{
    "text_curriculum\\University of York.pdf": [
        "Course Outlines \nYear 1 \nFOUNDATIONS OF PROGRAMMING FOR COMPUTER SCIENCE \nModule aims \nStudents will be introduced to different programming constructs, basic data structures, \ncommand line tools, integrated development environments and unit testing of programs. \nStudents will learn how to describe well-defined tasks using pseudocode and translate them into \nprograms using a procedural programming paradigm. The module will be taught using a \nprocedural language for practising these skills. \nModule learning outcomes \nS101\u200b Describe and apply the fundamental concepts of procedural programming. Write small \nprocedural programs from scratch to perform well-defined tasks, following well-defined \nrequirements, in a procedural programming language like Python. Relate the syntax of the \nlanguage to its semantics, and analyse the result of executing fragments of syntax. Integrate \nlibrary code with their own programs using appropriate software tools. \nS102\u200b Implement bespoke data structures to store states of a process. Implement simple \nalgorithms written in pseudocode. Develop programs incrementally, using simple tests \n(automated where appropriate) to check each increment. \nS103\u200b Store data in memory in standard built-in collection types, and to store and retrieve data \nfrom simple text files such as CSV and JSON files. \nS104\u200b  \nUse an appropriate software development environment, such as Eclipse, IntelliJ or VS Code. \nGiven a program and a debugging tool, students will be able to identify and correct bugs which \nprevent the program from functioning as intended. \nS105\u200b  \nOrganise and document program code following the principles of software engineering. Write \ndocumentation to explain the design and implementation of their own code, or example code \nwhich is supplied to them. \n \nIndicative assessment \nTask\u200b\n% of module mark \nOnline Exam -less than 24hrs (Centrally scheduled)\u200b100 \nSpecial assessment rules \nNone \n \nIndicative reassessment \nTask\u200b\n% of module mark \nOnline Exam -less than 24hrs (Centrally scheduled)\u200b100 \nModule feedback \nFeedback is provided through work in practical sessions, formative assessments, and after the \nfinal assessment as per normal University guidelines. \n \nIndicative reading \n",
        "Allen B. Downey - Think Python: How to Think Like a Computer Scientist - 2nd ed. (2015), \nO'Reilly Media \n \nMike Dawson - Python programming for the absolute beginner - 3rd ed. (2010) - Course \nTechnology \n \nKent D. Lee and Steve Hubbard - Data structures and algorithms with Python (2015), Springer \n \n",
        "MATHEMATICAL FOUNDATIONS OF COMPUTER SCIENCE \nModule aims \nStudents will be introduced to the key discrete mathematics concepts that are the foundation of \ncomputer science. Seven topics are covered as follows: i) counting (combinatorics), ii) discrete \nprobability, iii) graphs, iv) propositional and predicate logic, v) proofs and sets, vi) relations on \nsets, and vii) relations on a single set. After studying the module, students will be able to apply \nthe learnt concepts, theories and formulae in real-world examples of computational problems. \n \nModule learning outcomes \nT101\u200b Define, read and apply mathematical notations for the purpose of describing \nmathematical concepts from across discrete mathematics. \nT102\u200b Select appropriate techniques to prove properties related to discrete mathematics \nconcepts. \nT103\u200b Understand how to construct sets of elements with certain properties and determine their \ncardinality using counting formulae from combinatorics. \nT104\u200b Understand and apply basic set theory, including formally defining set relations and \noperations. \nT105\u200b Describe and use the basic concepts of discrete probability to describe events, with an \nunderstanding of joint, conditional and marginal probabilities, Bayes\u2019 theorem, expectation, \ncovariance and correlation. \nT106\u200b Formally define and illustrate by example graphs of different graph classes, such as \nsimple, undirected, directed, weighted, directed acyclic, connected, disconnected and trees - \nwith an understanding of how they may be used in real-world computational problems. \nT107\u200b Apply a variety of techniques to identify whether logical expressions are true or false, \nvalid or invalid or equivalent to one another, and be able to apply logical statements to describe \nreal-world logical problems. \n \n \nIndicative assessment \nTask\u200b\n% of module mark \nClosed/in-person Exam (Centrally scheduled)\u200b\n100 \nSpecial assessment rules \nNone \n \nIndicative reassessment \nTask\u200b\n% of module mark \nClosed/in-person Exam (Centrally scheduled)\u200b\n100 \nModule feedback \nFeedback is provided through work in practical sessions, and after the final assessment as per \nnormal University guidelines. \n \nIndicative reading \n** Dean N., The Essence of Discrete Mathematics, Prentice Hall, 1997 \n \n",
        "** Haggarty R., Discrete Mathematics for Computing, Addison Wesley, 2002 \n \n** Truss J., Discrete Mathematics for Computer Scientists, Addison Wesley, 1999 \n \n** Gordon H., Discrete Probability, Springer, 1997 \n \n* Solow D., How to Read and Do Proofs, Wiley, 2005 \n \n \n \n",
        "HUMAN-COMPUTER INTERACTION \nModule summary \nHCIN introduces user-centred design. Where other modules focus on technical understanding \nof computers and how they work, this module is instead about understanding the relationship \nbetween computer systems and people. It discusses how this can be used to improve system \ndevelopment, and how it can go wrong. We will explore the nature of and barriers to people's \ninteractions with computers and how systems can be designed to optimise and facilitate these \ninteractions. We will also consider how to evaluate the people's experience - what makes a \ngood, enjoyable human-computer interaction. \n \nRelated modules \nThe assessment is undertaken as a group open assessment in which groups will work together \nto complete and present a user-centred design task. The assessment also includes as part of \nthe submission an individual reflection exercise which is worth 10% of the overall marks. This \ntask asks students to comment on their development of transferable skills (using the York \nStrengths Framework as a basis) and how they have developed these during HCIN. The \nassessment also has a peer-assessment exercise, which ensures that individual contributions to \nthe group are accounted for in the final marks. \n \nReassessment is by an individual open assessment which requires students to understand and \nimplement the processes and techniques discussed during the module. \n \nModule will run \nOccurrence\u200b\nTeaching period \nModule aims \n \nStudents taking this module will be familiarised with how to design user-centred systems that \nmeet the needs and preferences of diverse users. Students will be introduced to the notion of \nengineering lifecycles, and in particular building requirements from user needs, iterative \nprototyping and evaluation of interactive systems. Students will undertake group work in \npracticals, giving them opportunities to develop communication and conflict resolution skills. The \nassessment will evaluate knowledge of the user-centred design process and interaction design \nprinciples. \n \nModule learning outcomes \nDescribe why user-centred design in software development is important to usable and inclusive \ndesign. \n \nUndertake a user-centred design process as a cyclical approach through the key stages of user \nneeds elicitation, conceptual design, prototyping, and evaluation. \n \nApply appropriate interaction design concepts in describing user-system interaction including: \naffordances, feedforward, feedback, conceptual model. \n \n",
        "Advocate for the ethical treatment of participants throughout the user-centred design lifecycle, \nand explain how user diversity can impact on the inclusiveness of a system. \n \nDescribe how interactive systems are embedded in societal structures, and how they are used \nto invoke change at the personal, community, national or international level. \n \nPlan and manage deliverables to set deadlines throughout a project lifecycle, and use a \nself-reflective skills assessment to improve student team working and team performance. \n \nIndicative assessment \nTask\u200b\n% of module mark \nEssay/coursework\u200b\n10 \nGroupwork\u200b\n90 \nSpecial assessment rules \nNone \n \nIndicative reassessment \nTask\u200b\n% of module mark \nEssay/coursework\u200b\n100 \nModule feedback \nFeedback is provided through work in practical sessions, formative assessments, and after the \nfinal assessment as per normal University guidelines. \n \nIndicative reading \n*** Preece, J., Rogers, Y., Sharp, H., Interaction Design, 4th edn Wiley, 2015 \n \n*** Cooper, A., Reimann., R., Cronin., D., Noessel., C. About Face: The Essentials of Interaction \nDesign. 4th edn Wiley, 2014. \n \n* Mackenzie, I.S. Human-Computer Interaction. Elsevier Inc., 2013. \n \n* Norman, D. The Design of Everyday Things. Any edition. \n \n",
        "OBJECT-ORIENTED DATA STRUCTURES AND ALGORITHMS \n \nModule aims \nStudents begin to program key data structures such as stacks, queues, trees and graphs. They \nare introduced to the idea of complexity of an algorithm, and how to characterise time and \nspace through formal notations and proof techniques. Students are taught using an object \noriented language like Java, and learn the basics of test driven development for testing their \ncode and demonstrating its successful running. Students are also introduced to several \nalgorithm design paradigms such as greedy algorithms. \n \nModule learning outcomes \nS201 \n \nImplement an object oriented design. This includes organising program code into modules using \nmethods following the software engineering principles of modularity and abstraction, and \nassembling data and methods into classes at an introductory level following the software \nengineering principles of encapsulation and data hiding. Integrate standard library code with \ntheir own programs using appropriate software tools. \n \nS202 \n \nWrite and test code that conforms to specific interfaces. Identify and correct bugs which prevent \nthe program from functioning as intended. \n \nS203 \n \nOrganise and document program code following the principles of software engineering. \nGenerate documentation, manually and programmatically, to explain the design and \nimplementation of their own code, or example code which is supplied to them. \n \nS204 \n \nAnalyse problems in order to confidently design algorithms to solve simple problems,and be \nable to explain how algorithms and Processing programs work. Develop small programs that \nimplement basic algorithmic designs. Argue the correctness of algorithms using inductive proofs \nand invariants. \n \nS205 \n \nAnalyse worst-case running times of algorithms using asymptotic analysis and apply the \nknowledge to sorting and searching algorithms, categorising efficiency in time and memory use. \n \nS206 \n \n",
        "Compare between different abstract data structures from linked lists to graphs in order to be \nable to choose an appropriate data structure for a design situation. This includes the major \nsearch, sort, and graph algorithms and their analyses. \n \nS207 \n \n.Apply algorithmic design paradigms such as greedy and dynamic programming paradigms. \nPresent an argumentation for the choice of a paradigm for a given problem. Describe what an \napproximation algorithm is, the benefit of using approximation algorithms, and analyse the \napproximation factor for such an algorithm. \n \n \nIndicative assessment \nTask\u200b\n% of module mark \nOnline Exam -less than 24hrs (Centrally scheduled)\u200b50 \nOnline Exam -less than 24hrs (Centrally scheduled)\u200b50 \nSpecial assessment rules \nNone \n \nIndicative reassessment \nTask\u200b\n% of module mark \nOnline Exam -less than 24hrs (Centrally scheduled)\u200b50 \nOnline Exam -less than 24hrs (Centrally scheduled)\u200b50 \nModule feedback \nFeedback is provided through work in practical sessions, formative assessments, and after the \nfinal assessment as per normal University guidelines. \n \nIndicative reading \nSteven S. Skiena - The algorithm design manual - 2nd ed., (2010), Springer \n \nMichael T. Goodrich, Roberto Tamassia, Michael H. Goldwasser - Data Structures and \nAlgorithms in Java (2014), Wiley Etextbooks \n \nMitsunori Ogihara - Fundamentals of Java programming (2018), Springer \n \n",
        " \n \nINTRODUCTION TO COMPUTER ARCHITECTURES \nModule will run \nOccurrence\u200b\nTeaching period \nModule aims \nStudents taking this module will gain foundations in the key architectural components of a \ncomputer system, how a high level program is executed upon that system, and how a computer \nsystem is constructed in hardware. Students will be introduced to how a computer system is \nconstructed, and how a program implemented in a high level programming language (for \nexample C) executes on that system. Students will be introduced to the basics of computer \narchitecture and program language construction, providing a basis for further study in later \nyears. Students will be introduced to a bottom-up approach, motivated by real examples, taught \nas both lectures and laboratory practicals. Students will be able to describe and apply their \nprogramming skills on real devices and computer systems that are used in many real \napplications today. \n \nModule learning outcomes \nIdentify the purpose of key computer hardware components such as processors, memories and \nbusses. \nDescribe different data types commonly found in binary systems (e.g. signed vs. unsigned \nintegers), and show how to convert, perform arithmetic, and perform logical operations. \nExpress logical expressions as basic gates, transistors and combinatoric logic circuits \nDescribe the function and limitations of a variety of logical building blocks in the context of \nprocessor architectures \nDescribe the von Neumann Model paradigm of computer architecture, including the fetch \nexecute cycle of instruction processing. \nExplain how operations executed in a processor can be used to implement to higher level \nsequential, conditional and iterative programming language constructs \nBuild a simple system comprised of a CPU, memory and input/output. \nExplain the use of assemblers, compilers and linkers to create executable code for a processor, \nand use such a toolchain develop software for the simple system built in the module \nIdentify potential security problems associated with architecture design. \nIndicative assessment \nTask\u200b\n% of module mark \nEssay/coursework\u200b\n100 \nSpecial assessment rules \nNone \n \nAdditional assessment information \nOpen assessment is a long-running task that takes place over a number of weeks. \n \nIndicative reassessment \nTask\u200b\n% of module mark \n",
        "Essay/coursework\u200b\n100 \nModule feedback \nFeedback is provided through work in practical sessions, and after the final assessment as per \nnormal University guidelines. \n \nIndicative reading \n*** J.Hennessy, D.Patterson Computer Architecture: A Quantitative Approach (2nd Edition) \nMorgan Kaufmannn 1990 \n \n*** W.Stallings Computer Organization and Architecture: Design For Performance (8th Edition) \nPearson 2010 \n \n",
        "FORMAL LANGUAGES AND AUTOMATA \nModule will run \nOccurrence\u200b\nTeaching period \nModule aims \nStudents taking this module will be introduced to the concepts of formal languages and the \nabstract machines that accept them as a way of describing computation. Students will have a \ndeep understanding of finite automata and pushdown automata, with their associated languages \nand related proof techniques, and will be introduced to more complex machines accepting \ncontext sensitive and recursively enumerable languages for purposes of being able to identify \nand describe them. \n \nModule learning outcomes \nDescribe and illustrate the concepts of formal languages, automata and grammars, and the \nrelations between them; \nConstruct a variety of abstract machines including: deterministic and non-deterministic finite \nautomata, deterministic and non-deterministic pushdown automata and Turing machines; \nDistinguish different classes of automata, and the languages they accept; \nApply a variety of operations to transform and convert between automata; \nConvert between grammars and automata for regular and context-free languages; \nDemonstrate that a grammar is ambiguous; \nApply the pumping lemma for regular and context-free languages to show a language is not \nregular or context-free respectively; \nDescribe the Chomsky hierarchy; \nIdentify key applications in computing where regular and context-free languages are used in \npractice; and \nUse automata theory as the basis for building lexers and parsers. \nIndicative assessment \nTask\u200b\n% of module mark \nOpen Examination\u200b\n100 \nSpecial assessment rules \nNone \n \nIndicative reassessment \nTask\u200b\n% of module mark \nOpen Examination\u200b\n100 \nModule feedback \nFeedback is provided through work in practical sessions, and after the final assessment as per \nnormal University guidelines. \n \nIndicative reading \n*** Peter Linz, An introduction to formal languages and automata. Sixth Edition. Jones and \nBartlett Computer Science. 2017 \n** D.I. Cohen, Introduction to Computer Theory. 2nd Edition. Wiley. 1997 \n",
        "** Hopcroft, John E. and Motwani, Rajeev and Ullman, Jeffrey D., Introduction to Automata \nTheory, Languages and Computation (3rd ed.), Pearson Education, 2013 \n** S.H. Rodger and T.W. Finley, JFLAP: An interactive formal language and automata package. \nJones and Bartlett Computer Science. 2006. Available online \n* Martin, John C., Introduction to Languages and the Theory of Computation (4th ed.), McGraw \nHill, 2010 \n* Rich, Elaine, Automata, Computability and Complexity, Pearson Education, 2008 \n* Sipser, Michael, Introduction to the Theory of Computation (3rd ed.), South-Western College \nPublishing, 2012\n \n",
        "Year 2 \nDATA: INTRODUCTION TO DATA SCIENCE \n \nModule aims \nStudents will be introduced to key concepts required to undertake rigorous and valid data \nanalysis. Students will be introduced to processes for collecting, manipulating and cleaning \ndata, while gaining experience in judging the quality of data sources. Students will be introduced \nto statistical analysis in data science, including correlation, inferential statistics and regression, \nand how to use these tests in a programming environment. Relational databases, SQL, and and \nother database paradigms such as NoSQL, are covered as a way of storing and accessing data. \nA key aim of the module is to solve complex problems and deliver insights about \nmulti-dimensional data. \n \nModule learning outcomes \nDistinguish between different types of data that are generated in science, engineering and \ndesign, and employ strategies for ensuring data quality. \nRetrieve data from a variety of different data sources in a variety of different formats. \nApply inferential statistics and statistical procedures to test hypotheses about features and \nrelationships within data sets. \nUse appropriate visualisations to present and explore data sets. \nUse databases, both relational and of other paradigms, to store and query data. \nIdentify the ethical concerns regarding the provenance of data, the privacy of individuals, and \nthe impact data analytics can have on society, and apply topics from the code of ethics of a \nprofessional data protection body. \n \nIndicative assessment \nTask\u200b\n% of module mark \nEssay/coursework\u200b\n100 \nSpecial assessment rules \nNone \n \nIndicative reassessment \nTask\u200b\n% of module mark \nEssay/coursework\u200b\n100 \nModule feedback \nFeedback is provided through work in practical sessions, and after the final assessment as per \nnormal University guidelines. \n \nIndicative reading \n*** Spiegelhalter, D., The Art of Statistics: Learning from Data, Pelican, 2019. \n \n*** VanderPlas, J. Python Data Science Handbook: Essential Tools for Working with Data, \nO\u2019Reilly, 2016. \n \n",
        "** Igual, L. Segui, S. Introduction to Data Science: A Python Approach to Concepts, Techniques \nand Applications, Springer, 2017\n \n",
        "SYSTEMS & DEVICES 2: OPERATING SYSTEMS, SECURITY, AND NETWORKING \nRelated modules \nPre-requisite modules \nSystems & Devices 1: Introduction to Computer Architectures (COM00011C) \n  \nCo-requisite modules \nNone \n  \nProhibited combinations \nNone \nModule aims \nThis module builds on Systems and Devices 1 by examining the system software that executes \nupon a computer system. Students will learn how the resources of the system can be shared by \nmultiple programmes and users, and how networking can be used to communicate between \nprogrammes. One important aspect is how basic security and protection mechanisms are \nprovided by the processor and memory system. Throughout, the module students will consider \npractical examples based on computer systems used today. This module also introduces \nstudents to the core concepts of computer networking by covering the layered network model, \nand discussing the utility and motivation for such an approach. Services that are layered on this \nmodel (such as UNIX sockets, DNS, TCP, IP) are detailed and students will develop software to \nexperiment with these features. After taking this module, students will have an understanding of \nthe role of an operating system, how computers can support multiple time-sliced programmes, \nand how all kinds of computer networks, including the Internet, are created. \n \n \nModule learning outcomes \nDemonstrate application programming of OS-supported concurrency, communication and I/O. \nShow how the structure of the OS is supported by computer hardware, with specific reference to \nthe hardware features that extend the basic systems introduced in S&D1. \n \nUse basic resource management mechanisms provided by common OSes, including time and \nmemory. \n \nDemonstrate use of the memory protection mechanisms provided by hardware and OSes, \nincluding memory mapped I/O. \n \nDemonstrate use of the information security provided by the OS in terms of file systems. \n \nDemonstrate concurrent programming at the process level and show how it is supported by, and \nimplemented on, the system hardware. \n \nLearn to recognise and avoid issues of deadlock, livelock, and starvation. \n \nBe able to articulate the motivation behind the layered network model \n",
        " \nDevelop software using OS-level networking concepts (i.e. sockets) to communicate with other \nsystems. \n \nDemonstrate understanding of networked architectures, how they are integrated into an \noperating system, and develop simple applications using this knowledge. \n \nIndicative assessment \nTask\u200b\n% of module mark \nEssay/coursework\u200b\n50 \nOnline Exam -less than 24hrs (Centrally scheduled)\u200b50 \nSpecial assessment rules \nNone \n \nAdditional assessment information \nStudents are only required to resit any failed assessment component. \n \nIndicative reassessment \nTask\u200b\n% of module mark \nEssay/coursework\u200b\n50 \nOnline Exam -less than 24hrs (Centrally scheduled)\u200b50 \nModule feedback \nFeedback is provided through work in practical sessions, and after the final assessment as per \nnormal University guidelines. \n \nIndicative reading \n*** Operating System Concepts, Tenth Edition by Silberschatz, Galvin and Gagne, Wiley (2018) \n \n* A. S. Tanenbaum, Modern Operating Systems, Prentice Hall (2014). \n* W. Stallings, Operating Systems, Internals and Design Principles (9th Edition) Ninth Edition, \nPrentice Hall (2017). \n \n \n \n",
        "THEORY 3: COMPUTABILITY, COMPLEXITY & LOGIC \n \nModule aims \nThis module covers computability theory and complexity theory. In particular, students will learn \nthe concepts of semi-decidable and decidable languages, and Turing-computable functions. \nThey will be able to explain the difference between solvable and unsolvable problems and prove \nunsolvability by reduction. They will understand the time and space complexity of Turing \nmachines, complexity classes such as P, NP, PSpace, NPSpace and NPC, and prove \nNP-completeness by reduction. The module will also introduce basic concepts and results in \npropositional logic, predicate logic, and program verification. In particular, students will learn to \ndistinguish between syntax and semantics, and be able to use formal proof systems such as \nnatural deduction. They will understand the limitations of logic in terms of decidability and \nexpressiveness, and how to use a formal calculus such as Hoare logic to specify programs and \nprove them correct. \n \nModule learning outcomes \n \nUse unrestricted grammars and Turing machines to specify semi-decidable languages. \n \nProvide examples of unsolvable problems and prove that a problem is unsolvable by reducing a \nknown unsolvable problem to it. \n \nExplain the Church-Turing thesis and its significance. \n \nDefine the classes P and NP, and explain their relation to the class ExpTime. \n \nExplain the significance of NP-completeness and provide examples of NP-complete problems. \n \nExplain the meaning of formulas in propositional and predicate logic, and translate such \nformulas into English and vice-versa. \n \nExplain the fundamental difference between syntax and semantics. \n \nApply the rules of natural deduction to construct proofs, and determine the truth or falsity of \nformulas in a given model. \n \nExplain the limitations of logic and the relationship between logic and computability. \n \nReason deductively about programs using formalisms such as Hoare logic and weakest \npreconditions \n \nIndicative assessment \nTask\u200b\n% of module mark \nClosed/in-person Exam (Centrally scheduled)\u200b\n100 \n",
        "Special assessment rules \nNone \n \nIndicative reassessment \nTask\u200b\n% of module mark \nClosed/in-person Exam (Centrally scheduled)\u200b\n100 \nModule feedback \nFeedback is provided through work in practical sessions, revision classes, and after the final \nassessment as per normal University guidelines. \n \nIndicative reading \n**** Martin, John C., Introduction to Languages and the Theory of Computation (4th ed.), \nMcGraw Hill, 2010 \n \n** Rich, Elaine, Automata, Computability and Complexity, Pearson Education, 2008 \n \n** Sipser, Michael, Introduction to the Theory of Computation (3rd ed.), South-Western College \nPublishing, 2012 \n \n* Hopcroft, John E. and Motwani, Rajeev and Ullman, Jeffrey D., Introduction to Automata \nTheory, Languages and Computation (3rd ed.), Pearson Education, 2013 \n \n* Arora, Sanjeev and Barak, Boaz, Computational Complexity: A Modern Approach, Cambridge \nUniversity Press, 2009 \n \n++ Garey, Michael R. and Johnson, David S., Computers and Intractability: A Guide to the \nTheory of NP-Completeness, W.H. Freeman, 1979 \n \n\u200b\n \n",
        "INTELLIGENT SYSTEMS: MACHINE LEARNING & OPTIMISATION \n \nModule aims \nThis module introduces the field of Artificial Intelligence, key approaches within the field and \nphilosophical questions such as what it means for a machine to understand. Students will learn \nthe theory and practice of machine learning techniques covering linear regression, simple neural \nnetworks, linear algebra and continuous optimisation. Students will see motivating real world \nproblems, the ML techniques required to solve them, the underlying mathematics needed for the \ntechnique and their practical implementation. Practicals will be taught using Python, and the \ngroup project will introduce the students to a Python-based modern machine learning library \nsuch as TensorFlow or PyTorch. \n \n \nModule learning outcomes \nExplain the difference between strong, weak and general AI, understand the relationship \nbetween computation and AI, define the machine learning paradigm, and distinguish it from the \nwider field of AI \n \nCompute partial derivatives and understand the concept of the gradient as a generalisation of \nthe derivative \n \nExpress, manipulate and solve systems of linear equations using linear algebra, and apply \nlinear regression and logistic regression \n \nOptimise multivariate functions using gradient descent \n \nExplain the concept of overfitting and how regularisation can be used to prevent it \n \nConstruct a basic neural network using a modern machine learning library and learn its weights \nvia optimisation using the backpropagation algorithm \n \nDeconstruct ethical arguments relating to AI and its applications, and appreciate the ethical and \nprivacy implications of machine learning \n \nIndicative assessment \nTask\u200b\n% of module mark \nEssay/coursework\u200b\n30 \nOnline Exam -less than 24hrs (Centrally scheduled)\u200b70 \nSpecial assessment rules \nNone \n \nIndicative reassessment \nTask\u200b\n% of module mark \nEssay/coursework\u200b\n30 \n",
        "Online Exam -less than 24hrs (Centrally scheduled)\u200b70 \nModule feedback \nFeedback is provided through work in practical sessions, and after the assessments as per \nnormal University guidelines. \n \nIndicative reading \nArtificial Intelligence: A Modern Approach by Russell and Norvig\n \n",
        "ENGINEERING 1: SOFTWARE & SYSTEMS ENGINEERING \nRelated modules \nPre-requisite modules \nSoftware 2: Object Oriented Data Structures & Algorithms (COM00016C) \n  \nCo-requisite modules \nNone \n  \nProhibited combinations \nNone \nModule aims \nThis is the students' first opportunity to integrate their skills into a development project. Starting \nfrom a broad problem description and working in groups, students will design, develop and test \na complex system. The students will be introduced to the software engineering terminology, \nlifecycle and processes and will become familiar with principles, techniques and tools for, and \ndevelop hands-on experience of eliciting requirements; defining software architectures; \ndesigning and implementing software in an object-oriented way using established patterns; \nreviewing, testing and refactoring software systems; and setting up continuous integration and \ndelivery processes. Students will also develop an appreciation of how to identify, mitigate and \nmonitor risks, how to manage software projects, and how to reuse and extend 3rd-party \ncode/libraries. Overarching themes of the module will include traceability, cyber-security and \nethical considerations across the engineering lifecycle. \n \nModule learning outcomes \nE101 \n \nApply an understanding of software engineering terminology, lifecycles and process models, to \nhelp with undertaking a project. \n \nE102 \n \nElicit and document user and system requirements. \n \nE103 \n \nArchitect, design and implement software in an object-oriented way. \n \nE104 \n \nDemonstrate how the estimation of risk can be used to improve decision-making, and to make \nrealistic estimates for a project. \n \nE105 \n \n",
        "Define unit- and system-level tests for software, and use continuous integration processes. \n \nE106 \n \nApply mechanisms for working in teams to successfully undertake a group project. \n \nE107 \n \nApply different models for software licensing and reuse of 3rd party software to the artefacts \ndeveloped in the module. \n \nE108 \n \nDemonstrate consideration of cyber-security and ethical considerations in the engineering \nlifecycle, through practice and documentation. \n \nE109 \n \nWrite documentation and design/deliver presentations for the different stages of the engineering \nlifecycle. \n \n \nIndicative assessment \nTask\u200b\n% of module mark \nGroupwork\u200b\n35 \nGroupwork\u200b\n65 \nSpecial assessment rules \nNone \n \nIndicative reassessment \nTask\u200b\n% of module mark \nEssay/coursework\u200b\n35 \nEssay/coursework\u200b\n65 \nModule feedback \nFeedback is provided through work in practical sessions, and after the final assessment as per \nnormal University guidelines \n \nIndicative reading \n**** Ian Sommerville, Software Engineering (latest edition), Addison-Wesley, 2010 \n \n*** Roger Pressman, Software Engineering (latest edition), McGraw-Hill, 2010\n \n",
        "SYSTEMS & DEVICES 3: ADVANCED COMPUTER SYSTEMS \n \nModule aims \nThis module continues the Systems and Devices stream by considering advanced computer \nsystems in terms of their structure and how it affects their programming. To this point, the \nsystems introduced have been basic uniprocessors with simple memory. The module starts by \nintroducing the concept of multiple processor architectures, pipelined and superscalar \nprocessors, systems-on-chip and advanced memory structures, including caches. \n \nThe rest of the module considers how these hardware features affect the way that such modern \nsystems are efficiently programmed. Issues of programming for pipelines, caches, etc. are \nconsidered. Then how processes running in parallel on separate processors can share \nunderlying resources safely. Finally, the module considers building blocks for parallel \nprogramming, e.g. threads and communication. It will also consider how such constructs map to \nprocessor instructions. The module will consider real-world examples throughout. \n \nModule learning outcomes \n \nS301 \n \nAppreciate how, and why, the hardware structure of multiprocessor architectures differs from the \narchitectures discussed in S&D1. Use this knowledge to develop software applications for such \narchitectures. \n \nS302 \n \nUse knowledge of computer system design concepts to evaluate the likely performance of a \nproposed computer architecture. \n \nS303 \n \nDemonstrate the ability to optimise software for advanced hardware features like caches, \nbranch predictors, and pipelines. \n \nS304\u200b Demonstrate through software development, how data structures are laid out in memory, \nhow they can be accessed from a program, and how processor instructions are generated to \nmanipulate them. \n \nIndicative assessment \nTask\u200b\n% of module mark \nOnline Exam -less than 24hrs (Centrally scheduled)\u200b100 \nSpecial assessment rules \nNone \n \n",
        "Additional assessment information \nStudents will only need to retake failed assessment components. \n \nIndicative reassessment \nTask\u200b\n% of module mark \nOnline Exam -less than 24hrs (Centrally scheduled)\u200b100 \nModule feedback \nFeedback is provided through work in practical sessions, and after the final assessment as per \nnormal University guidelines \n \nIndicative reading \n**** H. Abelson, and G.J. Sussman, Structure and Interpretation of Computer Programs, MIT \nPress, 1996 \n \n*** R.W. Sebesta, Concepts of Programming Languages, Addison Wesley, 2009 \n \n*** A. Burns and G. Davis, Concurrent Programming, Addison-Wesley, 1993 \n \n** D. Watt and W. Findlay, Programming Language Design Concepts, Wiley, 2004 \n \n** T.W. Pratt and M.V. Zelkowitz, Programming Languages: Design and Implementation, \nPrentice Hall, 2001 \n \n** D. Lea, Concurrent Programming in Java, Addison Wesley, 1996 \n \n** A. Burns and A. Wellings, Concurrent and Real-Time Programming in Ada, CUP, 2007 \n \n** F. Casarini and S. Thompson, Erlang Programming, O'Reilly, 2009 \n \n** A. Burns and A. Wellings, Real-Time Systems and Languages 4/e, Addison Wesley\n \n",
        "Year 3 \nCAPSTONE PROJECT (PRBX): COMPUTER SCIENCE (UG) \nProfessional requirements \nPlease see the additional assessment information above. \n \nModule will run \nOccurrence\u200b\nTeaching period \nModule aims \nThe aims of this module are to provide a culmination of three years' teaching in a substantial \nCapstone Project Module (CPM).The project provides an introduction to independent study in \nan engineering context, supports synthesis and application of material from the degree course, \nand gives the student the opportunity to demonstrate an appreciation of engineering methods \nand techniques, through coverage of requirements, ethical considerations, specification, design, \nimplementation and evaluation. On completion of the project, the student will have gained the \npractical skills that can only be gleaned from the experience of undertaking independent \n(supervised) study. The student will also have the experience of having written a substantial \nacademic report. \n \nModule learning outcomes \n \nDemonstrate acquired specialisation in a particular part of the subject area, including enhanced \nor new technical skills that build on taught theory. \n \nDemonstrate acquired skills to undertake a computer systems (software and/or hardware) \nengineering project, including design, implementation and evaluation. \n \nDemonstrate a practical understanding of how established techniques of research and enquiry \nare used to create and interpret knowledge. \n \nRecognise alternatives, selecting and justifying the approach taken at each point in the report, \nidentifying parts of the project area that are feasible within the time (etc) constraints of the \nproject. \n \nAppreciate the latent issues of the subject area (for example, in software engineering they might \nmeet and tackle such as emergent requirements, design flaws, equipment/application \nproblems). \n \nPrepare a written report on the work done, according to the defined criteria. In particular, the \nstudent should be able to prepare a report with a good structure and clear presentation, and in \nwhich the referencing is of publishable academic standard. The report must demonstrate critical \nabilities and evaluation of work done and methods applied. \n \nArticulates an understanding of legal, ethical, social, professional and commercial issues \ninvolved in the project, detailing potential issues and mitigation strategies. \n",
        " \nSummarise the context, method, results, and implications of the project in an engaging form for \na non-expert audience. \n \nIndicative assessment \nTask\u200b\n% of module mark \nEssay/coursework\u200b\n100 \nSpecial assessment rules \nNon-compensatable \n \nAdditional assessment information \nPlease note, as per University of York assessment regulations (see the Rules for Progression \nand Award), it is only possible to resit UG Capstone Project Modules (CPMs) when this is \npermitted within the total module credit resit threshold for the year. Currently, this threshold is 60 \nmodule credits. \n \nDue to PSRB (accreditation requirements) there are also special assessment rules that apply to \nPRBX. First PRBX cannot be compensated. Second, only in the case of marginal fail of PRBX \n(marks that fall within 30-39), reassessment is permitted. Students are given the opportunity to \nmake amendments to enable them to reach a pass threshold, within a specified time frame. The \nmark for the resubmitted PRBX will be capped at the pass mark (40). There will only be one \nsuch reassessment. These rules are applicable to all students who take PRBX, regardless of \nprogramme. \n \nIndicative reassessment \nTask\u200b\n% of module mark \nEssay/coursework\u200b\n100 \nModule feedback \nFeedback on written report draft (where draft provided to supervisor in a timely manner). \nWritten feedback after written project report. \n \nIndicative reading \n*** Dawson, C. W Projects in Computing and Information Systems. Addison-Wesley 2005 \n \n*** Gowers, E. The complete plain words. Penguin 1987 \n \n*** Kopka, H and Daly, P.W. A guide to LATEX : document preparation for beginners and \nadvanced users, 3rd edn. Addison-Wesley 1999 \n \n*** Zobel, J. Writing for computer science, 2nd edn. Springer 2004\n \n",
        "AI PROBLEM SOLVING WITH SEARCH AND LOGIC \nElective Pre-Requisites \n These pre-requisites only apply to students taking this module as an elective. \n \nA Level Maths or Equivalent, plus good knowledge of AI fundamentals, predicate logic and \nprogramming. \n \nModule will run \nOccurrence\u200b\nTeaching period \nModule aims \nThis module will introduce key approaches in Artificial Intelligence for tasks such as: finding a \nsequence of actions to achieve a goal; playing adversarial games; and solving discrete \noptimization problems such as configuration and scheduling. Students will learn the theory and \npractice of AI search, logic, and constraint-based approaches. The module aims to equip \nstudents with a wide range of problem-solving tools, how to design effective heuristics for them, \nand enable comparison of methods to determine which are best suited to a given problem. \nSome of the tools covered are state-space search algorithms (i.e. A* Search, IDA*, and Greedy \nBest-First Search), game-tree search algorithms (i.e. Minimax and Monte-Carlo Tree Search), \nlocal search methods for solving discrete optimization problems, constraint programming, and \nthe satisfiability (SAT) problem in knowledge representation and reasoning. \n \nModule learning outcomes \nRepresent a given search problem in terms of states, actions, and a goal, and identify a suitable \nheuristic. \n \nRepresent a given scenario using propositional logic to enable logical inference (for example, \nusing a SAT solver). \n \nModel (represent) and solve discrete optimization problems using a modern constraint \nprogramming system. \n \nSelect and apply an appropriate AI state-space search algorithm for a given problem, identifying \nreasons for the choice of algorithm in comparison to others. \n \nSelect and apply an appropriate adversarial (game-tree) search method to solve a given game, \nincluding design of a suitable heuristic if required. \n \nDescribe the algorithms commonly used in SAT (propositional satisfiability) solvers, local search \nsolvers, and constraint solvers, and apply them to small examples. \n \nIndicative assessment \nTask\u200b\n% of module mark \nClosed/in-person Exam (Centrally scheduled)\u200b\n100 \nSpecial assessment rules \n",
        "None \n \nIndicative reassessment \nTask\u200b\n% of module mark \nClosed/in-person Exam (Centrally scheduled)\u200b\n100 \nModule feedback \nFeedback is provided throughout the sessions, and after the assessment as per normal \nUniversity guidelines.\n \n",
        "AUTONOMOUS ROBOTIC SYSTEMS ENGINEERING \nModule aims \nThis module will introduce students to the theoretical concepts and practical skills required to \nengineer autonomous robotic systems. It will cover fundamental aspects of sensors/actuators \nand control systems, then build upon this foundation with high-level algorithms for autonomous \nlocalisation, mapping, navigation, and multi-robot coordination. This module will also explore \nsafety considerations and ethical implications of the design, implementation, and deployment of \nautonomous robotic systems. \n \nModule learning outcomes \nDescribe the degrees of autonomy that robotic systems can achieve \n \nDiscuss the safety considerations and ethical implications of the design, implementation, and \ndeployment of autonomous robotic systems \n \nExplain methodological principles for engineering autonomous robotic systems \n \nDemonstrate an understanding of modern robotics middleware and its application \n \nImplement an autonomous robotic solution for a predefined problem \n \nDiscuss the strengths and weaknesses of an implemented autonomous robotic solution \n \nIndicative assessment \nTask\u200b\n% of module mark \nEssay/coursework\u200b\n100 \nSpecial assessment rules \nNone \n \nIndicative reassessment \nTask\u200b\n% of module mark \nEssay/coursework\u200b\n100 \nModule feedback \nFeedback is provided throughout the sessions, and after the assessment as per normal \nUniversity guidelines. \n \nIndicative reading \nCorrell, Nikolaus, et al. Introduction to Autonomous Robots: Mechanisms, Sensors, Actuators, \nand Algorithms. MIT Press, 2022. \n \nHerath, Damith, and David St-Onge. Foundations of Robotics: A Multidisciplinary Approach with \nPython and ROS. Springer Nature Singapore, 2022.\n \n",
        "EVOLUTIONARY & ADAPTIVE COMPUTING \nModule summary \nThis module introduces a range of biologically-inspired approaches to computing. \nModule aims \nThis module introduces a range of biologically-inspired approaches to computing. It provides a \nfoundation of both theoretical and practical knowledge on the subject of evolutionary \ncomputation, an optimisation technique inspired by biological evolution. Students will have \nhands-on experience implementing a number of types of evolutionary algorithms using Python \nand the library DEAP: Distributed Evolutionary Algorithms in Python, to solve a range of different \ntypes of problems. The module also studies the use of Agents and Multi-agent Systems as a \nmodelling paradigm, with a focus on evolutionary adaptation and learning. \n \nModule learning outcomes \nDesign and implement evolutionary systems to address a given problem. \nUnderstand the biological underpinnings of evolutionary algorithms, and use them to optimise \nmathematical functions and agent behaviours. \nDefine a range of agent behaviours and represent them in a form that is well suited to natural \nselection \nModel processes in populations of agents using hand-written mathematical models \nCritically evaluate the performance and implementation of evolutionary and multi agent systems. \nIndicative assessment \nTask\u200b\n% of module mark \nEssay/coursework\u200b\n100 \nSpecial assessment rules \nNone \n \nIndicative reassessment \nTask\u200b\n% of module mark \nEssay/coursework\u200b\n100 \nModule feedback \nFeedback is provided through work in practical sessions, and after the final assessment as per \nnormal University guidelines. \n \nIndicative reading \n++ Banzhaf et al, Genetic Programming: An Introduction, Morgan Kaufmann , 1999 \n \n++ M. Mitchell, An Introduction to Genetic Algorithms, MIT Press, 1998\n \n",
        "COMPUTER VISION & GRAPHICS \nModule summary \nThis module will introduce modern computer vision approaches, including discussion of the \nmain applications and challenges. \nModule aims \nThis module will introduce modern computer vision approaches, including discussion of the \nmain applications and challenges. It will cover issues of image formation, camera geometry, \nfeature detection, motion estimation and tracking, image classification and scene \nunderstanding, using a range of model-based approaches. \n \nModule learning outcomes \nDemonstrate a detailed understanding of the image formation process, its modelling in \ncomputer vision and its simulation in computer graphics \n \nDescribe and implement techniques for rendering images including modelling light/material \ninteraction \n \nUnderstand a range of methods for inferring 3D shape from images \n \nApply fundamental machine learning methods for image understanding \n \nIndicative assessment \nTask\u200b\n% of module mark \nClosed/in-person Exam (Centrally scheduled)\u200b\n100 \nSpecial assessment rules \nNone \n \nIndicative reassessment \nTask\u200b\n% of module mark \nClosed/in-person Exam (Centrally scheduled)\u200b\n100 \nModule feedback \nFeedback is provided through work in practical sessions, and after the final assessment as per \nnormal University guidelines. \n \n \nIndicative reading \n** Forsyth and Ponce Computer Vision a Modern Approach Prentice Hall \n \n** Anil K. Jain Fundamentals of Digital Image Processing Prentice Hall\n \n",
        "CRYPTOGRAPHY THEORY & PRACTICE \nModule summary \nThe module aims to provide a broad overview of modern cryptography. The module will cover \nthe fundamental security goals achieved through cryptographic algorithms and protocols, how \nthey are formalised, designs that enable achieving those goals, and how formal security \narguments can be made for the design achieving the security goals. \n \nElective Pre-Requisites \n These pre-requisites only apply to students taking this module as an elective. \n \nModule will run \nOccurrence\u200b\nTeaching period \nModule aims \nThe module aims to provide a broad overview of modern cryptography. The module will cover \nthe fundamental security goals achieved through cryptographic algorithms and protocols, how \nthey are formalised, designs that enable achieving those goals, and how formal security \narguments can be made for the design achieving the security goals. \n \nModule learning outcomes \nBy the end of the module the students will be able to: \n \ndescribe and apply the fundamental security properties provided by cryptographic algorithms \nand protocols; \n \ndescribe how these security properties are formalised; \n \ndescribe the implications of cryptographic security arguments; \n \nidentify security requirements for a given practical scenario and identify the appropriate \ncryptographic tools to achieve the security requirements; \n \nassess whether given cryptographic algorithms and protocols meet identified security \nrequirements; and \n \nanalyse the security and efficiency of cryptographic protocols from both theoretical and practical \npoints of view. \n \nIndicative assessment \nTask\u200b\n% of module mark \nClosed/in-person Exam (Centrally scheduled)\u200b\n100 \nSpecial assessment rules \nNone \n \nIndicative reassessment \n",
        "Task\u200b\n% of module mark \nClosed/in-person Exam (Centrally scheduled)\u200b\n100 \nModule feedback \nFeedback is provided through work in practical sessions, and after the final assessment as per \nnormal University guidelines. \n \nIndicative reading \n[1] J. Katz, Introduction to modern cryptography, Third edition :: Chapman & Hall/CRC, 2020 \n[2] J.-P. Aumasson, Serious cryptography : a practical introduction to modern encryption :: No \nStarch Press, 2018 \n[3] C. Paar, Understanding cryptography : a textbook for students and practitioners :: Springer, \n2009 \n[4] A. McAndrew, Introduction to cryptography with open-source software :: CRC Press, 2011 \n[5] W. Stallings, Cryptography and network security : principles and practice, Seventh edition :: \nPearson, 2017 \n[6] A. J. Menezes, Handbook of applied cryptography :: CRC Press, 1997 \n \n \n",
        "EMBEDDED SYSTEMS DESIGN & IMPLEMENTATION \nModule will run \nOccurrence\u200b\nTeaching period \nModule aims \nThe aim of this module is to understand the need for embedded computer systems, and the \nengineering process to design, implement and validate them. \n \nModule learning outcomes \nArticulate the need for embedded computer systems, and the requirements imposed on them by \ntheir application scenario. \nBe able to apply an engineering process to design, implement and validate embedded systems. \nDemonstrate the different levels of abstraction that are used throughout the design process, and \nbe able to decide the most appropriate abstractions at each step. \nExplore hardware/software implementation trade-offs, and a number of partitioning, mapping \nand evaluation techniques that can be used to analyse that trade-off for a particular application \nscenario. \nProgram computing platforms that have limited performance, energy, memory and storage \ncapacity. \nDesign and evaluate custom hardware architectures. \nUnderstand the process to improve and assure timing properties, including priority assignment, \nscheduling of real-time tasks and schedulability analysis. \nIndicative assessment \nTask\u200b\n% of module mark \nEssay/coursework\u200b\n100 \nSpecial assessment rules \nNone \n \nIndicative reassessment \nTask\u200b\n% of module mark \nEssay/coursework\u200b\n100 \nModule feedback \nFeedback is given to each student on each of the two assessments, highlighting the strengths \nand weaknesses of the proposed design. Additional feedback is also given during practical \nsessions, which are also based on design problems. \n \nIndicative reading \nP. Marwedel, Embedded System Design, Springer, 2011\n \n",
        "ENGINEERING 2: AUTOMATED SOFTWARE ENGINEERING \nModule will run \nOccurrence\u200b\nTeaching period \nModule aims \nThis module builds on ENG1 and introduces advanced techniques for engineering large, \ndata-intensive software systems that consist of components written in different programming \nlanguages and operating on a variety of platforms. In order to manage the complexity of \nengineering such data-intensive systems, ENG2 will introduce the theory, principles and \npractices of model-driven engineering (MDE), focusing on technical topics. These include \nmodelling, metamodelling, model management, model transformation, model-driven traceability, \nworkflows, model validation, and model evolution. Non-technical issues including standards, \ndomain-specific MDE versus general-purpose MDE and MDE processes will also be \nconsidered. The discussion of concepts and technologies for engineering modern data-intensive \nsoftware systems will start from a general discussion about architectural concerns, then move \non to discuss their design, implementation as a collaboration of microservices, and using \ncontainers for reproducible and scalable deployment. ENG2 will discuss assurance practices \naround data-intensive software systems, such as testing and hardening. \n \nModule learning outcomes \nDemonstrate the principles and practices of modelling and metamodelling. \n \nImplement and orchestrate domain-specific models, metamodels and model management \noperations. \n \nDemonstrate the design principles for data-intensive software systems, including their \narchitecture characteristics like availability, scalability, reliability and security, and the \ncharacteristics of the organisation. \n \nStructure data-intensive systems as combinations of independently maintained microservices, \nand propose benefits and challenges in terms of complexity management, organisational \nscalability, and resource usage. \n \nDeploy the components of data-intensive systems in a scalable and reproducible manner, by \npackaging them as containers and orchestrating these containers to work with each other. \n \nDescribe assurance practices in the development and deployment of large data-intensive \nsystems, including testing and hardening. \n \nArticulate how Model-Driven Engineering integrates with wider system engineering processes \nand policies, including those of a data-intensive software system. \n \nIndicative assessment \nTask\u200b\n% of module mark \nEssay/coursework\u200b\n100 \n",
        "Special assessment rules \nNone \n \nIndicative reassessment \nTask\u200b\n% of module mark \nEssay/coursework\u200b\n100 \nModule feedback \nFeedback is provided throughout the sessions, and after the assessment as per normal \nUniversity guidelines. \n \nIndicative reading \nNewman, Sam. Building Microservices, O'Reilly Media, 2021. \nKleppmann, Martin. Designing Data-Intensive Applications: The Big Ideas Behind Reliable, \nScalable, and Maintainable Systems, O'Reilly Media, 2017. \nWasowski A. Domain-Specific Languages: Effective Modeling, Automation, and Reuse. 1st ed. \n2023. (Berger T, ed.). Springer International Publishing; Imprint Springer; 2023. \n \n",
        "ETHICAL HACKING, ANALYSIS & INVESTIGATION \nModule aims \nThe module aims to provide an introductory range of theoretical and practical skills to undertake \nethical hacking, analysis, and investigation of modern computer systems and networks. \n \nThe module will explore ethical hacking topics such as vulnerability exploitation of cryptographic \nand network protocols, reverse engineering, and other penetration testing techniques, as well as \nmalware analysis and digital investigation of cyber incidents. \n \nThe module will familiarise students with a range of tools used by ethical hackers and security \npractitioners. \n \nModule learning outcomes \nBy the end of the module the students will be able to: \n \nExplain the fundamental concepts of cyber security in systems and networks \n \nUnderstand the security strengths & weaknesses in network mechanisms \n \nUnderstand the major threats and attacks in systems and networks under various scenarios, \narchitectures, and threat models \n \nAnalyse control solutions for network security \n \nAssess the relative merits of different solution approaches in various security-related contexts \n \nIndicative assessment \nTask\u200b\n% of module mark \nClosed/in-person Exam (Centrally scheduled)\u200b\n50 \nClosed/in-person Exam (Centrally scheduled)\u200b\n50 \nSpecial assessment rules \nNone \n \nIndicative reassessment \nTask\u200b\n% of module mark \nClosed/in-person Exam (Centrally scheduled)\u200b\n50 \nClosed/in-person Exam (Centrally scheduled)\u200b\n50 \nModule feedback \nFeedback is provided throughout the sessions, and after the assessment as per normal \nUniversity guidelines. \n \nIndicative reading \nJon Erickson. Hacking: The Art of Exploitation. No Starch (2008) \n \n",
        "Sikorski, Michael ; Honig, Andrew. Practical Malware Analysis: A Hands-On Guide to Dissecting \nMalicious Software. No Starch Press, Incorporated (2012) \n \n \n",
        "HIGH-INTEGRITY SYSTEMS ENGINEERING \nModule summary \nThis module teaches students to adapt their software development practice to take account of \nthe general criticality and the specific risks of the software they are developing. \n \nRelated modules \nStudents on the joint Mathematics and Computer Science programmes who are interested in \nthis module should discuss the necessary prerequisite knowledge with the module leader. \n \nElective Pre-Requisites \n These pre-requisites only apply to students taking this module as an elective. \n \nGood knowledge of software engineering practice. \n \nModule will run \nOccurrence\u200b\nTeaching period \nModule aims \nThis module teaches students to adapt their software development practice to take account of \nthe general criticality and the specific risks of the software they are developing. Students will \ngain a broad understanding of the issues involved in designing and implementing critical \nsystems, be aware of the methods used to construct critical systems, and understand the \nlimitations of the various methods, analysis techniques and tools currently in use. \n \nModule learning outcomes \nUnderstand and articulate the issues involved in designing and implementing critical systems \n \nApply a range of techniques to the design, and validation of high integrity systems, such as \nfault-tree analysis and failure analysis. \n \nDiscuss issues of high integrity engineering, both technical and social. \n \nParticipate in significant discussion periods brainstorming scenarios and discussing previous \nwell-documented examples of system failures \n \nIntroduce existing software engineering concepts for the development of critical systems \n \nIndicative assessment \nTask\u200b\n% of module mark \nOnline Exam -less than 24hrs (Centrally scheduled)\u200b100 \nSpecial assessment rules \nNone \n \nIndicative reassessment \nTask\u200b\n% of module mark \n",
        "Online Exam -less than 24hrs (Centrally scheduled)\u200b100 \nModule feedback \nFeedback is provided through work in practical sessions, through discussion sessions, and after \nthe final assessment as per normal University guidelines. \n \n \nIndicative reading \nN.G. Leveson, Safeware: System Safety and Computers, Addison-Wesley, 1995 \n \n \n",
        "HIGH-PERFORMANCE PARALLEL & DISTRIBUTED SYSTEMS \nModule summary \nThis module introduces and explores the use of high-performance computing, and related \ntechnologies. \nModule aims \nThis module introduces and explores the use of HPC and related technologies, covering on and \noff node parallelism, accelerators, and memory management. Students will understand the \nincreasingly important role that HPC plays in science and engineering. Throughout the module, \nstudents will practice by developing programs using a range of parallel programming paradigms \nlike OpenMP and MPI. \n \nModule learning outcomes \nDemonstrate the development of parallel programs on shared memory systems using popular \nprogramming paradigms. \nBe able to understand appropriate metrics to assess the performance of applications and \nsystems. \nDemonstrate the development of parallel programs on distributed memory systems using \nappropriate parallel programming libraries. \nBe able to explore the design space afforded by parallel and distributed systems, including \nissues of GPU programming, high-speed networking, and I/O. \nBe able to reason about the performance, portability, and productivity of different approaches to \ndeveloping HPC applications. \nIndicative assessment \nTask\u200b\n% of module mark \nEssay/coursework\u200b\n100 \nSpecial assessment rules \nNone \n \nIndicative reassessment \nTask\u200b\n% of module mark \nEssay/coursework\u200b\n100 \nModule feedback \nFeedback is provided through work in practical sessions, and after the final assessment as per \nnormal University guidelines. \n \n \nIndicative reading \nIntroduction to High Performance Scientific Computing, Eijkhout, Victor; van de Geijn, Robert; \nChow, Edmond \n \n \nHUMAN FACTORS: TECHNOLOGY IN CONTEXT \nAdditional information \nPre-Requisite Module: \n",
        " \nHuman-Computer Interaction (HCIN) - COM00018C \n \nPre-Requisite Knowledge: \n \nResearch Methods \n \nElective Pre-Requisites \n These pre-requisites only apply to students taking this module as an elective. \n \nNone. \nModule will run \nOccurrence\u200b\nTeaching period \nModule aims \nThis module will equip students with an understanding of how human characteristics and design \ninfluence interactions with technical systems in organisational and industry contexts. The \nmodule uses real-world case studies to introduce and critically evaluate the contribution of \nhumans in developing and maintaining safe, ethical and secure systems. The module aims to \ndevelop students' skills in: \n \nAnalysis techniques \n \nProposing and applying appropriate criteria to assess the rigour and suitability of industrial and \nacademic approaches to human reliability. \n \nModule learning outcomes \nRecognise the role of human characteristics/factors/mental models in relation to the safe, \nsecure and effective operation and maintenance of interactive systems across applied/industry \ncontexts \n \nCritically evaluate the quality of designs and propose solutions to security and safety issues \n \nDemonstrate the ability to locate and, where relevant, apply existing academic research to \ninform the consideration of issues arising within human-technology interactions \n \nIdentify, apply and critique appropriate techniques for human reliability and error analysis \n \nIdentify, apply and critique appropriate techniques for security analysis, including issues of \nauthentication, privacy and foreseeable misuse \n \nUse appropriate frameworks and design techniques to investigate ethical issues in relation to \nhuman-machine interaction \n \nIndicative assessment \n",
        "Task\u200b\n% of module mark \nEssay/coursework\u200b\n100 \nSpecial assessment rules \nNone \n \nIndicative reassessment \nTask\u200b\n% of module mark \nEssay/coursework\u200b\n100 \nModule feedback \nFeedback is provided throughout the sessions, and after the assessment as per normal \nUniversity guidelines.\n \n",
        "INTELLIGENT SYSTEMS: PROBABILISTIC & DEEP LEARNING \nAdditional information \nPre-Requisite knowledge - Understanding of the theory and practice of Machine Learning. For \nundergraduates this is covered in intelligent systems modules (for example, INT2 -  Intelligent \nSystems 2: Machine Learning & Optimisation COM00024I or IMLO - Intelligent Systems: \nMachine Learning & Optimisation COM00026I). \n \n  \n \nElective Pre-Requisites \n These pre-requisites only apply to students taking this module as an elective. \n \nA Level Maths or Equivalent plus good understanding of the theory and practice of machine \nlearning. \n \nModule will run \nOccurrence\u200b\nTeaching period \nModule aims \nThis module builds on the basic machine learning covered previously in the programme, and \ntakes students up to state of the art methods in modern deep learning. It introduces probabilistic \nmethods, where we can reason about uncertainty, and deep learning based methods, where \nneural networks with many layers prove to be the most powerful general model for learning. We \nwill see a range of methods and architectures for classification and regression problems, \nunsupervised generative models and the mathematics that underlies these techniques. We will \ncover both theory and practicalities: how are these ideas actually implemented in a modern \nmachine learning library like PyTorch? \n \nModule learning outcomes \nExplain the probabilistic basis of machine learning \n \nDemonstrate a working knowledge of manifold embedding and kernel methods \n \nApply a range of Bayesian methods for regression, classification and clustering \n \nBe familiar with the main deep learning architectures \n \nDemonstrate the optimisation process and different variants (i.e. gradient descent, stochastic \nalgorithms, ADAM) \n \nIndicative assessment \nTask\u200b\n% of module mark \nEssay/coursework\u200b\n100 \nSpecial assessment rules \nNone \n",
        " \nIndicative reassessment \nTask\u200b\n% of module mark \nEssay/coursework\u200b\n100 \nModule feedback \nFeedback is provided through work in practical sessions, and after the final assessment as per \nnormal University guidelines \n \nIndicative reading \nSolomon, Justin. Numerical Algorithms. AK Peters/CRC Press, 2015\n \n",
        "LEGAL PRACTICE, TECHNOLOGY & COMPUTER SCIENCE \nModule summary \nLaw and computer science students will work collaboratively in teams to develop a \ntechnology-based solution to a real-life legal practice process problem. From initial analysis of \nthe problem, teams will work to develop a solution to the problem, assessing both legal and \nprocedural issues, system and user design requirements, functionality, costs, benefits and risks. \nStudents from each discipline will contribute know-how from their discipline to the project, whilst \ngaining new understanding and skills. The module will be supported by a leading international \nlaw firm with a technology hub in the region, providing students with access to external \nexpertise in the field, in addition to disciplinary tutor support. \n \nModule aims \nThe aim of the module is to provide law and computer science students with an opportunity to \ndevelop an applied understanding of how technology and computer science are being applied in \ndeveloping new approaches to the provision of legal services and how these can offer greater \naccessibility to justice, as well as efficiency, quality and costs gains. Students will develop \nknowledge both within their own and the colloborative discipline, as well as a range of analytical, \nproblem-solving, planning, communication and interpersonal skills. The module also aims to \nprovide students with opportunities to interact with experts in legal practice technology and thus \ngain contemporary professional perspectives on the areas covered in the module. \n \nModule learning outcomes \nOn completion of this module, students should be able to: \n \nAnalyse a legal process which may be improved for users by application of computer science \nApply design thinking to identify procedural, technical, legal and user issues \nCommunicate orally and in writing relevant principles from their primary discipline to colleagues \nfrom another discipline \nCollaborate and synthesise disciplinary principles to develop potential solutions \nEvaluate potential solutions against user requirements \nFormulate a costed implementation plan for an agreed solution \nPresent a persuasive written and oral case for the solution \nExplain the potential applications of computer science and technology in the development of \nlegal services \nReflect on learning gains and challenges from the module, including cross-disciplinary \ncollaboration \nModule content \n \nLaw and computer science students will work collaboratively to develop a solution to a legal \nprocess problem, based on a real-life access to justice scenario. This will be developed in \ncollaboration with a law firm. \n \n \n",
        "The module will use problem-based learning (PBL) techniques, and be predominantly \ngroup-assessed. Students will receive a problem and, working in teams comprising equal \nnumbers from each discipline will apply PBL techniques to identify: \n \nsubstantive and procedural issues \nclient and internal commercial issues \nrisks \nprocess requirements \nUsing these as the basis to develop a solution, there will be an element of cross-discipline \nteaching: law students will have to be able to explain law, procedure and process requirements \nto computer science students and the latter will have to explain tech functionality and \ncapabilities, in each case in language understandable to those from the other discipline. \n \n \nFollowing initial detailed analysis of the problem, teams will work to develop a solution to the \nproblem, with activity moving from analysis to development. They will plan a programme of work \noutside class activity sessions, with the latter acting as formal workshops/surgeries, during \nwhich teams can obtain feedback from facilitators from both disciplines. There will also be an \nopportunity to obtain feedback from a legal technology expert from a law firm, as part of a \nplenary \"masterclass\". \n \n \nThe assessment will require submission of the solution - e.g., system requirements; \nfunctionality; process map; costs; benefits; risk; time-line to implementation - in the form of a \nbusiness proposal, together with an oral presentation by each group. Individual students will \nalso submit a personal reflection on learning gained against the module outcomes. \n \nIndicative assessment \nTask\u200b\n% of module mark \nEssay/coursework\u200b\n20 \nGroupwork\u200b\n50 \nOral presentation/seminar/exam\u200b\n30 \nSpecial assessment rules \nNone \n \nAdditional assessment information \nFormative feedback will be provided on a rolling basis by facilitators as teams progress through \nthe analysis and development stages of the module, especially during workshops. \n \nIndicative reassessment \nTask\u200b\n% of module mark \nEssay/coursework\u200b\n20 \nGroupwork\u200b\n50 \nOral presentation/seminar/exam\u200b\n30 \n",
        "Module feedback \nStudents will receive summative feedback as follows: \n \noral feedback on group presentation \nwritten feedback on group submission \nwritten feedback on individual reflective submission \nIndicative reading \nThe Future of the Professions: Susskind & Susskind \u2013 OUP 2017 \n \n \nTomorrow\u2019s Lawyers \u2013 Susskind \u2013 OUP 2017 \n \n \nThe End of Lawyers? Susskind \u2013 OUP 2008 \n \n \n",
        "NETWORK SECURITY \nModule summary \nNetwork Security (NETS): This module covers the basic concepts of cyber security, how these \nare modelled, threat models, and the mechanisms to enforce security policies. \n \nElective Pre-Requisites \n These pre-requisites only apply to students taking this module as an elective. \n \nA Level Maths or Equivalent. \n \nModule will run \nOccurrence\u200b\nTeaching period \nModule aims \nThis module aims to provide a broad knowledge of network and system security, addressing \nthreats over a range of network layers and detailing corresponding defensive countermeasures \nand protocols. The module will cover the basic concepts of cyber security (confidentiality, \nintegrity and availability), how these are modelled, threat models (adversary capabilities and \ngoals), and basic control mechanisms to enforce security policies (e.g. access control). Students \nwill learn to understand network security, threats, and the mechanisms that have been \ndeveloped to counter them. It explores a range of different networked systems, the main \nnetwork attacks, and their defence mechanisms. \n \nModule learning outcomes \nBy the end of the module the students will be able to: \n \nDescribe the fundamental concepts of cyber security in systems and networks \nIdentify security strengths & weaknesses in network mechanisms \nIdentify major threats and attacks in systems and networks under various scenarios, \narchitectures, and threat models \nPropose control solutions for network security \nAssess the relative merits of different solution approaches in various security-related contexts \nIndicative assessment \nTask\u200b\n% of module mark \nEssay/coursework\u200b\n100 \nSpecial assessment rules \nNone \n \nIndicative reassessment \nTask\u200b\n% of module mark \nEssay/coursework\u200b\n100 \nModule feedback \nFeedback is provided through work in practical sessions, discussion in seminars, and after each \nassessment as per normal University guidelines. \n \n",
        "Indicative reading \nSherri Davidoff, Jonathan Ham, Network Forensics: Tracking Hackers Through Cyberspace, \nPrentice Hall, 2012 \n \nKevin R Fall, W Richard Stevens, TCP/IP Illustrated, Volume 1: The protocols, Addison Wesley, \n2012 \n \nAndrew Tannenbaum, Computer Networks, Prentice Hall, 2002\n \n",
        "PLAYER EXPERIENCES IN DIGITAL GAMES \nModule aims \nThis module will provide students with a comprehensive understanding of player experience and \nthe different ways in which games can impact players. In addition to learning about what player \nexperience is and how to evaluate it, the module will also cover the effects of games on players \n(e.g. in relation to wellbeing) and the use of games for applied purposes (e.g. behaviour \nchange). \n \nModule learning outcomes \nDemonstrate an understanding of the breadth of player experience \n \nSelect appropriate methods for evaluating different kinds of player experience \n \nConduct playtesting sessions and report on the findings \n \nCompare and contrast the different ways in which digital games can affect players \n \nPlan appropriate evaluations for applied games \n \nIndicative assessment \nTask\u200b\n% of module mark \nEssay/coursework\u200b\n60 \nGroupwork\u200b\n40 \nSpecial assessment rules \nNone \n \nIndicative reassessment \nTask\u200b\n% of module mark \nEssay/coursework\u200b\n60 \nGroupwork\u200b\n40 \nModule feedback \nFeedback is provided throughout the sessions, and after the assessment as per normal \nUniversity guidelines. \n \nIndicative reading \nDrachen, A., Mirza-Babaei, P., & Nacke, L. E. (Eds.). (2018). Games user research. Oxford \nUniversity Press. \n \nStahlke, S., & Mirza-Babaei, P. (2022). The Game Designer's Playbook: An Introduction to \nGame Interaction Design. Oxford University Press.\n \n",
        "QUALITATIVE APPROACHES TO INVESTIGATING UX \nModule aims \nThis module will provide students the methodological approaches to interrogating concepts and \nusers\u2019 subjective experiences with interactive systems. The module provides theoretical and \npractical grounding in methods used for collection and analysis of qualitative data that are used \nin industry and academic research. Students will learn how different qualitative methods can be \nused for a range of purposes, from informing to design to understanding technology use in \ncontext. \n \nModule learning outcomes \nIdentify and justify use of appropriate qualitative methodology \n \nRecognise and acknowledge the underpinning epistemological positions of qualitative methods \n \nRecognise the role of the researcher in the research process \n \nCritique and contrast the methods of studies against the standards of rigour and validity of the \nchosen methodology \n \nReport and discuss the results of an analysis appropriate to the method used \n \nDescribe the important aspects of research governance including ethical conduct and data \ngovernance \n \nIndicative assessment \nTask\u200b\n% of module mark \nEssay/coursework\u200b\n100 \nSpecial assessment rules \nNone \n \nIndicative reassessment \nTask\u200b\n% of module mark \nEssay/coursework\u200b\n100 \nModule feedback \nFeedback is provided throughout the sessions, and after the assessment as per normal \nUniversity guidelines.\n \n",
        "QUANTUM COMPUTATION \nModule summary \nThe aim of this module is to introduce the theory of quantum computation. In it we will learn \nabout the pioneering quantum algorithms that promise a qualitative leap in computation power \nover conventional computers. \n \nElective Pre-Requisites \n These pre-requisites only apply to students taking this module as an elective. \n \nA Level Maths or Equivalent; good understanding of programming. \n \nModule will run \nOccurrence\u200b\nTeaching period \nModule aims \nIntroducing both the promise and limitations of quantum computation. Gate operations, evolving \nquantum states, calculating the result of measurements on quantum states, designing and \nanalyzing quantum computational circuits, key algorithms (e.g., Shor's, Grover's and the \nDeutsch-Jozsa algorithms). \n \n \nModule learning outcomes \nArticulate both the promise and limitations of quantum computation. \n \nApply some of the many concepts and techniques in quantum computation (e.g., applying gate \noperations and evolving quantum states, calculating the result of measurements on quantum \nstates, designing and analyzing quantum computational circuits); \n \nExplain some of the key algorithms (e.g., Shor's, Grover's and the Deutsch-Jozsa algorithms) \nand their implications, and are able to simulate these algorithms on quantum states. \n \nIndicative assessment \nTask\u200b\n% of module mark \nOnline Exam -less than 24hrs (Centrally scheduled)\u200b100 \nSpecial assessment rules \nNone \n \nIndicative reassessment \nTask\u200b\n% of module mark \nOnline Exam -less than 24hrs (Centrally scheduled)\u200b100 \nModule feedback \nFeedback is provided through work in practical sessions, and after the final assessment as per \nnormal University guidelines. \n \nIndicative reading \n",
        "*** G. Benenti, et al., Principles of Quantum Computation and Information, vol I, World Scientific, \n2004 \n \n*** G. Benenti, et al., Principles of Quantum Computation and Information, vol II, World \nScientific, 2007 \n \n*** P. Kaye, et al., An Introduction to Quantum Computing, Oxford University Press, 2007\n",
        "RESEARCH METHODS IN COMPUTER SCIENCE \nModule summary \nA core module that equips postgraduate taught students with core theorectical and practical \nresearch skills. \nModule will run \nOccurrence\u200b\nTeaching period \nModule aims \nResearch is about both generating new knowledge and evaluating confidence in knowledge. \nThere are three skills associated with the conduct of good research: \n \nThe ability to accurately identify from existing literature and systems a meaningful or important \ngap in knowledge, and therefore what constitutes new knowledge in the domain of Computer \nScience \n \nThe ability to competently intervene in the world (e.g. through developing systems, \nimplementing data collection procedures, conducting experiments\u2026) in order to generate \nknowledge that causes positive change. \n \nThe ability to evaluate the quality of evidence stemming from an intervention using sound \nanalysis, and communicate that analysis to the scientific community. \n \nAs the ways of intervening in the world depend strongly on your disciplinary area, the aim of this \nmodule is to provide an introduction to the first and third pillars, namely the conduct of literature \nreviews to identify a research gap; and the methods for analysing research data, interpreting \nsaid analysis, and accurately presenting research outcomes. \n \nThe module will also cover general principles related to the second pillar that apply across \ndisciplinary areas in the development of novel methods and conduct of experiments. \n \nModule learning outcomes \nBy the end of this module, students will be able to\u2026 \n \nIdentify a gap in the evidence base within the structure of a formal academic literature review. \n \nDiscuss the validity and reliability of methods used in extant or novel research. \n \nDescribe and apply principles of responsible research and innovation to a research project. \n \nAnalyse a variety of quantitative research data using an array of appropriate inferential \nstatistical tests. \n \nPresent the outcomes of a research project. \n \nIndicative assessment \n",
        "Task\u200b\n% of module mark \nEssay/coursework\u200b\n50 \nGroupwork\u200b\n50 \nSpecial assessment rules \nNone \n \nIndicative reassessment \nTask\u200b\n% of module mark \nEssay/coursework\u200b\n50 \nEssay/coursework\u200b\n50 \nModule feedback \nFeedback is provided throughout the sessions, and after the assessment as per normal \nUniversity guidelines. \n \nIndicative reading \nHowell DC. Fundamental Statistics for the Behavioral Sciences . 9th edition, student edition. \nCengage Learning; 2017. \nGoldbort R. Writing for Science . Yale University Press; 2006.\n \n",
        " \n"
    ],
    "semesters_-5718966191674416995": [
        "Year 1 \nFOUNDATIONS OF PROGRAMMING FOR COMPUTER SCIENCE \nModule aims \nStudents will be introduced to different programming constructs, basic data structures, \ncommand line tools, integrated development environments and unit testing of programs. \nStudents will learn how to describe well-defined tasks using pseudocode and translate them into \nprograms using a procedural programming paradigm. The module will be taught using a \nprocedural language for practising these skills. \nModule learning outcomes \nS101\u200b Describe and apply the fundamental concepts of procedural programming. Write small \nprocedural programs from scratch to perform well-defined tasks, following well-defined \nrequirements, in a procedural programming language like Python. Relate the syntax of the \nlanguage to its semantics, and analyse the result of executing fragments of syntax. Integrate \nlibrary code with their own programs using appropriate software tools. \nS102\u200b Implement bespoke data structures to store states of a process. Implement simple \nalgorithms written in pseudocode. Develop programs incrementally, using simple tests \n(automated where appropriate) to check each increment. \nS103\u200b Store data in memory in standard built-in collection types, and to store and retrieve data \nfrom simple text files such as CSV and JSON files. \nS104\u200b  \nUse an appropriate software development environment, such as Eclipse, IntelliJ or VS Code. \nGiven a program and a debugging tool, students will be able to identify and correct bugs which \nprevent the program from functioning as intended. \nS105\u200b  \nOrganise and document program code following the principles of software engineering. Write \ndocumentation to explain the design and implementation of their own code, or example code \nwhich is supplied to them. \n \nIndicative assessment \nTask\u200b\n% of module mark \nOnline Exam -less than 24hrs (Centrally scheduled)\u200b100 \nSpecial assessment rules \nNone \n \nIndicative reassessment \nTask\u200b\n% of module mark \nOnline Exam -less than 24hrs (Centrally scheduled)\u200b100 \nModule feedback \nFeedback is provided through work in practical sessions, formative assessments, and after the \nfinal assessment as per normal University guidelines. \n \nIndicative reading \n\nAllen B. Downey - Think Python: How to Think Like a Computer Scientist - 2nd ed. (2015), \nO'Reilly Media \n \nMike Dawson - Python programming for the absolute beginner - 3rd ed. (2010) - Course \nTechnology \n \nKent D. Lee and Steve Hubbard - Data structures and algorithms with Python (2015), Springer \n \n\nMATHEMATICAL FOUNDATIONS OF COMPUTER SCIENCE \nModule aims \nStudents will be introduced to the key discrete mathematics concepts that are the foundation of \ncomputer science. Seven topics are covered as follows: i) counting (combinatorics), ii) discrete \nprobability, iii) graphs, iv) propositional and predicate logic, v) proofs and sets, vi) relations on \nsets, and vii) relations on a single set. After studying the module, students will be able to apply \nthe learnt concepts, theories and formulae in real-world examples of computational problems. \n \nModule learning outcomes \nT101\u200b Define, read and apply mathematical notations for the purpose of describing \nmathematical concepts from across discrete mathematics. \nT102\u200b Select appropriate techniques to prove properties related to discrete mathematics \nconcepts. \nT103\u200b Understand how to construct sets of elements with certain properties and determine their \ncardinality using counting formulae from combinatorics. \nT104\u200b Understand and apply basic set theory, including formally defining set relations and \noperations. \nT105\u200b Describe and use the basic concepts of discrete probability to describe events, with an \nunderstanding of joint, conditional and marginal probabilities, Bayes\u2019 theorem, expectation, \ncovariance and correlation. \nT106\u200b Formally define and illustrate by example graphs of different graph classes, such as \nsimple, undirected, directed, weighted, directed acyclic, connected, disconnected and trees - \nwith an understanding of how they may be used in real-world computational problems. \nT107\u200b Apply a variety of techniques to identify whether logical expressions are true or false, \nvalid or invalid or equivalent to one another, and be able to apply logical statements to describe \nreal-world logical problems. \n \n \nIndicative assessment \nTask\u200b\n% of module mark \nClosed/in-person Exam (Centrally scheduled)\u200b\n100 \nSpecial assessment rules \nNone \n \nIndicative reassessment \nTask\u200b\n% of module mark \nClosed/in-person Exam (Centrally scheduled)\u200b\n100 \nModule feedback \nFeedback is provided through work in practical sessions, and after the final assessment as per \nnormal University guidelines. \n \nIndicative reading \n** Dean N., The Essence of Discrete Mathematics, Prentice Hall, 1997 \n \n\n** Haggarty R., Discrete Mathematics for Computing, Addison Wesley, 2002 \n \n** Truss J., Discrete Mathematics for Computer Scientists, Addison Wesley, 1999 \n \n** Gordon H., Discrete Probability, Springer, 1997 \n \n* Solow D., How to Read and Do Proofs, Wiley, 2005 \n \n \n \n\nHUMAN-COMPUTER INTERACTION \nModule summary \nHCIN introduces user-centred design. Where other modules focus on technical understanding \nof computers and how they work, this module is instead about understanding the relationship \nbetween computer systems and people. It discusses how this can be used to improve system \ndevelopment, and how it can go wrong. We will explore the nature of and barriers to people's \ninteractions with computers and how systems can be designed to optimise and facilitate these \ninteractions. We will also consider how to evaluate the people's experience - what makes a \ngood, enjoyable human-computer interaction. \n \nRelated modules \nThe assessment is undertaken as a group open assessment in which groups will work together \nto complete and present a user-centred design task. The assessment also includes as part of \nthe submission an individual reflection exercise which is worth 10% of the overall marks. This \ntask asks students to comment on their development of transferable skills (using the York \nStrengths Framework as a basis) and how they have developed these during HCIN. The \nassessment also has a peer-assessment exercise, which ensures that individual contributions to \nthe group are accounted for in the final marks. \n \nReassessment is by an individual open assessment which requires students to understand and \nimplement the processes and techniques discussed during the module. \n \nModule will run \nOccurrence\u200b\nTeaching period \nModule aims \n \nStudents taking this module will be familiarised with how to design user-centred systems that \nmeet the needs and preferences of diverse users. Students will be introduced to the notion of \nengineering lifecycles, and in particular building requirements from user needs, iterative \nprototyping and evaluation of interactive systems. Students will undertake group work in \npracticals, giving them opportunities to develop communication and conflict resolution skills. The \nassessment will evaluate knowledge of the user-centred design process and interaction design \nprinciples. \n \nModule learning outcomes \nDescribe why user-centred design in software development is important to usable and inclusive \ndesign. \n \nUndertake a user-centred design process as a cyclical approach through the key stages of user \nneeds elicitation, conceptual design, prototyping, and evaluation. \n \nApply appropriate interaction design concepts in describing user-system interaction including: \naffordances, feedforward, feedback, conceptual model. \n \n\nAdvocate for the ethical treatment of participants throughout the user-centred design lifecycle, \nand explain how user diversity can impact on the inclusiveness of a system. \n \nDescribe how interactive systems are embedded in societal structures, and how they are used \nto invoke change at the personal, community, national or international level. \n \nPlan and manage deliverables to set deadlines throughout a project lifecycle, and use a \nself-reflective skills assessment to improve student team working and team performance. \n \nIndicative assessment \nTask\u200b\n% of module mark \nEssay/coursework\u200b\n10 \nGroupwork\u200b\n90 \nSpecial assessment rules \nNone \n \nIndicative reassessment \nTask\u200b\n% of module mark \nEssay/coursework\u200b\n100 \nModule feedback \nFeedback is provided through work in practical sessions, formative assessments, and after the \nfinal assessment as per normal University guidelines. \n \nIndicative reading \n*** Preece, J., Rogers, Y., Sharp, H., Interaction Design, 4th edn Wiley, 2015 \n \n*** Cooper, A., Reimann., R., Cronin., D., Noessel., C. About Face: The Essentials of Interaction \nDesign. 4th edn Wiley, 2014. \n \n* Mackenzie, I.S. Human-Computer Interaction. Elsevier Inc., 2013. \n \n* Norman, D. The Design of Everyday Things. Any edition. \n \n\nOBJECT-ORIENTED DATA STRUCTURES AND ALGORITHMS \n \nModule aims \nStudents begin to program key data structures such as stacks, queues, trees and graphs. They \nare introduced to the idea of complexity of an algorithm, and how to characterise time and \nspace through formal notations and proof techniques. Students are taught using an object \noriented language like Java, and learn the basics of test driven development for testing their \ncode and demonstrating its successful running. Students are also introduced to several \nalgorithm design paradigms such as greedy algorithms. \n \nModule learning outcomes \nS201 \n \nImplement an object oriented design. This includes organising program code into modules using \nmethods following the software engineering principles of modularity and abstraction, and \nassembling data and methods into classes at an introductory level following the software \nengineering principles of encapsulation and data hiding. Integrate standard library code with \ntheir own programs using appropriate software tools. \n \nS202 \n \nWrite and test code that conforms to specific interfaces. Identify and correct bugs which prevent \nthe program from functioning as intended. \n \nS203 \n \nOrganise and document program code following the principles of software engineering. \nGenerate documentation, manually and programmatically, to explain the design and \nimplementation of their own code, or example code which is supplied to them. \n \nS204 \n \nAnalyse problems in order to confidently design algorithms to solve simple problems,and be \nable to explain how algorithms and Processing programs work. Develop small programs that \nimplement basic algorithmic designs. Argue the correctness of algorithms using inductive proofs \nand invariants. \n \nS205 \n \nAnalyse worst-case running times of algorithms using asymptotic analysis and apply the \nknowledge to sorting and searching algorithms, categorising efficiency in time and memory use. \n \nS206 \n \n\nCompare between different abstract data structures from linked lists to graphs in order to be \nable to choose an appropriate data structure for a design situation. This includes the major \nsearch, sort, and graph algorithms and their analyses. \n \nS207 \n \n.Apply algorithmic design paradigms such as greedy and dynamic programming paradigms. \nPresent an argumentation for the choice of a paradigm for a given problem. Describe what an \napproximation algorithm is, the benefit of using approximation algorithms, and analyse the \napproximation factor for such an algorithm. \n \n \nIndicative assessment \nTask\u200b\n% of module mark \nOnline Exam -less than 24hrs (Centrally scheduled)\u200b50 \nOnline Exam -less than 24hrs (Centrally scheduled)\u200b50 \nSpecial assessment rules \nNone \n \nIndicative reassessment \nTask\u200b\n% of module mark \nOnline Exam -less than 24hrs (Centrally scheduled)\u200b50 \nOnline Exam -less than 24hrs (Centrally scheduled)\u200b50 \nModule feedback \nFeedback is provided through work in practical sessions, formative assessments, and after the \nfinal assessment as per normal University guidelines. \n \nIndicative reading \nSteven S. Skiena - The algorithm design manual - 2nd ed., (2010), Springer \n \nMichael T. Goodrich, Roberto Tamassia, Michael H. Goldwasser - Data Structures and \nAlgorithms in Java (2014), Wiley Etextbooks \n \nMitsunori Ogihara - Fundamentals of Java programming (2018), Springer \n \n\n \n \nINTRODUCTION TO COMPUTER ARCHITECTURES \nModule will run \nOccurrence\u200b\nTeaching period \nModule aims \nStudents taking this module will gain foundations in the key architectural components of a \ncomputer system, how a high level program is executed upon that system, and how a computer \nsystem is constructed in hardware. Students will be introduced to how a computer system is \nconstructed, and how a program implemented in a high level programming language (for \nexample C) executes on that system. Students will be introduced to the basics of computer \narchitecture and program language construction, providing a basis for further study in later \nyears. Students will be introduced to a bottom-up approach, motivated by real examples, taught \nas both lectures and laboratory practicals. Students will be able to describe and apply their \nprogramming skills on real devices and computer systems that are used in many real \napplications today. \n \nModule learning outcomes \nIdentify the purpose of key computer hardware components such as processors, memories and \nbusses. \nDescribe different data types commonly found in binary systems (e.g. signed vs. unsigned \nintegers), and show how to convert, perform arithmetic, and perform logical operations. \nExpress logical expressions as basic gates, transistors and combinatoric logic circuits \nDescribe the function and limitations of a variety of logical building blocks in the context of \nprocessor architectures \nDescribe the von Neumann Model paradigm of computer architecture, including the fetch \nexecute cycle of instruction processing. \nExplain how operations executed in a processor can be used to implement to higher level \nsequential, conditional and iterative programming language constructs \nBuild a simple system comprised of a CPU, memory and input/output. \nExplain the use of assemblers, compilers and linkers to create executable code for a processor, \nand use such a toolchain develop software for the simple system built in the module \nIdentify potential security problems associated with architecture design. \nIndicative assessment \nTask\u200b\n% of module mark \nEssay/coursework\u200b\n100 \nSpecial assessment rules \nNone \n \nAdditional assessment information \nOpen assessment is a long-running task that takes place over a number of weeks. \n \nIndicative reassessment \nTask\u200b\n% of module mark \n\nEssay/coursework\u200b\n100 \nModule feedback \nFeedback is provided through work in practical sessions, and after the final assessment as per \nnormal University guidelines. \n \nIndicative reading \n*** J.Hennessy, D.Patterson Computer Architecture: A Quantitative Approach (2nd Edition) \nMorgan Kaufmannn 1990 \n \n*** W.Stallings Computer Organization and Architecture: Design For Performance (8th Edition) \nPearson 2010 \n \n\nFORMAL LANGUAGES AND AUTOMATA \nModule will run \nOccurrence\u200b\nTeaching period \nModule aims \nStudents taking this module will be introduced to the concepts of formal languages and the \nabstract machines that accept them as a way of describing computation. Students will have a \ndeep understanding of finite automata and pushdown automata, with their associated languages \nand related proof techniques, and will be introduced to more complex machines accepting \ncontext sensitive and recursively enumerable languages for purposes of being able to identify \nand describe them. \n \nModule learning outcomes \nDescribe and illustrate the concepts of formal languages, automata and grammars, and the \nrelations between them; \nConstruct a variety of abstract machines including: deterministic and non-deterministic finite \nautomata, deterministic and non-deterministic pushdown automata and Turing machines; \nDistinguish different classes of automata, and the languages they accept; \nApply a variety of operations to transform and convert between automata; \nConvert between grammars and automata for regular and context-free languages; \nDemonstrate that a grammar is ambiguous; \nApply the pumping lemma for regular and context-free languages to show a language is not \nregular or context-free respectively; \nDescribe the Chomsky hierarchy; \nIdentify key applications in computing where regular and context-free languages are used in \npractice; and \nUse automata theory as the basis for building lexers and parsers. \nIndicative assessment \nTask\u200b\n% of module mark \nOpen Examination\u200b\n100 \nSpecial assessment rules \nNone \n \nIndicative reassessment \nTask\u200b\n% of module mark \nOpen Examination\u200b\n100 \nModule feedback \nFeedback is provided through work in practical sessions, and after the final assessment as per \nnormal University guidelines. \n \nIndicative reading \n*** Peter Linz, An introduction to formal languages and automata. Sixth Edition. Jones and \nBartlett Computer Science. 2017 \n** D.I. Cohen, Introduction to Computer Theory. 2nd Edition. Wiley. 1997 \n\n** Hopcroft, John E. and Motwani, Rajeev and Ullman, Jeffrey D., Introduction to Automata \nTheory, Languages and Computation (3rd ed.), Pearson Education, 2013 \n** S.H. Rodger and T.W. Finley, JFLAP: An interactive formal language and automata package. \nJones and Bartlett Computer Science. 2006. Available online \n* Martin, John C., Introduction to Languages and the Theory of Computation (4th ed.), McGraw \nHill, 2010 \n* Rich, Elaine, Automata, Computability and Complexity, Pearson Education, 2008 \n* Sipser, Michael, Introduction to the Theory of Computation (3rd ed.), South-Western College \nPublishing, 2012\n \n\n",
        "Year 2 \nDATA: INTRODUCTION TO DATA SCIENCE \n \nModule aims \nStudents will be introduced to key concepts required to undertake rigorous and valid data \nanalysis. Students will be introduced to processes for collecting, manipulating and cleaning \ndata, while gaining experience in judging the quality of data sources. Students will be introduced \nto statistical analysis in data science, including correlation, inferential statistics and regression, \nand how to use these tests in a programming environment. Relational databases, SQL, and and \nother database paradigms such as NoSQL, are covered as a way of storing and accessing data. \nA key aim of the module is to solve complex problems and deliver insights about \nmulti-dimensional data. \n \nModule learning outcomes \nDistinguish between different types of data that are generated in science, engineering and \ndesign, and employ strategies for ensuring data quality. \nRetrieve data from a variety of different data sources in a variety of different formats. \nApply inferential statistics and statistical procedures to test hypotheses about features and \nrelationships within data sets. \nUse appropriate visualisations to present and explore data sets. \nUse databases, both relational and of other paradigms, to store and query data. \nIdentify the ethical concerns regarding the provenance of data, the privacy of individuals, and \nthe impact data analytics can have on society, and apply topics from the code of ethics of a \nprofessional data protection body. \n \nIndicative assessment \nTask\u200b\n% of module mark \nEssay/coursework\u200b\n100 \nSpecial assessment rules \nNone \n \nIndicative reassessment \nTask\u200b\n% of module mark \nEssay/coursework\u200b\n100 \nModule feedback \nFeedback is provided through work in practical sessions, and after the final assessment as per \nnormal University guidelines. \n \nIndicative reading \n*** Spiegelhalter, D., The Art of Statistics: Learning from Data, Pelican, 2019. \n \n*** VanderPlas, J. Python Data Science Handbook: Essential Tools for Working with Data, \nO\u2019Reilly, 2016. \n \n\n** Igual, L. Segui, S. Introduction to Data Science: A Python Approach to Concepts, Techniques \nand Applications, Springer, 2017\n \n\nSYSTEMS & DEVICES 2: OPERATING SYSTEMS, SECURITY, AND NETWORKING \nRelated modules \nPre-requisite modules \nSystems & Devices 1: Introduction to Computer Architectures (COM00011C) \n  \nCo-requisite modules \nNone \n  \nProhibited combinations \nNone \nModule aims \nThis module builds on Systems and Devices 1 by examining the system software that executes \nupon a computer system. Students will learn how the resources of the system can be shared by \nmultiple programmes and users, and how networking can be used to communicate between \nprogrammes. One important aspect is how basic security and protection mechanisms are \nprovided by the processor and memory system. Throughout, the module students will consider \npractical examples based on computer systems used today. This module also introduces \nstudents to the core concepts of computer networking by covering the layered network model, \nand discussing the utility and motivation for such an approach. Services that are layered on this \nmodel (such as UNIX sockets, DNS, TCP, IP) are detailed and students will develop software to \nexperiment with these features. After taking this module, students will have an understanding of \nthe role of an operating system, how computers can support multiple time-sliced programmes, \nand how all kinds of computer networks, including the Internet, are created. \n \n \nModule learning outcomes \nDemonstrate application programming of OS-supported concurrency, communication and I/O. \nShow how the structure of the OS is supported by computer hardware, with specific reference to \nthe hardware features that extend the basic systems introduced in S&D1. \n \nUse basic resource management mechanisms provided by common OSes, including time and \nmemory. \n \nDemonstrate use of the memory protection mechanisms provided by hardware and OSes, \nincluding memory mapped I/O. \n \nDemonstrate use of the information security provided by the OS in terms of file systems. \n \nDemonstrate concurrent programming at the process level and show how it is supported by, and \nimplemented on, the system hardware. \n \nLearn to recognise and avoid issues of deadlock, livelock, and starvation. \n \nBe able to articulate the motivation behind the layered network model \n\n \nDevelop software using OS-level networking concepts (i.e. sockets) to communicate with other \nsystems. \n \nDemonstrate understanding of networked architectures, how they are integrated into an \noperating system, and develop simple applications using this knowledge. \n \nIndicative assessment \nTask\u200b\n% of module mark \nEssay/coursework\u200b\n50 \nOnline Exam -less than 24hrs (Centrally scheduled)\u200b50 \nSpecial assessment rules \nNone \n \nAdditional assessment information \nStudents are only required to resit any failed assessment component. \n \nIndicative reassessment \nTask\u200b\n% of module mark \nEssay/coursework\u200b\n50 \nOnline Exam -less than 24hrs (Centrally scheduled)\u200b50 \nModule feedback \nFeedback is provided through work in practical sessions, and after the final assessment as per \nnormal University guidelines. \n \nIndicative reading \n*** Operating System Concepts, Tenth Edition by Silberschatz, Galvin and Gagne, Wiley (2018) \n \n* A. S. Tanenbaum, Modern Operating Systems, Prentice Hall (2014). \n* W. Stallings, Operating Systems, Internals and Design Principles (9th Edition) Ninth Edition, \nPrentice Hall (2017). \n \n \n \n\nTHEORY 3: COMPUTABILITY, COMPLEXITY & LOGIC \n \nModule aims \nThis module covers computability theory and complexity theory. In particular, students will learn \nthe concepts of semi-decidable and decidable languages, and Turing-computable functions. \nThey will be able to explain the difference between solvable and unsolvable problems and prove \nunsolvability by reduction. They will understand the time and space complexity of Turing \nmachines, complexity classes such as P, NP, PSpace, NPSpace and NPC, and prove \nNP-completeness by reduction. The module will also introduce basic concepts and results in \npropositional logic, predicate logic, and program verification. In particular, students will learn to \ndistinguish between syntax and semantics, and be able to use formal proof systems such as \nnatural deduction. They will understand the limitations of logic in terms of decidability and \nexpressiveness, and how to use a formal calculus such as Hoare logic to specify programs and \nprove them correct. \n \nModule learning outcomes \n \nUse unrestricted grammars and Turing machines to specify semi-decidable languages. \n \nProvide examples of unsolvable problems and prove that a problem is unsolvable by reducing a \nknown unsolvable problem to it. \n \nExplain the Church-Turing thesis and its significance. \n \nDefine the classes P and NP, and explain their relation to the class ExpTime. \n \nExplain the significance of NP-completeness and provide examples of NP-complete problems. \n \nExplain the meaning of formulas in propositional and predicate logic, and translate such \nformulas into English and vice-versa. \n \nExplain the fundamental difference between syntax and semantics. \n \nApply the rules of natural deduction to construct proofs, and determine the truth or falsity of \nformulas in a given model. \n \nExplain the limitations of logic and the relationship between logic and computability. \n \nReason deductively about programs using formalisms such as Hoare logic and weakest \npreconditions \n \nIndicative assessment \nTask\u200b\n% of module mark \nClosed/in-person Exam (Centrally scheduled)\u200b\n100 \n\nSpecial assessment rules \nNone \n \nIndicative reassessment \nTask\u200b\n% of module mark \nClosed/in-person Exam (Centrally scheduled)\u200b\n100 \nModule feedback \nFeedback is provided through work in practical sessions, revision classes, and after the final \nassessment as per normal University guidelines. \n \nIndicative reading \n**** Martin, John C., Introduction to Languages and the Theory of Computation (4th ed.), \nMcGraw Hill, 2010 \n \n** Rich, Elaine, Automata, Computability and Complexity, Pearson Education, 2008 \n \n** Sipser, Michael, Introduction to the Theory of Computation (3rd ed.), South-Western College \nPublishing, 2012 \n \n* Hopcroft, John E. and Motwani, Rajeev and Ullman, Jeffrey D., Introduction to Automata \nTheory, Languages and Computation (3rd ed.), Pearson Education, 2013 \n \n* Arora, Sanjeev and Barak, Boaz, Computational Complexity: A Modern Approach, Cambridge \nUniversity Press, 2009 \n \n++ Garey, Michael R. and Johnson, David S., Computers and Intractability: A Guide to the \nTheory of NP-Completeness, W.H. Freeman, 1979 \n \n\u200b\n \n\nINTELLIGENT SYSTEMS: MACHINE LEARNING & OPTIMISATION \n \nModule aims \nThis module introduces the field of Artificial Intelligence, key approaches within the field and \nphilosophical questions such as what it means for a machine to understand. Students will learn \nthe theory and practice of machine learning techniques covering linear regression, simple neural \nnetworks, linear algebra and continuous optimisation. Students will see motivating real world \nproblems, the ML techniques required to solve them, the underlying mathematics needed for the \ntechnique and their practical implementation. Practicals will be taught using Python, and the \ngroup project will introduce the students to a Python-based modern machine learning library \nsuch as TensorFlow or PyTorch. \n \n \nModule learning outcomes \nExplain the difference between strong, weak and general AI, understand the relationship \nbetween computation and AI, define the machine learning paradigm, and distinguish it from the \nwider field of AI \n \nCompute partial derivatives and understand the concept of the gradient as a generalisation of \nthe derivative \n \nExpress, manipulate and solve systems of linear equations using linear algebra, and apply \nlinear regression and logistic regression \n \nOptimise multivariate functions using gradient descent \n \nExplain the concept of overfitting and how regularisation can be used to prevent it \n \nConstruct a basic neural network using a modern machine learning library and learn its weights \nvia optimisation using the backpropagation algorithm \n \nDeconstruct ethical arguments relating to AI and its applications, and appreciate the ethical and \nprivacy implications of machine learning \n \nIndicative assessment \nTask\u200b\n% of module mark \nEssay/coursework\u200b\n30 \nOnline Exam -less than 24hrs (Centrally scheduled)\u200b70 \nSpecial assessment rules \nNone \n \nIndicative reassessment \nTask\u200b\n% of module mark \nEssay/coursework\u200b\n30 \n\nOnline Exam -less than 24hrs (Centrally scheduled)\u200b70 \nModule feedback \nFeedback is provided through work in practical sessions, and after the assessments as per \nnormal University guidelines. \n \nIndicative reading \nArtificial Intelligence: A Modern Approach by Russell and Norvig\n \n\nENGINEERING 1: SOFTWARE & SYSTEMS ENGINEERING \nRelated modules \nPre-requisite modules \nSoftware 2: Object Oriented Data Structures & Algorithms (COM00016C) \n  \nCo-requisite modules \nNone \n  \nProhibited combinations \nNone \nModule aims \nThis is the students' first opportunity to integrate their skills into a development project. Starting \nfrom a broad problem description and working in groups, students will design, develop and test \na complex system. The students will be introduced to the software engineering terminology, \nlifecycle and processes and will become familiar with principles, techniques and tools for, and \ndevelop hands-on experience of eliciting requirements; defining software architectures; \ndesigning and implementing software in an object-oriented way using established patterns; \nreviewing, testing and refactoring software systems; and setting up continuous integration and \ndelivery processes. Students will also develop an appreciation of how to identify, mitigate and \nmonitor risks, how to manage software projects, and how to reuse and extend 3rd-party \ncode/libraries. Overarching themes of the module will include traceability, cyber-security and \nethical considerations across the engineering lifecycle. \n \nModule learning outcomes \nE101 \n \nApply an understanding of software engineering terminology, lifecycles and process models, to \nhelp with undertaking a project. \n \nE102 \n \nElicit and document user and system requirements. \n \nE103 \n \nArchitect, design and implement software in an object-oriented way. \n \nE104 \n \nDemonstrate how the estimation of risk can be used to improve decision-making, and to make \nrealistic estimates for a project. \n \nE105 \n \n\nDefine unit- and system-level tests for software, and use continuous integration processes. \n \nE106 \n \nApply mechanisms for working in teams to successfully undertake a group project. \n \nE107 \n \nApply different models for software licensing and reuse of 3rd party software to the artefacts \ndeveloped in the module. \n \nE108 \n \nDemonstrate consideration of cyber-security and ethical considerations in the engineering \nlifecycle, through practice and documentation. \n \nE109 \n \nWrite documentation and design/deliver presentations for the different stages of the engineering \nlifecycle. \n \n \nIndicative assessment \nTask\u200b\n% of module mark \nGroupwork\u200b\n35 \nGroupwork\u200b\n65 \nSpecial assessment rules \nNone \n \nIndicative reassessment \nTask\u200b\n% of module mark \nEssay/coursework\u200b\n35 \nEssay/coursework\u200b\n65 \nModule feedback \nFeedback is provided through work in practical sessions, and after the final assessment as per \nnormal University guidelines \n \nIndicative reading \n**** Ian Sommerville, Software Engineering (latest edition), Addison-Wesley, 2010 \n \n*** Roger Pressman, Software Engineering (latest edition), McGraw-Hill, 2010\n \n\nSYSTEMS & DEVICES 3: ADVANCED COMPUTER SYSTEMS \n \nModule aims \nThis module continues the Systems and Devices stream by considering advanced computer \nsystems in terms of their structure and how it affects their programming. To this point, the \nsystems introduced have been basic uniprocessors with simple memory. The module starts by \nintroducing the concept of multiple processor architectures, pipelined and superscalar \nprocessors, systems-on-chip and advanced memory structures, including caches. \n \nThe rest of the module considers how these hardware features affect the way that such modern \nsystems are efficiently programmed. Issues of programming for pipelines, caches, etc. are \nconsidered. Then how processes running in parallel on separate processors can share \nunderlying resources safely. Finally, the module considers building blocks for parallel \nprogramming, e.g. threads and communication. It will also consider how such constructs map to \nprocessor instructions. The module will consider real-world examples throughout. \n \nModule learning outcomes \n \nS301 \n \nAppreciate how, and why, the hardware structure of multiprocessor architectures differs from the \narchitectures discussed in S&D1. Use this knowledge to develop software applications for such \narchitectures. \n \nS302 \n \nUse knowledge of computer system design concepts to evaluate the likely performance of a \nproposed computer architecture. \n \nS303 \n \nDemonstrate the ability to optimise software for advanced hardware features like caches, \nbranch predictors, and pipelines. \n \nS304\u200b Demonstrate through software development, how data structures are laid out in memory, \nhow they can be accessed from a program, and how processor instructions are generated to \nmanipulate them. \n \nIndicative assessment \nTask\u200b\n% of module mark \nOnline Exam -less than 24hrs (Centrally scheduled)\u200b100 \nSpecial assessment rules \nNone \n \n\nAdditional assessment information \nStudents will only need to retake failed assessment components. \n \nIndicative reassessment \nTask\u200b\n% of module mark \nOnline Exam -less than 24hrs (Centrally scheduled)\u200b100 \nModule feedback \nFeedback is provided through work in practical sessions, and after the final assessment as per \nnormal University guidelines \n \nIndicative reading \n**** H. Abelson, and G.J. Sussman, Structure and Interpretation of Computer Programs, MIT \nPress, 1996 \n \n*** R.W. Sebesta, Concepts of Programming Languages, Addison Wesley, 2009 \n \n*** A. Burns and G. Davis, Concurrent Programming, Addison-Wesley, 1993 \n \n** D. Watt and W. Findlay, Programming Language Design Concepts, Wiley, 2004 \n \n** T.W. Pratt and M.V. Zelkowitz, Programming Languages: Design and Implementation, \nPrentice Hall, 2001 \n \n** D. Lea, Concurrent Programming in Java, Addison Wesley, 1996 \n \n** A. Burns and A. Wellings, Concurrent and Real-Time Programming in Ada, CUP, 2007 \n \n** F. Casarini and S. Thompson, Erlang Programming, O'Reilly, 2009 \n \n** A. Burns and A. Wellings, Real-Time Systems and Languages 4/e, Addison Wesley\n \n\n",
        "Year 3 \nCAPSTONE PROJECT (PRBX): COMPUTER SCIENCE (UG) \nProfessional requirements \nPlease see the additional assessment information above. \n \nModule will run \nOccurrence\u200b\nTeaching period \nModule aims \nThe aims of this module are to provide a culmination of three years' teaching in a substantial \nCapstone Project Module (CPM).The project provides an introduction to independent study in \nan engineering context, supports synthesis and application of material from the degree course, \nand gives the student the opportunity to demonstrate an appreciation of engineering methods \nand techniques, through coverage of requirements, ethical considerations, specification, design, \nimplementation and evaluation. On completion of the project, the student will have gained the \npractical skills that can only be gleaned from the experience of undertaking independent \n(supervised) study. The student will also have the experience of having written a substantial \nacademic report. \n \nModule learning outcomes \n \nDemonstrate acquired specialisation in a particular part of the subject area, including enhanced \nor new technical skills that build on taught theory. \n \nDemonstrate acquired skills to undertake a computer systems (software and/or hardware) \nengineering project, including design, implementation and evaluation. \n \nDemonstrate a practical understanding of how established techniques of research and enquiry \nare used to create and interpret knowledge. \n \nRecognise alternatives, selecting and justifying the approach taken at each point in the report, \nidentifying parts of the project area that are feasible within the time (etc) constraints of the \nproject. \n \nAppreciate the latent issues of the subject area (for example, in software engineering they might \nmeet and tackle such as emergent requirements, design flaws, equipment/application \nproblems). \n \nPrepare a written report on the work done, according to the defined criteria. In particular, the \nstudent should be able to prepare a report with a good structure and clear presentation, and in \nwhich the referencing is of publishable academic standard. The report must demonstrate critical \nabilities and evaluation of work done and methods applied. \n \nArticulates an understanding of legal, ethical, social, professional and commercial issues \ninvolved in the project, detailing potential issues and mitigation strategies. \n\n \nSummarise the context, method, results, and implications of the project in an engaging form for \na non-expert audience. \n \nIndicative assessment \nTask\u200b\n% of module mark \nEssay/coursework\u200b\n100 \nSpecial assessment rules \nNon-compensatable \n \nAdditional assessment information \nPlease note, as per University of York assessment regulations (see the Rules for Progression \nand Award), it is only possible to resit UG Capstone Project Modules (CPMs) when this is \npermitted within the total module credit resit threshold for the year. Currently, this threshold is 60 \nmodule credits. \n \nDue to PSRB (accreditation requirements) there are also special assessment rules that apply to \nPRBX. First PRBX cannot be compensated. Second, only in the case of marginal fail of PRBX \n(marks that fall within 30-39), reassessment is permitted. Students are given the opportunity to \nmake amendments to enable them to reach a pass threshold, within a specified time frame. The \nmark for the resubmitted PRBX will be capped at the pass mark (40). There will only be one \nsuch reassessment. These rules are applicable to all students who take PRBX, regardless of \nprogramme. \n \nIndicative reassessment \nTask\u200b\n% of module mark \nEssay/coursework\u200b\n100 \nModule feedback \nFeedback on written report draft (where draft provided to supervisor in a timely manner). \nWritten feedback after written project report. \n \nIndicative reading \n*** Dawson, C. W Projects in Computing and Information Systems. Addison-Wesley 2005 \n \n*** Gowers, E. The complete plain words. Penguin 1987 \n \n*** Kopka, H and Daly, P.W. A guide to LATEX : document preparation for beginners and \nadvanced users, 3rd edn. Addison-Wesley 1999 \n \n*** Zobel, J. Writing for computer science, 2nd edn. Springer 2004\n \n\nAI PROBLEM SOLVING WITH SEARCH AND LOGIC \nElective Pre-Requisites \n These pre-requisites only apply to students taking this module as an elective. \n \nA Level Maths or Equivalent, plus good knowledge of AI fundamentals, predicate logic and \nprogramming. \n \nModule will run \nOccurrence\u200b\nTeaching period \nModule aims \nThis module will introduce key approaches in Artificial Intelligence for tasks such as: finding a \nsequence of actions to achieve a goal; playing adversarial games; and solving discrete \noptimization problems such as configuration and scheduling. Students will learn the theory and \npractice of AI search, logic, and constraint-based approaches. The module aims to equip \nstudents with a wide range of problem-solving tools, how to design effective heuristics for them, \nand enable comparison of methods to determine which are best suited to a given problem. \nSome of the tools covered are state-space search algorithms (i.e. A* Search, IDA*, and Greedy \nBest-First Search), game-tree search algorithms (i.e. Minimax and Monte-Carlo Tree Search), \nlocal search methods for solving discrete optimization problems, constraint programming, and \nthe satisfiability (SAT) problem in knowledge representation and reasoning. \n \nModule learning outcomes \nRepresent a given search problem in terms of states, actions, and a goal, and identify a suitable \nheuristic. \n \nRepresent a given scenario using propositional logic to enable logical inference (for example, \nusing a SAT solver). \n \nModel (represent) and solve discrete optimization problems using a modern constraint \nprogramming system. \n \nSelect and apply an appropriate AI state-space search algorithm for a given problem, identifying \nreasons for the choice of algorithm in comparison to others. \n \nSelect and apply an appropriate adversarial (game-tree) search method to solve a given game, \nincluding design of a suitable heuristic if required. \n \nDescribe the algorithms commonly used in SAT (propositional satisfiability) solvers, local search \nsolvers, and constraint solvers, and apply them to small examples. \n \nIndicative assessment \nTask\u200b\n% of module mark \nClosed/in-person Exam (Centrally scheduled)\u200b\n100 \nSpecial assessment rules \n\nNone \n \nIndicative reassessment \nTask\u200b\n% of module mark \nClosed/in-person Exam (Centrally scheduled)\u200b\n100 \nModule feedback \nFeedback is provided throughout the sessions, and after the assessment as per normal \nUniversity guidelines.\n \n\nAUTONOMOUS ROBOTIC SYSTEMS ENGINEERING \nModule aims \nThis module will introduce students to the theoretical concepts and practical skills required to \nengineer autonomous robotic systems. It will cover fundamental aspects of sensors/actuators \nand control systems, then build upon this foundation with high-level algorithms for autonomous \nlocalisation, mapping, navigation, and multi-robot coordination. This module will also explore \nsafety considerations and ethical implications of the design, implementation, and deployment of \nautonomous robotic systems. \n \nModule learning outcomes \nDescribe the degrees of autonomy that robotic systems can achieve \n \nDiscuss the safety considerations and ethical implications of the design, implementation, and \ndeployment of autonomous robotic systems \n \nExplain methodological principles for engineering autonomous robotic systems \n \nDemonstrate an understanding of modern robotics middleware and its application \n \nImplement an autonomous robotic solution for a predefined problem \n \nDiscuss the strengths and weaknesses of an implemented autonomous robotic solution \n \nIndicative assessment \nTask\u200b\n% of module mark \nEssay/coursework\u200b\n100 \nSpecial assessment rules \nNone \n \nIndicative reassessment \nTask\u200b\n% of module mark \nEssay/coursework\u200b\n100 \nModule feedback \nFeedback is provided throughout the sessions, and after the assessment as per normal \nUniversity guidelines. \n \nIndicative reading \nCorrell, Nikolaus, et al. Introduction to Autonomous Robots: Mechanisms, Sensors, Actuators, \nand Algorithms. MIT Press, 2022. \n \nHerath, Damith, and David St-Onge. Foundations of Robotics: A Multidisciplinary Approach with \nPython and ROS. Springer Nature Singapore, 2022.\n \n\nEVOLUTIONARY & ADAPTIVE COMPUTING \nModule summary \nThis module introduces a range of biologically-inspired approaches to computing. \nModule aims \nThis module introduces a range of biologically-inspired approaches to computing. It provides a \nfoundation of both theoretical and practical knowledge on the subject of evolutionary \ncomputation, an optimisation technique inspired by biological evolution. Students will have \nhands-on experience implementing a number of types of evolutionary algorithms using Python \nand the library DEAP: Distributed Evolutionary Algorithms in Python, to solve a range of different \ntypes of problems. The module also studies the use of Agents and Multi-agent Systems as a \nmodelling paradigm, with a focus on evolutionary adaptation and learning. \n \nModule learning outcomes \nDesign and implement evolutionary systems to address a given problem. \nUnderstand the biological underpinnings of evolutionary algorithms, and use them to optimise \nmathematical functions and agent behaviours. \nDefine a range of agent behaviours and represent them in a form that is well suited to natural \nselection \nModel processes in populations of agents using hand-written mathematical models \nCritically evaluate the performance and implementation of evolutionary and multi agent systems. \nIndicative assessment \nTask\u200b\n% of module mark \nEssay/coursework\u200b\n100 \nSpecial assessment rules \nNone \n \nIndicative reassessment \nTask\u200b\n% of module mark \nEssay/coursework\u200b\n100 \nModule feedback \nFeedback is provided through work in practical sessions, and after the final assessment as per \nnormal University guidelines. \n \nIndicative reading \n++ Banzhaf et al, Genetic Programming: An Introduction, Morgan Kaufmann , 1999 \n \n++ M. Mitchell, An Introduction to Genetic Algorithms, MIT Press, 1998\n \n\nCOMPUTER VISION & GRAPHICS \nModule summary \nThis module will introduce modern computer vision approaches, including discussion of the \nmain applications and challenges. \nModule aims \nThis module will introduce modern computer vision approaches, including discussion of the \nmain applications and challenges. It will cover issues of image formation, camera geometry, \nfeature detection, motion estimation and tracking, image classification and scene \nunderstanding, using a range of model-based approaches. \n \nModule learning outcomes \nDemonstrate a detailed understanding of the image formation process, its modelling in \ncomputer vision and its simulation in computer graphics \n \nDescribe and implement techniques for rendering images including modelling light/material \ninteraction \n \nUnderstand a range of methods for inferring 3D shape from images \n \nApply fundamental machine learning methods for image understanding \n \nIndicative assessment \nTask\u200b\n% of module mark \nClosed/in-person Exam (Centrally scheduled)\u200b\n100 \nSpecial assessment rules \nNone \n \nIndicative reassessment \nTask\u200b\n% of module mark \nClosed/in-person Exam (Centrally scheduled)\u200b\n100 \nModule feedback \nFeedback is provided through work in practical sessions, and after the final assessment as per \nnormal University guidelines. \n \n \nIndicative reading \n** Forsyth and Ponce Computer Vision a Modern Approach Prentice Hall \n \n** Anil K. Jain Fundamentals of Digital Image Processing Prentice Hall\n \n\nCRYPTOGRAPHY THEORY & PRACTICE \nModule summary \nThe module aims to provide a broad overview of modern cryptography. The module will cover \nthe fundamental security goals achieved through cryptographic algorithms and protocols, how \nthey are formalised, designs that enable achieving those goals, and how formal security \narguments can be made for the design achieving the security goals. \n \nElective Pre-Requisites \n These pre-requisites only apply to students taking this module as an elective. \n \nModule will run \nOccurrence\u200b\nTeaching period \nModule aims \nThe module aims to provide a broad overview of modern cryptography. The module will cover \nthe fundamental security goals achieved through cryptographic algorithms and protocols, how \nthey are formalised, designs that enable achieving those goals, and how formal security \narguments can be made for the design achieving the security goals. \n \nModule learning outcomes \nBy the end of the module the students will be able to: \n \ndescribe and apply the fundamental security properties provided by cryptographic algorithms \nand protocols; \n \ndescribe how these security properties are formalised; \n \ndescribe the implications of cryptographic security arguments; \n \nidentify security requirements for a given practical scenario and identify the appropriate \ncryptographic tools to achieve the security requirements; \n \nassess whether given cryptographic algorithms and protocols meet identified security \nrequirements; and \n \nanalyse the security and efficiency of cryptographic protocols from both theoretical and practical \npoints of view. \n \nIndicative assessment \nTask\u200b\n% of module mark \nClosed/in-person Exam (Centrally scheduled)\u200b\n100 \nSpecial assessment rules \nNone \n \nIndicative reassessment \n\nTask\u200b\n% of module mark \nClosed/in-person Exam (Centrally scheduled)\u200b\n100 \nModule feedback \nFeedback is provided through work in practical sessions, and after the final assessment as per \nnormal University guidelines. \n \nIndicative reading \n[1] J. Katz, Introduction to modern cryptography, Third edition :: Chapman & Hall/CRC, 2020 \n[2] J.-P. Aumasson, Serious cryptography : a practical introduction to modern encryption :: No \nStarch Press, 2018 \n[3] C. Paar, Understanding cryptography : a textbook for students and practitioners :: Springer, \n2009 \n[4] A. McAndrew, Introduction to cryptography with open-source software :: CRC Press, 2011 \n[5] W. Stallings, Cryptography and network security : principles and practice, Seventh edition :: \nPearson, 2017 \n[6] A. J. Menezes, Handbook of applied cryptography :: CRC Press, 1997 \n \n \n\nEMBEDDED SYSTEMS DESIGN & IMPLEMENTATION \nModule will run \nOccurrence\u200b\nTeaching period \nModule aims \nThe aim of this module is to understand the need for embedded computer systems, and the \nengineering process to design, implement and validate them. \n \nModule learning outcomes \nArticulate the need for embedded computer systems, and the requirements imposed on them by \ntheir application scenario. \nBe able to apply an engineering process to design, implement and validate embedded systems. \nDemonstrate the different levels of abstraction that are used throughout the design process, and \nbe able to decide the most appropriate abstractions at each step. \nExplore hardware/software implementation trade-offs, and a number of partitioning, mapping \nand evaluation techniques that can be used to analyse that trade-off for a particular application \nscenario. \nProgram computing platforms that have limited performance, energy, memory and storage \ncapacity. \nDesign and evaluate custom hardware architectures. \nUnderstand the process to improve and assure timing properties, including priority assignment, \nscheduling of real-time tasks and schedulability analysis. \nIndicative assessment \nTask\u200b\n% of module mark \nEssay/coursework\u200b\n100 \nSpecial assessment rules \nNone \n \nIndicative reassessment \nTask\u200b\n% of module mark \nEssay/coursework\u200b\n100 \nModule feedback \nFeedback is given to each student on each of the two assessments, highlighting the strengths \nand weaknesses of the proposed design. Additional feedback is also given during practical \nsessions, which are also based on design problems. \n \nIndicative reading \nP. Marwedel, Embedded System Design, Springer, 2011\n \n\nENGINEERING 2: AUTOMATED SOFTWARE ENGINEERING \nModule will run \nOccurrence\u200b\nTeaching period \nModule aims \nThis module builds on ENG1 and introduces advanced techniques for engineering large, \ndata-intensive software systems that consist of components written in different programming \nlanguages and operating on a variety of platforms. In order to manage the complexity of \nengineering such data-intensive systems, ENG2 will introduce the theory, principles and \npractices of model-driven engineering (MDE), focusing on technical topics. These include \nmodelling, metamodelling, model management, model transformation, model-driven traceability, \nworkflows, model validation, and model evolution. Non-technical issues including standards, \ndomain-specific MDE versus general-purpose MDE and MDE processes will also be \nconsidered. The discussion of concepts and technologies for engineering modern data-intensive \nsoftware systems will start from a general discussion about architectural concerns, then move \non to discuss their design, implementation as a collaboration of microservices, and using \ncontainers for reproducible and scalable deployment. ENG2 will discuss assurance practices \naround data-intensive software systems, such as testing and hardening. \n \nModule learning outcomes \nDemonstrate the principles and practices of modelling and metamodelling. \n \nImplement and orchestrate domain-specific models, metamodels and model management \noperations. \n \nDemonstrate the design principles for data-intensive software systems, including their \narchitecture characteristics like availability, scalability, reliability and security, and the \ncharacteristics of the organisation. \n \nStructure data-intensive systems as combinations of independently maintained microservices, \nand propose benefits and challenges in terms of complexity management, organisational \nscalability, and resource usage. \n \nDeploy the components of data-intensive systems in a scalable and reproducible manner, by \npackaging them as containers and orchestrating these containers to work with each other. \n \nDescribe assurance practices in the development and deployment of large data-intensive \nsystems, including testing and hardening. \n \nArticulate how Model-Driven Engineering integrates with wider system engineering processes \nand policies, including those of a data-intensive software system. \n \nIndicative assessment \nTask\u200b\n% of module mark \nEssay/coursework\u200b\n100 \n\nSpecial assessment rules \nNone \n \nIndicative reassessment \nTask\u200b\n% of module mark \nEssay/coursework\u200b\n100 \nModule feedback \nFeedback is provided throughout the sessions, and after the assessment as per normal \nUniversity guidelines. \n \nIndicative reading \nNewman, Sam. Building Microservices, O'Reilly Media, 2021. \nKleppmann, Martin. Designing Data-Intensive Applications: The Big Ideas Behind Reliable, \nScalable, and Maintainable Systems, O'Reilly Media, 2017. \nWasowski A. Domain-Specific Languages: Effective Modeling, Automation, and Reuse. 1st ed. \n2023. (Berger T, ed.). Springer International Publishing; Imprint Springer; 2023. \n \n\nETHICAL HACKING, ANALYSIS & INVESTIGATION \nModule aims \nThe module aims to provide an introductory range of theoretical and practical skills to undertake \nethical hacking, analysis, and investigation of modern computer systems and networks. \n \nThe module will explore ethical hacking topics such as vulnerability exploitation of cryptographic \nand network protocols, reverse engineering, and other penetration testing techniques, as well as \nmalware analysis and digital investigation of cyber incidents. \n \nThe module will familiarise students with a range of tools used by ethical hackers and security \npractitioners. \n \nModule learning outcomes \nBy the end of the module the students will be able to: \n \nExplain the fundamental concepts of cyber security in systems and networks \n \nUnderstand the security strengths & weaknesses in network mechanisms \n \nUnderstand the major threats and attacks in systems and networks under various scenarios, \narchitectures, and threat models \n \nAnalyse control solutions for network security \n \nAssess the relative merits of different solution approaches in various security-related contexts \n \nIndicative assessment \nTask\u200b\n% of module mark \nClosed/in-person Exam (Centrally scheduled)\u200b\n50 \nClosed/in-person Exam (Centrally scheduled)\u200b\n50 \nSpecial assessment rules \nNone \n \nIndicative reassessment \nTask\u200b\n% of module mark \nClosed/in-person Exam (Centrally scheduled)\u200b\n50 \nClosed/in-person Exam (Centrally scheduled)\u200b\n50 \nModule feedback \nFeedback is provided throughout the sessions, and after the assessment as per normal \nUniversity guidelines. \n \nIndicative reading \nJon Erickson. Hacking: The Art of Exploitation. No Starch (2008) \n \n\nSikorski, Michael ; Honig, Andrew. Practical Malware Analysis: A Hands-On Guide to Dissecting \nMalicious Software. No Starch Press, Incorporated (2012) \n \n \n\nHIGH-INTEGRITY SYSTEMS ENGINEERING \nModule summary \nThis module teaches students to adapt their software development practice to take account of \nthe general criticality and the specific risks of the software they are developing. \n \nRelated modules \nStudents on the joint Mathematics and Computer Science programmes who are interested in \nthis module should discuss the necessary prerequisite knowledge with the module leader. \n \nElective Pre-Requisites \n These pre-requisites only apply to students taking this module as an elective. \n \nGood knowledge of software engineering practice. \n \nModule will run \nOccurrence\u200b\nTeaching period \nModule aims \nThis module teaches students to adapt their software development practice to take account of \nthe general criticality and the specific risks of the software they are developing. Students will \ngain a broad understanding of the issues involved in designing and implementing critical \nsystems, be aware of the methods used to construct critical systems, and understand the \nlimitations of the various methods, analysis techniques and tools currently in use. \n \nModule learning outcomes \nUnderstand and articulate the issues involved in designing and implementing critical systems \n \nApply a range of techniques to the design, and validation of high integrity systems, such as \nfault-tree analysis and failure analysis. \n \nDiscuss issues of high integrity engineering, both technical and social. \n \nParticipate in significant discussion periods brainstorming scenarios and discussing previous \nwell-documented examples of system failures \n \nIntroduce existing software engineering concepts for the development of critical systems \n \nIndicative assessment \nTask\u200b\n% of module mark \nOnline Exam -less than 24hrs (Centrally scheduled)\u200b100 \nSpecial assessment rules \nNone \n \nIndicative reassessment \nTask\u200b\n% of module mark \n\nOnline Exam -less than 24hrs (Centrally scheduled)\u200b100 \nModule feedback \nFeedback is provided through work in practical sessions, through discussion sessions, and after \nthe final assessment as per normal University guidelines. \n \n \nIndicative reading \nN.G. Leveson, Safeware: System Safety and Computers, Addison-Wesley, 1995 \n \n \n\nHIGH-PERFORMANCE PARALLEL & DISTRIBUTED SYSTEMS \nModule summary \nThis module introduces and explores the use of high-performance computing, and related \ntechnologies. \nModule aims \nThis module introduces and explores the use of HPC and related technologies, covering on and \noff node parallelism, accelerators, and memory management. Students will understand the \nincreasingly important role that HPC plays in science and engineering. Throughout the module, \nstudents will practice by developing programs using a range of parallel programming paradigms \nlike OpenMP and MPI. \n \nModule learning outcomes \nDemonstrate the development of parallel programs on shared memory systems using popular \nprogramming paradigms. \nBe able to understand appropriate metrics to assess the performance of applications and \nsystems. \nDemonstrate the development of parallel programs on distributed memory systems using \nappropriate parallel programming libraries. \nBe able to explore the design space afforded by parallel and distributed systems, including \nissues of GPU programming, high-speed networking, and I/O. \nBe able to reason about the performance, portability, and productivity of different approaches to \ndeveloping HPC applications. \nIndicative assessment \nTask\u200b\n% of module mark \nEssay/coursework\u200b\n100 \nSpecial assessment rules \nNone \n \nIndicative reassessment \nTask\u200b\n% of module mark \nEssay/coursework\u200b\n100 \nModule feedback \nFeedback is provided through work in practical sessions, and after the final assessment as per \nnormal University guidelines. \n \n \nIndicative reading \nIntroduction to High Performance Scientific Computing, Eijkhout, Victor; van de Geijn, Robert; \nChow, Edmond \n \n \nHUMAN FACTORS: TECHNOLOGY IN CONTEXT \nAdditional information \nPre-Requisite Module: \n\n \nHuman-Computer Interaction (HCIN) - COM00018C \n \nPre-Requisite Knowledge: \n \nResearch Methods \n \nElective Pre-Requisites \n These pre-requisites only apply to students taking this module as an elective. \n \nNone. \nModule will run \nOccurrence\u200b\nTeaching period \nModule aims \nThis module will equip students with an understanding of how human characteristics and design \ninfluence interactions with technical systems in organisational and industry contexts. The \nmodule uses real-world case studies to introduce and critically evaluate the contribution of \nhumans in developing and maintaining safe, ethical and secure systems. The module aims to \ndevelop students' skills in: \n \nAnalysis techniques \n \nProposing and applying appropriate criteria to assess the rigour and suitability of industrial and \nacademic approaches to human reliability. \n \nModule learning outcomes \nRecognise the role of human characteristics/factors/mental models in relation to the safe, \nsecure and effective operation and maintenance of interactive systems across applied/industry \ncontexts \n \nCritically evaluate the quality of designs and propose solutions to security and safety issues \n \nDemonstrate the ability to locate and, where relevant, apply existing academic research to \ninform the consideration of issues arising within human-technology interactions \n \nIdentify, apply and critique appropriate techniques for human reliability and error analysis \n \nIdentify, apply and critique appropriate techniques for security analysis, including issues of \nauthentication, privacy and foreseeable misuse \n \nUse appropriate frameworks and design techniques to investigate ethical issues in relation to \nhuman-machine interaction \n \nIndicative assessment \n\nTask\u200b\n% of module mark \nEssay/coursework\u200b\n100 \nSpecial assessment rules \nNone \n \nIndicative reassessment \nTask\u200b\n% of module mark \nEssay/coursework\u200b\n100 \nModule feedback \nFeedback is provided throughout the sessions, and after the assessment as per normal \nUniversity guidelines.\n \n\nINTELLIGENT SYSTEMS: PROBABILISTIC & DEEP LEARNING \nAdditional information \nPre-Requisite knowledge - Understanding of the theory and practice of Machine Learning. For \nundergraduates this is covered in intelligent systems modules (for example, INT2 -  Intelligent \nSystems 2: Machine Learning & Optimisation COM00024I or IMLO - Intelligent Systems: \nMachine Learning & Optimisation COM00026I). \n \n  \n \nElective Pre-Requisites \n These pre-requisites only apply to students taking this module as an elective. \n \nA Level Maths or Equivalent plus good understanding of the theory and practice of machine \nlearning. \n \nModule will run \nOccurrence\u200b\nTeaching period \nModule aims \nThis module builds on the basic machine learning covered previously in the programme, and \ntakes students up to state of the art methods in modern deep learning. It introduces probabilistic \nmethods, where we can reason about uncertainty, and deep learning based methods, where \nneural networks with many layers prove to be the most powerful general model for learning. We \nwill see a range of methods and architectures for classification and regression problems, \nunsupervised generative models and the mathematics that underlies these techniques. We will \ncover both theory and practicalities: how are these ideas actually implemented in a modern \nmachine learning library like PyTorch? \n \nModule learning outcomes \nExplain the probabilistic basis of machine learning \n \nDemonstrate a working knowledge of manifold embedding and kernel methods \n \nApply a range of Bayesian methods for regression, classification and clustering \n \nBe familiar with the main deep learning architectures \n \nDemonstrate the optimisation process and different variants (i.e. gradient descent, stochastic \nalgorithms, ADAM) \n \nIndicative assessment \nTask\u200b\n% of module mark \nEssay/coursework\u200b\n100 \nSpecial assessment rules \nNone \n\n \nIndicative reassessment \nTask\u200b\n% of module mark \nEssay/coursework\u200b\n100 \nModule feedback \nFeedback is provided through work in practical sessions, and after the final assessment as per \nnormal University guidelines \n \nIndicative reading \nSolomon, Justin. Numerical Algorithms. AK Peters/CRC Press, 2015\n \n\nLEGAL PRACTICE, TECHNOLOGY & COMPUTER SCIENCE \nModule summary \nLaw and computer science students will work collaboratively in teams to develop a \ntechnology-based solution to a real-life legal practice process problem. From initial analysis of \nthe problem, teams will work to develop a solution to the problem, assessing both legal and \nprocedural issues, system and user design requirements, functionality, costs, benefits and risks. \nStudents from each discipline will contribute know-how from their discipline to the project, whilst \ngaining new understanding and skills. The module will be supported by a leading international \nlaw firm with a technology hub in the region, providing students with access to external \nexpertise in the field, in addition to disciplinary tutor support. \n \nModule aims \nThe aim of the module is to provide law and computer science students with an opportunity to \ndevelop an applied understanding of how technology and computer science are being applied in \ndeveloping new approaches to the provision of legal services and how these can offer greater \naccessibility to justice, as well as efficiency, quality and costs gains. Students will develop \nknowledge both within their own and the colloborative discipline, as well as a range of analytical, \nproblem-solving, planning, communication and interpersonal skills. The module also aims to \nprovide students with opportunities to interact with experts in legal practice technology and thus \ngain contemporary professional perspectives on the areas covered in the module. \n \nModule learning outcomes \nOn completion of this module, students should be able to: \n \nAnalyse a legal process which may be improved for users by application of computer science \nApply design thinking to identify procedural, technical, legal and user issues \nCommunicate orally and in writing relevant principles from their primary discipline to colleagues \nfrom another discipline \nCollaborate and synthesise disciplinary principles to develop potential solutions \nEvaluate potential solutions against user requirements \nFormulate a costed implementation plan for an agreed solution \nPresent a persuasive written and oral case for the solution \nExplain the potential applications of computer science and technology in the development of \nlegal services \nReflect on learning gains and challenges from the module, including cross-disciplinary \ncollaboration \nModule content \n \nLaw and computer science students will work collaboratively to develop a solution to a legal \nprocess problem, based on a real-life access to justice scenario. This will be developed in \ncollaboration with a law firm. \n \n \n\nThe module will use problem-based learning (PBL) techniques, and be predominantly \ngroup-assessed. Students will receive a problem and, working in teams comprising equal \nnumbers from each discipline will apply PBL techniques to identify: \n \nsubstantive and procedural issues \nclient and internal commercial issues \nrisks \nprocess requirements \nUsing these as the basis to develop a solution, there will be an element of cross-discipline \nteaching: law students will have to be able to explain law, procedure and process requirements \nto computer science students and the latter will have to explain tech functionality and \ncapabilities, in each case in language understandable to those from the other discipline. \n \n \nFollowing initial detailed analysis of the problem, teams will work to develop a solution to the \nproblem, with activity moving from analysis to development. They will plan a programme of work \noutside class activity sessions, with the latter acting as formal workshops/surgeries, during \nwhich teams can obtain feedback from facilitators from both disciplines. There will also be an \nopportunity to obtain feedback from a legal technology expert from a law firm, as part of a \nplenary \"masterclass\". \n \n \nThe assessment will require submission of the solution - e.g., system requirements; \nfunctionality; process map; costs; benefits; risk; time-line to implementation - in the form of a \nbusiness proposal, together with an oral presentation by each group. Individual students will \nalso submit a personal reflection on learning gained against the module outcomes. \n \nIndicative assessment \nTask\u200b\n% of module mark \nEssay/coursework\u200b\n20 \nGroupwork\u200b\n50 \nOral presentation/seminar/exam\u200b\n30 \nSpecial assessment rules \nNone \n \nAdditional assessment information \nFormative feedback will be provided on a rolling basis by facilitators as teams progress through \nthe analysis and development stages of the module, especially during workshops. \n \nIndicative reassessment \nTask\u200b\n% of module mark \nEssay/coursework\u200b\n20 \nGroupwork\u200b\n50 \nOral presentation/seminar/exam\u200b\n30 \n\nModule feedback \nStudents will receive summative feedback as follows: \n \noral feedback on group presentation \nwritten feedback on group submission \nwritten feedback on individual reflective submission \nIndicative reading \nThe Future of the Professions: Susskind & Susskind \u2013 OUP 2017 \n \n \nTomorrow\u2019s Lawyers \u2013 Susskind \u2013 OUP 2017 \n \n \nThe End of Lawyers? Susskind \u2013 OUP 2008 \n \n \n\nNETWORK SECURITY \nModule summary \nNetwork Security (NETS): This module covers the basic concepts of cyber security, how these \nare modelled, threat models, and the mechanisms to enforce security policies. \n \nElective Pre-Requisites \n These pre-requisites only apply to students taking this module as an elective. \n \nA Level Maths or Equivalent. \n \nModule will run \nOccurrence\u200b\nTeaching period \nModule aims \nThis module aims to provide a broad knowledge of network and system security, addressing \nthreats over a range of network layers and detailing corresponding defensive countermeasures \nand protocols. The module will cover the basic concepts of cyber security (confidentiality, \nintegrity and availability), how these are modelled, threat models (adversary capabilities and \ngoals), and basic control mechanisms to enforce security policies (e.g. access control). Students \nwill learn to understand network security, threats, and the mechanisms that have been \ndeveloped to counter them. It explores a range of different networked systems, the main \nnetwork attacks, and their defence mechanisms. \n \nModule learning outcomes \nBy the end of the module the students will be able to: \n \nDescribe the fundamental concepts of cyber security in systems and networks \nIdentify security strengths & weaknesses in network mechanisms \nIdentify major threats and attacks in systems and networks under various scenarios, \narchitectures, and threat models \nPropose control solutions for network security \nAssess the relative merits of different solution approaches in various security-related contexts \nIndicative assessment \nTask\u200b\n% of module mark \nEssay/coursework\u200b\n100 \nSpecial assessment rules \nNone \n \nIndicative reassessment \nTask\u200b\n% of module mark \nEssay/coursework\u200b\n100 \nModule feedback \nFeedback is provided through work in practical sessions, discussion in seminars, and after each \nassessment as per normal University guidelines. \n \n\nIndicative reading \nSherri Davidoff, Jonathan Ham, Network Forensics: Tracking Hackers Through Cyberspace, \nPrentice Hall, 2012 \n \nKevin R Fall, W Richard Stevens, TCP/IP Illustrated, Volume 1: The protocols, Addison Wesley, \n2012 \n \nAndrew Tannenbaum, Computer Networks, Prentice Hall, 2002\n \n\nPLAYER EXPERIENCES IN DIGITAL GAMES \nModule aims \nThis module will provide students with a comprehensive understanding of player experience and \nthe different ways in which games can impact players. In addition to learning about what player \nexperience is and how to evaluate it, the module will also cover the effects of games on players \n(e.g. in relation to wellbeing) and the use of games for applied purposes (e.g. behaviour \nchange). \n \nModule learning outcomes \nDemonstrate an understanding of the breadth of player experience \n \nSelect appropriate methods for evaluating different kinds of player experience \n \nConduct playtesting sessions and report on the findings \n \nCompare and contrast the different ways in which digital games can affect players \n \nPlan appropriate evaluations for applied games \n \nIndicative assessment \nTask\u200b\n% of module mark \nEssay/coursework\u200b\n60 \nGroupwork\u200b\n40 \nSpecial assessment rules \nNone \n \nIndicative reassessment \nTask\u200b\n% of module mark \nEssay/coursework\u200b\n60 \nGroupwork\u200b\n40 \nModule feedback \nFeedback is provided throughout the sessions, and after the assessment as per normal \nUniversity guidelines. \n \nIndicative reading \nDrachen, A., Mirza-Babaei, P., & Nacke, L. E. (Eds.). (2018). Games user research. Oxford \nUniversity Press. \n \nStahlke, S., & Mirza-Babaei, P. (2022). The Game Designer's Playbook: An Introduction to \nGame Interaction Design. Oxford University Press.\n \n\nQUALITATIVE APPROACHES TO INVESTIGATING UX \nModule aims \nThis module will provide students the methodological approaches to interrogating concepts and \nusers\u2019 subjective experiences with interactive systems. The module provides theoretical and \npractical grounding in methods used for collection and analysis of qualitative data that are used \nin industry and academic research. Students will learn how different qualitative methods can be \nused for a range of purposes, from informing to design to understanding technology use in \ncontext. \n \nModule learning outcomes \nIdentify and justify use of appropriate qualitative methodology \n \nRecognise and acknowledge the underpinning epistemological positions of qualitative methods \n \nRecognise the role of the researcher in the research process \n \nCritique and contrast the methods of studies against the standards of rigour and validity of the \nchosen methodology \n \nReport and discuss the results of an analysis appropriate to the method used \n \nDescribe the important aspects of research governance including ethical conduct and data \ngovernance \n \nIndicative assessment \nTask\u200b\n% of module mark \nEssay/coursework\u200b\n100 \nSpecial assessment rules \nNone \n \nIndicative reassessment \nTask\u200b\n% of module mark \nEssay/coursework\u200b\n100 \nModule feedback \nFeedback is provided throughout the sessions, and after the assessment as per normal \nUniversity guidelines.\n \n\nQUANTUM COMPUTATION \nModule summary \nThe aim of this module is to introduce the theory of quantum computation. In it we will learn \nabout the pioneering quantum algorithms that promise a qualitative leap in computation power \nover conventional computers. \n \nElective Pre-Requisites \n These pre-requisites only apply to students taking this module as an elective. \n \nA Level Maths or Equivalent; good understanding of programming. \n \nModule will run \nOccurrence\u200b\nTeaching period \nModule aims \nIntroducing both the promise and limitations of quantum computation. Gate operations, evolving \nquantum states, calculating the result of measurements on quantum states, designing and \nanalyzing quantum computational circuits, key algorithms (e.g., Shor's, Grover's and the \nDeutsch-Jozsa algorithms). \n \n \nModule learning outcomes \nArticulate both the promise and limitations of quantum computation. \n \nApply some of the many concepts and techniques in quantum computation (e.g., applying gate \noperations and evolving quantum states, calculating the result of measurements on quantum \nstates, designing and analyzing quantum computational circuits); \n \nExplain some of the key algorithms (e.g., Shor's, Grover's and the Deutsch-Jozsa algorithms) \nand their implications, and are able to simulate these algorithms on quantum states. \n \nIndicative assessment \nTask\u200b\n% of module mark \nOnline Exam -less than 24hrs (Centrally scheduled)\u200b100 \nSpecial assessment rules \nNone \n \nIndicative reassessment \nTask\u200b\n% of module mark \nOnline Exam -less than 24hrs (Centrally scheduled)\u200b100 \nModule feedback \nFeedback is provided through work in practical sessions, and after the final assessment as per \nnormal University guidelines. \n \nIndicative reading \n\n*** G. Benenti, et al., Principles of Quantum Computation and Information, vol I, World Scientific, \n2004 \n \n*** G. Benenti, et al., Principles of Quantum Computation and Information, vol II, World \nScientific, 2007 \n \n*** P. Kaye, et al., An Introduction to Quantum Computing, Oxford University Press, 2007\n\nRESEARCH METHODS IN COMPUTER SCIENCE \nModule summary \nA core module that equips postgraduate taught students with core theorectical and practical \nresearch skills. \nModule will run \nOccurrence\u200b\nTeaching period \nModule aims \nResearch is about both generating new knowledge and evaluating confidence in knowledge. \nThere are three skills associated with the conduct of good research: \n \nThe ability to accurately identify from existing literature and systems a meaningful or important \ngap in knowledge, and therefore what constitutes new knowledge in the domain of Computer \nScience \n \nThe ability to competently intervene in the world (e.g. through developing systems, \nimplementing data collection procedures, conducting experiments\u2026) in order to generate \nknowledge that causes positive change. \n \nThe ability to evaluate the quality of evidence stemming from an intervention using sound \nanalysis, and communicate that analysis to the scientific community. \n \nAs the ways of intervening in the world depend strongly on your disciplinary area, the aim of this \nmodule is to provide an introduction to the first and third pillars, namely the conduct of literature \nreviews to identify a research gap; and the methods for analysing research data, interpreting \nsaid analysis, and accurately presenting research outcomes. \n \nThe module will also cover general principles related to the second pillar that apply across \ndisciplinary areas in the development of novel methods and conduct of experiments. \n \nModule learning outcomes \nBy the end of this module, students will be able to\u2026 \n \nIdentify a gap in the evidence base within the structure of a formal academic literature review. \n \nDiscuss the validity and reliability of methods used in extant or novel research. \n \nDescribe and apply principles of responsible research and innovation to a research project. \n \nAnalyse a variety of quantitative research data using an array of appropriate inferential \nstatistical tests. \n \nPresent the outcomes of a research project. \n \nIndicative assessment \n\nTask\u200b\n% of module mark \nEssay/coursework\u200b\n50 \nGroupwork\u200b\n50 \nSpecial assessment rules \nNone \n \nIndicative reassessment \nTask\u200b\n% of module mark \nEssay/coursework\u200b\n50 \nEssay/coursework\u200b\n50 \nModule feedback \nFeedback is provided throughout the sessions, and after the assessment as per normal \nUniversity guidelines. \n \nIndicative reading \nHowell DC. Fundamental Statistics for the Behavioral Sciences . 9th edition, student edition. \nCengage Learning; 2017. \nGoldbort R. Writing for Science . Yale University Press; 2006.\n \n\n \n"
    ],
    "text_C:\\Users\\miafo\\Downloads\\curriculum-skills\\tests\\sample_pdfs\\University of Cambridge.pdf": [
        "Course Outlines \n \n1st Semester \n \nDATABASES \n \nAims \nThis course introduces basic concepts for database systems as seen from the perspective of \napplication designers. That is, the focus is on the abstractions supported by database \nmanagement systems and not on how those abstractions are implemented. \n \nThe database world is currently undergoing swift and dramatic transformations largely driven by \nInternet-oriented applications and services. Today many more options are available to database \napplication developers than in the past and so it is becoming increasingly difficult to sort fact \nfrom fiction. The course attempts to cut through the fog with a practical approach that \nemphasises engineering tradeoffs that underpin these recent developments and also guide our \nselection of \u201cthe right tool for the job.\u201d \n \nThis course covers three approaches. First, the traditional mainstay of the database industry -- \nthe relational approach -- is described with emphasis on eliminating logical redundancy in data. \nThen two representatives of recent trends are presented -- graph-oriented and \ndocument-oriented databases. The lectures are supported with two Help and Tick sessions, \nwhere students gain hands-on experience and guidance with the Assessed Exercises (ticks). \n \nLectures \nL1 Introduction. What is a database system? What is a data model? Typical DBMS \nconfigurations and variations (fast queries or reliable update). In-core, in secondary store or \ndistributed. \nL2 Conceptual modelling. Entities, relations, E/R diagrams and implementation-independent \nmodelling, weak entities, cardinality. \nL3+4 The relational database model. Implementing E/R models with relational tables. Relational \nalgebra and SQL. Basic query primitives. Update anomalies caused by redundancy. Minimising \nredundancy with normalised schemas. \nL5 Transactions. On-Line Transaction Processing. On-line Analytical Processing. Reliability, \nthroughput, normal forms, ACID, BASE, eventual consistency. \nL6 Documents and semi-structured data. The NoSQL, schema-free movement. XML/JSON. \nKey/value stores. Embracing data redundancy: representing data for fast, application-specific \naccess. Path queries (if time permits). \nL7 Further SQL. Multisets, NULL values, aggregates, transitive closure, expressibility (nested \nqueries and recursive SQL). \nL8 Graph databases. Optimised for processing enormous numbers of nodes and edges. \nImplementing E/R models in a graph-oriented database. Comparison of the presented models. \nObjectives \nAt the end of the course students should \n",
        " \nbe able to design entity-relationship diagrams to represent simple database application \nscenarios \nknow how to convert entity-relationship diagrams to relational- and graph-oriented \nimplementations \nunderstand the fundamental tradeoff between the ease of updating data and the response time \nof complex queries \nunderstand that no single data architecture can be used to meet all data management \nrequirements \nbe familiar with recent trends in the database area. \nRecommended reading \nLemahieu, W., Broucke, S. van den and Baesens, B. (2018) Principles of database \nmanagement. Cambridge University Press.\n \n",
        "DIGITAL ELECTRONICS \n \nAims \nThe aims of this course are to present the principles of combinational and sequential digital logic \ndesign and optimisation at a gate level. The use of n and p channel MOSFETs for building logic \ngates is also introduced. \n \nTopics \nIntroduction. Semiconductors to computers. Logic variables. Examples of simple logic. Logic \ngates. Boolean algebra. De Morgan\u2019s theorem. \nLogic minimisation. Truth tables and normal forms. Karnaugh maps. Quine-McCluskey method. \nBinary adders. Half adder, full adder, ripple carry adder, fast carry generation. \nCombinational logic design: further considerations. Multilevel logic. Gate propagation delay. An \nintroduction to timing diagrams. Hazards and hazard elimination. Other ways to implement \ncombinational logic. \nIntroduction to practical classes. Prototyping box. Breadboard and Dual in line (DIL) packages. \nWiring. \nSequential logic. Memory elements. RS latch. Transparent D latch. Master-slave D flip-flop. T \nand JK flip-flops. Setup and hold times. \nSequential logic. Counters: Ripple and synchronous. Shift registers. System timing - setup time \nconstraint, clock skew, metastability. \nSynchronous State Machines. Moore and Mealy finite state machines (FSMs). Reset and self \nstarting. State transition diagrams. Elimination of redundant states - row matching and state \nequivalence/implication table. \nFurther state machines. State assignment: sequential, sliding, shift register, one hot. \nImplementation of FSMs. \nIntroduction to microprocessors. Microarchitecture, fetch, register access, memory access, \nbranching, execution time. \nElectronics, Devices and Circuits. Current and voltage, conductors/insulators/semiconductors, \nresistance, basic circuit theory, the potential divider. Solving non-linear circuits. P-N junction \n(forward and reverse bias), N and p channel MOSFETs (operation and characteristics) and \nn-MOSFET logic, e.g., n-MOSFET inverter. Power consumption and switching time problems \nproblems in n-MOSFET logic. CMOS logic (NOT, NAND and NOR gates), logic families, noise \nmargin. \nObjectives \nAt the end of the course students should \n \nunderstand the relationships between combination logic and boolean algebra, and between \nsequential logic and finite state machines; \nbe able to design and minimise combinational logic; \nappreciate tradeoffs in complexity and speed of combinational designs; \nunderstand how state can be stored in a digital logic circuit; \nknow how to design a simple finite state machine from a specification and be able to implement \nthis in gates and edge triggered flip-flops; \n",
        "understand how to use MOSFETs to build digital logic circuits. \nRecommended reading \n* Harris, D.M. and Harris, S.L. (2013). Digital design and computer architecture. Morgan \nKaufmann (2nd ed.). The first edition is still relevant. \nKatz, R.H. (2004). Contemporary logic design. Benjamin/Cummings. The 1994 edition is more \nthan sufficient. \nHayes, J.P. (1993). Introduction to digital logic design. Addison-Wesley. \n \nBooks for reference: \n \nHorowitz, P. and Hill, W. (1989). The art of electronics. Cambridge University Press (2nd ed.) \n(more analog). \nWeste, N.H.E. and Harris, D. (2005). CMOS VLSI Design - a circuits and systems perspective. \nAddison-Wesley (3rd ed.). \nMead, C. and Conway, L. (1980). Introduction to VLSI systems. Addison-Wesley. \nCrowe, J. and Hayes-Gill, B. (1998). Introduction to digital electronics. Butterworth-Heinemann. \nGibson, J.R. (1992). Electronic logic circuits. Butterworth-Heinemann.\n \n",
        "DISCRETE MATHEMATICS \n \nAims \nThe course aims to introduce the mathematics of discrete structures, showing it as an essential \ntool for computer science that can be clever and beautiful. \n \nLectures \nProof [5 lectures]. \nProofs in practice and mathematical jargon. Mathematical statements: implication, bi-implication, \nuniversal quantification, conjunction, existential quantification, disjunction, negation. Logical \ndeduction: proof strategies and patterns, scratch work, logical equivalences. Proof by \ncontradiction. Divisibility and congruences. Fermat\u2019s Little Theorem. \n \nNumbers [5 lectures]. \nNumber systems: natural numbers, integers, rationals, modular integers. The Division Theorem \nand Algorithm. Modular arithmetic. Sets: membership and comprehension. The greatest \ncommon divisor, and Euclid\u2019s Algorithm and Theorem. The Extended Euclid\u2019s Algorithm and \nmultiplicative inverses in modular arithmetic. The Diffie-Hellman cryptographic method. \nMathematical induction: Binomial Theorem, Pascal\u2019s Triangle, Fundamental Theorem of \nArithmetic, Euclid\u2019s infinity of primes. \n \nSets [9 lectures]. \nExtensionality Axiom: subsets and supersets. Separation Principle: Russell\u2019s Paradox, the \nempty set. Powerset Axiom: the powerset Boolean algebra, Venn and Hasse diagrams. Pairing \nAxiom: singletons, ordered pairs, products. Union axiom: big unions, big intersections, disjoint \nunions. Relations: composition, matrices, directed graphs, preorders and partial orders. Partial \nand (total) functions. Bijections: sections and retractions. Equivalence relations and set \npartitions. Calculus of bijections: characteristic (or indicator) functions. Finite cardinality and \ncounting. Infinity axiom. Surjections. Enumerable and countable sets. Axiom of choice. \nInjections. Images: direct and inverse images. Replacement Axiom: set-indexed constructions. \nSet cardinality: Cantor-Schoeder-Bernstein Theorem, unbounded cardinality, diagonalisation, \nfixed-points. Foundation Axiom. \n \nFormal languages and automata [5 lectures]. \nIntroduction to inductive definitions using rules and proof by rule induction. Abstract syntax \ntrees. Regular expressions and their algebra. Finite automata and regular languages: Kleene\u2019s \ntheorem and the Pumping Lemma. \n \nObjectives \nOn completing the course, students should be able to \n \nprove and disprove mathematical statements using a variety of techniques; \napply the mathematical principle of induction; \nknow the basics of modular arithmetic and appreciate its role in cryptography; \n",
        "understand and use the language of set theory in applications to computer science; \ndefine sets inductively using rules and prove properties about them; \nconvert between regular expressions and finite automata; \nuse the Pumping Lemma to prove that a language is not regular. \nRecommended reading \nBiggs, N.L. (2002). Discrete mathematics. Oxford University Press (Second Edition). \nDavenport, H. (2008). The higher arithmetic: an introduction to the theory of numbers. \nCambridge University Press. \nHammack, R. (2013). Book of proof. Privately published (Second edition). Available at: \nhttp://www.people.vcu.edu/ rhammack/BookOfProof/index.html \nHouston, K. (2009). How to think like a mathematician: a companion to undergraduate \nmathematics. Cambridge University Press. \nKozen, D.C. (1997). Automata and computability. Springer. \nLehman, E.; Leighton, F.T.; Meyer, A.R. (2014). Mathematics for computer science. Available \non-line. \nVelleman, D.J. (2006). How to prove it: a structured approach. Cambridge University Press \n(Second Edition).\n \n",
        "FOUNDATIONS OF COMPUTER SCIENCE \nAims \nThe main aim of this course is to present the basic principles of programming. As the \nintroductory course of the Computer Science Tripos, it caters for students from all backgrounds. \nTo those who have had no programming experience, it will be comprehensible; to those \nexperienced in languages such as C, it will attempt to correct any bad habits that they have \nlearnt. \n \nA further aim is to introduce the principles of data structures and algorithms. The course will \nemphasise the algorithmic side of programming, focusing on problem-solving rather than on \nhardware-level bits and bytes. Accordingly it will present basic algorithms for sorting, searching, \netc., and discuss their efficiency using O-notation. Worked examples (such as polynomial \narithmetic) will demonstrate how algorithmic ideas can be used to build efficient applications. \n \nThe course will use a functional language (OCaml). OCaml is particularly appropriate for \ninexperienced programmers, since a faulty program cannot crash and OCaml\u2019s unobtrusive type \nsystem captures many program faults before execution. The course will present the elements of \nfunctional programming, such as curried and higher-order functions. But it will also introduce \ntraditional (procedural) programming, such as assignments, arrays and references. \n \nLectures \nIntroduction to Programming. The role of abstraction and representation. Introduction to integer \nand floating-point arithmetic. Declaring functions. Decisions and booleans. Example: integer \nexponentiation. \nRecursion and Efficiency. Examples: Exponentiation and summing integers. Iteration versus \nrecursion. Examples of growth rates. Dominance and O-Notation. The costs of some \nrepresentative functions. Cost estimation. \nLists. Basic list operations. Append. Na\u00efve versus efficient functions for length and reverse. \nStrings. \nMore on lists. The utilities take and drop. Pattern-matching: zip, unzip. A word on polymorphism. \nThe \u201cmaking change\u201d example. \nSorting. A random number generator. Insertion sort, mergesort, quicksort. Their efficiency. \nDatatypes and trees. Pattern-matching and case expressions. Exceptions. Binary tree traversal \n(conversion to lists): preorder, inorder, postorder. \nDictionaries and functional arrays. Functional arrays. Dictionaries: association lists (slow) versus \nbinary search trees. Problems with unbalanced trees. \nFunctions as values. Nameless functions. Currying. The \u201capply to all\u201d functional, map. \nExamples: The predicate functionals filter and exists. \nSequences, or lazy lists. Non-strict functions such as IF. Call-by-need versus call-by-name. Lazy \nlists. Their implementation in OCaml. Applications, for example Newton-Raphson square roots. \nQueues and search strategies. Depth-first search and its limitations. Breadth-first search (BFS). \nImplementing BFS using lists. An efficient representation of queues. Importance of efficient data \nrepresentation. \n",
        "Elements of procedural programming. Address versus contents. Assignment versus binding. \nOwn variables. Arrays, mutable or not. Introduction to linked lists. \nObjectives \nAt the end of the course, students should \n \nbe able to write simple OCaml programs; \nunderstand the fundamentals of using a data structure to represent some mathematical \nabstraction; \nbe able to estimate the efficiency of simple algorithms, using the notions of average-case, \nworse-case and amortised costs; \nknow the comparative advantages of insertion sort, quick sort and merge sort; \nunderstand binary search and binary search trees; \nknow how to use currying and higher-order functions; \nunderstand how OCaml combines imperative and functional programming in a single language. \nRecommended reading \nJohn Whitington. OCaml from the Very Beginning. (http://ocaml-book.com). \n \nOkasaki, C. (1998). Purely functional data structures. Cambridge University Press.\n \n",
        "HARDWARE PRACTICAL CLASSES \nThe Hardware Practical Classes accompany the Digital Electronics series of lectures. The aim \nof the Practical Classes is to enable students to get hands-on experience of designing, building, \nand testing and debugging of digital electronic circuits. The labs will take place in the Intel Lab. \nlocated in the William Gates Building (WGB). \n \nThe Digital Electronics lecture series occupies 12 lectures in the first 4 weeks of the Michaelmas \nTerm, while the Practical Classes occupy the latter 6 weeks of the Michaelmas Term and the \nfirst 6 weeks of the Lent Term. If required, extra sessions will be available for any students \nneeding to 'catch-up' on missed sessions or to complete any remaining practical work. \n \nThe Practical Classes take the form of 4 workshops, specifically: \n \nWorkshop 1 \u2013 Electronic Die; \nWorkshop 2 \u2013 Shaft Position Encoder; \nWorkshop 3 \u2013 Debouncing a Switch; \nWorkshop 4 \u2013 Framestore for an LED Array. \nIn general, the workshops require some preparatory work to be done prior to the practical \nsession. These tasks are highlighted at the beginning of each Worksheet. Typically this involves \npreparing a design that you will then build, test and modify during the practical class. Note that \ninsufficient preparation prior to the practical classes may compromise effective use of your time \nin the Intel lab. \n \nIn the Practical Classes you will usually work on your own, and you are expected to complete \none Workshop in each of your scheduled sessions. Demonstrators are available during the \nsessions to assist you with any queries or problems you may have. \n \nImportant: Remember to get your work ticked.\n \n",
        "INTRODUCTION TO GRAPHICS \nAims \nTo introduce the necessary background, the basic algorithms, and the applications of computer \ngraphics and graphics hardware. \n \nLectures \nBackground. What is an image? Resolution and quantisation. Storage of images in memory. [1 \nlecture] \nRendering. Perspective. Reflection of light from surfaces and shading. Geometric models. Ray \ntracing. [2 lectures] \nGraphics pipeline. Polygonal mesh models. Transformations using matrices in 2D and 3D. \nHomogeneous coordinates. Projection: orthographic and perspective. [1 lecture] \nGraphics hardware and modern OpenGL. GPU rendering. GPU frameworks and APIs. Vertex \nprocessing. Rasterisation. Fragment processing. Working with meshes and textures. Z-buffer. \nDouble-buffering and frame synchronization. [2 lectures] \n  \nHuman vision, colour and tone mapping. Perception of colour. Tone mapping. Colour spaces. [2 \nlectures] \nObjectives \nBy the end of the course students should be able to: \n \nunderstand and apply in practice basic concepts of ray-tracing: ray-object intersection, \nreflections, refraction, shadow rays, distributed ray-tracing, direct and indirect illumination; \ndescribe and explain the following algorithms: z-buffer, texture mapping, double buffering, \nmip-map, and normal-mapping; \nuse matrices and homogeneous coordinates to represent and perform 2D and 3D \ntransformations; understand and use 3D to 2D projection, the viewing volume, and 3D clipping; \nimplement OpenGL code for rendering of polygonal objects, control camera and lighting, work \nwith vertex and fragment shaders; \ndescribe a number of colour spaces and their relative merits. \nexplain the need for tone mapping and colour processing in rendering pipeline. \nRecommended reading \n* Shirley, P. and Marschner, S. (2009). Fundamentals of Computer Graphics. CRC Press (3rd \ned.). \n \nFoley, J.D., van Dam, A., Feiner, S.K. and Hughes, J.F. (1990). Computer graphics: principles \nand practice. Addison-Wesley (2nd ed.). \n \nKessenich, J.M., Sellers, G. and Shreiner, D (2016). OpenGL Programming Guide: The Official \nGuide to Learning OpenGL, Version 4.5 with SPIR-V, [seventh edition and later]\n \n",
        "OBJECT-ORIENTED PROGRAMMING \n \nAims \nThe goal of this course is to provide students with an understanding of Object-Oriented \nProgramming. Concepts are demonstrated in multiple languages, but the primary language is \nJava. \n \nLecture syllabus \nTypes, Objects and Classes Moving from functional to imperative. Functions, methods. Control \nflow. values, variables and types. Primitive Types. Classes as custom types. Objects vs \nClasses. Class definition, constructors. Static data and methods. \nDesigning Classes Identifying classes. UML class diagrams. Modularity. Encapsulation/data \nhiding. Immutability. Access modifiers. Parameterised types (Generics). \nPointers, References and Memory Pointers and references. Reference types in Java. The call \nstack. The heap. Iteration and recursion. Pass-by-value and pass-by-reference. \nInheritance Inheritance. Casting. Shadowing. Overloading. Overriding. Abstract Methods and \nClasses. \nPolymorphism and Multiple Inheritance Polymorphism in ML and Java. Multiple inheritance. \nInterfaces in Java. \nLifecycle of an Object Constructors and chaining. Destructors. Finalizers. Garbage Collection: \nreference counting, tracing. \nJava Collections and Object Comparison Java Collection interface. Key classes. Collections \nclass. Iteration options and the use of Iterator. Comparing primitives and objects. Operator \noverloading. \nError Handling Types of errors. Limitations of return values. Deferred error handling. Exceptions. \nCustom exceptions. Checked vs unchecked. Inappropriate use of exceptions. Assertions. \nDesignLanguage evolution Need for languages to evolve. Generics in Java. Type erasure. \nIntroduction to Java 8: Lambda functions, functions as values, method references, streams. \nDesign Patterns Introduction to design patterns. Open-closed principle. Examples of Singleton, \nDecorator, State, Composite, Strategy, Observer. [2 lectures] \nObjectives \nAt the end of the course students should \n \nProvide an overview of key OOP concepts that are transferable in mainstream programming \nlanguages; \nGain an understanding of software development approaches adopted in the industry including \nmaintainability, testing, and software design patterns; \nIncrease programming familiarity with Java; \nRecommended reading \n1. Real-World Software Development: A Project-Driven Guide to Fundamentals in Java [Urma et \nal.] \n2. Modern Java in Action (2nd edition) [Urma et al.]. \n3. Java How to Program: early objects [Deitel et al.]\n \n",
        "OCAML PRACTICAL CLASSES \n \nThe Role of Tickers \nA ticking session is a short (5 minute) one-to-one session with a ticker conducted on Thursday \nafternoons. The role of the ticker is twofold: \n \nto check you understand your solution (and didn\u2019t just have some lucky guesses or copy code \nfrom elsewhere) \nto give you general feedback on ways to improve your code. \nTo those ends a Ticker will typically ask you questions related to the core material of the tick and \ndiscuss your code directly. \n \nDeadline Extensions \nDeadline extensions can be granted for illness or similar reasons. To obtain an extension please \nemail Jon Ludlam in the first instance, clearly stating your justification for an extension and \nCCing your Director of Studies, who will need to support your request.\n \n",
        "SCIENTIFIC COMPUTING \n \nAims \nThis course is a hands-on introduction to using computers to investigate scientific models and \ndata. \n \nSyllabus \nPython notebooks. Overview of the Python programming language. Use of notebooks for \nscientific computing. \nNumerical computation. Writing fast vectorized code in numpy. Optimization and fitting. \nSimulation. \nWorking with data. Data import. Common ways to summarize and plot data, for univariate and \nmultivariate analysis. \nObjectives \nAt the end of the course students should \n \nbe able to import data, plot it, and summarize it appropriately \nbe able to write fast vectorized code for scientific / data work\n \n",
        "2nd Semester \nALGORITHMS 1 \nAims \nThe aim of this course is to provide an introduction to computer algorithms and data structures, \nwith an emphasis on foundational material. \n \nLectures \nSorting. Review of complexity and O-notation. Trivial sorting algorithms of quadratic complexity. \nReview of merge sort and quicksort, understanding their memory behaviour on statically \nallocated arrays. Heapsort. Stability. Other sorting methods including sorting in linear time. \nMedian and order statistics. [Ref: CLRS3 chapters 1, 2, 3, 6, 7, 8, 9] [about 4 lectures] \nStrategies for algorithm design. Dynamic programming, divide and conquer, greedy algorithms \nand other useful paradigms. [Ref: CLRS3 chapters 4, 15, 16] [about 3 lectures] \nData structures. Elementary data structures: pointers, objects, stacks, queues, lists, trees. \nBinary search trees. Red-black trees. B-trees. Hash tables. Priority queues and heaps. [Ref: \nCLRS3 chapters 6, 10, 11, 12, 13, 18] [about 5 lectures]. \nObjectives \nAt the end of the course students should: \n \nhave a thorough understanding of several classical algorithms and data structures; \nbe able to analyse the space and time efficiency of most algorithms; \nhave a good understanding of how a smart choice of data structures may be used to increase \nthe efficiency of particular algorithms; \nbe able to design new algorithms or modify existing ones for new applications and reason about \nthe efficiency of the result. \nRecommended reading \n* Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest and Clifford Stein Introduction to \nAlgorithms, Fourth Edition ISBN 9780262046305 Published: April 5, 2022. \n \nSedgewick, R., Wayne, K. (2011). Algorithms. Addison-Wesley. ISBN 978-0-321-57351-3. \n \nKleinberg, J. and Tardos, \u00c9. (2006). Algorithm design. Addison-Wesley. ISBN \n978-0-321-29535-4. \n \nKnuth, D.A. (2011). The Art of Computer Programming. Addison-Wesley. ISBN \n978-0-321-75104-1.\n \n",
        "ALGORITHMS 2 \n \nAims \nThe aim of this course is to provide an introduction to computer algorithms and data structures, \nwith an emphasis on foundational material. \n \nLectures \nGraphs and path-finding algorithms. Graph representations. Breadth-first and depth-first search. \nSingle-source shortest paths: Bellman-Ford and Dijkstra algorithms. All-pairs shortest paths: \ndynamic programming and Johnson\u2019s algorithms. [About 4 lectures] \nGraphs and subgraphs. Maximum flow: Ford-Fulkerson method, Max-flow min-cut theorem. \nMatchings in bipartite graphs. Minimum spanning tree: Kruskal and Prim algorithms. Topological \nsort. [About 4 lectures] \nAdvanced data structures. Binomial heap. Amortized analysis: aggregate analysis, potential \nmethod. Fibonacci heaps. Disjoint sets. [About 4 lectures] \nObjectives \nAt the end of the course students should: \n \nhave a thorough understanding of several classical algorithms and data structures; \nbe able to analyse the space and time efficiency of most algorithms; \nhave a good understanding of how a smart choice of data structures may be used to increase \nthe efficiency of particular algorithms; \nbe able to design new algorithms or modify existing ones for new applications and reason about \nthe efficiency of the result. \nRecommended reading \n* Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest and Clifford Stein Introduction to \nAlgorithms, Fourth Edition ISBN 9780262046305 Published: April 5, 2022. \n \nSedgewick, R., Wayne, K. (2011). Algorithms. Addison-Wesley. ISBN 978-0-321-57351-3. \n \nKleinberg, J. and Tardos, \u00c9. (2006). Algorithm design. Addison-Wesley. ISBN \n978-0-321-29535-4. \n \nKnuth, D.A. (2011). The Art of Computer Programming. Addison-Wesley. ISBN \n978-0-321-75104-1.\n \n",
        "MACHINE LEARNING AND REAL-WORLD DATA \n \nAims \nThis course introduces students to machine learning algorithms as used in real-world \napplications, and to the experimental methodology necessary to perform statistical analysis of \nlarge-scale data from unpredictable processes. Students will perform 3 extended practicals, as \nfollows: \n \nStatistical classification: Determining movie review sentiment using Naive Bayes; \nSequence Analysis: Hidden Markov Modelling and its application to a task from biology \n(predicting protein interactions with a cell membrane); \nAnalysis of social networks, including detection of cliques and central nodes. \nSyllabus \nTopic One: Statistical Classification \nIntroduction to sentiment classification. \nNaive Bayes parameter estimation. \nStatistical laws of language. \nStatistical tests for classification tasks. \nCross-validation and test sets. \nUncertainty and human agreement. \nTopic Two: Sequence Analysis  \nHidden Markov Models (HMM) and HMM training. \nThe Viterbi algorithm. \nUsing an HMM in a biological application. \nTopic Three: Social Networks  \nProperties of networks: Degree, Diameter. \nBetweenness Centrality. \nClustering using betweenness centrality. \nObjectives \nBy the end of the course students should be able to: \n \nunderstand and program two simple supervised machine learning algorithms; \nuse these algorithms in statistically valid experiments, including the design of baselines, \nevaluation metrics, statistical testing of results, and provision against overtraining; \nvisualise the connectivity and centrality in large networks; \nuse clustering (i.e., a type of unsupervised machine learning) for detection of cliques in \nunstructured networks. \nRecommended reading \nJurafsky, D. and Martin, J. (2008). Speech and language processing. Prentice Hall. \n \nEasley, D. and Kleinberg, J. (2010). Networks, crowds, and markets: reasoning about a highly \nconnected world. Cambridge University Press.\n \n",
        "OPERATING SYSTEMS \n \nAims \nThe overall aim of this course is to provide a general understanding of the structure and key \nfunctions of the operating system. Case studies will be used to illustrate and reinforce \nfundamental concepts. \n \nLectures \nIntroduction to operating systems. Elementary computer organisaton. Abstract view of an \noperating system. Key concepts: layering, multiplexing, performance, caching, buffering, policies \nversus mechanisms. OS evolution: multi-programming, time-sharing. Booting. [1 lecture] \nProtection. Dual-mode operation. Protecting CPU, memory, I/O, storage. Kernels, micro-kernels, \nand modules. System calls. Virtual machines and containers. Subjects and objects. \nAuthentication. Access matrix: ACLs and capabilities. Covert channels. [1 lecture] \nProcesses. Job/process concepts. Process lifecycle. Process management. Context switching. \nInter-process communication. [1 lecture] \nScheduling. CPU-I/O burst cycle. Scheduling basics: preemption and non-preemption. \nScheduling algorithms: FCFS, SJF, SRTF, Priority, and Round Robin. Multilevel scheduling. [2 \nlectures] \nMemory management. Processes in memory. Logical versus physical addresses. Partitions: \nstatic versus dynamic, free space management, external fragmentation. Segmented memory. \nPaged memory: concepts, internal fragmentation, page tables. Demand paging/segmentation. \nPage replacement strategies: FIFO, OPT, and LRU with approximations including NRU, LFU, \nMFU, MRU. Working set schemes. Thrashing. [3 lectures] \nI/O subsystem. General structure. Polled mode versus interrupt-driven I/O. Programmed I/O \n(PIO) versus Direct Memory Access (DMA). Application I/O interface: block and character \ndevices, buffering, blocking, non-blocking, asynchronous, and vectored I/O. Other issues: \ncaching, scheduling, spooling, performance. [1 lecture] \nFile management. File concept. Directory and storage services. File names and metadata. \nDirectory name-space: hierarchies, DAGs, hard and soft links. File operations. Access control. \nExistence and concurrency control. [1 lecture] \nUnix case study. History. General structure. Unix file system: file abstraction, directories, mount \npoints, implementation details. Processes: memory image, lifecycle, start of day. The shell: \nbasic operation, commands, standard I/O, redirection, pipes, signals. Character and block I/O. \nProcess scheduling. [2 lectures] \nObjectives \nAt the end of the course students should be able to: \n \ndescribe the general structure and purpose of an operating system; \nexplain the concepts of process, address space, and file; \ncompare and contrast various CPU scheduling algorithms; \ncompare and contrast mechanisms involved in memory and storage management; \ncompare and contrast polled, interrupt-driven, and DMA-based access to I/O devices. \nRecommended reading \n",
        "* Silberschatz, A., Galvin, P.C., and Gagne, G. (2018). Operating systems concepts. Wiley (10th \ned.). Prior editions, at least back to 8th ed., should also be acceptable. \nAnderson, T. and Dahlin, M. (2014). Operating Systems: Principles and Practice. Recursive \nBooks (2nd ed.). \nBacon, J. and Harris, T. (2003). Operating systems. Addison-Wesley (3rd ed.). Currently \nappears out-of-print. \nLeffler, S. (1989). The design and implementation of the 4.3BSD Unix operating system. \nAddison-Wesley. \nMcKusick, M.K., Neville-Neil, G.N. and Watson, R.N.M. (2014) The Design and Implementation \nof the FreeBSD Operating System. Pearson Education. (2nd ed.).\n \n",
        "INTERACTION DESIGN \nAims \nThe aim of this course is to provide an introduction to interaction design, with an emphasis on \nunderstanding and experiencing the user-centred design process, from conducting user \nresearch and requirements development to implementation and evaluation, while understanding \nthe background to human-computer interaction. \n \nLectures \nCourse overview and user research methods. Introduction to the course and practicals. \nUser-Centred Design. User research methods. \nKeeping usera in mind. User research data analysis. Identifying users and stakeholders. \nRepresenting user goals and activities. Identifying and establishing requirements. \nDesign and prototyping. Methods for exploring the design space. Prototyping and different kinds \nof prototypes. \nVisual and interaction design. Memory, perception, attention, and their implications for \ninteraction design. Modalities of interaction, interaction design patterns, information architecture, \nand their implications for interaction design. \nEvaluation. Practical methods for evaluating designs. Evaluation methods without users. \nEvaluation methods with users. \nCase studies from industry and research. Guest lectures (the topics of these lectures are \nsubject to change). \nObjectives \nBy the end of the course students should \n \nhave a thorough understanding of the user-centred design process and be able to apply it to \ninteraction design; \nbe able to design new user interfaces that are informed by principles of good design, and the \nprinciples of human visual perception, cognition and communication; \nbe able to prototype and implement interactive user interfaces with a strong emphasis on users, \nusability and appearance; \nbe able to evaluate existing or new user interfaces using multiple techniques; \nbe able to compare and contrast different design techniques and to critique their applicability to \nnew domains. \nRecommended reading \n* Preece, J., Rogers, Y. and Sharp, H. (2015). Interaction design. Wiley (4th ed.).\n \n",
        "INTRODUCTION TO PROBABILITY \n \nAims \nThis course provides an elementary introduction to probability and statistics with applications. \nProbability theory and the related field of statistical inference provide the foundations for \nanalysing and making sense of data. The focus of this course is to introduce the language and \ncore concepts of probability theory. The course will also cover some applications of statistical \ninference and algorithms in order to equip students with the ability to represent problems using \nthese concepts and analyse the data within these principles. \n \nLectures \nPart 1 - Introduction to Probability \n \nIntroduction. Counting/Combinatorics (revision), Probability Space, Axioms, Union Bound. \nConditional probability. Conditional Probabilities and Independence, Bayes\u2019Theorem, Partition \nTheorem \nPart 2 - Discrete Random Variables \n \nRandom variables. Definition of a Random Variable, Probability Mass Function, Cumulative \nDistribution, Expectation. \nProbability distributions. Definition and Properties of Expectation, Variance, different ways of \ncomputing them, Examples of important Distributions (Bernoulli, Binomial, Geometric, Poisson), \nPrimer on Continuous Distributions including Normal and Exponential Distributions. \nMultivariate distributions. Multiple Random Variables, Joint and Marginal Distributions, \nIndependence of Random Variables, Covariance. \nPart 3 - Moments and Limit Theorems \n \nIntroduction. Law of Average, Useful inequalities (Markov and Chebyshef), Weak Law of Large \nNumbers (including Proof using Chebyshef\u2019s inequality), Examples. \nMoments and Central Limit Theorem. Introduction to Moments of Random Variables, Central \nLimit Theorem (Proof using Moment Generating functions), Example. \nPart 4 - Applications/Statistics \n \nStatistics. Classical Parameter Estimation (Maximum-Likelihood-Estimation), bias, sample \nmean, sample variance), Examples (Collision-Sampling, Estimating Population Size). \nAlgorithms. Online Algorithms (Secretary Problem, Odd\u2019s Algorithm). \nObjectives \nAt the end of the course students should \n \nunderstand the basic principles of probability spaces and random variables \nbe able to formulate problems using concepts from probability theory and compute or estimate \nprobabilities \nbe familiar with more advanced concepts such as moments, limit theorems and applications \nsuch as parameter estimation \n",
        "Recommended reading \n* Ross, S.M. (2014). A First course in probability. Pearson (9th ed.). \n \nBertsekas, D.P. and Tsitsiklis, J.N. (2008). Introduction to probability. Athena Scientific. \n \nGrimmett, G. and Welsh, D. (2014). Probability: an Introduction. Oxford University Press (2nd \ned.). \n \nDekking, F.M., et. al. (2005) A modern introduction to probability and statistics. Springer.\n",
        "SOFTWARE AND SECURITY ENGINEERING \n \nAims \nThis course aims to introduce students to software and security engineering, and in particular to \nthe problems of building large systems, safety-critical systems and systems that must withstand \nattack by capable opponents. Case histories of failure are used to illustrate what can go wrong, \nwhile current software and security engineering practice is studied as a guide to how failures \ncan be avoided. \n \nLectures \n1. What is a security policy or a safety case? Definitions and examples; one-way flows for both \nconfidentiality and safety properties; separation of duties. Top-down and bottom-up analysis \nmethods. What architecture can do, versus benefits of decoupling policy from mechanism. \n \n2. Examples of safety and security policies. Safety and security usability; the pyramid of harms. \nPredicting and mitigating user errors. The prevention of fraud and error in accounting systems; \nthe safety usability of medical devices. \n \n3. Attitudes to risk: expected  utility, prospect theory, framing, status quo bias. Authority, \nconformity and gender; mental models, affordances and defaults. The characteristics of human \nmemory; forgetting passwords versus guessing them. \n \n4. Security protocols; how to enforce policy using  structured human interaction, cryptography or \nboth. Middleperson attacks.The role of verification and its limitations. \n \n5. Attacks on TLS, from rogue CAs through side channels to Heartbleed. Other types of \nsoftware bugs: syntactic, timing, concurrency, code injection, buffer overflows. Defensive \nprogramming: secure coding, contracts. Fuzzing. \n \n6. The software crisis. Examples of large-scale project failure, such as the London Ambulance \nService system and the NHS National Programme for IT. Intrinsic difficulties with complex \nsoftware. \n \n7. Software engineering as the management of complexity. The software life cycle; requirements \nanalysis methods; modular design; the role of prototyping; the waterfall, spiral and agile models. \n \n8. The economics of software as a Service (SaaS); the impact SaaS has on software \nengineering. Continuous integration, release engineering, behavioural analytics and experiment \nframeworks, rearchitecting systems while in operation. \n \n9. Critical systems: safety as an emergent system property. Examples of catastrophic failure: \nfrom Therac-25 to the Boeing 737Max. The problems of managing redundancy. The overall \nprocess of safety engineering. \n \n",
        "10. Managing the development of critical systems: tools and methods, individual versus group \nproductivity, economics of testing and agile development, measuring outcomes versus process, \nthe technical and human aspects of management, post-market surveillance and coordinated \ndisclosure. The sustainability of products with software components. \n \nAt the end of the course students should know how writing programs with tough assurance \ntargets, in large teams, or both, differs from the programming exercises they have engaged in \nso far. They should understand the different models of software development described in the \ncourse as well as the value of various development and management tools. They should \nunderstand the development life cycle and its basic economics. They should understand the \nvarious types of bugs, vulnerabilities and hazards, how to find them, and how to avoid \nintroducing them. Finally, they should be prepared for the organizational aspects of their Part IB \ngroup project. \n \nRecommended reading \nAnderson, R. (Third Edition 2020). Security engineering (Part 1 and Chapters 27-28). Wiley. \nAvailable at: http://www.cl.cam.ac.uk/users/rja14/book.html\n \n",
        "3rd Semester \nCONCURRENT AND DISTRIBUTED SYSTEMS \n \nAims \nThis course considers two closely related topics, Concurrent Systems and Distributed Systems, \nover 16 lectures. The aim of the first half of the course is to introduce concurrency control \nconcepts and their implications for system design and implementation. The aims of the latter \nhalf of the course are to study the fundamental characteristics of distributed systems, including \ntheir models and architectures; the implications for software design; some of the techniques that \nhave been used to build them; and the resulting details of good distributed algorithms and \napplications. \n \nLectures: Concurrency \nIntroduction to concurrency, threads, and mutual exclusion. Introduction to concurrent systems; \nthreads; interleaving; preemption; parallelism; execution orderings; processes and threads; \nkernel vs. user threads; M:N threads; atomicity; mutual exclusion; and mutual exclusion locks \n(mutexes). \nAutomata Composition. Synchronous and asynchronous parallelism; sequential consistency; \nrendezvous. Safety, liveness and deadlock; the Dining Philosophers; Hardware foundations for \natomicity: test-and-set, load-linked/store-conditional and fence instructions. Lamport bakery \nalgorithm. \nCommon design patterns: semaphores, producer-consumer, and MRSW. Locks and invariants; \nsemaphores; condition synchronisation; N-resource allocation; two-party and generalised \nproducer-consumer; Multi-Reader, Single-Write (MRSW) locks. \nCCR, monitors, and concurrency in practice. Conditional critical regions (CCR); monitors; \ncondition variables; signal-wait vs. signal-continue semantics; concurrency in practice (kernels, \npthreads, Java, Cilk, OpenMP). \nDeadlock and liveness guarantees Offline vs. online; model checking; resource allocation \ngraphs; lock order checking; deadlock prevention, avoidance, detection, and recovery; livelock; \npriority inversion; auto parallelisation. \nConcurrency without shared data; transactions. Active objects; message passing; tuple spaces; \nCSP; and actor models. Composite operations; transactions; ACID; isolation; and serialisability. \nFurther transactions History graphs; good and bad schedules; isolation vs. strict isolation; \n2-phase locking; rollback; timestamp ordering (TSO); and optimistic concurrency control (OCC). \nCrash recovery, lock-free programming, and transactional memory. Write-ahead logging, \ncheckpoints, and recovery. Lock-free programming. Hardware and software transactional \nmemories. \n  \n \nLectures: Distributed Systems \nIntroduction to distributed systems; RPC. Avantages and challenges of distributed systems; \nunbounded delay and partial failure; network protocols; transparency; client-server systems; \nremote procedure call (RPC); marshalling; interface definition languages (IDLs). \n",
        "System models and faults. Synchronous, partially synchronous, and asynchronous network \nmodels; crash-stop, crash-recovery, and Byzantine faults; failures, faults, and fault tolerance; \ntwo generals problem. \nTime, clocks, and ordering of events. Physical clocks; leap seconds; UTC; clock synchronisation \nand drift; Network Time Protocol (NTP). Causality; happens-before relation. \nLogical time; Lamport clocks; vector clocks. Broadcast (FIFO, causal, total order); gossip \nprotocols. \nReplication. Quorums; idempotence; replica consistency; read-after-write consistency. State \nmachine replication; leader-based replication. \nConsensus and total order broadcast. FLP result; leader election; the Raft consensus algorithm. \nReplica consistency. Two-phase commit; relationship between 2PC and consensus; \nlinearizability; ABD algorithm; eventual consistency; CAP theorem. \nCase studies. Conflict-free Replicated Data Types (CRDTs); collaborative text editing. Google's \nSpanner; TrueTime. \nObjectives \nAt the end of Concurrent Systems portion of the course, students should: \n \nunderstand the need for concurrency control in operating systems and applications, both mutual \nexclusion and condition synchronisation; \nunderstand how multi-threading can be supported and the implications of different approaches; \nbe familiar with the support offered by various programming languages for concurrency control \nand be able to judge the scope, performance implications and possible applications of the \nvarious approaches; \nbe aware that dynamic resource allocation can lead to deadlock; \nunderstand the concept of transaction; the properties of transactions, how they can be \nimplemented, and how their performance can be optimised based on optimistic assumptions; \nunderstand how the persistence properties of transactions are addressed through logging; and \nhave a high-level understanding of the evolution of software use of concurrency in the \noperating-system kernel case study. \nAt the end of the Distributed Systems portion of the course, students should: \n \nunderstand the difference between shared-memory concurrency and distributed systems; \nunderstand the fundamental properties of distributed systems and their implications for system \ndesign; \nunderstand notions of time, including logical clocks, vector clocks, and physical time \nsynchronisation; \nbe familiar with various approaches to data and service replication, as well as the concept of \nreplica consistency; \nunderstand the effects of large scale on the provision of fundamental services and the tradeoffs \narising from scale; \nappreciate the implications of individual node and network communications failures on \ndistributed computation; \nbe aware of a variety of programming models and abstractions for distributed systems, such as \nRPC, middleware, and total order broadcast; \n",
        "be familiar with a range of distributed algorithms, such as consensus, causal broadcast, and \ntwo-phase commit. \nRecommended reading \nModern Operating Systems (free PDF available online) by Andrew S Tanenbaum, Herbert Bos \nJava Concurrency in Practice' (2006 but still highly relevant) by Brian Goetz \nKleppmann, M. (2017). Designing data-intensive applications. O\u2019Reilly. \nTanenbaum, A.S. and van Steen, M. (2017). Distributed systems, 3rd edition. available online. \nCachin, C., Guerraoui, R. and Rodrigues, L. (2011) Introduction to Reliable and Secure \nDistributed Programming. Springer (2nd edition).\n \n",
        "DATA SCIENCE \n \nAims \nThis course introduces fundamental tools for describing and reasoning about data. There are \ntwo themes: designing probability models to describe systems; and drawing conclusions based \non data generated by such systems. \n \nLectures \nSpecifying and fitting probability models. Random variables. Maximum likelihood estimation. \nGenerative and supervised models. Goodness of fit. \nFeature spaces. Vector spaces, bases, inner products, projection. Linear models. Model fitting \nas projection. Design of features. \nHandling probability models. Handling pdf and cdf. Bayes\u2019s rule. Monte Carlo estimation. \nEmpirical distribution. \nInference. Bayesianism. Frequentist confidence intervals, hypothesis testing. Bootstrap \nresampling. \nRandom processes. Markov chains. Stationarity, and drift analysis. Processes with memory. \nLearning a random process. \nObjectives \nAt the end of the course students should \n \nbe able to formulate basic probabilistic models, including discrete time Markov chains and linear \nmodels \nbe familiar with common random variables and their uses, and with the use of empirical \ndistributions rather than formulae \nunderstand different types of inference about noisy data, including model fitting, hypothesis \ntesting, and making predictions \nunderstand the fundamental properties of inner product spaces and orthonormal systems, and \ntheir application to modelling \nRecommended reading \n* F.M. Dekking, C. Kraaikamp, H.P. Lopuha\u00e4, L.E. Meester (2005). A modern introduction to \nprobability and statistics: understanding why and how. Springer. \n \nS.M. Ross (2002). Probability models for computer science. Harcourt / Academic Press. \n \nM. Mitzenmacher and E. Upfal (2005). Probability and computing: randomized algorithms and \nprobabilistic analysis. Cambridge University Press.\n \n",
        "ECAD AND ARCHITECTURE PRACTICAL CLASSES \nAims \nThe aims of this course are to enable students to apply the concepts learned in the Computer \nDesign course. In particular a web based tutor is used to introduce the SystemVerilog hardware \ndescription language, while the remaining practical classes will then allow students to implement \nthe design of components in this language. \n \nPractical Classes \nWeb tutor The first class uses a web based tutor to rapidly teach the SystemVerilog language. \nFPGA design flow Test driven hardware development for FPGA including an embedded \nprocessor and peripherals [3 classes] \nEmbedded system implementation Embedded system implementation on FPGA [3-4 classes] \nObjectives \nGain experience in electronic computer aided design (ECAD) through learning a design-flow for \nfield programmable gate arrays (FPGAs). \nLearn how to interface to peripherals like a touch screen. \nLearn how to debug hardware and software systems in simulation. \nUnderstand how to construct and program a heterogeneous embedded system. \nRecommended reading \n* Harris, D.M. and Harris, S.L. (2007). Digital design and computer architecture: from gates to \nprocessors. Morgan Kaufmann. \n \nPointers to sources of more specialist information are included on the associated course web \npage. \n \nIntroduction \nThe ECAD and Architecture Laboratory sessions are a companion to the Introduction to \nComputer Architecture course. The objective is to provide experience of hardware design for \nFPGA including use of a small embedded processor. It covers hardware design in \nSystemVerilog, embedded software design in RISC-V assembly and C, and use of FPGA tools. \n \nPrerequisites \nThis course assumes familiarity with the material from Part IA Digital Electronics, although we \nwill use automated tools to perform many of the steps you might have done there by hand (for \nexample, logic minimisation). \n \nWe will be using a Linux command-line environment. We provide scripts and guidance on what \nyou need, however you may find it useful to review Unix Tools, in particular basic navigation \nsuch as ls, cd, mkdir, cp, mv. We'll be using the GCC compiler and Makefiles, described in parts \n31-32. We will also be using git for revision control and submission of work for ticking - you \nshould review at least parts 23-25 and 29. \n \nPracticalities \n",
        "The course runs over the 8 weeks of Michaelmas term. The course has been designed so you \ncan do most of the work at home or at any time you wish. You should be able to run all the \nnecessary tools on your own machine, through the use of virtual machines and Docker \ncontainers. We have a number of routes to do this in case some of them aren't suitable for the \nmachine you have. \n \nWe're running the course in a hybrid mode this year. We will provide online help sessions via \nMicrosoft Teams as well as in-person lab sessions on Fridays and Tuesdays 2-5pm during term. \n \nThe core components of the course, being the RISC-V architecture and software and ticked \nexercise, can be done entirely online, without needing access to hardware. They can be \nundertaken with any of the platforms we support, including MCS Linux (on the Intel Lab \nworkstations and remotely) if your own machine isn't suitable. We expect everyone to complete \nthese parts. \n \nThe rest of the course is optional. The hardware simulation and FPGA compilation can be done \non your machine without access to hardware, if you are able to run the virtual machine. \n \nThe parts that require access to an FPGA board will need the ability to run the virtual machine \nand you be able to collect and return an FPGA board from the Department. That process will be \ndescribed when you come to those parts. There is an optional starred tick available for \ncompletion of this part, which will need assessment by a demonstrator in person. \n \nWe'll do our best to support all the different platforms and variations, but please bear with us - \nit's quite possible we'll come across a problem we haven't seen before! \n \nWe have provided four routes you can use different tools to complete the course. Some routes \nnecessarily exclude some material but this material is optional. \n \nAssessment \nIn a similar way to other practical courses, this course carries one 'tick'. Everyone should expect \nto receive this tick, and those who do not should expect to lose marks on their final exams. \n \nThis year we have adopted the 'chime' autoticker used by Further Java. You will check out the \ninitial files as a git repository from the chime server, commit your work, and push the repository \nback to submit it. You can then press a button to run tests against your code and the autoticker \nwill tell you whether you have passed. Additionally you may be selected for a (real/video) \ninterview with a ticker to discuss your code and understanding of the material. \n \nTicking procedures will only be available during weeks 1-8 of Michaelmas term. The deadline for \nsubmitting Tick 1 is 5pm on the Tuesday of week 6 (19 November 2024). After passing the \nautoticker you may be asked for a tick interview with a demonstrator (online or in person). You \ncan gain Tick 1* during the timetabled lab sessions, assuming demonstrators are available. \n \n",
        "If you do not submit a successful Tick 1 to the autoticker by 5pm on the Tuesday of week 6 you \ncan self-certify an extension. If you need more time than this you will need to ask your Directory \nof Studies to organise a further extension. The hard deadline by which all ticks must be \ncompleted is noon on Friday 31 January 2025 (the Head of Department Notices is the definitive \ndocument). Extensions beyond this date due to exceptional circumstances require a formal \napplication from your college to the Examiners. \n \nScheduling \nWhile the full course contains 8 exercises, there is not a tight binding to doing one exercise per \nweek. Students should expect to spend about three hours per week on the course, however it is \nrecommended that you make a start on the next exercise if you have time in hand. \n \nDemonstrator help will be focused on the Tuesday and Friday 2-5pm slots during term the lab \nwould normally operate in, so you may find it helpful to work in these times as that will provide \nthe quickest response to questions. \n \nCollaboration \nWhen we have done these labs in person, we have often found they worked well when doing \nthem collaboratively. While your work must be your own, students can work together and help \neach other, and demonstrators contribute advice and experience with the tools. \n \nFor the timetabled sessions, which are Tuesdays and Fridays 2-5pm UK time during term, there \nwill be demonstrators available in the lab and at the same time we'll use the ECAD group on \nMicrosoft Teams. With this we can provide online audio/video help, screensharing and (for the \nWindows and possibly Mac Teams apps) optional remote control of your development \nenvironment. You will be added to the Team in week 1 - if you believe you have been missed \nplease post in the Moodle forum. In Teams there are a number of Helpdesk Channels (A-C) - if \nyou need help during lab times, post in Help Centre and a demonstrator can ask you to go to a \nspecific channel. There are also Student Breakout Channels (D-F) which are available for \nstudents to chat amongst themselves. \n \nFor help outside of timetabled sessions we've set up a Moodle forum where you can post \nquestions - we'll keep an eye on this at other times, but students are encouraged to use it to \nsupport each other. \n \nStudents are of course free to use other platforms to help each other. If you find anything useful \nthat we might use in future, please let us know! \n \nIf you are having difficulty accessing both Moodle and Teams, please email theo.markettos at \ncl.cam.ac.uk. \n \nFeedback \nWe revise the course each year, which may cause new bugs in the code or the notes. The \ncreators (Theo Markettos and Simon Moore) appreciate constructive feedback. \n",
        " \nCredits \nThis course was written by Theo Markettos and Simon Moore, with portions developed by \nRobert Eady, Jonas Fiala, Paul Fox, Vlad Gavrila, Brian Jones and Roy Spliet. We would like to \nthank Altera, Intel and Terasic for funding and other contributions.\n \n",
        "ECONOMICS, LAW AND ETHICS \n \nAims \nThis course aims to give students an introduction to some basic concepts in economics, law and \nethics. \n \nLectures \nClassical economics and consumer theory. Prices and markets; Pareto efficiency; preferences; \nutility; supply and demand; the marginalist revolution; elasticity; the welfare theorems; \ntransaction costs. \nInformation economics. The discriminating monopolist; marginal costs; effects of technology on \nsupply and demand; competition and information; lock in; real and virtual networks; Metcalfe\u2019s \nlaw; the dominant firm model; price discrimination; bundling; income distribution. \nMarket failure and behavioural economics. Market failure: the business cycle; recession and \ntechnology; tragedy of the commons; externalities; monopoly rents; asymmetric information: the \nmarket for lemons; adverse selection; moral hazard; signalling. Behavioural economics: \nbounded rationality, heuristics and biases; nudge theory; the power of defaults; agency effects. \nAuction theory and game theory. Auction theory: types of auctions; strategic equivalence; the \nrevenue equivalence theorem; the winner\u2019s curse; problems with real auctions; mechanism \ndesign and the combinatorial auction; applicability of auction mechanisms in computer science; \nadvertising auctions. Game theory: the choice between cooperation and conflict; strategic \nforms; dominant strategy equilibrium; Nash equilibrium; the prisoners\u2019 dilemma; evolution of \nstrategies; stag hunt; volunteer\u2019s dilemma; chicken; iterated games; hawk-dove; application to \ncomputer science. \nPrinciples of law. Criminal and civil law; contract law; choice of law and jurisdiction; arbitration; \ntort; negligence; defamation; intellectual property rights. \nLaw and the Internet. Computer evidence; the General Data Protection Regulation; UK laws that \nspecifically affect the Internet; e-commerce regulations; privacy and electronic communications. \nPhilosophies of ethics. Authority, intuitionist, egoist and deontological theories; utilitarian and \nRawlsian models; morality; insights from evolutionary psychology, neurology, and experimental \nethics; professional codes of ethics; research ethics. \nContemporary ethical issues. The Internet and social policy; current debates on privacy, \nsurveillance, and censorship; responsible vulnerability disclosure; algorithmic bias; predictive \npolicing; gamification and engagement; targeted political advertising; environmental impacts. \nObjectives \nOn completion of this course, students should be able to: \n \nReflect on and discuss professional, economic, social, environmental, moral and ethical issues \nrelating to computer science \nDefine and explain economic and legal terminology and arguments \nApply the philosophies and theories covered to computer science problems and scenarios \nReflect on the main constraints that market, legislation and ethics place on firms dealing in \ninformation goods and services \nRecommended reading \n",
        "* Shapiro, C. & Varian, H. (1998). Information rules. Harvard Business School Press. \nHare, S. (2022). Technology is not neutral: A short guide to technology ethics. London \nPublishing Partnership. \n \nFurther reading: \nSmith, A. (1776). An inquiry into the nature and causes of the wealth of nations, available at    \nhttp://www.econlib.org/library/Smith/smWN.html \nThaler, R.H. (2016). Misbehaving. Penguin. \nGalbraith, J.K. (1991). A history of economics. Penguin. \nPoundstone, W. (1992). Prisoner\u2019s dilemma. Anchor Books. \nPinker, S (2011). The Better Angels of our Nature. Penguin. \nAnderson, R. (2008). Security engineering (Chapter 7). Wiley. \nVarian, H. (1999). Intermediate microeconomics - a modern approach. Norton. \nNuffield Council on Bioethics (2015) The collection, linking and use of data in biomedical \nresearch and health care.\n \n",
        "FURTHER GRAPHICS \n \nAims \nThe course introduces fundamental concepts of modern graphics pipelines. \n \nLectures \nThe order and content of lectures is provisional and subject to change. \n \nGeometry Representations. Parametric surfaces, implicit surfaces, meshes, point-set surfaces, \ngeometry processing pipeline. [1 lecture] \nDiscrete Differential Geometry. Surface normal and curvature, Laplace-Beltrami operator, heat \ndiffusion. [1 lecture] \nGeometry Processing.  Parametrization, filtering, 3D capture. [1 lecture] \nAnimation I. Animation types, animation pipeline, rigging/skinning, character animation. [1 \nlecture] \nAnimation II. Blending transformations, pose-space animation, controllers. [1 lecture] \nThe Rendering Equation. Radiosity, reflection models, BRDFs, local vs. global illumination. [1 \nlecture] \nDistributed Ray Tracing. Quadrature, importance sampling, recursive ray tracing. [1 lecture] \nInverse Rendering. Differentiable rendering, inverse problems. [1 lecture] \nObjectives \nOn completing the course, students should be able to \n \nunderstand and use fundamental 3D geometry/scene representations and operations; \nlearn animation techniques and controls; \nlearn how light transport is modeled and simulated in rendering; \nunderstand how graphics and rendering can be used to solve computer perception problems. \nRecommended reading \nStudents should expect to refer to one or more of these books, but should not find it necessary \nto purchase any of them. \n \nShirley, P. and Marschner, S. (2009). Fundamentals of Computer Graphics. CRC Press (3rd \ned.). \nWatt, A. (2000). 3D Computer Graphics. Addison-Wesley (3rd ed). \nHughes, van Dam, McGuire, Skalar, Foley, Feiner and Akeley (2013). Computer Graphics: \nPrinciples and Practice. Addison-Wesley (3rd edition) \nAkenine-M\u00f6ller, et. al. (2018). Real-time rendering. CRC Press (4th ed.).\n \n",
        "FURTHER JAVA \n \nAims \nThe goal of this course is to provide students with the ability to understand the advanced \nprogramming features available in the Java programming language, completing the coverage of \nthe language started in the Programming in Java course. The course is designed to \naccommodate students with diverse programming backgrounds; consequently Java is taught \nfrom first principles in a practical class setting where students can work at their own pace from a \ncourse handbook. \n \nObjectives \n \nAt the end of the course students should \n \nunderstand different mechanisms for communication between distributed applications and be \nable to evaluate their trade-offs; \nbe able to use Java generics and annotations to improve software usability, readability and \nsafety; \nunderstand and be able to exploit the Java class-loading mechansim; \nunderstand and be able to use concurrency control correctly; \nbe able to implement a vector clock algorithm and the happens-before relation. \nRecommended reading \n* Goetz, B. (2006). Java concurrency in practice. Addison-Wesley. \nGosling, J., Joy, B., Steele, G., Bracha, G. and Buckley, A. (2014). The Java language \nspecification, Java SE 8 Edition. Addison-Wesley. \nhttp://docs.oracle.com/javase/specs/jls/se8/html/\n \n",
        "GROUP PROJECTS \nExample Risk Report \nThe risk report should focus on the risks to your project succeeding on time (and not, say, on \ngeneral health and safety points). It is only 50 words. Example: \n \n\"Identified risks: \n * Training data requires access authorisation that has not been approved, and we don\u2019t know if \nclient can do this \n * Two group members have unreliable network connections and may miss meetings \n * One group member is in timezone UTC+8, and will have to attend meetings late at night \n * Bug reports on Github for a library we plan to use suggest maintenance updates will be \nnecessary for recent Android release\"\n \n",
        "INTRODUCTION TO COMPUTER ARCHITECTURE \n \nAims \nThe aims of this course are to introduce a hardware description language (SystemVerilog) and \ncomputer architecture concepts in order to design computer systems. The parallel ECAD+Arch \npractical classes will allow students to apply the concepts taught in lectures. \n \nLectures \nPart 1 - Gates to processors  \n \nTechnology trends and design challenges. Current technology, technology trends, ECAD trends, \nchallenges. [1 lecture] \nDigital system design. Practicalities of mapping SystemVerilog descriptions of hardware \n(including a processor) onto an FPGA board. Tips and pitfalls when generating larger modular \ndesigns. [1 lecture] \nEight great ideas in computer architecture. [1 lecture] \nReduced instruction set computers and RISC-V. Introduction to the RISC-V processor design. [1 \nlecture] \nExecutable and synthesisable models. [1 lecture] \nPipelining. [2 lectures] \nMemory hierarchy and caching. Caching, etc. [1 lecture] \nSupport for operating systems. Memory protection, exceptions, interrupts, etc. [1 lecture] \nOther instruction set architectures. CISC, stack, accumulator. [1 lecture] \nPart 2  \n \nOverview of Systems-on-Chip (SoCs) and DRAM. [1 lecture] High-level SoCs, DRAM storage \nand accessing. \nMulticore Processors. [2 lectures] Communication, cache coherence, barriers and \nsynchronisation primitives. \nGraphics processing units (GPUs) [2 lectures] Basic GPU architecture and programming. \nFuture Directions [1 lecture] Where is computer architecture heading? \nObjectives \nAt the end of the course students should \n \nbe able to read assembler given a guide to the instruction set and be able to write short pieces \nof assembler if given an instruction set or asked to invent an instruction set; \nunderstand the differences between RISC and CISC assembler; \nunderstand what facilities a processor provides to support operating systems, from memory \nmanagement to software interrupts; \nunderstand memory hierarchy including different cache structures and coherency needed for \nmulticore systems; \nunderstand how to implement a processor in SystemVerilog; \nappreciate the use of pipelining in processor design; \nhave an appreciation of control structures used in processor design; \n",
        "have an appreciation of system-on-chips and their components; \nunderstand how DRAM stores data; \nunderstand how multicore processors communicate; \nunderstand how GPUs work and have an appreciation of how to program them. \nRecommended reading \n* Patterson, D.A. and Hennessy, J.L. (2017). Computer organization and design: The \nhardware/software interface RISC-V edition. Morgan Kaufmann. ISBN 978-0-12-812275-4. \n \nRecommended further reading: \n \nHarris, D.M. and Harris, S.L. (2012). Digital design and computer architecture. Morgan \nKaufmann. ISBN 978-0-12-394424-5. \nHennessy, J. and Patterson, D. (2006). Computer architecture: a quantitative approach. Elsevier \n(4th ed.). ISBN 978-0-12-370490-0. (Older versions of the book are also still generally relevant.) \n \nPointers to sources of more specialist information are included in the lecture notes and on the \nassociated course web page\n \n",
        "PROGRAMMING IN C AND C++ \n \nAims \nThis course aims \n \nto provide a solid introduction to programming in C and to provide an overview of the principles \nand constraints that affect the way in which the C programming language has been designed \nand is used; \nto introduce the key additional features of C++. \n  \n \nLectures \nIntroduction to the C language. Background and goals of C. Types, expressions, control flow \nand strings. \n(continued) Functions. Multiple compilation units. Scope. Segments. Incremental compilation. \nPreprocessor. \n(continued) Pointers and pointer arithmetic. Function pointers. Structures and Unions. \n(continued) Const and volatile qualifiers. Typedefs. Standard input/output. Heap allocation. \nMiscellaneous Features, Hints and Tips. \nC semantics and tools. Undefined vs implementation-defined behaviour. Buffer and integer \noverflows. ASan, MSan, UBsan, Valgrind checkers. \nMemory allocation, data structures and aliasing. Malloc/free, tree and DAG examples and their \ndeallocation using a memory arena. \nFurther memory management. Reference Counting and Garbage Collection \nMemory hierarchy and cache optimization. Data structure layouts. Intrusive lists. Array-of-structs \nvs struct-of-arrays representations. Loop blocking. \nDebugging. Using print statements, assertions and an interactive debugger. Unit testing and \nregression. \nIntroduction to C++. Goals of C++. Differences between C and C++. References versus \npointers. Overloaded functions. \nObjects in C++. Classes and structs. Destructors. Resource Acquisition is Initialisation. Operator \noverloading. Virtual functions. Casts. Multiple inheritance. Virtual base classes. \nOther C++ concepts. Templates and meta-programming. \nObjectives \nAt the end of the course students should \n \nbe able to read and write C programs; \nunderstand the interaction between C programs and the host operating system; \nbe familiar with the structure of C program execution in machine memory; \nunderstand the potential dangers of writing programs in C; \nunderstand the object model and main additional features of C++. \nRecommended reading \n* Kernighan, B.W. and Ritchie, D.M. (1988). The C programming language. Prentice Hall (2nd \ned.).\n \n",
        "SEMANTICS OF PROGRAMMING LANGUAGES \n \nAims \nThe aim of this course is to introduce the structural, operational approach to programming \nlanguage semantics. It will show how to specify the meaning of typical programming language \nconstructs, in the context of language design, and how to reason formally about semantic \nproperties of programs. \n \nLectures \nIntroduction. Transition systems. The idea of structural operational semantics. Transition \nsemantics of a simple imperative language. Language design options. [2 lectures] \nTypes. Introduction to formal type systems. Typing for the simple imperative language. \nStatements of desirable properties. [2 lectures] \nInduction. Review of mathematical induction. Abstract syntax trees and structural induction. \nRule-based inductive definitions and proofs. Proofs of type safety properties. [2 lectures] \nFunctions. Call-by-name and call-by-value function application, semantics and typing. Local \nrecursive definitions. [2 lectures] \nData. Semantics and typing for products, sums, records, references. [1 lecture] \nSubtyping. Record subtyping and simple object encoding. [1 lecture] \nSemantic equivalence. Semantic equivalence of phrases in a simple imperative language, \nincluding the congruence property. Examples of equivalence and non-equivalence. [1 lecture] \nConcurrency. Shared variable interleaving. Semantics for simple mutexes; a serializability \nproperty. [1 lecture] \nObjectives \nAt the end of the course students should \n \nbe familiar with rule-based presentations of the operational semantics and type systems for \nsome simple imperative, functional and interactive program constructs; \nbe able to prove properties of an operational semantics using various forms of induction \n(mathematical, structural, and rule-based); \nbe familiar with some operationally-based notions of semantic equivalence of program phrases \nand their basic properties. \nRecommended reading \n* Pierce, B.C. (2002). Types and programming languages. MIT Press. \nHennessy, M. (1990). The semantics of programming languages. Wiley. Out of print, but \navailable on the web at \nhttp://www.cs.tcd.ie/matthew.hennessy/splexternal2015/resources/sembookWiley.pdf \nWinskel, G. (1993). The formal semantics of programming languages. MIT Press.\n \n",
        "UNIX TOOLS \n \nAims \nThis video-lecture course gives students who have already basic Unix/Linux experience some \nadditional practical software-engineering knowledge: how to use the shell and related tools as \nan efficient working environment, how to automate routine tasks, and how version-control and \nautomated-build tools can help to avoid confusion and accidents, especially when working in \nteams. These are essential skills, both in industrial software development and student projects. \n \nLectures \nUnix concepts. Brief review of Unix history and design philosophy, documentation, terminals, \ninter-process communication mechanisms and conventions, shell, command-line arguments, \nenvironment variables, file descriptors. \nShell concepts. Program invocation, redirection, pipes, file-system navigation, argument \nexpansion, quoting, job control, signals, process groups, variables, locale, history and alias \nfunctions, security considerations. \nScripting. Plain-text formats, executables, #!, shell control structures and functions. Startup \nscripts. \nText, file and networking tools. sed, grep, chmod, find, ssh, rsync, tar, zip, etc. \nVersion control. diff, patch, RCS, Subversion, git. \nSoftware development tools. C compiler, linker, debugger, make. \nPerl. Introduction to a powerful scripting and text-manipulation language. [2 lectures] \nObjectives \nAt the end of the course students should \n \nbe confident in performing routine user tasks on a POSIX system, understand command-line \nuser-interface conventions and know how to find more detailed documentation; \nappreciate how simple tools can be combined to perform a large variety of tasks; \nbe familiar with the most common tools, file formats and configuration practices; \nbe able to understand, write, and maintain shell scripts and makefiles; \nappreciate how using a version-control system and fully automated build processes help to \nmaintain reproducibility and audit trails during software development; \nknow enough about basic development tools to be able to install, modify and debug C source \ncode; \nhave understood the main concepts of, and gained initial experience in, writing Perl scripts \n(excluding the facilities for object-oriented programming). \nRecommended reading \nRobbins, A. (2005). Unix in a nutshell. O\u2019Reilly (4th ed.). \nSchwartz, R.L., Foy, B.D. and Phoenix, T. (2011). Learning Perl. O\u2019Reilly (6th ed.).\n \n",
        "4th Semester \n \nCOMPILER CONSTRUCTION \n \nAims \nThis course aims to cover the main concepts associated with implementing compilers for \nprogramming languages. We use a running example called SLANG (a Small LANGuage) \ninspired by the languages described in 1B Semantics. A toy compiler (written in ML) is provided, \nand students are encouraged to extend it in various ways. \n \nLectures \nOverview of compiler structure The spectrum of interpreters and compilers; compile-time and \nrun-time. Compilation as a sequence of translations from higher-level to lower-level intermediate \nlanguages, where each translation preserves semantics. The structure of a simple compiler: \nlexical analysis and syntax analysis, type checking, intermediate representations, optimisations, \ncode generation. Overview of run-time data structures: stack and heap. Virtual machines. [1 \nlecture] \nLexical analysis and syntax analysis. Lexical analysis based on regular expressions and finite \nstate automata. Using LEX-tools. How does LEX work? Parsing based on context-free \ngrammars and push-down automata. Grammar ambiguity, left- and right-associativity and \noperator precedence. Using YACC-like tools. How does YACC work? LL(k) and LR(k) parsing \ntheory. [3 lectures] \nCompiler Correctness Recursive functions can be transformed into iterative functions using the \nContinuation-Passing Style (CPS) transformation. CPS applied to a (recursive) SLANG \ninterpreter to derive, in a step-by-step manner, a correct stack-based compiler. [3 lectures] \nData structures, procedures/functions Representing tuples, arrays, references. Procedures and \nfunctions: calling conventions, nested structure, non-local variables. Functions as first-class \nvalues represented as closures. Simple optimisations: inline expansion, constant folding, \nelimination of tail recursion, peephole optimisation. [5 lectures] \nAdvanced topics Run-time memory management (garbage collection). Static and dynamic \nlinking. Objects and inheritance; implementation of method dispatch. Try-catch exception \nmechanisms. Compiling a compiler via bootstrapping. [4 lectures] \nObjectives \nAt the end of the course students should understand the overall structure of a compiler, and will \nknow significant details of a number of important techniques commonly used. They will be \naware of the way in which language features raise challenges for compiler builders. \n \nRecommended reading \n* Aho, A.V., Sethi, R. and Ullman, J.D. (2007). Compilers: principles, techniques and tools. \nAddison-Wesley (2nd ed.). \nMogensen, T. \u00c6. (2011). Introduction to compiler design. Springer. http://www.diku.dk/ \ntorbenm/Basics.\n \n",
        "COMPUTATION THEORY \n \nAims \nThe aim of this course is to introduce several apparently different formalisations of the informal \nnotion of algorithm; to show that they are equivalent; and to use them to demonstrate that there \nare uncomputable functions and algorithmically undecidable problems. \n \nLectures \nIntroduction: algorithmically undecidable problems. Decision problems. The informal notion of \nalgorithm, or effective procedure. Examples of algorithmically undecidable problems. [1 lecture] \nRegister machines. Definition and examples; graphical notation. Register machine computable \nfunctions. Doing arithmetic with register machines. [1 lecture] \nUniversal register machine. Natural number encoding of pairs and lists. Coding register machine \nprograms as numbers. Specification and implementation of a universal register machine. [2 \nlectures] \nUndecidability of the halting problem. Statement and proof. Example of an uncomputable partial \nfunction. Decidable sets of numbers; examples of undecidable sets of numbers. [1 lecture] \nTuring machines. Informal description. Definition and examples. Turing computable functions. \nEquivalence of register machine computability and Turing computability. The Church-Turing \nThesis. [2 lectures] \nPrimitive and partial recursive functions. Definition and examples. Existence of a recursive, but \nnot primitive recursive function. A partial function is partial recursive if and only if it is \ncomputable. [2 lectures] \nLambda-Calculus. Alpha and beta conversion. Normalization. Encoding data. Writing recursive \nfunctions in the lambda-calculus. The relationship between computable functions and \nlambda-definable functions. [3 lectures] \nObjectives \nAt the end of the course students should \n \nbe familiar with the register machine, Turing machine and lambda-calculus models of \ncomputability; \nunderstand the notion of coding programs as data, and of a universal machine; \nbe able to use diagonalisation to prove the undecidability of the Halting Problem; \nunderstand the mathematical notion of partial recursive function and its relationship to \ncomputability. \nRecommended reading \n* Hopcroft, J.E., Motwani, R. and Ullman, J.D. (2001). Introduction to automata theory, \nlanguages, and computation. Addison-Wesley (2nd ed.). \n* Hindley, J.R. and Seldin, J.P. (2008). Lambda-calculus and combinators, an introduction. \nCambridge University Press (2nd ed.). \nCutland, N.J. (1980). Computability: an introduction to recursive function theory. Cambridge \nUniversity Press. \nDavis, M.D., Sigal, R. and Weyuker, E.J. (1994). Computability, complexity and languages. \nAcademic Press (2nd ed.). \n",
        "Sudkamp, T.A. (2005). Languages and machines. Addison-Wesley (3rd ed.).\n \n",
        "COMPUTER NETWORKING \n \nAims \nThe aim of this course is to introduce key concepts and principles of computer networks. The \ncourse will use a top-down approach to study the Internet and its protocol stack. Instances of \narchitecture, protocol, application-examples will include email, web and media-streaming. We \nwill cover communications services (e.g., TCP/IP) required to support such network \napplications. The implementation and deployment of communications services in practical \nnetworks: including wired and wireless LAN environments, will be followed by a discussion of \nissues of network-management. Throughout the course, the Internet\u2019s architecture and \nprotocols will be used as the primary examples to illustrate the fundamental principles of \ncomputer networking. \n \nLectures \nIntroduction. Overview of networking using the Internet as an example. LANs and WANs. OSI \nreference model, Internet TCP/IP Protocol Stack. Circuit-switching, packet-switching, Internet \nstructure, networking delays and packet loss. [3 lectures] \nLink layer and local area networks. Link layer services, error detection and correction, Multiple \nAccess Protocols, link layer addressing, Ethernet, hubs and switches, Point-to-Point Protocol. [2 \nlectures] \nWireless and mobile networks. Wireless links and network characteristics, Wi-Fi: IEEE 802.11 \nwireless LANs. [1 lecture] \nNetwork layer addressing. Network layer services, IP, IP addressing, IPv4, DHCP, NAT, ICMP, \nIPv6. [3 lectures] \nNetwork layer routing. Routing and forwarding, routing algorithms, routing in the Internet, \nmulticast. [3 lectures] \nTransport layer. Service models, multiplexing/demultiplexing, connection-less transport (UDP), \nprinciples of reliable data transfer, connection-oriented transport (TCP), TCP congestion control, \nTCP variants. [6 lectures] \nApplication layer. Client/server paradigm, WWW, HTTP, Domain Name System, P2P. [1.5 \nlectures] \nMultimedia networking. Networked multimedia applications, multimedia delivery requirements, \nmultimedia protocols (SIP), content distribution networks. [0.5 lecture] \nObjectives \nAt the end of the course students should \n \nbe able to analyse a communication system by separating out the different functions provided \nby the network; \nunderstand that there are fundamental limits to any communications system; \nunderstand the general principles behind multiplexing, addressing, routing, reliable transmission \nand other stateful protocols as well as specific examples of each; \nunderstand what FEC is; \nbe able to compare communications systems in how they solve similar problems; \n",
        "have an informed view of both the internal workings of the Internet and of a number of common \nInternet applications and protocols. \nRecommended reading \n* Peterson, L.L. and Davie, B.S. (2011). Computer networks: a systems approach. Morgan \nKaufmann (5th ed.). ISBN 9780123850591 \nKurose, J.F. and Ross, K.W. (2009). Computer networking: a top-down approach. \nAddison-Wesley (5th ed.). \nComer, D. and Stevens, D. (2005). Internetworking with TCP-IP, vol. 1 and 2. Prentice Hall (5th \ned.). \nStevens, W.R., Fenner, B. and Rudoff, A.M. (2003). UNIX network programming, Vol.I: The \nsockets networking API. Prentice Hall (3rd ed.).\n \n",
        "FURTHER HUMAN\u2013COMPUTER INTERACTION \n \nAims \nThis aim of this course is to provide an introduction to the theoretical foundations of Human \nComputer Interaction, and an understanding of how these can be applied to the design of \ncomplex technologies. \n \nLectures \nTheory driven approaches to HCI. What is a theory in HCI? Why take a theory driven approach \nto HCI? \nDesign of visual displays. Segmentation and variables of the display plane. Modes of \ncorrespondence. \nGoal-oriented interaction. Using cognitive theories of planning, learning and understanding to \nunderstand user behaviour, and what they find hard. \nDesigning smart systems. Using statistical methods to anticipate user needs and actions with \nBayesian strategies. \nDesigning efficient systems. Measuring and optimising human performance through quantitative \nexperimental methods. \nDesigning meaningful systems. Qualitative research methods to understand social context and \nrequirements of user experience. \nEvaluating interactive system designs. Approaches to evaluation in systems research and \nengineering, including Part II Projects. \nDesigning complex systems. Worked case studies of applying the theories to a hard HCI \nproblem. Research directions in HCI. \nObjectives \nAt the end of the course students should be able to apply theories of human performance and \ncognition to system design, including selection of appropriate techniques to analyse, observe \nand improve the usability of a wide range of technologies. \n \nRecommended reading \n* Preece, J., Sharp, H. and Rogers, Y. (2015). Interaction design: beyond human-computer \ninteraction. Wiley (Currently in 4th edition, but earlier editions will suffice). \n \nFurther reading: \n \nCarroll, J.M. (ed.) (2003). HCI models, theories and frameworks: toward a multi-disciplinary \nscience. Morgan Kaufmann.\n \n",
        "LOGIC AND PROOF \n \nAims \nThis course will teach logic, especially the predicate calculus. It will present the basic principles \nand definitions, then describe a variety of different formalisms and algorithms that can be used \nto solve problems in logic. Putting logic into the context of Computer Science, the course will \nshow how the programming language Prolog arises from the automatic proof method known as \nresolution. It will introduce topics that are important in mechanical verification, such as binary \ndecision diagrams (BDDs), SAT solvers and modal logic. \n \nLectures \nIntroduction to logic. Schematic statements. Interpretations and validity. Logical consequence. \nInference. \nPropositional logic. Basic syntax and semantics. Equivalences. Normal forms. Tautology \nchecking using CNF. \nThe sequent calculus. A simple (Hilbert-style) proof system. Natural deduction systems. \nSequent calculus rules. Sample proofs. \nFirst order logic. Basic syntax. Quantifiers. Semantics (truth definition). \nFormal reasoning in FOL. Free versus bound variables. Substitution. Equivalences for \nquantifiers. Sequent calculus rules. Examples. \nClausal proof methods. Clause form. A SAT-solving procedure. The resolution rule. Examples. \nRefinements. \nSkolem functions, Unification and Herbrand\u2019s theorem. Prenex normal form. Skolemisation. \nMost general unifiers. A unification algorithm. Herbrand models and their properties. \nResolution theorem-proving and Prolog. Binary resolution. Factorisation. Example of Prolog \nexecution. Proof by model elimination. \nSatisfiability Modulo Theories. Decision problems and procedures. How SMT solvers work. \nBinary decision diagrams. General concepts. Fast canonical form algorithm. Optimisations. \nApplications. \nModal logics. Possible worlds semantics. Truth and validity. A Hilbert-style proof system. \nSequent calculus rules. \nTableaux methods. Simplifying the sequent calculus. Examples. Adding unification. \nSkolemisation. The world\u2019s smallest theorem prover? \nObjectives \nAt the end of the course students should \n \nbe able to manipulate logical formulas accurately; \nbe able to perform proofs using the presented formal calculi; \nbe able to construct a small BDD; \nunderstand the relationships among the various calculi, e.g. SAT solving, resolution and Prolog; \nunderstand the concept of a decision procedure and the basic principles of \u201csatisfiability modulo \ntheories\u201d. \nbe able to apply the unification algorithm and to describe its uses. \nRecommended reading \n",
        "* Huth, M. and Ryan, M. (2004). Logic in computer science: modelling and reasoning about \nsystems. Cambridge University Press (2nd ed.). \nBen-Ari, M. (2001). Mathematical logic for computer science. Springer (2nd ed.).\n \n",
        "PROLOG \n \nAims \nThe aim of this course is to introduce programming in the Prolog language. Prolog encourages \na different programming style to Java or ML and particular focus is placed on programming to \nsolve real problems that are suited to this style. Practical experimentation with the language is \nstrongly encouraged. \n \nLectures \nIntroduction to Prolog. The structure of a Prolog program and how to use the Prolog interpreter. \nUnification. Some simple programs. \nArithmetic and lists. Prolog\u2019s support for evaluating arithmetic expressions and lists. The space \ncomplexity of program evaluation discussed with reference to last-call optimisation. \nBacktracking, cut, and negation. The cut operator for controlling backtracking. Negation as \nfailure and its uses. \nSearch and cut. Prolog\u2019s search method for solving problems. Graph searching exploiting \nProlog\u2019s built-in search mechanisms. \nDifference structures. Difference lists: introduction and application to example programs. \nBuilding on Prolog. How particular limitations of Prolog programs can be addressed by \ntechniques such as Constraint Logic Programming (CLP) and tabled resolution. \nObjectives \nAt the end of the course students should \n \nbe able to write programs in Prolog using techniques such as accumulators and difference \nstructures; \nknow how to model the backtracking behaviour of program execution; \nappreciate the unique perspective Prolog gives to problem solving and algorithm design; \nunderstand how larger programs can be created using the basic programming techniques used \nin this course. \nRecommended reading \n* Bratko, I. (2001). PROLOG programming for artificial intelligence. Addison-Wesley (3rd or 4th \ned.). \nSterling, L. and Shapiro, E. (1994). The art of Prolog. MIT Press (2nd ed.). \n \nFurther reading: \n \nO\u2019Keefe, R. (1990). The craft of Prolog. MIT Press. [This book is beyond the scope of this \ncourse, but it is very instructive. If you understand its contents, you\u2019re more than prepared for \nthe examination.]\n \n",
        "ARTIFICIAL INTELLIGENCE \n \nPrerequisites: Algorithms 1, Algorithms 2. In addition the course requires some mathematics, in \nparticular some use of vectors and some calculus. Part IA Natural Sciences Mathematics or \nequivalent and Discrete Mathematics are likely to be helpful although not essential. Similarly, \nelements of Machine Learning and Real World Data, Foundations of Data Science, Logic and \nProof, Prolog and Complexity Theory are likely to be useful. This course is a prerequisite for the \nPart II courses Machine Learning and Bayesian Inference and Natural Language Processing. \nThis course is a prerequisite for: Natural Language Processing \nExam: Paper 7 Question 1, 2 \nPast exam questions, Moodle, timetable \n \nAims \nThe aim of this course is to provide an introduction to some fundamental issues and algorithms \nin artificial intelligence (AI). The course approaches AI from an algorithmic, computer \nscience-centric perspective; relatively little reference is made to the complementary \nperspectives developed within psychology, neuroscience or elsewhere. The course aims to \nprovide some fundamental tools and algorithms required to produce AI systems able to exhibit \nlimited human-like abilities, particularly in the form of problem solving by search, game-playing, \nrepresenting and reasoning with knowledge, planning, and learning. \n \nLectures \nIntroduction. Alternate ways of thinking about AI. Agents as a unifying view of AI systems. [1 \nlecture] \nSearch I. Search as a fundamental paradigm for intelligent problem-solving. Simple, uninformed \nsearch algorithms. Tree search and graph search. [1 lecture] \nSearch II. More sophisticated heuristic search algorithms. The A* algorithm and its properties. \nImproving memory efficiency: the IDA* and recursive best first search algorithms. Local search \nand gradient descent. [1 lecture] \nGame-playing. Search in an adversarial environment. The minimax algorithm and its \nshortcomings. Improving minimax using alpha-beta pruning. [1 lecture] \nConstraint satisfaction problems (CSPs). Standardising search problems to a common format. \nThe backtracking algorithm for CSPs. Heuristics for improving the search for a solution. Forward \nchecking, constraint propagation and arc consistency. [1 lecture] \nBackjumping in CSPs. Backtracking, backjumping using Gaschnig\u2019s algorithm, graph-based \nbackjumping. [1 lecture] \nKnowledge representation and reasoning I. How can we represent and deal with commonsense \nknowledge and other forms of knowledge? Semantic networks, frames and rules. How can we \nuse inference in conjunction with a knowledge representation scheme to perform reasoning \nabout the world and thereby to solve problems? Inheritance, forward and backward chaining. [1 \nlecture] \nKnowledge representation and reasoning II. Knowledge representation and reasoning using first \norder logic. The frame, qualification and ramification problems. The situation calculus. [1 lecture] \n",
        "Planning I. Methods for planning in advance how to solve a problem. The STRIPS language. \nAchieving preconditions, backtracking and fixing threats by promotion or demotion: the \npartial-order planning algorithm. [1 lecture] \nPlanning II. Incorporating heuristics into partial-order planning. Planning graphs. The \nGRAPHPLAN algorithm. Planning using propositional logic. Planning as a constraint satisfaction \nproblem. [1 lecture] \nNeural Networks I. A brief introduction to supervised learning from examples. Learning as fitting \na curve to data. The perceptron. Learning by gradient descent. [1 lecture] \nNeural Networks II. Multilayer perceptrons and the backpropagation algorithm. [1 lecture] \nObjectives \nAt the end of the course students should: \n \nappreciate the distinction between the popular view of the field and the actual research results; \nappreciate the fact that the computational complexity of most AI problems requires us regularly \nto deal with approximate techniques; \nbe able to design basic problem solving methods based on AI-based search, knowledge \nrepresentation, reasoning, planning, and learning algorithms. \nRecommended reading \nThe recommended text is: \n \n* Russell, S. and Norvig, P. (2010). Artificial intelligence: a modern approach. Prentice Hall (3rd \ned.). \n \nThere are many good books available on artificial intelligence; one alternative is: \n \nPoole, D. L. and Mackworth, A. K. (2017). Artificial intelligence: foundations of computational \nagents. Cambridge University Press (2nd ed.). \n \nFor some of the material you might find it useful to consult more specialised texts, in particular: \n \nDechter, R. (2003). Constraint processing. Morgan Kaufmann. \n \nCawsey, A. (1998). The essence of artificial intelligence. Prentice Hall. \n \nGhallab, M., Nau, D. and Traverso, P. (2004). Automated planning: theory and practice. Morgan \nKaufmann. \n \nBishop, C.M. (2006). Pattern recognition and machine learning. Springer. \n \nBrachman, R.J and Levesque, H.J. (2004). Knowledge Representation and Reasoning. Morgan \nKaufmann.\n \n",
        "COMPLEXITY THEORY \n \nAims \nThe aim of the course is to introduce the theory of computational complexity. The course will \nexplain measures of the complexity of problems and of algorithms, based on time and space \nused on abstract models. Important complexity classes will be defined, and the notion of \ncompleteness established through a thorough study of NP-completeness. Applications to \ncryptography will be considered. \n \nSyllabus \nIntroduction to Complexity Theory. Decidability and complexity; lower and upper bounds; the \nChurch-Turing hypothesis. \nTime complexity. Time complexity classes; polynomial time computation and the class P. \nNon-determinism. Non-deterministic machines; the class NP; the problem 3SAT. \nNP-completeness. Reductions and completeness; the Cook-Levin theorem; graph-theoretic \nproblems. \nMore NP-complete problems. 3-colourability; scheduling, matching, set covering, and knapsack.  \ncoNP. Complement classes. NP \u2229 coNP; primality and factorisation. \nCryptography. One-way functions; the class UP. \nSpace complexity. Space complexity classes; Savitch\u2019s theorem. \nHierarchy theorems. Time and space hierarchy theorems. \nRandomised algorithms. The class BPP; derandomisation; error-correcting codes, \ncommunication complexity. \nQuantum Complexity. The classes BQP and QMA. \nInteractive Proofs. IP=PSPACE; Zero Knowledge Proofs; Probabilistically Checkable Proofs \n(PCPs).  \n  \nObjectives \nAt the end of the course students should \n \nbe able to analyse practical problems and classify them according to their complexity; \nbe familiar with the phenomenon of NP-completeness, and be able to identify problems that are \nNP-complete; \nbe aware of a variety of complexity classes and their interrelationships; \nunderstand the role of complexity analysis in cryptography. \nRecommended reading \nA Survey of P vs. NP by Scott Aaronson. \nComputational Complexity: A Modern Approach by Arora and Barak \nComputational Complexity: A Conceptual Perspective by Oded Goldreich \nMathematics and Computation by Avi Wigderson\n \n",
        "CYBERSECURITY \n \nAims \nIn today\u2019s digital society, computer security is vital for commercial competitiveness, national \nsecurity and privacy of individuals. Protection against both criminal and state-sponsored attacks \nwill need a large cohort of skilled individuals with an understanding of the principles of security \nand with practical experience of the application of these principles. We want you to become one \nof them. In this adversarial game, to defeat the bad guys, the good guys have to be at least as \nskilled at them. Therefore this course has a strong foundation of becoming proficient at actual \nattacks. A theoretical understanding is of course necessary, but without some practice the bad \nguys will run circles around the good guys. In 12 hours I can\u2019t bring you from zero to the stage \nwhere you can invent new attacks and countermeasures, but at least by practicing and \ndissecting common attacks (akin to \u201cstudying the classics\u201d) I\u2019ll give you a feeling for the kind of \nadversarial thinking that is required in this field. Large portions of this course are hands-on: you \nwill need to acquire the relevant skills rather than just reading about it. The recommended \ncourse textbook will be especially helpful to those with no prior low-level hacking experience. \n \nLectures \nIntroduction. \nThe adversarial nature of security; thinking like the attacker; confidentiality, integrity and \navailability; systems-level thinking; Trusted Computing Base; role of cryptography. \n \nFundamentals of access control (chapter 1). \nDiscretionary vs mandatory access control; discretionary access control in POSIX; file \npermissions, file ownership, groups, permission bits. \n \nSoftware security (spanning 4 book chapters) \nSetuid (chapter 2): privileged programs, attack surfaces, exploitable vulnerabilities, privilege \nescalation from regular user to root. \nBuffer overflow (chapter 4): stack memory layout, exploiting a buffer overflow vulnerability, \nguessing unknown addresses, payload, countermeasures and counter-countermeasures. \nReturn to libc and return-oriented programming (chapter 5): exploiting a non-executable stack, \nchaining function calls, chaining ROP gadgets. \nSQL injection (chapter 14): vulnerability and its exploitation, countermeasures, input filtering, \nprepared statements. \n \nAuthentication. \nSomething you know, have, are; passwords, their dominance, their shortcomings and the many \nattempts at replacing them; password cracking; tokens; biometrics; taxonomy of Single Sign-On \nsystems; password managers. \n \nWeb and internet security (spanning 3 book chapters). \nCross-Site Request Forgery (chapter 12): why cross-site requests, CSRF attacks on HTTP GET \nand HTTP POST, countermeasures. \n",
        "Cross-Site Scripting (chapter 13): non-persistent vs persistent XSS, javascript, self-propagating \nXSS worm, countermeasures; \n \nHuman factors. \nUsers are not the enemy; Compliance budget; Prospect theory; 7 principles for systems \nsecurity. \n \nAdditional topics. \nViruses, worms and quines; lockpicking; privilege escalation in physical locks; conclusions. \n \nTo complete the course successfully, students must acquire the practical ability to carry out (as \nopposed to just describing) the exploits mentioned in the syllabus, given a vulnerable system. \nThe low-level hands-on portions of the course are supported by the very helpful course \ntextbook. Those who choose not to study on the recommended textbook will be severely \ndisadvantaged. \n \nRecommended textbook: Wenliang Du. Computer Security: A Hands-on Approach. 3rd Edition. \nISBN: 978-17330039-5-7. Published: 1 May 2022. \n \nhttps://www.handsonsecurity.net. Some chapters freely available online. \n \n \nOptional additional books (not substitutes): \n \nRoss Anderson, Security Engineering 3rd Ed, Wiley, 2020. ISBN: 978-1-119-64278-7. \nhttps://www.cl.cam.ac.uk/~rja14/book.html. Some chapters freely available online. \n \nPaul van Oorschot, Computer Security and the Internet, Springer, 2020. ISBN: \n978-3-030-33648-6 (hardcopy), 978-3-030-33649-3 (eBook). \nhttps://people.scs.carleton.ca/~paulv/toolsjewels.html. All chapters freely available online.\n",
        "FORMAL MODELS OF LANGUAGE \n \nAims \nThis course studies formal models of language and considers how they might be relevant to the \nprocessing and acquisition of natural languages. The course will extend knowledge of formal \nlanguage theory; introduce several new grammars; and use concepts from information theory to \ndescribe natural language. \n \nLectures \nNatural language and the Chomsky hierarchy 1. Recap classes of language. Closure properties \nof language classes. Recap pumping lemma for regular languages. Discussion of relevance (or \nnot) to natural languages (example embedded clauses in English). \nNatural language and the Chomsky hierarchy 2. Pumping lemma for context free languages. \nDiscussion of relevance (or not) to natural languages (example Swiss-German cross serial \ndependancies). Properties of minimally context sensitive languages. Introduction to tree \nadjoining grammars. \nLanguage processing and context free grammar parsing 1. Recap of context free grammar \nparsing. Language processing predictions based on top down parsing models (example Yngve\u2019s \nlanguage processing predictions). Language processing predictions based on probabilistic \nparsing (example Halle\u2019s language processing predictions). \nLanguage processing and context free grammar parsing 2. Introduction to context free grammar \nequivalent dependancy grammars. Language processing predictions based on Shift-Reduce \nparsing (examples prosodic look-ahead parsers, Parsey McParseface). \nGrammar induction of language classes. Introduction to grammar induction. Discussion of \nrelevance (or not) to natural language acquisition. Gold\u2019s theorem. Introduction to context free \ngrammar equivalent categorial grammars and their learnable classes. \nNatural language and information theory 1. Entropy and natural language typology. Uniform \ninformation density as a predictor for language processing. \nNatural language and information theory 2. Noisy channel encoding as a model for spelling \nerror, translation and language processing. \nVector space models and word vectors. Introduction to word vectors (example Word2Vec). Word \nvectors as predictors for semantic language processing. \nObjectives \nAt the end of the course students should \n \nunderstand how known natural languages relate to formal languages in the Chomsky hierarchy; \nhave knowledge of several context free grammars equivalents; \nunderstand how we might make predictions about language processing and language \nacquisition from formal models; \nknow how to use information theoretic concepts to describe aspects of natural language. \nRecommended reading \n* Jurafsky, D. and Martin, J. (2008). Speech and language processing. Prentice Hall. \nManning, C.D. and Schutze, H. (1999) Foundations of statistical natural language processing. \nMIT Press. \n",
        "Ruslan, M. (2003) The Oxford handbook of computational linguistics. Oxford University Press. \nClark, A., Fox, C. and Lappin, S. (2010) The handbook of computational linguistics and natural \nlanguage processing. Wiley-Blackwell. \nKozen, D. (1997) Automata and computibility. Springer.\n \n",
        "5th Semester \nBIOINFORMATICS \nAims \nThis course focuses on algorithms used in Bioinformatics and System Biology. Most of the \nalgorithms are general and can be applied in other fields on multidimensional and noisy data. All \nthe necessary biological terms and concepts useful for the course and the examination will be \ngiven in the lectures. The most important software implementing the described algorithms will be \ndemonstrated. \n \nLectures \nIntroduction to biological data: Bioinformatics as an interesting field in computer science. \nComputing and storing information with DNA (including Adleman\u2019s experiment). \nDynamic programming. Longest common subsequence, DNA global and local alignment, linear \nspace alignment, Nussinov algorithm for RNA, heuristics for multiple alignment. (Vol. 1, chapter \n5) \nSequence database search. Blast. (see notes and textbooks) \nGenome sequencing. De Bruijn graph. (Vol. 1, chapter 3) \nPhylogeny. Distance based algorithms (UPGMA, Neighbour-Joining). Parsimony-based \nalgorithms. Examples in Computer Science. (Vol. 2, chapter 7) \nClustering. Hard and soft K-means clustering, use of Expectation Maximization in clustering, \nHierarchical clustering, Markov clustering algorithm. (Vol. 2, chapter 8) \nGenomics Pattern Matching. Suffix Tree String Compression and the Burrows-Wheeler \nTransform. (Vol. 2, chapter 9) \nHidden Markov Models. The Viterbi algorithm, profile HMMs for sequence alignment, classifying \nproteins with profile HMMs, soft decoding problem, Baum-Welch learning. (Vol. 2, chapter 10) \nObjectives \nAt the end of this course students should \n \nunderstand Bioinformatics terminology; \nhave mastered the most important algorithms in the field; \nbe able to work with bioinformaticians and biologists; \nbe able to find data and literature in repositories. \nRecommended reading \n* Compeau, P. and Pevzner, P.A. (2015). Bioinformatics algorithms: an active learning approach. \nActive Learning Publishers. \nDurbin, R., Eddy, S., Krough, A. and Mitchison, G. (1998). Biological sequence analysis: \nprobabilistic models of proteins and nucleic acids. Cambridge University Press. \nJones, N.C. and Pevzner, P.A. (2004). An introduction to bioinformatics algorithms. MIT Press. \nFelsenstein, J. (2003). Inferring phylogenies. Sinauer Associates.\n \n",
        "BUSINESS STUDIES \n \nAims \nHow to start and run a computer company; the aims of this course are to introduce students to \nall the things that go to making a successful project or product other than just the programming. \nThe course will survey some of the issues that students are likely to encounter in the world of \ncommerce and that need to be considered when setting up a new computer company. \n \nSee also Business Seminars in the Easter Term. \n \nLectures \nSo you\u2019ve got an idea? Introduction. Why are you doing it and what is it? Types of company. \nMarket analysis. The business plan. \nMoney and tools for its management. Introduction to accounting: profit and loss, cash flow, \nbalance sheet, budgets. Sources of finance. Stocks and shares. Options and futures. \nSetting up: legal aspects. Company formation. Brief introduction to business law; duties of \ndirectors. Shares, stock options, profit share schemes and the like. Intellectual Property Rights, \npatents, trademarks and copyright. Company culture and management theory. \nPeople. Motivating factors. Groups and teams. Ego. Hiring and firing: employment law. \nInterviews. Meeting techniques. \nProject planning and management. Role of a manager. PERT and GANTT charts, and critical \npath analysis. Estimation techniques. Monitoring. \nQuality, maintenance and documentation. Development cycle. Productization. Plan for quality. \nPlan for maintenance. Plan for documentation. \nMarketing and selling. Sales and marketing are different. Marketing; channels; marketing \ncommunications. Stages in selling. Control and commissions. \nGrowth and exit routes. New markets: horizontal and vertical expansion. Problems of growth; \nsecond system effects. Management structures. Communication. Exit routes: acquisition, \nfloatation, MBO or liquidation. Futures: some emerging ideas for new computer businesses. \nSummary. Conclusion: now you do it! \nObjectives \nAt the end of the course students should \n \nbe able to write and analyse a business plan; \nknow how to construct PERT and GANTT diagrams and perform critical path analysis; \nappreciate the differences between profitability and cash flow, and have some notion of budget \nestimation; \nhave an outline view of company formation, share structure, capital raising, growth and exit \nroutes; \nhave been introduced to concepts of team formation and management; \nknow about quality documentation and productization processes; \nunderstand the rudiments of marketing and the sales process. \nRecommended reading \n",
        "Lang, J. (2001). The high-tech entrepreneur\u2019s handbook: how to start and run a high-tech \ncompany. FT.COM/Prentice Hall. \n \nStudents will be expected to be able to use Microsoft Excel and Microsoft Project. \n \nFor additional reading on a lecture-by-lecture basis, please see the course website. \n \nStudents are strongly recommended to enter the CU Entrepreneurs Business Ideas Competition \nhttp://www.cue.org.uk/\n \n",
        "DENOTATIONAL SEMANTICS \n \nAims \nThe aims of this course are to introduce domain theory and denotational semantics, and to \nshow how they provide a mathematical basis for reasoning about the behaviour of programming \nlanguages. \n \nLectures \nIntroduction.The denotational approach to the semantics of programming \nlanguages.Recursively defined objects as limits of successive approximations. \nLeast fixed points.Complete partial orders (cpos) and least elements.Continuous functions and \nleast fixed points. \nConstructions on domains.Flat domains.Product domains. Function domains. \nScott induction.Chain-closed and admissible subsets of cpos and domains.Scott\u2019s fixed-point \ninduction principle. \nPCF.The Scott-Plotkin language PCF.Evaluation. Contextual equivalence. \nDenotational semantics of PCF.Denotation of types and terms. Compositionality. Soundness \nwith respect to evaluation. [2 lectures]. \nRelating denotational and operational semantics.Formal approximation relation and its \nfundamental property.Computational adequacy of the PCF denotational semantics with respect \nto evaluation. Extensionality properties of contextual equivalence. [2 lectures]. \nFull abstraction.Failure of full abstraction for the domain model. PCF with parallel or. \nObjectives \nAt the end of the course students should \n \nbe familiar with basic domain theory: cpos, continuous functions, admissible subsets, least fixed \npoints, basic constructions on domains; \nbe able to give denotational semantics to simple programming languages with simple types; \nbe able to apply denotational semantics; in particular, to understand the use of least fixed points \nto model recursive programs and be able to reason about least fixed points and simple \nrecursive programs using fixed point induction; \nunderstand the issues concerning the relation between denotational and operational semantics, \nadequacy and full abstraction, especially with respect to the language PCF. \nRecommended reading \nWinskel, G. (1993). The formal semantics of programming languages: an introduction. MIT \nPress. \nGunter, C. (1992). Semantics of programming languages: structures and techniques. MIT Press. \nTennent, R. (1991). Semantics of programming languages. Prentice Hall.\n \n",
        "INFORMATION THEORY \n \nAims \nThis course introduces the principles and applications of information theory: how information is \nmeasured in terms of probability and various entropies, how these are used to calculate the \ncapacity of communication channels, with or without noise, and to measure how much random \nvariables reveal about each other. Coding schemes including error correcting codes are studied \nalong with data compression, spectral analysis, transforms, and wavelet coding. Applications of \ninformation theory are reviewed, from astrophysics to pattern recognition. \n \nLectures \nInformation, probability, uncertainty, and surprise. How concepts of randomness and uncertainty \nare related to information. How the metrics of information are grounded in the rules of \nprobability. Shannon Information. Weighing problems and other examples. \nEntropy of discrete variables. Definition and link to Shannon information. Joint entropy, Mutual \ninformation. Visual depictions of the relationships between entropy types. Why entropy gives \nfundamental measures of information content. \nSource coding theorem and data compression; prefix, variable-, and fixed-length codes. \nInformation rates; Asymptotic equipartition principle; Symbol codes; Huffman codes and the \nprefix property. Binary symmetric channels. Capacity of a noiseless discrete channel. Stream \ncodes. \nNoisy discrete channel coding. Joint distributions; mutual information; Conditional Entropy; \nError-correcting codes; Capacity of a discrete channel. Noisy channel coding theorem. \nEntropy of continuous variables. Differential entropy; Mutual information; Channel Capacity; \nGaussian channels. \nEntropy for comparing probability distributions and for machine learning. Relative entropy/KL \ndivergence; cross-entropy; use as loss function. \nApplications of information theory in other sciences.  \nObjectives \nAt the end of the course students should be able to \n \ncalculate the information content of a random variable from its probability distribution; \nrelate the joint, conditional, and marginal entropies of variables in terms of their coupled \nprobabilities; \ndefine channel capacities and properties using Shannon\u2019s Theorems; \nconstruct efficient codes for data on imperfect communication channels; \ngeneralize the discrete concepts to continuous signals on continuous channels; \nunderstand encoding and communication schemes in terms of the spectral properties of signals \nand channels; \ndescribe compression schemes, and efficient coding using wavelets and other representations \nfor data. \nRecommended reading \n  Mackay's Information Theory, inference and Learning Algorithms \n \n",
        " Stone's Information Theory: A Tutorial Introduction.\n \n",
        "LATEX AND JULIA \n \nAims \nIntroduction to two widely-used languages for typesetting dissertations and scientific \npublications, for prototyping numerical algorithms and to visualize results. \n \nLectures \nLATEX. Workflow example, syntax, typesetting conventions, non-ASCII characters, document \nstructure, packages, mathematical typesetting, graphics and figures, cross references, build \ntools. \nJulia. Tools for technical computing and visualization. The Array type and its operators, 2D/3D \nplotting, functions and methods, notebooks, packages, vectorized audio demonstration. \nObjectives \nStudents should be able to avoid the most common LATEX mistakes, to prototype simple image \nand signal-processing algorithms in Julia, and to visualize the results. \n \nRecommended reading \n* Lamport, L. (1994). LATEX \u2013 a documentation preparation system user\u2019s guide and reference \nmanual. Addison-Wesley (2nd ed.). \nMittelbach, F., et al. (2023). The LATEX companion. Addison-Wesley (3rd ed.).\n \n",
        "PRINCIPLES OF COMMUNICATIONS \n \nAims \nThis course aims to provide a detailed understanding of the underlying principles for how \ncommunications systems operate. Practical examples (from wired and wireless \ncommunications, the Internet, and other communications systems) are used to illustrate the \nprinciples. \n \nLectures \nIntroduction. Course overview. Abstraction, layering. Review of structure of real networks, links, \nend systems and switching systems. [1 lecture] \nRouting. Central versus Distributed Routing Policy Routing. Multicast Routing Circuit Routing [6 \nlectures] \nFlow control and resource optimisation. 1[Control theory] is a branch of engineering familiar to \npeople building dynamic machines. It can be applied to network traffic. Stemming the flood, at \nsource, sink, or in between? Optimisation as a model of networkand user. TCP in the wild. [3 \nlectures] \nPacket Scheduling. Design choices for scheduling and queue management algorithms for \npacket forwarding, and fairness. [2 lectures] \nThe big picture for managing traffic. Economics and policy are relevant to networks in many \nways. Optimisation and game theory are both relevant topics discussed here. [2 lectures] \nSystem Structures and Summary. Abstraction, layering. The structure of real networks, links, \nend systems and switching. [2 lectures] \n1 Control theory was not taught and will not be the subject of any exam question. \n \nObjectives \nAt the end of the course students should be able to explain the underlying design and behaviour \nof protocols and networks, including capacity, topology, control and use. Several specific \nmathematical approaches are covered (control theory, optimisation). \n \nRecommended reading \n* Keshav, S. (2012). Mathematical Foundations of Computer Networking. Addison Wesley. ISBN \n9780321792105 \nBackground reading: \nKeshav, S. (1997). An engineering approach to computer networking. Addison-Wesley (1st ed.). \nISBN 0201634422 \nStevens, W.R. (1994). TCP/IP illustrated, vol. 1: the protocols. Addison-Wesley (1st ed.). ISBN \n0201633469\n \n",
        "TYPES \n \nAims \nThe aim of this course is to show by example how type systems for programming languages can \nbe defined and their properties developed, using techniques that were introduced in the Part IB \ncourse on Semantics of Programming Languages. The emphasis is on type systems for \nfunctional languages and their connection to constructive logic. \n \nLectures \nIntroduction. The role of type systems in programming languages. Review of rule-based \nformalisation of type systems. [1 lecture] \nPropositions as types. The Curry-Howard correspondence between intuitionistic propositional \ncalculus and simply-typed lambda calculus. Inductive types and iteration. Consistency and \ntermination. [2 lectures] \nPolymorphic lambda calculus (PLC). PLC syntax and reduction semantics. Examples of \ndatatypes definable in the polymorphic lambda calculus. Type inference. [3 lectures] \nMonads and effects. Explicit versus implicit effects. Using monadic types to control effects. \nReferences and polymorphism. Recursion and looping. [2 lectures] \nContinuations and classical logic. First-class continuations and control operators. Continuations \nas Curry-Howard for classical logic. Continuation-passing style. [2 lectures] \nDependent types. Dependent function types. Indexed datatypes. Equality types and combining \nproofs with programming. [2 lectures] \nObjectives \nAt the end of the course students should \n \nbe able to use a rule-based specification of a type system to carry out type checking and type \ninference; \nunderstand by example the Curry-Howard correspondence between type systems and logics; \nunderstand how types can be used to control side-effects in programming; \nappreciate the expressive power of parametric polymorphism and dependent types. \nRecommended reading \n* Pierce, B.C. (2002). Types and programming languages. MIT Press. \nPierce, B. C. (Ed.) (2005). Advanced Topics in Types and Programming Languages. MIT Press. \nGirard, J-Y. (tr. Taylor, P. and Lafont, Y.) (1989). Proofs and types. Cambridge University Press.\n",
        "ADVANCED DATA SCIENCE \n \nPracticals \nThe module will have a practical session for each of the three stages of the pipeline. Each of the \nparts will be tied into an aspect of the final project. \n \nObjectives \nAt the end of the course students will be familiar with the purpose of data science, how it differs \nfrom the closely related fields of machine learning, statistics and artificial intelligence and what a \ntypical data analysis pipeline looks like in practice. As well as emphasising the importance of \nanalysis methods we will introduce a formalism for organising how data science is done in \npractice and what the different aspects the data scientist faces when giving data-driven answers \nto questions of interest. \n \nRecommended reading \nSimon Rogers et al. (2016). A First Course in Machine Learning, Second Edition. [] Chapman \nand Hall/CRC, nil \nShai Shalev-Shwartz et al. (2014). Understanding Machine Learning: From Theory to \nAlgorithms. New York, NY, USA: Cambridge  University Press \nChristopher M. Bishop (2006). Pattern Recognition and Machine Learning (Information Science \nand Statistics). Secaucus, NJ, USA: Springer-Verlag New York, Inc. \nAssessment \nPractical There will be four practicals contributing 20% to the final module mark. Data science is \na topic where there is rarely a single \ncorrect answer therefore the important thing is that an informed attempt has been made to \naddress the questions that can be supported and motivated. \nReport The course will be concluded by an individual report covering the material in the course \nthrough \npractical exercise working with real data. This report will make up 80% of the mark for the \ncourse. \n \n \n",
        "AFFECTIVE ARTIFICIAL INTELLIGENCE \n \nSynopsis \nAffective Artificial Intelligence (Affective AI) aims to imbue machines with social and emotional \nintelligence (EQ). More specifically, Affective AI aims to create artificially intelligent systems and \nmachines that can recognize, interpret, process, and simulate human social signals and \nbehaviours, expressions, and emotions, to enhance human-AI interaction and communication. \n\\r\\n\\r\\nTo achieve this goal, Affective AI draws upon various scientific disciplines, including \nmachine learning, computer vision, speech / natural language / signal processing, psychology \nand cognitive science, and ethics and social sciences. \n \n  \nBackground & Aims \nAffective Artificial Intelligence (Affective AI) aims to imbue machines with social and emotional \nintelligence (EQ). More specifically, Affective AI aims to create artificially intelligent systems and \nmachines that can recognize, interpret, process, and simulate human social signals and \nbehaviours, expressions, and emotions, to enhance human-AI interaction and communication.  \n \nTo achieve this goal, Affective AI draws upon various scientific disciplines, including machine \nlearning, computer vision, speech / natural language / signal processing, psychology and \ncognitive science, and ethics and social sciences. \n \nAffective AI has direct applications in and implications for the design of innovative interactive \ntechnology (e.g., interaction with chat bots, virtual agents, robots), single and multi-user smart \nenvironments ( e.g., in-car/ virtual / augmented / mixed reality, serious games), public speaking \nand cognitive training, and clinical and biomedical studies (e.g., autism, depression, pain). \n \nThe aim of this module is to impart knowledge and ability needed to make informed choices of \nmodels, data, and machine learning techniques for sensing, recognition, and generation of \naffective and social behaviour (e.g., smile, frown, head nodding/shaking, \nagreement/disagreement) in order to create Affectively intelligent AI systems, with a \nconsideration for various ethical issues (e.g., privacy, bias) arising from the real-world \ndeployment of these systems. \n \n  \n \nSyllabus \nThe following list provides a representative list of topics: \n \nIntroduction, definitions, and overview \nTheories from various disciplines \nSensing from multiple modalities (e.g., vision, audio, bio signals, text) \nData acquisition and annotation \nSignal processing / feature extraction \n",
        "Learning / prediction / recognition and evaluation \nBehaviour synthesis / generation (e.g., for embodied agents / robots) \nAdvanced topics and ethical considerations (e.g., bias and fairness) \nCross-disciplinary applications (via seminar presentations and discussions) \nGuest lectures (diverse topics \u2013 changes each year) \nHands-on research and programming work (i.e., mini project & report)  \n \n  \n \nObjectives \nOn completion of this module, students will: \n \nunderstand and demonstrate knowledge in key characteristics of affectively intelligent AI, which \ninclude: \nRecognition: How to equip Affective AI systems with capabilities of analysing facial expressions, \nvocal intonations, gestures, and other physiological signals to infer human affective states? \nGeneration: How to enable affectively intelligent AI simulate expressions of emotions in \nmachines, allowing them to respond in a more human-like manner, such as virtual agents and \nhumanoid robots, showing empathy or sympathy? \nAdaptation and Personalization: How to enable affectively intelligent AI adapt system responses \nbased on users' affective states and/or needs, or tailor interactions and experiences to individual \nusers based on their expressivity / emotional profiles, personalities, and past interactions? \nEmpathetic Communication: How to design Affective AI systems to communicate with users in a \nway that demonstrates empathy, understanding, and sensitivity to their affective states and \nneeds? \nEthical and societal considerations: What are the various human differences, ethical guidelines, \nand societal impacts that need to be considered when designing and deploying Affective AI to \nensure that these systems respect users' privacy, autonomy, and well-being? \ncomprehend and apply (appropriate) methods for collection, analysis, representation, and \nevaluation of human affective and communicative behavioural data. \nenhance programming skills for creating and implementing (components of) Affective AI \nsystems. \ndemonstrate critical thinking, analysis and synthesis while deciding on 'when' and 'how' to \nincorporate human affect and social signals in a specific AI system context and gain practical \nexperience in proposing and justifying computational solution(s) of suitable nature and scope. \n  \n \nAssessment - Part II Students \nSeminar presentation: 20% \nParticipating in Q&A and discussions: 10% \nMid-term mini project report and presentation (as a team of 2): 10%  \nFinal mini project report & code (as a team of 2): 60% \n \n",
        "CATEGORY THEORY \n \nAims \nCategory theory provides a unified treatment of mathematical properties and constructions that \ncan be expressed in terms of 'morphisms' between structures. It gives a precise framework for \ncomparing one branch of mathematics (organized as a category) with another and for the \ntransfer of problems in one area to another. Since its origins in the 1940s motivated by \nconnections between algebra and geometry, category theory has been applied to diverse fields, \nincluding computer science, logic and linguistics. This course introduces the basic notions of \ncategory theory: adjunction, natural transformation, functor and category. We will use category \ntheory to organize and develop the kinds of structure that arise in models and semantics for \nlogics and programming languages. \n \nSyllabus \nIntroduction; some history. Definition of category. The category of sets and functions. \nCommutative diagrams. Examples of categories: preorders and monotone functions; monoids \nand monoid homomorphisms; a preorder as a category; a monoid as a category. Definition of \nisomorphism. Informal notion of a 'category-theoretic' property. \nTerminal objects. The opposite of a category and the duality principle. Initial objects. Free \nmonoids as initial objects. \nBinary products and coproducts. Cartesian categories. \nExponential objects: in the category of sets and in general. Cartesian closed categories: \ndefinition and examples. \nIntuitionistic Propositional Logic (IPL) in Natural Deduction style. Semantics of IPL in a cartesian \nclosed preorder. \nSimply Typed Lambda Calculus (STLC). The typing relation. Semantics of STLC types and \nterms in a cartesian closed category (ccc). The internal language of a ccc. The \nCurry-Howard-Lawvere correspondence. \nFunctors. Contravariance. Identity and composition for functors. \nSize: small categories and locally small categories. The category of small categories. Finite \nproducts of categories. \nNatural transformations. Functor categories. The category of small categories is cartesian \nclosed. \nHom functors. Natural isomorphisms. Adjunctions. Examples of adjoint functors. Theorem \ncharacterising the existence of right (respectively left) adjoints in terms of a universal property. \nDependent types. Dependent product sets and dependent function sets as adjoint functors. \nEquivalence of categories. Example: the category of I-indexed sets and functions is equivalent \nto the slice category Set/I. \nPresheaves. The Yoneda Lemma. Categories of presheaves are cartesian closed. \nMonads. Modelling notions of computation as monads. Moggi's computational lambda calculus. \nObjectives \nOn completion of this module, students should: \n \n",
        "be familiar with some of the basic notions of category theory and its connections with logic and \nprogramming language semantics \nAssessment \na graded exercise sheet (25% of the final mark), and \na take-home test (75%) \nRecommended reading \nAwodey, S. (2010). Category theory. Oxford University Press (2nd ed.). \n \nCrole, R. L. (1994). Categories for types. Cambridge University Press. \n \nLambek, J. and Scott, P. J. (1986). Introduction to higher order categorical logic. Cambridge \nUniversity Press. \n \nPitts, A. M. (2000). Categorical Logic. Chapter 2 of S. Abramsky, D. M. Gabbay and T. S. E. \nMaibaum (Eds) Handbook of Logic in Computer Science, Volume 5. Oxford University Press. \n(Draft copy available here.) \n \nClass Size \nThis module can accommodate upto 15 Part II students plus 15 MPhil / Part III students.\n",
        "DIGITAL SIGNAL PROCESSING \n \nAims \nThis course teaches basic signal-processing principles necessary to understand many modern \nhigh-tech systems, with examples from audio processing, image coding, radio communication, \nradar, and software-defined radio. Students will gain practical experience from numerical \nexperiments in programming assignments (in Julia, MATLAB or NumPy). \n \nLectures \nSignals and systems. Discrete sequences and systems: types and properties. Amplitude, \nphase, frequency, modulation, decibels, root-mean square. Linear time-invariant systems, \nconvolution. Some examples from electronics, optics and acoustics. \nPhasors. Eigen functions of linear time-invariant systems. Review of complex arithmetic. \nPhasors as orthogonal base functions. \nFourier transform. Forms and properties of the Fourier transform. Convolution theorem. Rect \nand sinc. \nDirac\u2019s delta function. Fourier representation of sine waves, impulse combs in the time and \nfrequency domain. Amplitude-modulation in the frequency domain. \nDiscrete sequences and spectra. Sampling of continuous signals, periodic signals, aliasing, \ninterpolation, sampling and reconstruction, sample-rate conversion, oversampling, spectral \ninversion. \nDiscrete Fourier transform. Continuous versus discrete Fourier transform, symmetry, linearity, \nFFT, real-valued FFT, FFT-based convolution, zero padding, FFT-based resampling, \ndeconvolution exercise. \nSpectral estimation. Short-time Fourier transform, leakage and scalloping phenomena, \nwindowing, zero padding. Audio and voice examples. DTFM exercise. \nFinite impulse-response filters. Properties of filters, implementation forms, window-based FIR \ndesign, use of frequency-inversion to obtain high-pass filters, use of modulation to obtain \nband-pass filters. \nInfinite impulse-response filters. Sequences as polynomials, z-transform, zeros and poles, some \nanalog IIR design techniques (Butterworth, Chebyshev I/II, elliptic filters, second-order cascade \nform). \nBand-pass signals. Band-pass sampling and reconstruction, IQ up and down conversion, \nsuperheterodyne receivers, software-defined radio front-ends, IQ representation of AM and FM \nsignals and their demodulation. \nDigital communication. Pulse-amplitude modulation. Matched-filter detector. Pulse shapes, \ninter-symbol interference, equalization. IQ representation of ASK, BSK, PSK, QAM and FSK \nsignals. [2 hours] \nRandom sequences and noise. Random variables, stationary and ergodic processes, \nautocorrelation, cross-correlation, deterministic cross-correlation sequences, filtered random \nsequences, white noise, periodic averaging. \nCorrelation coding. Entropy, delta coding, linear prediction, dependence versus correlation, \nrandom vectors, covariance, decorrelation, matrix diagonalization, eigen decomposition, \n",
        "Karhunen-Lo\u00e8ve transform, principal component analysis. Relation to orthogonal transform \ncoding using fixed basis vectors, such as DCT. \nLossy versus lossless compression. What information is discarded by human senses and can \nbe eliminated by encoders? Perceptual scales, audio masking, spatial resolution, colour \ncoordinates, some demonstration experiments. \nQuantization, image coding standards. Uniform and logarithmic quantization, A/\u00b5-law coding, \ndithering, JPEG. \nObjectives \napply basic properties of time-invariant linear systems; \nunderstand sampling, aliasing, convolution, filtering, the pitfalls of spectral estimation; \nexplain the above in time and frequency domain representations; \nuse filter-design software; \nvisualize and discuss digital filters in the z-domain; \nuse the FFT for convolution, deconvolution, filtering; \nimplement, apply and evaluate simple DSP applications; \nfamiliarity with a number of signal-processing concepts used in digital communication systems \nRecommended reading \nLyons, R.G. (2010). Understanding digital signal processing. Prentice Hall (3rd ed.). \nOppenheim, A.V. and Schafer, R.W. (2007). Discrete-time digital signal processing. Prentice \nHall (3rd ed.). \nStein, J. (2000). Digital signal processing \u2013 a computer science perspective. Wiley. \n \nClass size \nThis module can accommodate a maximum of 24 students (16 Part II students and 8 MPhil \nstudents) \n \nAssessment - Part II students \nThree homework programming assignments, each comprising 20% of the mark \nWritten test, comprising 40% of the total mark.\n \n",
        "MACHINE VISUAL PERCEPTION \n \nAims \nThis course aims at introducing the theoretical foundations and practical techniques for machine \nperception, the capability of computers to interpret data resulting from sensor measurements. \nThe course will teach the fundamentals and modern techniques of machine perception, i.e. \nreconstructing the real world starting from sensor measurements with a focus on machine \nperception for visual data. The topics covered will be image/geometry representations for \nmachine perception, semantic segmentation, object detection and recognition, geometry \ncapture, appearance modeling and acquisition, motion detection and estimation, \nhuman-in-the-loop machine perception, select topics in applied machine perception. \n \nMachine perception/computer vision is a rapidly expanding area of research with real-world \napplications. An understanding of machine perception is also important for robotics, interactive \ngraphics (especially AR/VR), applied machine learning, and several other fields and industries. \nThis course will provide a fundamental understanding of and practical experience with the \nrelevant techniques. \n \nLearning outcomes \nStudents will understand the theoretical underpinnings of the modern machine perception \ntechniques for reconstructing models of reality starting from an incomplete and imperfect view of \nreality. \nStudents will be able to apply machine perception theory to solve practical problems, e.g. \nclassification of images, geometry capture. \nStudents will gain an understanding of which machine perception techniques are appropriate for \ndifferent tasks and scenarios. \nStudents will have hands-on experience with some of these techniques via developing a \nfunctional machine perception system in their projects. \nStudents will have practical experience with the current prominent machine perception \nframeworks. \nSyllabus \nThe fundamentals of machine learning for machine perception \nDeep neural networks and frameworks for machine perception \nSemantic segmentation of objects and humans \nObject detection and recognition \nMotion estimation, tracking and recognition \n3D geometry capture \nAppearance modeling and acquisition \nSelect topics in applied machine perception \nAssessment \nA practical exercise, worth 20% of the mark. This will cover the basics and theory of machine \nperception and some of the practical techniques the students will likely use in their projects. This \nis individual work. No GPU hours will be needed for the practical work. \nA machine perception project worth 80% of the marks: \n",
        "Course projects will be selected by the students following the possible project themes proposed \nby the lecturer, and will be checked by the lecturer for appropriateness.  \nThe students will form groups of 2-3 to design, implement, report, and present a project to tackle \na given task in machine perception. \nThe final mark will be composed of an implementation/report mark (60%) and a presentation \nmark (20%). Each team member will be evaluated based on her/his contribution. \nEach project will have extensions to be completed only by the ACS students. Each student will \nwrite a different part of the report, whose author will be clearly marked. Each student will further \nsummarise her/his contributions to the project in the same report. \nRecommended Reading List \nComputer Vision: Algorithms and Applications, Richard Szeliski, Springer, 2010. \nDeep Learning, Ian Goodfellow, Yoshua Bengio, and Aaron Courville, MIT Press, 2016. \nMachine Learning and Visual Perception, Baochang Zhang, De Gruyter, 2020.\n \n",
        "NATURAL LANGUAGE PROCESSING \n \nAims \nThis course introduces the fundamental techniques of natural language processing. It aims to \nexplain the potential and the main limitations of these techniques. Some current research issues \nare introduced and some current and potential applications discussed and evaluated. Students \nwill also be introduced to practical experimentation in natural language processing. \n \nLectures \nOverview. Brief history of NLP research, some current applications, components of NLP \nsystems. \nMorphology and Finite State Techniques. Morphology in different languages, importance of \nmorphological analysis in NLP, finite-state techniques in NLP. \nPart-of-Speech Tagging and Log-Linear Models. Lexical categories, word tagging, corpora and \nannotations, empirical evaluation. \nPhrase Structure and Structure Prediction. Phrase structures, structured prediction, context-free \ngrammars, weights and probabilities. Some limitations of context-free grammars. \nDependency Parsing. Dependency structure, grammar-free parsing, incremental processing.  \nGradient Descent and Neural Nets. Parameter optimisation by gradient descent. Non-linear \nfunctions with neural network layers. Log-linear model as softmax layer. Current findings of \nNeural NLP. \nWord representations. Representing words with vectors, count-based and prediction-based \napproaches, similarity metrics. \nRecurrent Neural Networks. Modelling sequences, parameter sharing in recurrent neural \nnetworks, neural language models, word prediction. \nCompositional Semantics. Logical representations, compositional semantics, lambda calculus, \ninference and robust entailment. \nLexical Semantics. Semantic relations, WordNet, word senses. \nDiscourse. Discourse relations, anaphora resolution, summarization. \nNatural Language Generation. Challenges of natural language generation (NLG), tasks in NLG, \nsurface realisation. \nPractical and assignments. Students will build a natural language processing system which will \nbe trained and evaluated on supplied data. The system will be built from existing components, \nbut students will be expected to compare approaches and some programming will be required \nfor this. Several assignments will be set during the practicals for assessment. \nObjectives \nBy the end of the course students should: \n \nbe able to discuss the current and likely future performance of several NLP applications; \nbe able to describe briefly a fundamental technique for processing language for several \nsubtasks, such as morphological processing, parsing, word sense disambiguation etc.; \nunderstand how these techniques draw on and relate to other areas of computer science. \nRecommended reading \n",
        "Jurafsky, D. & Martin, J. (2023). Speech and language processing. Prentice Hall (3rd ed. draft, \nonline). \n \nAssessment - Part II Students \nAssignment 1 - 10% of marks \nAssignment 2 - 60% of marks \nAssignment 3 - 30% of marks\n \n",
        "PRACTICAL RESEARCH IN HUMAN-CENTRED AI \n \nAims \nThis is an advanced course in human-computer interaction, with a specialist focus on intelligent \nuser interfaces and interaction with machine-learning and artificial intelligence technologies. The \nformat will be largely Practical, with students carrying out a mini-project involving empirical \nresearch investigation. These studies will investigate human interaction with some kind of \nmodel-based system for planning, decision-making, automation etc. Possible study formats \nmight include: System evaluation, Field observation, Hypothesis testing experiment, Design \nintervention or Corpus analysis, following set examples from recent research publications. \nProject work will be formally evaluated through a report and presentation. \n \nLectures \n(note that Lectures 2-7 also include one hour class discussion of practical work) \n        \u2022 Current research themes in intelligent user interfaces \n        \u2022 Program synthesis \n        \u2022 Mixed initiative interaction \n        \u2022 Interpretability / explainable AI \n        \u2022 Labelling as a fundamental problem \n        \u2022 Machine learning risks and bias \n        \u2022 Visualisation and visual analytics \n        \u2022 Student research presentations \n \nObjectives \nBy the end of the course students should: \n        \u2022 be familiar with current state of the art in intelligent interactive systems \n        \u2022 understand the human factors that are most critical in the design of such systems \n        \u2022 be able to evaluate evidence for and against the utility of novel systems \n        \u2022 have experience of conducting user studies meeting the quality criteria of this field \n        \u2022 be able to write up and present user research in a professional manner \n \nClass Size \nThis module can accommodate upto 20 Part II, Part III and MPhil students. \n \nRecommended reading \nBrad A. Myers and Richard McDaniel (2000). Demonstrational Interfaces: Sometimes You Need \na Little Intelligence, Sometimes You Need a Lot. \n \nAlan Blackwell (2024). Moral Codes: Designing alternatives to AI \n \nAssessment - Part II Students \nThe format will be largely practical, with students carrying out an individual mini-project involving \nempirical research investigation. \n \n",
        "Assignment 1: six incremental submissions which together contribute 20% to the final module \nmark. \n \nAssignment 2: Final report - 80% of the final module mark \n \n",
        "6th Semester \nADVANCED COMPUTER ARCHITECTURE \nAims \nThis course examines the techniques and underlying principles that are used to design \nhigh-performance computers and processors. Particular emphasis is placed on understanding \nthe trade-offs involved when making design decisions at the architectural level. A range of \nprocessor architectures are explored and contrasted. In each case we examine their merits and \nlimitations and how ultimately the ability to scale performance is restricted. \n \nLectures \nIntroduction. The impact of technology scaling and market trends. \nFundamentals of Computer Design. Amdahl\u2019s law, energy/performance trade-offs, ISA design. \nAdvanced pipelining. Pipeline hazards; exceptions; optimal pipeline depth; branch prediction; \nthe branch target buffer [2 lectures] \nSuperscalar techniques. Instruction-Level Parallelism (ILP); superscalar processor architecture \n[2 lectures] \nSoftware approaches to exploiting ILP. VLIW architectures; local and global instruction \nscheduling techniques; predicated instructions and support for speculative compiler \noptimisations. \nMultithreaded processors. Coarse-grained, fine-grained, simultaneous multithreading \nThe memory hierarchy. Caches; programming for caches; prefetching [2 lectures] \nVector processors. Vector machines; short vector/SIMD instruction set extensions; stream \nprocessing \nChip multiprocessors. The communication model; memory consistency models; false sharing; \nmultiprocessor memory hierarchies; cache coherence protocols; synchronization [2 lectures] \nOn-chip interconnection networks. Bus-based interconnects; on-chip packet switched networks \nSpecial-purpose architectures. Converging approaches to computer design \nObjectives \nAt the end of the course students should \n \nunderstand what determines processor design goals; \nappreciate what constrains the design process and how architectural trade-offs are made within \nthese constraints; \nbe able to describe the architecture and operation of pipelined and superscalar processors, \nincluding techniques such as branch prediction, register renaming and out-of-order execution; \nhave an understanding of vector, multithreaded and multi-core processor architectures; \nfor the architectures discussed, understand what ultimately limits their performance and \napplication domain. \nRecommended reading \n* Hennessy, J. and Patterson, D. (2012). Computer architecture: a quantitative approach. \nElsevier (5th ed.) ISBN 9780123838728. (the 3rd and 4th editions are also good)\n \n",
        "CRYPTOGRAPHY \n \nAims \nThis course provides an overview of basic modern cryptographic techniques and covers \nessential concepts that users of cryptographic standards need to understand to achieve their \nintended security goals. \n \nLectures \nCryptography. Overview, private vs. public-key ciphers, MACs vs. signatures, certificates, \ncapabilities of adversary, Kerckhoffs\u2019 principle. \nClassic ciphers. Attacks on substitution and transposition ciphers, Vigen\u00e9re. Perfect secrecy: \none-time pads. \nPrivate-key encryption. Stream ciphers, pseudo-random generators, attacking \nlinear-congruential RNGs and LFSRs. Semantic security definitions, oracle queries, advantage, \ncomputational security, concrete-security proofs. \nBlock ciphers. Pseudo-random functions and permutations. Birthday problem, random \nmappings. Feistel/Luby-Rackoff structure, DES, TDES, AES. \nChosen-plaintext attack security. Security with multiple encryptions, randomized encryption. \nModes of operation: ECB, CBC, OFB, CNT. \nMessage authenticity. Malleability, MACs, existential unforgeability, CBC-MAC, ECBC-MAC, \nCMAC, birthday attacks, Carter-Wegman one-time MAC. \nAuthenticated encryption. Chosen-ciphertext attack security, ciphertext integrity, \nencrypt-and-authenticate, authenticate-then-encrypt, encrypt-then-authenticate, padding oracle \nexample, GCM. \nSecure hash functions. One-way functions, collision resistance, padding, Merkle-Damg\u00e5rd \nconstruction, sponge function, duplex construct, entropy pool, SHA standards. \nApplications of secure hash functions. HMAC, stream authentication, Merkle tree, commitment \nprotocols, block chains, Bitcoin. \nKey distribution problem. Needham-Schroeder protocol, Kerberos, hardware-security modules, \npublic-key encryption schemes, CPA and CCA security for asymmetric encryption. \nNumber theory, finite groups and fields. Modular arithmetic, Euclid\u2019s algorithm, inversion, \ngroups, rings, fields, GF(2n), subgroup order, cyclic groups, Euler\u2019s theorem, Chinese \nremainder theorem, modular roots, quadratic residues, modular exponentiation, easy and \ndifficult problems. [2 lectures] \nDiscrete logarithm problem. Baby-step-giant-step algorithm, computational and decision \nDiffie-Hellman problem, DH key exchange, ElGamal encryption, hybrid cryptography, Schnorr \ngroups, elliptic-curve systems, key sizes. [2 lectures] \nTrapdoor permutations. Security definition, turning one into a public-key encryption scheme, \nRSA, attacks on \u201ctextbook\u201d RSA, RSA as a trapdoor permutation, optimal asymmetric \nencryption padding, common factor attacks. \nDigital signatures. One-time signatures, RSA signatures, Schnorr identification scheme, \nElGamal signatures, DSA, PS3 hack, certificates, PKI. \nObjectives \nBy the end of the course students should \n",
        " \nbe familiar with commonly used standardized cryptographic building blocks; \nbe able to match application requirements with concrete security definitions and identify their \nabsence in naive schemes; \nunderstand various adversarial capabilities and basic attack algorithms and how they affect key \nsizes; \nunderstand and compare the finite groups most commonly used with discrete-logarithm \nschemes; \nunderstand the basic number theory underlying the most common public-key schemes, and \nsome efficient implementation techniques. \nRecommended reading \nKatz, J., Lindell, Y. (2015). Introduction to modern cryptography. Chapman and Hall/CRC (2nd \ned.).\n \n",
        "E-COMMERCE \n \nAims \nThis course aims to give students an outline of the issues involved in setting up an e-commerce \nsite. \n \nLectures \nThe history of electronic commerce. Mail order; EDI; web-based businesses, credit card \nprocessing, PKI, identity and other hot topics. \nNetwork economics. Real and virtual networks, supply-side versus demand-side scale \neconomies, Metcalfe\u2019s law, the dominant firm model, the differentiated pricing model Data \nProtection Act, Distance Selling regulations, business models. \nWeb site design. Stock and price control; domain names, common mistakes, dynamic pages, \ntransition diagrams, content management systems, multiple targets. \nWeb site implementation. Merchant systems, system design and sizing, enterprise integration, \npayment mechanisms, CRM and help desks. Personalisation and internationalisation. \nThe law and electronic commerce. Contract and tort; copyright; binding actions; liabilities and \nremedies. Legislation: RIP; Data Protection; EU Directives on Distance Selling and Electronic \nSignatures. \nPutting it into practice. Search engine interaction, driving and analysing traffic; dynamic pricing \nmodels. Integration with traditional media. Logs and audit, data mining modelling the user. \ncollaborative filtering and affinity marketing brand value, building communities, typical behaviour. \nFinance. How business plans are put together. Funding Internet ventures; the recent hysteria; \nmaximising shareholder value. Future trends. \nUK and International Internet Regulation. Data Protection Act and US Privacy laws; HIPAA, \nSarbanes-Oxley, Security Breach Disclosure, RIP Act 2000, Electronic Communications Act \n2000, Patriot Act, Privacy Directives, data retention; specific issues: deep linking, Inlining, brand \nmisuse, phishing. \nObjectives \nAt the end of the course students should know how to apply their computer science skills to the \nconduct of e-commerce with some understanding of the legal, security, commercial, economic, \nmarketing and infrastructure issues involved. \n \nRecommended reading \nShapiro, C. and Varian, H. (1998). Information rules. Harvard Business School Press. \n \nAdditional reading: \n \nStandage, T. (1999). The Victorian Internet. Phoenix Press. Klemperer, P. (2004). Auctions: \ntheory and practice. Princeton Paperback ISBN 0-691-11925-2.\n \n",
        "MACHINE LEARNING AND BAYESIAN INFERENCE \nAims \nThe Part 1B course Artificial Intelligence introduced simple neural networks for supervised \nlearning, and logic-based methods for knowledge representation and reasoning. This course \nhas two aims. First, to provide a rigorous introduction to machine learning, moving beyond the \nsupervised case and ultimately presenting state-of-the-art methods. Second, to provide an \nintroduction to the wider area of probabilistic methods for representing and reasoning with \nknowledge. \n \nLectures \nIntroduction to learning and inference. Supervised, unsupervised, semi-supervised and \nreinforcement learning. Bayesian inference in general. What the naive Bayes method actually \ndoes. Review of backpropagation. Other kinds of learning and inference. [1 lecture] \nHow to classify optimally. Treating learning probabilistically. Bayesian decision theory and Bayes \noptimal classification. Likelihood functions and priors. Bayes theorem as applied to supervised \nlearning. The maximum likelihood and maximum a posteriori hypotheses. What does this teach \nus about the backpropagation algorithm? [2 lectures] \nLinear classifiers I. Supervised learning via error minimization. Iterative reweighted least \nsquares. The maximum margin classifier. [2 lectures] \nGaussian processes. Learning and inference for regression using Gaussian process models. [2 \nlectures] \nSupport vector machines (SVMs). The kernel trick. Problem formulation. Constrained \noptimization and the dual problem. SVM algorithm. [2 lectures] \nPractical issues. Hyperparameters. Measuring performance. Cross-validation. Experimental \nmethods. [1 lecture] \nLinear classifiers II. The Bayesian approach to neural networks. [1 lecture] \nUnsupervised learning I. The k-means algorithm. Clustering as a maximum likelihood problem. \n[1 lecture] \nUnsupervised learning II. The EM algorithm and its application to clustering. [1 lecture] \nBayesian networks I. Representing uncertain knowledge using Bayesian networks. Conditional \nindependence. Exact inference in Bayesian networks. [2 lectures] \nBayesian networks II. Markov random fields. Approximate inference. Markov chain Monte Carlo \nmethods. [1 lecture] \nObjectives \nAt the end of this course students should: \n \nUnderstand how learning and inference can be captured within a probabilistic framework, and \nknow how probability theory can be applied in practice as a means of handling uncertainty in AI \nsystems. \nUnderstand several algorithms for machine learning and apply those methods in practice with \nproper regard for good experimental practice. \nRecommended reading \nIf you are going to buy a single book for this course I recommend: \n \n",
        "* Bishop, C.M. (2006). Pattern recognition and machine learning. Springer. \n \nThe course text for Artificial Intelligence I: \n \nRussell, S. and Norvig, P. (2010). Artificial intelligence: a modern approach. Prentice Hall (3rd \ned.). \n \ncovers some relevant material but often in insufficient detail. Similarly: \n \nMitchell, T.M. (1997). Machine Learning. McGraw-Hill. \n \ngives a gentle introduction to some of the course material, but only an introduction. \n \nRecently a few new books have appeared that cover a lot of relevant ground well. For example: \n \nBarber, D. (2012). Bayesian Reasoning and Machine Learning. Cambridge University Press. \nFlach, P. (2012). Machine Learning: The Art and Science of Algorithms that Make Sense of \nData. Cambridge University Press. \nMurphy, K.P. (2012). Machine Learning: A Probabilistic Perspective. MIT Press.\n \n",
        "OPTIMISING COMPILERS \n \nAims \nThe aims of this course are to introduce the principles of program optimisation and related \nissues in decompilation. The course will cover optimisations of programs at the abstract syntax, \nflowgraph and target-code level. It will also examine how related techniques can be used in the \nprocess of decompilation. \n \nLectures \nIntroduction and motivation. Outline of an optimising compiler. Optimisation partitioned: analysis \nshows a property holds which enables a transformation. The flow graph; representation of \nprogramming concepts including argument and result passing. The phase-order problem. \nKinds of optimisation. Local optimisation: peephole optimisation, instruction scheduling. Global \noptimisation: common sub-expressions, code motion. Interprocedural optimisation. The call \ngraph. \nClassical dataflow analysis. Graph algorithms, live and avail sets. Register allocation by register \ncolouring. Common sub-expression elimination. Spilling to memory; treatment of \nCSE-introduced temporaries. Data flow anomalies. Static Single Assignment (SSA) form. \nHigher-level optimisations. Abstract interpretation, Strictness analysis. Constraint-based \nanalysis, Control flow analysis for lambda-calculus. Rule-based inference of program properties, \nTypes and effect systems. Points-to and alias analysis. \nTarget-dependent optimisations. Instruction selection. Instruction scheduling and its phase-order \nproblem. \nDecompilation. Legal/ethical issues. Some basic ideas, control flow and type reconstruction. \nObjectives \nAt the end of the course students should \n \nbe able to explain program analyses as dataflow equations on a flowgraph; \nknow various techniques for high-level optimisation of programs at the abstract syntax level; \nunderstand how code may be re-scheduled to improve execution speed; \nknow the basic ideas of decompilation. \nRecommended reading \n* Nielson, F., Nielson, H.R. and Hankin, C.L. (1999). Principles of program analysis. Springer. \nGood on part A and part B. \nAppel, A. (1997). Modern compiler implementation in Java/C/ML (3 editions). \nMuchnick, S. (1997). Advanced compiler design and implementation. Morgan Kaufmann. \nWilhelm, R. (1995). Compiler design. Addison-Wesley. \nAho, A.V., Sethi, R. and Ullman, J.D. (2007). Compilers: principles, techniques and tools. \nAddison-Wesley (2nd ed.).\n \n",
        "QUANTUM COMPUTING \n \nAims \nThe principal aim of the course is to introduce students to the basics of the quantum model of \ncomputation. The model will be used to study algorithms for searching, factorisation and \nquantum chemistry as well as other important topics in quantum information such as \ncryptography and super-dense coding. Issues in the complexity of computation will also be \nexplored. A second aim of the course is to introduce student to near-term quantum computing. \nTo this end, error-correction and adiabatic quantum computing are studied. \n \nLectures \nBits and qubits. Introduction to quantum states and measurements with motivating examples. \nComparison with discrete classical states. \nLinear algebra. Review of linear algebra: vector spaces, linear operators, Dirac notation, the \ntensor product. \nThe postulates of quantum mechanics. Postulates of quantum mechanics, incl. evolution and \nmeasurement. \nImportant concepts in quantum mechanics. Entanglement, distinguishing orthogonal and \nnon-orthogonal quantum states, no-cloning and no signalling. \nThe quantum circuit model. The circuit model of quantum computation. Quantum gates and \ncircuits. Universality of the quantum circuit model, and efficient simulation of arbitrary two-qubit \ngates with a standard universal set of gates. \nSome applications of quantum information. Applications of quantum information (other than \nquantum computation): quantum key distribution, superdense coding and quantum teleportation. \nDeutsch-Jozsa algorithm. Introducing Deutsch\u2019s problem and Deutsch\u2019s algorithm leading onto \nits generalisation, the Deutsch-Jozsa algorithm. \nQuantum search. Grover\u2019s search algorithm: analysis and lower bounds. \nQuantum Fourier Transform and Quantum Phase Estimation. Definition of the Quantum Fourier \nTransform (QFT), and efficient representation thereof as a quantum circuit. Application of the \nQFT to enable Quantum Phase Estimation (QPE). \nApplication 1 of QFT / QPE: Factoring. Shor\u2019s algorithm: reduction of factoring to period finding \nand then using the QFT for period finding. \nApplication 2 of QFT / QPE: Quantum Chemistry. Efficient simulation of quantum systems, and \napplications to real-world problems in quantum chemistry. \nQuantum complexity. Quantum complexity classes and their relationship to classical complexity. \nComparison with probabilistic computation. \nQuantum error correction. Introducing the concept of quantum error correction required for the \nfollowing lecture on fault-tolerance. \nFault tolerant quantum computing. Elements of fault tolerant computing; the threshold theorem \nfor efficient suppression of errors. \nAdiabatic quantum computing and quantum optimisation. The quantum adiabatic theorem, and \nadiabatic optimisation. Quantum annealing and D-Wave. \nCase studies in near-term quantum computation. Examples of state-of-the-art quantum \nalgorithms and computers, including superconducting and networked quantum computers. \n",
        "Objectives \nAt the end of the course students should: \n \nunderstand the quantum model of computation and the basic principles of quantum mechanics; \nbe familiar with basic quantum algorithms and their analysis; \nbe familiar with basic quantum protocols such as teleportation and superdense coding; \nsee how the quantum model relates to classical models of deterministic and probabilistic \ncomputation. \nappreciate the importance of efficient error-suppression if quantum computation is to yield an \nadvantage over classical computation. \ngain a general understanding of the important topics in near-term quantum computing, including \nadiabatic quantum computing. \nRecommended reading \nBooks: \nNielsen M.A., Chuang I.L. (2010). Quantum Computation and Quantum Information. Cambridge \nUniversity Press. \nMermin N.D. (2007). Quantum Computer Science: An Introduction. Cambridge University Press. \nMcGeoch, C. (2014). Adiabatic Quantum Computation and Quantum Annealing Theory and \nPractice. Morgan and Claypool. https://ieeexplore.ieee.org/document/7055969 \n \nPapers: \n \nBraunstein S.L. (2003). Quantum computation tutorial. Available at: \nhttps://www-users.cs.york.ac.uk/~schmuel/comp/comp_best.pdf \nAharonov D., Quantum computation [arXiv:quant-ph/9812037] \nAlbash T., Adiabatic Quantum Computing https://arxiv.org/pdf/1611.04471.pdf \nMcCardle S. et al, Quantum computational chemistry https://arxiv.org/abs/1808.10402 \n \nOther lecture notes: \n \nAndrew Childs (University of Maryland): http://cs.umd.edu/~amchilds/qa/ \n \n",
        "RANDOMISED ALGORITHMS \n \nAims: \nThe aim of this course is to introduce advanced techniques in the design and analysis \nalgorithms, with a strong focus on randomised algorithms. It covers essential tools and ideas \nfrom probability, optimisation and graph theory, and develops this knowledge through a variety \nof examples of algorithms and processes. This course will demonstrate that randomness is not \nonly an important design technique which often leads to simpler and more elegant algorithms, \nbut may also be essential in settings where time and space are restricted.   \n \nLectures: \nIntroduction. Course Overview. Why Randomised Algorithms? A first Randomised Algorithm for \nthe MAX-CUT problem. [1 Lecture]  \n \nConcentration Inequalities. Moment-Generating Functions and Chernoff Bounds. Extension: \nMethod of Bounded Independent Differences. Applications: Balls-into-Bins, Quick-Sort and Load \nBalancing. [approx. 2 Lectures] \n \nMarkov Chains and Mixing Times. Random Walks on Graphs. Application: Randomised \nAlgorithm for the 2-SAT problem. Mixing Times of Markov Chains. Application: Load Balancing \non Networks. [approx. 2 Lectures] \n \nLinear Programming and Applications. Definitions and Applications. Formulating Linear \nPrograms. The Simplex Algorithm. Finding Initial Solutions. How to use Linear Programs and \nBranch & Bound to Solve a Classical TSP instance. [approx. 3 Lectures] \n \nRandomised Approximation Algorithms. Randomised Approximation Schemes. Linearity of \nExpectations, Derandomisation. Deterministic and Randomised Rounding of Linear Programs. \nApplications: MAX3-SAT problem, Weighted Vertex Cover, Weighted Set Cover, MAX-SAT. \n[approx. 2 Lectures] \n \nSpectral Graph Theory and Clustering. Eigenvalues of Graphs and Matrices: Relations between \nEigenvalues and Graph Properties, Spectral Graph Drawing. Spectral Clustering: Conductance, \nCheeger's Inequality. Spectral Partitioning Algorithm [approx. 2 Lectures] \n \nObjectives: \nBy the end of the course students should be able to: \n \nlearn how to use randomness in the design of algorithms, in particular, approximation \nalgorithms; \nlearn the basics of linear programming, integer programming and randomised rounding; \napply randomisation to various problems coming from optimisation, machine learning and data \nscience; \nuse results from probability theory to analyse the performance of randomised algorithms. \n",
        "Recommended reading \n* Michael Mitzenmacher and Eli Upfal. Probability and Computing: Randomized Algorithms and \nProbabilistic Analysis., Cambridge University Press, 2nd edition. \n \n* David P. Williamson and David B. Shmoys. The Design of Approximation Algorithms, \nCambridge University Press, 2011 \n \n* Cormen, T.H., Leiserson, C.D., Rivest, R.L. and Stein, C. (2009). Introduction to Algorithms. \nMIT Press (3rd ed.). ISBN 978-0-262-53305-8 \n \n \n",
        "CLOUD COMPUTING \nAims \nThis module aims to teach students the fundamentals of Cloud Computing covering topics such \nas virtualization, data centres, cloud resource management, cloud storage and popular cloud \napplications including batch and data stream processing. Emphasis is given on the different \nbackend technologies to build and run efficient clouds and the way clouds are used by \napplications to realise computing on demand. The course will include practical tutorials on cloud \ninfrastructure technologies. Students will be assessed via a Cloud-based coursework project. \n \nLectures \nIntroduction to Cloud Computing \nData centres \nVirtualization I \nVirtualization II \nMapReduce \nMapReduce advanced \nResource management for virtualized data centres \nCloud storage \nCloud-based data stream processing \nObjectives \nBy the end of the course students should: \n \nunderstand how modern clouds operate and provide computing on demand; \nunderstand about cloud availability, performance, scalability and cost; \nknow about cloud infrastructure technologies including virtualization, data centres, resource \nmanagement and storage; \nknow how popular applications such as batch and data stream processing run efficiently on \nclouds; \nAssessment \nAssignment 1, worth 55% of the final mark. Develop batch processing applications running on a \ncluster assessed through automatic testing, code inspection and a report. \nAssignment 2, worth 30% of the final mark: Design and develop a framework for running the \nbatch processing applications from Assignment 1 on a cluster according to certain resource \nallocation policies. This assigment will be assessed by an expert, testing, code inspection and a \nreport. \nAssignment 3, worth 15% of the final mark: Write two individual project reports describing your \nwork for assignments 1 and 2. Write two sets of scripts to manage and run your code. Reports \nand scripts will be submitted with each assignment. \nRecommended reading \nMarinescu, D.C. Cloud Computing, Theory and Practice. Morgan Kaufmann. \nBarham, P., et. al. (2003). \u201cXen and the Art of Virtualization\u201d. In Proceedings of SOSP 2003. \nCharkasova, L., Gupta, D. and Vahdat, A. (2007). \u201cComparison of the Three CPU Schedulers in \nXen\u201d. In SIGMETRICS 2007. \n",
        "Dean, J. and Ghemawat, S. (2004). \u201cMapReduce: Simplified Data Processing on Large \nClusters\u201d. In Proceedings of OSDI 2004. \nZaharia, M, et al. (2008). \u201cImproving MapReduce Performance in Heterogeneous \nEnvironments\u201d. In Proceedings of OSDI 2008. \nHindman, A., et al. (2011). \u201cMesos: A Platform for Fine-Grained Resource Sharing in Data \nCenter\u201d. In Proceedings of NSDI 2011. \nSchwarzkopf, M., et al. (2013). \u201cOmega: Flexible, Scalable Schedulers for Large Compute \nClusters\u201d. In EuroSys 2013. \nGhemawat, S. (2003). \u201cThe Google File System\u201d. In Proceedings of SOSP 2003. \nChang, F. (2006). \u201cBigtable: A Distributed Storage System for Structured Data\u201d. In Proceedings \nof OSDI 2006. \nFernandez, R.C., et al. (2013). \u201cIntegrating Scale Out and Fault Tolerance in Stream Processing \nusing Operator State Management\u201d. In SIGMOD 2013.\n \n",
        "COMPUTER SYSTEMS MODELLING \n \nAims \n \nThe aims of this course are to introduce the concepts and principles of mathematical modelling \n \nand simulation, with particular emphasis on using queuing theory and control theory for \nunderstanding the behaviour of computer and communications systems. \n \nSyllabus \n \n1.     Overview of computer systems modeling using both analytic techniques and simulation. \n \n2.     Introduction to discrete event simulation. \n \na.     Simulation techniques \n \nb.     Random number generation methods \n \nc.     Statistical aspects: confidence intervals, stopping criteria \n \nd.     Variance reduction techniques. \n \n3.     Stochastic processes (builds on starred material in Foundations of Data Science) \n \na.     Discrete and continuous stochastic processes. \n \nb.     Markov processes and Chapman-Kolmogorov equations. \n \nc.     Discrete time Markov chains. \n \nd.     Ergodicity and the stationary distribution. \n \ne.     Continuous time Markov chains. \n \nf.      Birth-death processes, flow balance equations. \n \ng.     The Poisson process. \n \n4.     Queueing theory \n \na.     The M/M/1 queue in detail. \n \n",
        "b.     The equilibrium distribution with conditions for existence and common performance \nmetrics. \n \nc.     Extensions of the M/M/1 queue: the M/M/k queue, the M/M/infinity queue. \n \nd.     Queueing networks. Jacksonian networks. \n \ne.     The M/G/1 queue. \n \n5.     Signals, systems, and transforms \n \na.     Discrete- and continuous-time convolution. \n \nb.     Signals. The complex exponential signal. \n \nc.     Linear Time-Invariant Systems. Modeling practical systems as an LTI system. \n \nd.     Fourier and Laplace transforms. \n \n6.     Control theory. \n \na.     Controlled systems. Modeling controlled systems. \n \nb.     State variables. The transfer function model. \n \nc.     First-order and second-order systems. \n \nd.     Feedback control. PID control. \n \ne.     Stability. BIBO stability. Lyapunov stability. \n \nf.      Introduction to Model Predictive Control. \n \nObjectives \n \nAt the end of the course students should \n \n\u00b7       Be aware of different approaches to modeling a computer system; their pros and cons \n \n\u00b7       Be aware of the issues in building a simulation of a computer system and analysing the \nresults obtained \n \n\u00b7       Understand the concept of a stochastic process and how they arise in practice \n \n",
        "\u00b7       Be able to build simple Markov models and understand the critical modelling assumptions \n \n\u00b7       Be able to solve simple birth-death processes \n \n\u00b7       Understand and use M/M/1 queues to model computer systems \n \n\u00b7       Be able to model a computer system as a linear time-invariant system \n \n\u00b7       Understand the dynamics of a second-order controlled system \n \n\u00b7       Design a PID control for an LTI system \n \n\u00b7       Understand what is meant by BIBO and Lyapunov stability \n \nAssessment \n \nThere will be one programming assignment to build a discrete event simulator and using it to \nsimulate an M/M/1 and an M/D/1 queue (worth 15% of the final marks). There will also be two \none-hour in-person assessments for the course on: Stochastic processes and Queueing theory \n(40%) and Signals, Systems, Transforms, and Control Theory (45%). \n \nReference books \n \nKeshav, S. (2012)*. Mathematical Foundations of Computer Networking. Addison-Wesley. A \ncustomized PDF will be made available to class participants. \n \nKleinrock, L. (1975). Queueing systems, vol. 1. Theory. Wiley. \n \nKraniauskas, Peter. Transforms in signals and systems. Addison-Wesley Longman, 1992. \n \nJain, R. (1991). The art of computer systems performance analysis. Wiley. \n \n \n",
        "COMPUTING EDUCATION \n \nAims \nThis module considers various aspects of the teaching and learning of computer science in \nschool, including practical experience, and provides an introduction to the field of computing \neducation research. It is offered by the Raspberry Pi Computing Education Research Centre, \npart of the Department of Computer Science and Technology, in conjunction with local schools \nin the Cambridge area. Students will have the opportunity to observe, plan and deliver a lesson, \nand explore the teaching of computing in a secondary education setting. The lesson taught will \nbe on a topic within the computing/computer science curriculum in England but will be \nnegotiated with the teacher mentoring the school placement. In the sessions in the Department, \nstudents will be introduced to elements of pedagogy and assessment alongside current issues \nin computing education. The course can also serve as a useful introduction to research in \ncomputing education for those interested in this area. Assessment will include lesson planning, \npresentations, an in-person viva and a 3000-word report which will include evidence of \nengagement with relevant research underpinning the lessons and activities undertaken. An \ninformation session relating to the module will be available in the preceding Easter Term. The \narrangement of school placements and DBS checks will be carried out in Michaelmas Term, so \nstudents should be prepared to engage in these preparatory activities. \n \nLectures \nThis is not a traditional lecture course and the term will be structured as follows: \n \nWeek 1: Introduction to computing education & visit to school placement \nWeek 2: In school (observation, supporting teacher) \nWeek 3: In school (observation, supporting teacher) \nWeek 4: In school (co-teach / small group teaching) \nWeek 5: Computing pedagogy and assessment (school half-term) \nWeek 6: In school (co-teach / small group teaching) \nWeek 7: In school (teach part/whole lesson) \nWeek 8: Presentations \nObjectives \nBy the end of the course students should: \n \ngain experience of computing education in practice \ndemonstrate an ability to communicate an aspect of computer science clearly and effectively \nbe able to plan and design a teaching activity/lesson with consideration of the audience \nbe able to thoroughly evaluate the design and implementation of a teaching/activity lesson \nbe able to demonstrate an understanding of relevant theories of learning and pedagogy from the \nliterature and how they apply to the learning/teaching of the topic under consideration \ngain an understanding of the breadth and depth of the field of computing education research \nClass Size \nThis module can accommodate up to 10 Part II students. \n \n",
        "Recommended reading \nBrown, N. C., Sentance, S., Crick, T., & Humphreys, S. (2014). Restart: The resurgence of \ncomputer science in UK schools. ACM Transactions on Computing Education (TOCE), 14(2), \n1-22. https://doi.org/10.1145/2602484 \n \nSentance, S., Barendsen, E., Howard, N. R., & Schulte, C. (Eds.). (2023). Computer science \neducation: Perspectives on teaching and learning in school. Bloomsbury Publishing. \n \nGrover, S. (Ed). 2020. Computer Science in K-12: An A-to-Z Handbook on Teaching \nProgramming. Edfinity. https://www.shuchigrover.com/atozk12cs/ \n \nRaspberry Pi Foundation (2022). Hello World Big Book of Pedagogy. \nhttps://www.raspberrypi.org/hello-world/issues/the-big-book-of-computing-pedagogy \n \nAssessment \nWeek 2: Reflections on an observed lesson (10%, graded out of 20) \nWeek 4: Submission of a lesson plan outline (10% graded out of 20) \nWeek 8: Delivery of an oral presentation relating to the lesson delivery (10% graded out of 20) \nAfter course completion: 3000 word report (60%), graded out of 20 \nViva with assessor (10%) (pass/fail) \nFurther Information \nWe will find placements for students in secondary schools within cycling or bus distance from \nthe Department/centre of Cambridge. There will be a timetabled slot for the course that students \ncan use to go to their school placement, but students may need to be flexible and liaise with the \nschool to find the best time for working with a specific class. \n \n \n",
        "DEEP NEURAL NETWORKS \n \nObjectives \nYou will gain detailed knowledge of \n \nCurrent understanding of generalization in neural networks vs classical statistical models. \nOptimization procedures for neural network models such as stochastic gradient descent and \nADAM. \nAutomatic differentiation and at least one software framework (PyTorch, TensorFolow) as well as \nan overview of other software approaches. \nArchitectures that are deployed to deal with different data types such as images or sequences \nincluding a. convolutional networks b. recurrent networks and c. transformers. \nIn addition you will gain knowledge of more advanced topics reflecting recent research in \nmachine learning. These change from year to year, examples include: \n \nApproaches to unsupervised learning including autoencoders and self-supervised learning.. \nTechniques for deploying models in low data regimes such as transfer learning and \nmeta-learning. \nTechniques for propagating uncertainty such as Bayesian neural networks. \nReinforcement learning and its applications to robotics or games. \nGraph Neural Networks and Geometric Deep Learning. \nTeaching Style \nThe start of the course will focus on the latest undertanding of current theory of neural networks, \ncontrasting with previous classical understandings of generalization performance. Then we will \nmove to practical examples of network architectures and deployment. We will end with more \nadvanced topics reflecting current research, mostly delivered by guest lecturers from the \nDepartment and beyond. \n \nSchedule \nWeek 1 \nTwo lectures: Generalization and Approximation with Neural Networks \n \nWeek 2 \nTwo lectures: Optimization: Stochastic Gradient Descent and ADAM \n \nWeek 3 \nTwo lectures: Hardware Acceleration and Convolutional Neural Networks \n \nWeek 4 \nTwo lectures: Vanishing Gradients, Recurrent and Residual Networks, Sequence Modeling. \n \nWeek 5 \nTwo lectures: Attention, The Transformer Architecture, Large Language Models \n \n",
        "Week 6-8 \nLectures and guest lectures from the special topics below. \n \nSpecial topics \nSelf-Supervised Learning, AutoEncoders, Generative Modeling of Images \nHardware Implementations \nReinforcement learning \nTransfer learning and meta-learning \nUncertainty and Bayesian Neural Networks \nGraph Neural Networks \nAssessment \nAssessment involves solving a number of exercises in a jupyter notebook environment. \nFamiliarity with the python programming language is assumed. The exercises rely on the \npytorch deep learning framework, the syntax of which we don\u2019t cover in detail in the lectures, \nstudents are referred to documentation and tutorials to learn this component of the assessment. \n \nAssignment 1 \n \n30% of the total marks, with guided exercises. \n \nAssignment 2 \n \n70% of the total marks, a combination of guided exercises and a more exploratory mini-project.\n",
        "EXTENDED REALITY \n \nSyllabus & Schedule \nWeek 1 \nIntroduction \nCourse schedule and concept \nThe history of AR/VR/XR \nApplications of AR/VR/XR \nOverview of the XR pipeline \u2013 3 main components of XR \nDisplay technologies and rendering \nMachine perception and input interfaces \nContent generation \u2013 geometry, appearance, motion \nOverview of the AR/VR/XR frameworks \nPractical 1 \nProject structure \nProject themes \nIntroduction to the XR programming platform \nWeek 2 \n3D scene representations and rendering \nObject representations with polygonal meshes \nEfficient rendering via rasterization \nPractical XR rendering of textured geometries \nTracking and pose estimation for XR \nRigid transforms \nCamera pose estimation \nBody/ hand tracking \nPractical 2 \nProject draft proposal due \nProject feedback \nRendering and displaying a first object \nWeek 3 \n3D scene capture and understanding \nDepth estimation \n3D geometry capture \nAppearance acquisition \nLighting estimation and relighting \nLighting models for 3D scenes \nEstimating lighting in a scene \nRelighting rendered models \nPractical 3 \nAcquiring and utilizing depth from sensors \nRelighting a displayed object \nWeek 4 \nPhysically-based simulation of rigid objects \n",
        "Rigid-body dynamics \nCollision detection \nTime integration \nPhysically-based simulation of non-rigid volumetric phenomena \nParticle systems \nBasic fluid dynamics \nPractical 4 \nPractical due \nProject prototype due \nProject check-in \nSimulating and rendering a simple object \nWeek 5 \nAdvanced topics in XR I: AR/VR displays technologies \nMicro LED-based displays \nWaveguide-based displays \nNext generation displays \nWeek 6 \nAdvanced topics in XR II \u2013 Mobile AR \nComputer vision on limited hardware \nUX/UI challenges \nContent challenges \nProject due \n  \n \nAssessment \nA practical exercise, worth 20% of the mark. This will cover the basics of AR/VR development \nand some of the practical techniques the students will use in their projects. This is individual \nwork. No mobile device or GPU hours are needed. \nAn AR/VR project worth 80% of the marks: \nCourse projects will be designed by the students following the possible project themes proposed \nby the lecturer and will be checked by the lecturer for appropriateness. \nThe students will form groups of 2-3 to design, implement, report, and present the course \nproject. \nThe final mark will be an implementation mark (40%), a report mark (20%), and a \npresentation/demo mark (20%). Each team member will be evaluated based on their \ncontribution. \nEach project will have extensions to be completed only by the ACS students. Each student will \nwrite a different part of the report, whose author will be clearly marked. Each student will further \nsummarise her/his contributions to the project in the same report. \nNote: Submission is by one submission per group but work by each individual must be clearly \nlabelled.\n \n",
        "FEDERATED LEARNING: THEORY AND PRACTICE \n \nObjectives \nThis course aims to extend the machine learning knowledge available to students in Part I (or \npresent in typical undergraduate degrees in other universities), and allow them to understand \nhow these concepts can manifest in a decentralized setting. The course will consider both \ntheoretical (e.g., decentralized optimization) and practical (e.g., networking efficiency) aspects \nthat combine to define this growing area of machine learning.  \n \nAt the end of the course students should: \n \nUnderstand popular methods used in federated learning \nBe able to construct and scale a simple federated system \nHave gained an appreciation of the core limitations to existing methods, and the approaches \navailable to cope with these issues \nDeveloped an intuition for related technologies like differential privacy and secure aggregation, \nand are able to use them within typical federated settings  \nCan reason about the privacy and security issues with federated systems \nLectures \nCourse Overview. Introduction to Federated Learning. \nDecentralized Optimization. \nStatistical and Systems Heterogeneity. \nVariations of Federated Aggregation. \nSecure Aggregation. \nDifferential Privacy within Federated Systems. \nExtensions to Federated Analytics. \nApplications to Speech, Video, Images and Robotics. \nLab sessions \nFederating a Centralized ML Classifier. \nBehaviour under Heterogeneity. \nScaling a Federated Implementation. \nExploring Privacy with Federated Settings \nRecommended Reading \nReadings will be assigned for each lecture. Readings will be taken from either research papers, \ntutorials, source code or blogs that provide more comprehensive treatment of taught concepts. \n \nAssessment - Part II Students \nFour labs are performed during the course, and students receive 10% of their total grade for \nwork done as part of each lab. (For a total of 40% of the total grade from lab work alone). Labs \nwill primarily provide hands-on teaching opportunities, that are then utilized within the lab \nassignment which is completed outside of the lab contact time. MPhil and Part III students will \nbe given additional questions to answer within their version of the lab assignment which will \ndiffer from the assignment given to Part II CST students. \n \n",
        "The remainder of the course grade (60%) will be given based on a hands-on project that applies \nthe concepts taught in lectures and labs. This hands-on project will be assessed based on upon \na combination of source code, related documentation and brief 8-minute pre-recorded talk that \nsummarizes key project elements (any slides used are also submitted as part of the project). \nPlease note, that in the case of Part II CST students, the talk itself is not examinable -- as such \nwill be made optional to those students. \n \nA range of possible practical projects will be described and offered to students to select from, or \nalternatively students may propose their own. MPhil and Part III students will select from a \nproject pool that is separate from those offered to Part II CST students. MPhil and Part III \nprojects will contain a greater emphasis on a research element, and the pre-recorded talks \nprovided by this student group will focus on this research contribution. The project will be \nassessed on the level of student understanding demonstrated, the degree of difficulty, \ncorrectness of implementation -- and for Part III/MPhil students the additional criteria of the \nquality and execution of the research methodology, and depth and quality of results analysis. \n \nThis project can be done individually or in groups -- although individual projects will be strongly \nencouraged. It will be required the project is performed using a code repository that also will \ncontain all documentation -- access to this repository will be shared with course staff (e.g., \nlecturer and TAs). Where needed, marks assigned to students within a group will be \ndifferentiated using this repository as an input. Furthermore if groups are formed, members must \nbe either entirely from Part III/MPhil students or Part II CST, i.e., these two student groups \nshould not mix to form a project group. \n \nProjects will be made available publicly. A maximum word count for written contributions for the \nproject will be enforced.\n \n",
        "MOBILE HEALTH \n \nAims \nThe course aims to explore how wearable and mobile systems sensors can be used to gather \ndata relevant to understand health, how the data can be analysed with advanced signal \nprocessing and machine learning and the performance of these systems in terms of diagnostics \nand disease progression detection. \n \nSyllabus \nCourse Overview. Introduction to Mobile Health. Evaluation metrics and methodology. Basics of \nSignal Processing. \nInertial Measurement Units, Human Activity Recognition (HAR) and Gait Analysis and Machine \nLearning for IMU data. \nRadios, Bluetooth, GPS and Cellular. Epidemiology and contact tracing, Social interaction \nsensing and applications. Location tracking in health monitoring and public health. \nAudio Signal Processing. Voice and Speech Analysis: concepts and data analysis. Body \nSounds analysis. \nPhotoplethysmogram and Light sensing for health (heart and sleep) \nContactless and wireless behaviour and physiological monitoring \nGenerative Models for Wearable Health Data \nTopical Guest Lectures \nObjectives \nThe course aims to explore how wearable and mobile systems sensors can be used to gather \ndata relevant to understand health, how the data can be analysed with advanced signal \nprocessing and machine learning and the performance of these systems in terms of diagnostics \nand disease progression detection. \n \nRoughly, each lecture contains a theory part about the working of \u201csensor signals\u201d or \u201cdata \nanalysis methods\u201d and an application part which contextualises the concepts. \n \nAt the end of the course students should: Understand how mobile/wearable sensors capture \ndata and their working. Understand different approaches to acquiring and analysing sensor data \nfrom different types of sensors. Understand the concept of signal processing applied to time \nseries data and their practical application in health. Be able to extract sensor data and analyse it \nwith basic signal processing and machine learning techniques. Be aware of the different health \napplications of the various sensor techniques. The course will also touch on privacy and ethics \nimplications of the approaches developed in an orthogonal fashion. \n \nRecommended Reading \nPlease see Course Materials for recommended reading for each session. \n \nAssessment - Part II students \nTwo assignments will be based on two datasets which will be provided to the students: \n \n",
        "Assignment 1 (shared for Part II and Part III/MPhil): this will be based on a dataset and will be \nworth 40% of the final mark. The task of the assessment will be to perform pre-processing and \nbasic data analysis in a \"colab\" and an answer sheet of no more than 1000 words. \n \nAssignment 2 (Part II): This assignment (worth 60% of the final mark) will be a fuller analysis of \na dataset focusing on machine learning algorithms and metrics. Discussion and interpretation of \nthe findings will be reported in a colab and a report of no more than 1200 words.\n \n",
        "MULTICORE SEMANTICS AND PROGRAMMING \n \nAims \nIn recent years multiprocessors have become ubiquitous, but building reliable concurrent \nsystems with good performance remains very challenging. The aim of this module is to \nintroduce some of the theory and the practice of concurrent programming, from hardware \nmemory models and the design of high-level programming languages to the correctness and \nperformance properties of concurrent algorithms. \n \nLectures \nPart 1: Introduction and relaxed-memory concurrency [Professor P. Sewell] \n \nIntroduction. Sequential consistency, atomicity, basic concurrent problems. [1 block] \nConcurrency on real multiprocessors: the relaxed memory model(s) for x86, ARM, and IBM \nPower, and theoretical tools for reasoning about x86-TSO programs. [2 blocks] \nHigh-level languages. An introduction to C/C++11 and Java shared-memory concurrency. [1 \nblock] \nPart 2: Concurrent algorithms [Dr T. Harris] \n \nConcurrent programming. Simple algorithms (readers/writers, stacks, queues) and correctness \ncriteria (linearisability and progress properties). Advanced synchronisation patterns (e.g. some \nof the following: optimistic and lazy list algorithms, hash tables, double-checked locking, RCU, \nhazard pointers), with discussion of performance and on the interaction between algorithm \ndesign and the underlying relaxed memory models. [3 blocks] \nResearch topics, likely to include one hour on transactional memory and one guest lecture. [1 \nblock] \nObjectives \nBy the end of the course students should: \n \nhave a good understanding of the semantics of concurrent programs, both at the multprocessor \nlevel and the C/Java programming language level; \nhave a good understanding of some key concurrent algorithms, with practical experience. \nAssessment - Part II Students \nTwo assignments each worth 50% \n \nRecommended reading \nHerlihy, M. and Shavit, N. (2008). The art of multiprocessor programming. Morgan Kaufmann.\n",
        "BUSINESS STUDIES SEMINARS \nAims \nThis course is a series of seminars by former members and friends of the Laboratory about their \nreal-world experiences of starting and running high technology companies. It is a follow on to \nthe Business Studies course in the Michaelmas Term. It provides practical examples and case \nstudies, and the opportunity to network with and learn from actual entrepreneurs. \n \nLectures \nEight lectures by eight different entrepreneurs. \n \nObjectives \nAt the end of the course students should have a better knowledge of the pleasures and pitfalls \nof starting a high tech company. \n \nRecommended reading \nLang, J. (2001). The high-tech entrepreneur\u2019s handbook: how to start and run a high-tech \ncompany. FT.COM/Prentice Hall. \nMaurya, A. (2012). Running Lean: Iterate from Plan A to a Plan That Works. O\u2019Reilly. \nOsterwalder, A. and Pigneur, Y. (2010). Business Model Generation: A Handbook for \nVisionaires, Game Changers, and Challengers. Wiley. \nKim, W. and Mauborgne, R. (2005). Blue Ocean Strategy. Harvard Business School Press. \n \nSee also the additional reading list on the Business Studies web page.\n \n",
        "HOARE LOGIC AND MODEL CHECKING \n \nAims \nThe course introduces two verification methods, Hoare Logic and Temporal Logic, and uses \nthem to formally specify and verify imperative programs and systems. \n \nThe first aim is to introduce Hoare logic for a simple imperative language and then to show how \nit can be used to formally specify programs (along with discussion of soundness and \ncompleteness), and also how to use it in a mechanised program verifier. \n \nThe second aim is to introduce model checking: to show how temporal models can be used to \nrepresent systems, how temporal logic can describe the behaviour of systems, and finally to \nintroduce model-checking algorithms to determine whether properties hold, and to find \ncounter-examples. \n \nCurrent research trends also will be outlined. \n \nLectures \nPart 1: Hoare logic. Formal versus informal methods. Specification using preconditions and \npostconditions. \nAxioms and rules of inference. Hoare logic for a simple language with assignments, sequences, \nconditionals and while-loops. Syntax-directedness. \nLoops and invariants. Various examples illustrating loop invariants and how they can be found. \nPartial and total correctness. Hoare logic for proving termination. Variants. \nSemantics and metatheory Mathematical interpretation of Hoare logic. Semantics and \nsoundness of Hoare logic. \nSeparation logic Separation logic as a resource-aware reinterpretation of Hoare logic to deal \nwith aliasing in programs with pointers. \nPart 2: Model checking. Models. Representation of state spaces. Reachable states. \nTemporal logic. Linear and branching time. Temporal operators. Path quantifiers. CTL, LTL, and \nCTL*. \nModel checking. Simple algorithms for verifying that temporal properties hold. \nApplications and more recent developments Simple software and hardware examples. CEGAR \n(counter-example guided abstraction refinement). \nObjectives \nAt the end of the course students should \n \nbe able to prove simple programs correct by hand and implement a simple program verifier; \nbe familiar with the theory and use of separation logic; \nbe able to write simple models and specify them using temporal logic; \nbe familiar with the core ideas of model checking, and be able to implement a simple model \nchecker. \nRecommended reading \n",
        "Huth, M. and Ryan M. (2004). Logic in Computer Science: Modelling and Reasoning about \nSystems. Cambridge University Press (2nd ed.).\n \n",
        "7th Semester \nADVANCED TOPICS IN COMPUTER ARCHITECTURE \nAims \nThis course aims to provide students with an introduction to a range of advanced topics in \ncomputer architecture. It will explore the current and future challenges facing the architects of \nmodern computers. These will also be used to illustrate the many different influences and \ntrade-offs involved in computer architecture. \n \nObjectives \nOn completion of this module students should: \n \nunderstand the challenges of designing and verifying modern microprocessors \nbe familiar with recent research themes and emerging challenges \nappreciate the complex trade-offs at the heart of computer architecture \nSyllabus \nEach seminar will focus on a different topic. The proposed topics are listed below but there may \nbe some minor changes to this: \n \nTrends in computer architecture \nState-of-the-art microprocessor design \nMemory system design \nHardware reliability \nSpecification, verification and test (may be be replaced with a different topic) \nHardware security (2) \nHW accelerators and accelerators for machine learning \nEach two hour seminar will include three student presentations (15mins) questions (5mins) and \na broader discussion of the topics (around 30mins). The last part of the seminar will include a \nshort scene setting lecture (around 20mins) to introduce the following week's topic. \n \nAssessment \nEach week students will compare and contrast two of the main papers and submit a written \nsummary and review in advance of each seminar (except when presenting). \n \nStudents will be expected to give a number of 15 minute presentations. \n \nEssays and presentations will be marked out of 10. After dropping the lowest mark, the \nremaining marks will be scaled to give a final score out of 100. \n \nStudents will give at least one presentation during the course. They will not be required to \nsubmit an essay during the weeks they are presenting. \n \nEach presentation will focus on a single paper from the reading list. Marks will be awarded for \nclarity and the communication of the paper's key ideas, an analysis of the work's strengths and \nweaknesses and the work\u2019s relationship to related work and broader trends and constraints. \n",
        " \nRecommended prerequisite reading \nPatterson, D. A., Hennessy, J. L. (2017). Computer organization and design: The \nHardware/software interface RISC-V edition Morgan Kaufmann. ISBN 978-0-12-812275-4. \n \nHennessy, J. and Patterson, D. (2012). Computer architecture: a quantitative approach. Elsevier \n(5th ed.) ISBN 9780123838728. (the 3rd and 4th editions are also relevant)\n \n",
        "ADVANCED TOPICS IN PROGRAMMING LANGUAGES \nAims \nThis module explores various topics in programming languages beyond the scope of \nundergraduate courses. It aims to introduce students to ideas, results and techniques found in \nthe literature and prepare them for research in the field. \n \nSyllabus and structure \nThe module consists of eight two-hour seminars, each on a particular topic. Topics will vary from \nyear to year, but may include, for example, \n \nAbstract interpretation \nVerified software \nMetaprogramming \nBehavioural types \nProgram synthesis \nVerified compilation \nPartial evaluation \nGarbage collection \nDependent types \nAutomatic differentiation \nDelimited continuations \nModule systems \nThere will be three papers assigned for each topic, which students are expected to read before \nthe seminar. \nEach seminar will include three 20 minute student presentations (15 minutes + 5 minutes \nquestions), time for general discussion of the topic, and a brief overview lecture for the following \nweek\u2019s topic. \nBefore each seminar, except in weeks in which they give presentations, students will submit a \nshort essay about two of the papers. \n \n \nObjectives \nOn completion of this module, students should \n \nbe able to identify some major themes in programming language research \nbe familiar with some classic papers and recent advances \nhave an understanding of techniques used in the field \n \nAssessment \nAssessment consists of: \n \nPresentation of one of the papers from the reading list (typically once or twice in total for each \nstudent, depending on class numbers) \nOne essay per week (except on the first week and on presentation weeks) \n",
        "All essays and presentations carry equal numbers of marks. \n \nEssay marks are awarded for understanding, for insight and analysis, and for writing quality. \nEssays should be around 1500 words (with a lower limit of 1450 and upper limit of 1650). \nPresentation marks are awarded for clarity, for effective communication, and for selection and \norganisation of topics. \n \nThere will be seven submissions (essays or presentations) in total and, as in other courses, the \nlowest mark for each student will be disregarded when computing the final mark. \n \nMarking, deadlines and extensions will be handled in accordance with the MPhil Assessment \nGuidelines. \n \nRecommended reading material and resources \nResearch and survey papers from programming language conferences and journals (e.g. \nPOPL, PLDI, TOPLAS, FTPL) will be assigned each week. General background material may \nbe found in: \n \n\u2022 Types and Programming Languages (Benjamin C. Pierce) \n   The MIT Press \n   ISBN 0-262-16209-1 \n \n\u2022 Advanced Topics in Types and Programming Languages (ed. Benjamin C. Pierce) \n   The MIT Press \n   ISBN 0-262-16228-8 \n \n\u2022 Practical Foundations for Programming Languages (Robert Harper) \n   Cambridge University Press \n   9781107150300\n \n",
        "AFFECTIVE ARTIFICIAL INTELLIGENCE \n \nSynopsis \nAffective Artificial Intelligence (Affective AI) aims to imbue machines with social and emotional \nintelligence (EQ). More specifically, Affective AI aims to create artificially intelligent systems and \nmachines that can recognize, interpret, process, and simulate human social signals and \nbehaviours, expressions, and emotions, to enhance human-AI interaction and communication. \n\\r\\n\\r\\nTo achieve this goal, Affective AI draws upon various scientific disciplines, including \nmachine learning, computer vision, speech / natural language / signal processing, psychology \nand cognitive science, and ethics and social sciences. \n \n  \nBackground & Aims \nAffective Artificial Intelligence (Affective AI) aims to imbue machines with social and emotional \nintelligence (EQ). More specifically, Affective AI aims to create artificially intelligent systems and \nmachines that can recognize, interpret, process, and simulate human social signals and \nbehaviours, expressions, and emotions, to enhance human-AI interaction and communication.  \n \nTo achieve this goal, Affective AI draws upon various scientific disciplines, including machine \nlearning, computer vision, speech / natural language / signal processing, psychology and \ncognitive science, and ethics and social sciences. \n \nAffective AI has direct applications in and implications for the design of innovative interactive \ntechnology (e.g., interaction with chat bots, virtual agents, robots), single and multi-user smart \nenvironments ( e.g., in-car/ virtual / augmented / mixed reality, serious games), public speaking \nand cognitive training, and clinical and biomedical studies (e.g., autism, depression, pain). \n \nThe aim of this module is to impart knowledge and ability needed to make informed choices of \nmodels, data, and machine learning techniques for sensing, recognition, and generation of \naffective and social behaviour (e.g., smile, frown, head nodding/shaking, \nagreement/disagreement) in order to create Affectively intelligent AI systems, with a \nconsideration for various ethical issues (e.g., privacy, bias) arising from the real-world \ndeployment of these systems. \n \n  \n \nSyllabus \nThe following list provides a representative list of topics: \n \nIntroduction, definitions, and overview \nTheories from various disciplines \nSensing from multiple modalities (e.g., vision, audio, bio signals, text) \nData acquisition and annotation \nSignal processing / feature extraction \n",
        "Learning / prediction / recognition and evaluation \nBehaviour synthesis / generation (e.g., for embodied agents / robots) \nAdvanced topics and ethical considerations (e.g., bias and fairness) \nCross-disciplinary applications (via seminar presentations and discussions) \nGuest lectures (diverse topics \u2013 changes each year) \nHands-on research and programming work (i.e., mini project & report)  \n \n  \n \nObjectives \nOn completion of this module, students will: \n \nunderstand and demonstrate knowledge in key characteristics of affectively intelligent AI, which \ninclude: \nRecognition: How to equip Affective AI systems with capabilities of analysing facial expressions, \nvocal intonations, gestures, and other physiological signals to infer human affective states? \nGeneration: How to enable affectively intelligent AI simulate expressions of emotions in \nmachines, allowing them to respond in a more human-like manner, such as virtual agents and \nhumanoid robots, showing empathy or sympathy? \nAdaptation and Personalization: How to enable affectively intelligent AI adapt system responses \nbased on users' affective states and/or needs, or tailor interactions and experiences to individual \nusers based on their expressivity / emotional profiles, personalities, and past interactions? \nEmpathetic Communication: How to design Affective AI systems to communicate with users in a \nway that demonstrates empathy, understanding, and sensitivity to their affective states and \nneeds? \nEthical and societal considerations: What are the various human differences, ethical guidelines, \nand societal impacts that need to be considered when designing and deploying Affective AI to \nensure that these systems respect users' privacy, autonomy, and well-being? \ncomprehend and apply (appropriate) methods for collection, analysis, representation, and \nevaluation of human affective and communicative behavioural data. \nenhance programming skills for creating and implementing (components of) Affective AI \nsystems. \ndemonstrate critical thinking, analysis and synthesis while deciding on 'when' and 'how' to \nincorporate human affect and social signals in a specific AI system context and gain practical \nexperience in proposing and justifying computational solution(s) of suitable nature and scope. \n  \n \nAssessment - Part II Students \nSeminar presentation: 20% \nParticipating in Q&A and discussions: 10% \nMid-term mini project report and presentation (as a team of 2): 10%  \nFinal mini project report & code (as a team of 2): 60% \n  \n \n",
        "Assessment - MPhil / Part III Students \nSeminar presentation: 20% \nParticipating in Q&A and discussions: 10% \nMid-term mini project report and presentation: 10%  \nFinal mini project report & code: 60% \nFurther Information \nCurrent Cambridge undergraduate students who are continuing onto Part III or the MPhil in \nAdvanced Computer Science may only take this module if they did NOT take it as a Unit of \nAssessment in Part II. \n \nThis module is shared with ACS. Assessment will be adjusted for the two groups of students to \nbe at an appropriate level for whichever course the student is enrolled on. More information will \nfollow at the first lecture. \n \nThis module is shared with Part II of the Computer Science Tripos. Assessment will be adjusted \nfor the two groups of students to be at an appropriate level for whichever course the student is \nenrolled on. Further information about assessment and practicals will follow at the first lecture.\n",
        "CATEGORY THEORY \n \nAims \nCategory theory provides a unified treatment of mathematical properties and constructions that \ncan be expressed in terms of 'morphisms' between structures. It gives a precise framework for \ncomparing one branch of mathematics (organized as a category) with another and for the \ntransfer of problems in one area to another. Since its origins in the 1940s motivated by \nconnections between algebra and geometry, category theory has been applied to diverse fields, \nincluding computer science, logic and linguistics. This course introduces the basic notions of \ncategory theory: adjunction, natural transformation, functor and category. We will use category \ntheory to organize and develop the kinds of structure that arise in models and semantics for \nlogics and programming languages. \n \nSyllabus \nIntroduction; some history. Definition of category. The category of sets and functions. \nCommutative diagrams. Examples of categories: preorders and monotone functions; monoids \nand monoid homomorphisms; a preorder as a category; a monoid as a category. Definition of \nisomorphism. Informal notion of a 'category-theoretic' property. \nTerminal objects. The opposite of a category and the duality principle. Initial objects. Free \nmonoids as initial objects. \nBinary products and coproducts. Cartesian categories. \nExponential objects: in the category of sets and in general. Cartesian closed categories: \ndefinition and examples. \nIntuitionistic Propositional Logic (IPL) in Natural Deduction style. Semantics of IPL in a cartesian \nclosed preorder. \nSimply Typed Lambda Calculus (STLC). The typing relation. Semantics of STLC types and \nterms in a cartesian closed category (ccc). The internal language of a ccc. The \nCurry-Howard-Lawvere correspondence. \nFunctors. Contravariance. Identity and composition for functors. \nSize: small categories and locally small categories. The category of small categories. Finite \nproducts of categories. \nNatural transformations. Functor categories. The category of small categories is cartesian \nclosed. \nHom functors. Natural isomorphisms. Adjunctions. Examples of adjoint functors. Theorem \ncharacterising the existence of right (respectively left) adjoints in terms of a universal property. \nDependent types. Dependent product sets and dependent function sets as adjoint functors. \nEquivalence of categories. Example: the category of I-indexed sets and functions is equivalent \nto the slice category Set/I. \nPresheaves. The Yoneda Lemma. Categories of presheaves are cartesian closed. \nMonads. Modelling notions of computation as monads. Moggi's computational lambda calculus. \nObjectives \nOn completion of this module, students should: \n \n",
        "be familiar with some of the basic notions of category theory and its connections with logic and \nprogramming language semantics \nAssessment \na graded exercise sheet (25% of the final mark), and \na take-home test (75%) \nRecommended reading \nAwodey, S. (2010). Category theory. Oxford University Press (2nd ed.). \n \nCrole, R. L. (1994). Categories for types. Cambridge University Press. \n \nLambek, J. and Scott, P. J. (1986). Introduction to higher order categorical logic. Cambridge \nUniversity Press. \n \nPitts, A. M. (2000). Categorical Logic. Chapter 2 of S. Abramsky, D. M. Gabbay and T. S. E. \nMaibaum (Eds) Handbook of Logic in Computer Science, Volume 5. Oxford University Press. \n(Draft copy available here.) \n \nClass Size \nThis module can accommodate upto 15 Part II students plus 15 MPhil / Part III students. \n \nPre-registration evaluation form  \nStudents who are interested in taking this module will be asked to complete a pre-registration \nevaluation form to determine whether they have enough mathematical background to take the \nmodule.  \n \nThis will take place during the week Monday 12 August - Friday 16 August.  \n \nCoursework \nThe coursework in this module will consist of a graded exercise sheet. \n \nFurther Information \nCurrent Cambridge undergraduate students who are continuing onto Part III or the MPhil in \nAdvanced Computer Science may only take this module if they did NOT take 'Catgeory Theory' \nas a Unit of Assessment in Part II. \n \nThis module is shared with Part II of the Computer Science Tripos. Assessment will be adjusted \nfor the two groups of students to be at an appropriate level for whichever course the student is \nenrolled on. Further information about assessment and practicals will follow at the first lecture.\n",
        "DIGITAL MONEY AND DECENTRALISED FINANCE \n \nAims \nOur society is evolving towards digital payments and the elimination of physical cash. Digital \nalternatives to cash require trade-offs between various properties including unforgeability, \ntraceability, divisibility, transferability without intermediaries, privacy, redeemability, control over \nthe money supply and many more, often in conflict with each other. Since the 1980s, \ncryptographers have proposed a variety of clever technical solutions addressing some of these \nissues but it is only with the appearance of Bitcoin, blockchain and a plethora of copycat \ncryptocurrencies that a new asset class has now emerged, worth in excess of two trillion dollars \n(although plagued by extreme volatility). Meanwhile, most major countries have been planning \nthe introduction of Central Bank Digital Currencies in an attempt to retain control. This \nseminar-style interactive module encourages the students to understand what's happening, \nwhat the underlying problems and opportunities are (technical, social and economic) and where \nwe should go from here. We are at a historical turning point where an enterprising candidate \nmight disrupt the status quo and truly make a difference. \n \nSyllabus \nUnderstanding money: from gold standard to digital cash \nIntro to the Decentralised Finance (DeFi) ecosystem: Bitcoin, blockchain, wallets, smart \ncontracts \nStablecoins and Central Bank Digital Currencies \nDecentralised Autonomous Organisations and Automated Market Makers \nYield Farming, Perpetual Futures and Maximal Extractable Value \nWeb3, Non Fungible Tokens \nCryptocurrency crashes; crimes facilitated by cryptocurrencies \nNew areas of development. Where from here? \nThe module is structured as a reading club with a high degree of interactivity. Aside from the first \nand last session, which are treated differently, the regular two-hour sessions have the following \nstructure (not necessarily in this order). \n \nTwo half-hour debates on each of two papers \nTwo rapid-fire presentations on open questions previously assigned by the lecturers. \nHalf an hour of presentation by the lecturers. \nAbout ten minutes of buffer that can account for switchover time or be absorbed into the \nlecturers' presentation. \nThe first session is scene-setting by the lecturers, with no student presentations. The last \nsession has no paper debates, only one rapid-fire presentation on an open question from each \nof the students. \n \nObjectives \nOn completion of this module, students will gain an in-depth understanding of digital money \nalternatives as well as many modern applications and components of Decentralised Finance. By \ncomparing DeFi with current processes in traditional banking, students will be able to discuss \n",
        "the merits and limitations of these new technologies as well as to identify potential new areas of \ndevelopment. \n \nThrough having to write, present and defend very brief extended abstracts, the students will \nimprove their communication skills and in particular the ability to distil and convey the most \nimportant points forcefully and concisely. \n \n \nAssessment \nThe module is an interactive reading club. Each student produces four output triplets throughout \nthe course. Each output triplet consists of three parts: a 1200-word essay, a 200-word extended \nabstract and a 7-minute presentation. \n \nThe essay and abstract must be submitted in advance, using the LaTeX templates provided. \nThe abstract must consist of full sentences, not bullet points, and must fit on a single slide. \nDuring the presentation, the slide with the abstract will be projected on screen as the only visual \naid; the essay will not be accessible to either presenter or audience. Reading out the slide \nverbatim will obviously not count as a great presentation. Presenters must be able to expand \nand defend their ideas live, and answer questions from lecturers and audience. \n \nThe essays, the abstracts and the presentations are graded separately, each out of 10, and \nassigned weights of 60%, 20%, 20%, respectively, within the triplet. \n \nIn a regular 2-hour session (all but the first and last) there will be two paper slots and two \ninvestigation slots, plus a lecturer slot, described next. \n \nEach paper slot (30 min) involves a \u201csupporter\u201d and a \u201cdissenter\u201d for the paper, each \ncontributing one output triplet, and then a panel Q&A where all the other participants quiz the \nsupporter and dissenter. \n \nEach investigation slot (10 min) involves a single student contributing an output triplet on a \ngiven theme. \n \nRoles, papers and themes are assigned by the lecturers the previous week. \n \nIn the first session there will be no student presentations, only lecturer-led exploration. In the \nlast session there will be 12 investigation slots and no paper slots. \n \nOverall, each student will be supporter once, dissenter once and investigator twice, for a total of \n4 output triplets. The output triplets of the last session will be weighed twice as much as the \nothers, i.e. 40% each, while the others 20% each. \n \nAttendance is required at all sessions. Missing a session in which the candidate was due to \npresent will result in the loss of the corresponding presentation marks, while the written work will \n",
        "still have to be submitted and will be marked as usual. Missing a session in which the candidate \nwas not due to present will result in the loss of 3%. \n \nRecommended reading \nThere isn't a textbook covering all the material in this course. The following textbook, also \nadopted by R47 Distributed Ledger Technologies (also freely available online), is a thorough \nintroduction to Bitcoin and Blockchain, but we will complement it with research papers and \nindustry white papers for each lecture. \n \nNarayanan, A. , Bonneau, J., Felten, E., Miller, A. and Goldfeder, S. (2016). Bitcoin and \nCryptocurrency Technologies: A Comprehensive Introduction. Princeton University Press. \n(2016 Draft available here: \nhttps://d28rh4a8wq0iu5.cloudfront.net/bitcointech/readings/princeton_bitcoin_book.pdf)\n \n",
        "DIGITAL SIGNAL PROCESSING \n \nAims \nThis course teaches basic signal-processing principles necessary to understand many modern \nhigh-tech systems, with examples from audio processing, image coding, radio communication, \nradar, and software-defined radio. Students will gain practical experience from numerical \nexperiments in programming assignments (in Julia, MATLAB or NumPy). \n \nLectures \nSignals and systems. Discrete sequences and systems: types and properties. Amplitude, \nphase, frequency, modulation, decibels, root-mean square. Linear time-invariant systems, \nconvolution. Some examples from electronics, optics and acoustics. \nPhasors. Eigen functions of linear time-invariant systems. Review of complex arithmetic. \nPhasors as orthogonal base functions. \nFourier transform. Forms and properties of the Fourier transform. Convolution theorem. Rect \nand sinc. \nDirac\u2019s delta function. Fourier representation of sine waves, impulse combs in the time and \nfrequency domain. Amplitude-modulation in the frequency domain. \nDiscrete sequences and spectra. Sampling of continuous signals, periodic signals, aliasing, \ninterpolation, sampling and reconstruction, sample-rate conversion, oversampling, spectral \ninversion. \nDiscrete Fourier transform. Continuous versus discrete Fourier transform, symmetry, linearity, \nFFT, real-valued FFT, FFT-based convolution, zero padding, FFT-based resampling, \ndeconvolution exercise. \nSpectral estimation. Short-time Fourier transform, leakage and scalloping phenomena, \nwindowing, zero padding. Audio and voice examples. DTFM exercise. \nFinite impulse-response filters. Properties of filters, implementation forms, window-based FIR \ndesign, use of frequency-inversion to obtain high-pass filters, use of modulation to obtain \nband-pass filters. \nInfinite impulse-response filters. Sequences as polynomials, z-transform, zeros and poles, some \nanalog IIR design techniques (Butterworth, Chebyshev I/II, elliptic filters, second-order cascade \nform). \nBand-pass signals. Band-pass sampling and reconstruction, IQ up and down conversion, \nsuperheterodyne receivers, software-defined radio front-ends, IQ representation of AM and FM \nsignals and their demodulation. \nDigital communication. Pulse-amplitude modulation. Matched-filter detector. Pulse shapes, \ninter-symbol interference, equalization. IQ representation of ASK, BSK, PSK, QAM and FSK \nsignals. [2 hours] \nRandom sequences and noise. Random variables, stationary and ergodic processes, \nautocorrelation, cross-correlation, deterministic cross-correlation sequences, filtered random \nsequences, white noise, periodic averaging. \nCorrelation coding. Entropy, delta coding, linear prediction, dependence versus correlation, \nrandom vectors, covariance, decorrelation, matrix diagonalization, eigen decomposition, \n",
        "Karhunen-Lo\u00e8ve transform, principal component analysis. Relation to orthogonal transform \ncoding using fixed basis vectors, such as DCT. \nLossy versus lossless compression. What information is discarded by human senses and can \nbe eliminated by encoders? Perceptual scales, audio masking, spatial resolution, colour \ncoordinates, some demonstration experiments. \nQuantization, image coding standards. Uniform and logarithmic quantization, A/\u00b5-law coding, \ndithering, JPEG. \nObjectives \napply basic properties of time-invariant linear systems; \nunderstand sampling, aliasing, convolution, filtering, the pitfalls of spectral estimation; \nexplain the above in time and frequency domain representations; \nuse filter-design software; \nvisualize and discuss digital filters in the z-domain; \nuse the FFT for convolution, deconvolution, filtering; \nimplement, apply and evaluate simple DSP applications; \nfamiliarity with a number of signal-processing concepts used in digital communication systems \nRecommended reading \nLyons, R.G. (2010). Understanding digital signal processing. Prentice Hall (3rd ed.). \nOppenheim, A.V. and Schafer, R.W. (2007). Discrete-time digital signal processing. Prentice \nHall (3rd ed.). \nStein, J. (2000). Digital signal processing \u2013 a computer science perspective. Wiley. \n \nClass size \nThis module can accommodate a maximum of 24 students (16 Part II students and 8 MPhil \nstudents) \n \nAssessment - Part II students \nThree homework programming assignments, each comprising 20% of the mark \nWritten test, comprising 40% of the total mark. \nAssessment - Part III and MPhil students \nThree homework programming exercises, each comprising 10% of the mark. \nWritten test, comprising 20% of the total mark. \nIndividual implementation project with 8-page written report, topic agreed with lecturer, \ncomprising 50% of the mark. \nFurther Information \nCurrent Cambridge undergraduate students who are continuing onto Part III or the MPhil in \nAdvanced Computer Science may only take this module if they did NOT take it as a Unit of \nAssessment in Part II. \n \nThis module is shared with Part II of the Computer Science Tripos. Assessment will be adjusted \nfor the two groups of students to be at an appropriate level for whichever course the student is \nenrolled on. Further information about assessment and practicals will follow at the first lecture.\n",
        "INTRODUCTION TO COMPUTATIONAL SEMANTICS \n \nAims \nThis is a lecture-style course that introduces students to various aspects of the semantics of \nNatural Languages (mainly English): \n \nLexical Semantics, with an emphasis on theory and phenomenology  \nCompositional Semantics \nDiscourse and pragmatics-related aspects of semantics \nObjectives \nGive an operational definition of what is meant by \u201cmeaning\u201d (for instance, above and beyond \nsyntax); \nName the types of phenomena in language that require semantic consideration, in terms of \nlexical, compositional and discourse/pragmatic aspects, in other words, argue why semantics is \nimportant; \nDemonstrate an understanding of the basics of various semantic representations, including \nlogic-based and graph-based semantic representations, their properties, how they are used and \nwhy they are important, and how they are different from syntactic representations; \nKnow how such semantic representations are derived during or after parsing, and how they can \nbe analysed and mapped to surface strings; \nUnderstand applications of semantic representations e.g. reasoning, validation, and methods \nhow these are approached. \nWhen designing NL tasks that clearly require semantic processing (e.g. knowledge-based QA), \nto be aware of and reuse semantic representations and algorithms when designing the task, \nrather than reinventing the wheel. \nPractical advantages of this course for NLP students \nKnowledge of underlying semantic effects helps improve NLP evaluation, for instance by \nproviding more meaningful error analysis. You will be able to link particular errors to design \ndecisions inside your system. \nYou will learn methods for better benchmarking of your system, whatever the task may be. \nSupervised ML systems (in particular black-box systems such as Deep Learning) are only as \nclever as the datasets they are based on. In this course, you will learn to design datasets so that \nthey are harder to trick without real understanding, or critique existing datasets. \nYou will be able to design tests for ML systems that better pinpoint which aspects of language \nan end-to-end system has \u201cunderstood\u201d. \nYou will learn to detect ambiguity and ill-formed semantics in human-human communication. \nThis can serve to write more clearly and logically. \nYou will learn about decomposing complex semantics-reliant tasks sensibly so that you can \nreuse the techniques underlying semantic analyzers in a modular way. In this way, rather than \nbeing forced to treat complex tasks in an end-to-end manner, you will be able to profit from \npartial explanations and a better error analysis already built into the system. \nSyllabus \nOverview of the course \nEvents and semantic role labelling \n",
        "Referentiality and coreference resolution \nTruth-conditional semantics \nGraph-based meaning representation \nCompositional semantics \nContext-free Graph Rewriting \nSurface realisation \nNegation and psychological approach to semantics \nDynamic semantics \nGricean pragmatics \nVector space models \nCross-modality \nAcquisition of semantics \nDiachronic change of semantics \nSummary of the course \n  \n \nAssessment \n5 take-home exercises worth 20% each: \n \nTake-home assignment 1 is given in Lecture 4 and due is Lecture 6 \n \nStudents are given 10 English sentences and asked to provide their semantic analysis \naccording to truth-conditions. Students will be expected to return 10 logical expressions as their \nanswers. No word limit. Assessment criteria: correctness of the 10 logical expressions; 2 points \nfor each logical expression. \n \nTake-home assignment 2 is given in Lecture 7 and due is Lecture 9  \n \nStudents are given 10 English sentences and asked to provide their syntactico-semantic \nderivations according to the compositionality principle. Students will be expected to return \nderivation graphs as their answers. No word limit. Assessment criteria: correctness of the 10 \nderivation graphs; 2 points for each derivation. \n \nTake-home assignment 3 is given in Lecture 11 and due is Lecture 13  \n \nAll students are assigned with a paper on modelling common ground in dialogue system. \nStudents will receive related but different papers. Each student will write a review of their \nassigned paper, including a comprehensive summary and their own thoughts. Word limit: 1000 \nwords. Assessment criteria: 15 points on whether a student understands the paper correctly; 5 \npoints on whether a student is able to think critically. \n \nTake-home assignment 4 is given in Lecture 13 and due is Lecture 15 \n \n",
        "All students are assigned with a paper on language-vision interaction. Students will receive \nrelated but different papers. Each student will write a review of their assigned paper, including a \ncomprehensive summary and their own thoughts. Word limit: 1000 words. Assessment criteria: \n15 points on whether a student understands the paper correctly; 5 points on whether a student \nis able to think critically. \n \nTake-home assignment 5 is given in Lecture 14 and due is two weeks later. \n \nContent: All students are assigned with a paper on bootstrapping language acquisition. All \nstudents will receive the same paper. Each student will write a review of the paper, including a \ncomprehensive summary and their own thoughts. Word limit: 1000 words. Assessment criteria: \n15 points on whether a student understands the paper correctly; 5 points on whether a student \nis able to think critically.\n \n",
        "INTRODUCTION TO NATURAL LANGUAGE SYNTAX AND PARSING \n \nAims \nThis module aims to provide a brief introduction to linguistics for computer scientists and then \ngoes on to cover some of the core tasks in natural language processing (NLP), focussing on \nstatistical parsing of sentences to yield syntactic representations. We will look at how to \nevaluate parsers and see how well state-of-the-art tools perform given current techniques. \n \nSyllabus \nLinguistics for NLP focusing on morphology and syntax \nGrammars and representations \nStatistical and neural parsing \nTreebanks and evaluation \nObjectives \nOn completion of this module, students should: \n \nunderstand the basic properties of human languages and be familiar with descriptive and \ntheoretical frameworks for handling these properties; \nunderstand the design of tools for NLP tasks such as parsing and be able to apply them to text \nand evaluate their performance; \nPractical work \nWeek 1-7: There will be non-assessed practical exercises between sessions and during \nsessions. \nWeek 8: Download and apply two parsers to a designated text. Evaluate the performance of the \ntools quantitatively and qualitatively. \nAssessment \nThere will be one presentation worth 10% of the final mark; presentation topics will be allocated \nat the start of term. \nAn assessed practical report of not more 8 pages in ACL conference format. It will contribute \n90% of the final mark. \nRecommended reading \nJurafsky, D. and Martin, J. (2008). Speech and Language Processing. Prentice-Hall (2nd ed.). \n(See also 3rd ed. available online.)\n \n",
        "INTRODUCTION TO NETWORKING AND SYSTEMS MEASUREMENTS \n \nAims \nSystems research refers to the study of a broad range of behaviours arising from complex \nsystem design, including: resource sharing and scheduling; interactions between hardware and \nsoftware; network topology, protocol and device design and implementation; low-level operating \nsystems; Interconnect, storage and more. This module will: \n \nTeach performance characteristics and performance measurement methodology and practice \nthrough profiling experiments; \nExpose students to real-world systems artefacts evident through different measurement tools; \nDevelop scientific writing skills through a series of laboratory reports; \nProvide research skills for characterization and modelling of systems and networks using \nmeasurements. \nPrerequisites \nIt is strongly recommended that students have previously (and successfully) completed an \nundergraduate networking course -- or have equivalent experience through project or \nopen-source work. \n \nSyllabus \nIntroduction to performance measurements, performance characteristics [1 lecture] \nPerformance measurements tools and techniques [2 lectures + 2 lab sessions] \nReproducible experiments [1 lecture + 1 lab session] \nCommon pitfalls in measurements [1 lecture] \nDevice and system characterisation [1 lecture + 2 lab sessions] \nObjectives \nOn completion of this module, students should: \n \nDescribe the objectives of measurements, and what they can achieve; \nCharacterise and model a system using measurements; \nPerform reproducible measurements experiments; \nEvaluate the performance of a system using measurements; \nOperate measurements tools and be aware of their limitations; \nDetect anomalies in the network and avoid common measurements pitfalls; \nWrite system-style performance evaluations. \nPractical work \nFive 2-hour in-classroom labs will ask students to develop and use skills in performance \nmeasurements as applied to real-world systems and networking artefacts. Results from these \nlabs (and follow-up work by students outside of the classroom) will by the primary input to lab \nreports. \n \nThe first three labs will provide an introduction and hands on experience with measurement \ntools and measurements methodologies, while the last two labs will focus on practical \nmeasurements and evaluation of specific platforms. Students may find it useful to work in pairs \n",
        "within the lab, but must prepare lab reports independently. The module lecturer will give a short \nintroductory at the start of each lab, and instructors will be on-hand throughout labs to provide \nassistance. \n \nLab participation is not directly included in the final mark, but lab work is a key input to lab \nreports that are assessed. Guided lab experiments resulting in practical write ups. \n \nAssessment \nEach student will write two lab reports. The first lab report will summarise the experiments done \nin the first three labs (20%). The second will be a lab report (5000 words) summarising the \nevaluation of a device or a system (80%). \n \nRecommended reading \nThe following list provides some background to the course materials, but is not mandatory. A \nreading list, including research papers, will be provided in the course materials. \n \nGeorge Varghese. Network algorithmics. Chapman and Hall/CRC, 2010. \nMark Crovella and Balachander Krishnamurthy. Internet measurement: infrastructure, traffic and \napplications. John Wiley and Sons, Inc., 2006. \nBrendan Gregg. Systems Performance: Enterprise and the Cloud, Prentice Hall Press, Upper \nSaddle River, NJ, USA, October 2013. \nRaj Jain, The Art of Computer Systems Performance Analysis: Techniques for Experimental \nDesign, Measurement, Simulation, and Modeling, Wiley - Interscience, New York, NY, USA, \nApril 1991.\n \n",
        "LARGE-SCALE DATA PROCESSING AND OPTIMISATION \n \nAims \nThis module provides an introduction to large-scale data processing, optimisation, and the \nimpact on computer system's architecture. Large-scale distributed applications with high volume \ndata processing such as training of machine learning will grow ever more in importance. \nSupporting the design and implementation of robust, secure, and heterogeneous large-scale \ndistributed systems is essential. To deal with distributed systems with a large and complex \nparameter space, tuning and optimising computer systems is becoming an important and \ncomplex task, which also deals with the characteristics of input data and algorithms used in the \napplications. Algorithm designers are often unaware of the constraints imposed by systems and \nthe best way to consider these when designing algorithms with massive volume of data. On the \nother hand, computer systems often miss advances in algorithm design that can be used to cut \ndown processing time and scale up systems in terms of the size of the problem they can \naddress. Integrating machine learning approaches (e.g. Bayesian Optimisation, Reinforcement \nLearning) for system optimisation will be explored in this course. \n \nSyllabus \nThis course provides perspectives on large-scale data processing, including data-flow \nprogramming, graph data processing, probabilistic programming and computer system \noptimisation, especially using machine learning approaches, thus providing a solid basis to work \non the next generation of distributed systems. \n \nThe module consists of 8 sessions, with 5 sessions on specific aspects of large-scale data \nprocessing research. Each session discusses 3-4 papers, led by the assigned students. One \nsession is a hands-on tutorial on on dataflow programming fundamentals. The first session \nadvises on how to read/review a paper together with a brief introduction on different \nperspectives in large-scale data \nprocessing and optimisation. The last session is dedicated to the student presentation of \nopensource project studies. \n \nIntroduction to large-scale data processing and optimisation \nData flow programming: Map/Reduce to TensorFlow \nLarge-scale graph data processing: storage, processing model and parallel processing \nDataflow programming hands-on tutorial \nProbabilistic Programming \nOptimisations in ML Compiler \nOptimisations of Computer systems using ML \nPresentation of Open Source Project Study \nObjectives \nOn completion of this module, students should: \n \nUnderstand key concepts of scalable data processing approaches in future computer systems. \n",
        "Obtain a clear understanding of building distributed systems using data centric programming \nand large-scale data processing. \nUnderstand a large and complex parameter space in computer system's optimisation and \napplicability of Machine Learning approach. \nCoursework \nReading Club: \nThe preparation for the reading club will involve 1-3 papers every week. At each session, \naround 3-4 papers are selected under the given topic, and the students present their review \nwork. \nHands-on tutorial session of data flow programming including writing an application of \nprocessing streaming in Twitter data and/or Deep Neural Networks using Google TensorFlow \nusing cluster computing. \nReports \nThe following three reports are required, which could be extended from the assignment of the \nreading club, within the scope of data centric systems. \n \nReview report on a full length paper (max 1800 words) \nDescribe the contribution of the paper in depth with criticisms \nCrystallise the significant novelty in contrast to other related work \nSuggestions for future work \nSurvey report on sub-topic in large-scale data processing and optimisation (max 2000 words) \nPick up to 5 papers as core papers in the survey scope \nRead the above and expand reading through related work \nComprehend the view and finish an own survey paper \nProject study and exploration of a prototype (max 2500 words) \nWhat is the significance of the project in the research domain? \nCompare with similar and succeeding projects \nDemonstrate the project by exploring its prototype \nReports 1 should be handed in by the end of 5th week; Report 2 in the 8th week and Report 3 \non the first day of Lent Term. \n \nAssessment \nThe final grade for the course will be provided as a percentage, and the assessment will consist \nof two parts: \n \n25%: for reading club (participation, presentation) \n75%: for the three reports: \n15%: Intensive review report \n25%: Survey report \n35%: Project study \nRecommended reading \nM. Abadi et al. TensorFlow: A System for Large-Scale Machine Learning, OSDI, 2016. \nD. Aken et al.: Automatic Database Management System Tuning Through Large-scale Machine \nLearning, SIGMOD, 2017. \n",
        "J. Ansel et al. Opentuner: an extensible framework for program autotuning. PACT, 2014. \nV. Dalibard, M. Schaarschmidt, E. Yoneki. BOAT: Building Auto-Tuners with Structured Bayesian \nOptimization, WWW, 2017. \nJ. Dean et al. Large scale distributed deep networks. NIPS, 2012. \nG. Malewicz, M. Austern, A. Bik, J. Dehnert, I. Horn, N. Leiser, G. Czajkowski. Pregel: A System \nfor Large-Scale Graph Processing, SIGMOD, 2010. \nA. Mirhoseini et al. Device Placement Optimization with Reinforcement Learning, ICML, 2017. \nD. Murray, F. McSherry, R. Isaacs, M. Isard, P. Barham, M. Abadi. Naiad: A Timely Dataflow \nSystem, SOSP, 2013. \nM. Schaarschmidt, S. Mika, K. Fricke and E. Yoneki: RLgraph: Modular Computation Graphs for \nDeep Reinforcement Learning, SysML, 2019. \nZ. Jia, O. Padon, J. Thomas, T. Warszawski, M. Zaharia,  A. Aiken: TASO: Optimizing Deep \nLearning Computation with Automated Generation of Graph Substitutions: SOSP, 2019. \nH. Mao et al.: Park: An Open Platform for Learning-Augmented Computer Systems, \nOpenReview, 2019. \nJ. Shao, et al.: Tensor Program Optimization with Probabilistic Programs, NeurIPS, 2022.  \nA complete list can be found on the course material web page. See also 2023-2024 course \nmaterial on the previous course Large-Scale Data Processing and Optimisation.\n \n",
        "MACHINE LEARNING AND THE PHYSICAL WORLD \nAims \nThe module \u201cMachine Learning and the Physical World\u201d is focused on machine learning \nsystems that interact directly with the real world. Building artificial systems that interact with the \nphysical world have significantly different challenges compared to the purely digital domain. In \nthe real world data is scares, often uncertain and decisions can have costly and irreversible \nconsequences. However, we also have the benefit of centuries of scientific knowledge that we \ncan draw from. This module will provide the methodological background to machine learning \napplied in this scenario. We will study how we can build models with a principled treatment of \nuncertainty, allowing us to leverage prior knowledge and provide decisions that can be \ninterrogated. \n \nThere are three principle points about machine learning in the real world that will concern us. \n \nWe often have a mechanistic understanding of the real world which we should be able to \nbootstrap to make decisions. For example, equations from physics or an understanding of \neconomics. \nReal world decisions have consequences which may have costs, and often these cost functions \nneed to be assimilated into our machine learning system. \nThe real world is surprising, it does things that you do not expect and accounting for these \nchallenges requires us to build more robust and or interpretable systems. \nDecision making in the real world hasn\u2019t begun only with the advent of machine learning \ntechnologies. There are other domains which take these areas seriously, physics, environmental \nscientists, econometricians, statisticians, operational researchers. This course identifies how \nmachine learning can contribute and become a tool within these fields. It will equip you with an \nunderstanding of methodologies based on uncertainty and decision making functions for \ndelivering on these challenges. \n \nObjectives \nYou will gain detailed knowledge of \n \nsurrogate models and uncertainty \nsurrogate-based optimization \nsensitivity analysis \nexperimental design \nYou will gain knowledge of \n \ncounterfactual analysis \nsurrogate-based quadrature \nSchedule \nWeek 1: Introduction to the unit and foundation of probabilistic modelling \n \nWeek 2: Gaussian processes and probablistic inference \n \n",
        "Week 3: Simulation and Sequential decision making under uncertainty \n \nWeek 4: Emulation and Experimental Design \n \nWeek 5: Sensitivity Analysis and Multifidelity Modelling \n \nWeek 6-8: Case studies of applications and Projects \n \nPractical work \nDuring the first five weeks of the unit we will provide a weekly worksheet that will focus on \nimplementation and practical exploration of the material covered in the lectures. The worksheets \nwill allow you to build up a these methods without relying on extensive external libraries. You are \nfree to use any programming language of choice however we highly recommended the use of \n=Python=. \n \nAssessment \nTwo individual tasks (15% each) \nGroup Project (70%). Each group will work on an application of uncertainty that covers the \nmaterial of the first 5 weeks of lectures in the unit. Each group will submit a report which will \nform the basis of the assessment. In addition to the report each group will also attend a short \noral examination based on the material covered both in the report and the taught material. \nRecommended Reading \nRasmussen, C. E. and Williams, C. K. I. (2006). Gaussian Processes for Machine Learning. MIT \nPress \n \nBishop, C. (2006). Pattern recognition and machine learning. Springer. \nhttps://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-an\nd-Machine-Learning-2006.pdf \n \nLaplace, P. S. (1902). A Philosophical Essay on Probabilities. John Wiley & Sons. \nhttps://archive.org/details/philosophicaless00lapliala\n \n",
        "MACHINE VISUAL PERCEPTION \nAims \nThis course aims at introducing the theoretical foundations and practical techniques for machine \nperception, the capability of computers to interpret data resulting from sensor measurements. \nThe course will teach the fundamentals and modern techniques of machine perception, i.e. \nreconstructing the real world starting from sensor measurements with a focus on machine \nperception for visual data. The topics covered will be image/geometry representations for \nmachine perception, semantic segmentation, object detection and recognition, geometry \ncapture, appearance modeling and acquisition, motion detection and estimation, \nhuman-in-the-loop machine perception, select topics in applied machine perception. \n \nMachine perception/computer vision is a rapidly expanding area of research with real-world \napplications. An understanding of machine perception is also important for robotics, interactive \ngraphics (especially AR/VR), applied machine learning, and several other fields and industries. \nThis course will provide a fundamental understanding of and practical experience with the \nrelevant techniques. \n \nLearning outcomes \nStudents will understand the theoretical underpinnings of the modern machine perception \ntechniques for reconstructing models of reality starting from an incomplete and imperfect view of \nreality. \nStudents will be able to apply machine perception theory to solve practical problems, e.g. \nclassification of images, geometry capture. \nStudents will gain an understanding of which machine perception techniques are appropriate for \ndifferent tasks and scenarios. \nStudents will have hands-on experience with some of these techniques via developing a \nfunctional machine perception system in their projects. \nStudents will have practical experience with the current prominent machine perception \nframeworks. \nSyllabus \nThe fundamentals of machine learning for machine perception \nDeep neural networks and frameworks for machine perception \nSemantic segmentation of objects and humans \nObject detection and recognition \nMotion estimation, tracking and recognition \n3D geometry capture \nAppearance modeling and acquisition \nSelect topics in applied machine perception \nAssessment \nA practical exercise, worth 20% of the mark. This will cover the basics and theory of machine \nperception and some of the practical techniques the students will likely use in their projects. This \nis individual work. No GPU hours will be needed for the practical work. \nA machine perception project worth 80% of the marks: \n",
        "Course projects will be selected by the students following the possible project themes proposed \nby the lecturer, and will be checked by the lecturer for appropriateness.  \nThe students will form groups of 2-3 to design, implement, report, and present a project to tackle \na given task in machine perception. \nThe final mark will be composed of an implementation/report mark (60%) and a presentation \nmark (20%). Each team member will be evaluated based on her/his contribution. \nEach project will have extensions to be completed only by the ACS students. Each student will \nwrite a different part of the report, whose author will be clearly marked. Each student will further \nsummarise her/his contributions to the project in the same report. \nRecommended Reading List \nComputer Vision: Algorithms and Applications, Richard Szeliski, Springer, 2010. \nDeep Learning, Ian Goodfellow, Yoshua Bengio, and Aaron Courville, MIT Press, 2016. \nMachine Learning and Visual Perception, Baochang Zhang, De Gruyter, 2020. \nFurther Information \nCurrent Cambridge undergraduate students who are continuing onto Part III or the MPhil in \nAdvanced Computer Science may only take this module if they did NOT take it as a Unit of \nAssessment in Part II. \n \nThis module is shared with Part II of the Computer Science Tripos. Assessment will be adjusted \nfor the two groups of students to be at an appropriate level for whichever course the student is \nenrolled on. Further information about assessment and practicals will follow at the first lecture.\n",
        "MOBILE, WEARABLE SYSTEMS AND MACHINE LEARNING \n \nAims \nThis module aims to introduce the latest research advancements in mobile systems and mobile \ndata machine learning, spanning a range of domains including systems, data gathering, \nanalytics and machine learning, on device machine learning and applications such as health, \ntransportation, behavior monitoring, cyber-physical systems, autonomous vehicles, drones. The \ncourse will cover current and seminal research papers in the area of research. \n \nSyllabus \nThe course will consist of one introductory lecture, seven two-hour, and one three-hour, \nsessions covering a variety of topics roughly including the following material (some variation in \nthe topics might happen from year to year): \n \nSystem, Energy and Security \nBackscatter Communication, Battery Free and Energy Harvesting Devices \nNew Sensing Modalities \nMachine Learning on Wearable Data \nOn Device Machine Learning \nMobile and Wearable Health \nMobile and Wearable Systems of Sustainability \nEach week, three class participants will be assigned to introduce assigned three papers via \n20-minute presentations, conference-style and highlighting critically its features. Each \npresentation will be followed by 10 minutes of questions. This will be followed by 10 minutes of \ngeneral discussion. Slides will be used for presentation. \n \nStudents will give one or more presentations each term. Each student will submit a paper review \neach week for one of the three papers presented except for the week they will be presenting \nslides. Each review will follow a template and be up to 1,000 words. Each review will receive a \nmaximum of 10 points. As a result, each student will produce 6-7 reviews and at least one \npresentation, probably two. All participants are expected to attend and participate in every class; \nthe instructor must be notified of any absences in advance. \n \nObjectives \nOn completion of this module students should have an understanding of the recent key research \nin mobile and sensor systems and mobile analytics as well as an improved critical thinking over \nresearch papers. \n \nAssessment \nAggregate mark for 7 assignments from 6-7 reports and 1-2 presentations. Each report or \npresentation will contribute one seventh of the course mark. \nA tick for presence and participation to each class will also be awarded. \nRecommended reading \n",
        "Readings from the most recent conferences such as AAAI, ACM KDD, ACM MobiCom, ACM \nMobiSys, ACM SenSys, ACM UbiComp, ICLR, ICML Neurips, and WWW pertinent to mobile \nsystems and data.\n \n",
        "NETWORK ARCHITECTURES \nAims \nThis module aims to provide the world with more network architects. The 2011-2012 version \nwas oriented around the evolution of IP to support new services like multicast, mobility, \nmultihoming, pub/sub and, in general, data oriented networking. The course is a paper reading \nwhich puts the onus on the student to do the work. \n \nSyllabus \nIPng [2 lectures, Jon Crowcroft] \nNew Architectures [2 lectures, Jon Crowcroft] \nMulticast [2 lectures, Jon Crowcroft] \nContent Distribution and Content Centric Networks [2 lectures, Jon Crowcroft] \nResource Pooling [2 lectures, Jon Crowcroft] \nGreen Networking [2 lectures, Jon Crowcroft] \nAlternative Router Implementions [2 lectures, Jon Crowcroft] \nData Center Networks [2 Lectures, Jon Crowcroft] \nObjectives \nOn completion of this module, students should be able to: \n \ncontribute to new network system designs; \nengineer evolutionary changes in network systems; \nidentify and repair architectural design flaws in networked systems; \nsee that there are no perfect solutions (aside from academic ones) for routing, addressing, \nnaming; \nunderstand tradeoffs in modularisation and other pressures on clean software systems \nimplementation, and see how the world is changing the proper choices in protocol layering, or \nnon layered or cross-layered. \nCoursework \nAssessment is through three graded essays (each chosen individually from a number of \nsuggested or student-chosen topics), as follows: \n \nAnalysis of two different architectures for a particular scenario in terms of cost/performance \ntradeoffs for some functionality and design dimension, for example: \nATM \u2013 e.g. for hardware versus software tradeoff \nIP \u2013 e.g. for mobility, multi-homing, multicast, multipath \n3GPP \u2013 e.g. for plain complexity versus complicatedness \nA discursive essay on a specific communications systems component, in a particular context, \nsuch as ad hoc routing, or wireless sensor networks. \nA bespoke network design for a narrow, well specified specialised target scenario, for example: \nA customer baggage tracking network for an airport. \nin-flight entertainment system. \nin-car network for monitoring and control. \ninter-car sensor/control network for automatic highways. \nPractical work \n",
        "This course does not feature any implementation work due to time constraints. \n \nAssessment \nThree 1,200-word essays (worth 25% each), and \nan annotated bibliography (25%). \nRecommended reading \nPre-course reading: \n \nKeshav, S. (1997). An engineering approach to computer networking. Addison-Wesley (1st ed.). \nISBN 0201634422 \nPeterson, L.L. and Davie, B.S. (2007). Computer networks: a systems approach. Morgan \nKaufmann (4th ed.). \n \nDesign patterns: \n \nDay, John (2007). Patterns in network architecture: a return to fundamentals. Prentice Hall. \n \nExample systems: \n \nKrishnamurthy, B. and Rexford, J. (2001). Web protocols and practice: HTTP/1.1, Networking \nprotocols, caching, and traffic measurement. Addison-Wesley. \n \nEconomics and networks: \n \nFrank, Robert H. (2008). The economic naturalist: why economics explains almost everything. \n \nPapers: \n \nCertainly, a collection of papers (see ACM CCR which publishes notable network researchers' \nfavourite ten papers every 6 months or so).\n \n",
        "OVERVIEW OF NATURAL LANGUAGE PROCESSING \n \nAims \nThis course introduces the fundamental techniques of natural language processing. It aims to \nexplain the potential and the main limitations of these techniques. Some current research issues \nare introduced and some current and potential applications discussed and evaluated. Students \nwill also be introduced to practical experimentation in natural language processing. \n \nLectures \nOverview. Brief history of NLP research, some current applications, components of NLP \nsystems. \nMorphology and Finite State Techniques. Morphology in different languages, importance of \nmorphological analysis in NLP, finite-state techniques in NLP. \nPart-of-Speech Tagging and Log-Linear Models. Lexical categories, word tagging, corpora and \nannotations, empirical evaluation. \nPhrase Structure and Structure Prediction. Phrase structures, structured prediction, context-free \ngrammars, weights and probabilities. Some limitations of context-free grammars. \nDependency Parsing. Dependency structure, grammar-free parsing, incremental processing.  \nGradient Descent and Neural Nets. Parameter optimisation by gradient descent. Non-linear \nfunctions with neural network layers. Log-linear model as softmax layer. Current findings of \nNeural NLP. \nWord representations. Representing words with vectors, count-based and prediction-based \napproaches, similarity metrics. \nRecurrent Neural Networks. Modelling sequences, parameter sharing in recurrent neural \nnetworks, neural language models, word prediction. \nCompositional Semantics. Logical representations, compositional semantics, lambda calculus, \ninference and robust entailment. \nLexical Semantics. Semantic relations, WordNet, word senses. \nDiscourse. Discourse relations, anaphora resolution, summarization. \nNatural Language Generation. Challenges of natural language generation (NLG), tasks in NLG, \nsurface realisation. \nPractical and assignments. Students will build a natural language processing system which will \nbe trained and evaluated on supplied data. The system will be built from existing components, \nbut students will be expected to compare approaches and some programming will be required \nfor this. Several assignments will be set during the practicals for assessment. \nObjectives \nBy the end of the course students should: \n \nbe able to discuss the current and likely future performance of several NLP applications; \nbe able to describe briefly a fundamental technique for processing language for several \nsubtasks, such as morphological processing, parsing, word sense disambiguation etc.; \nunderstand how these techniques draw on and relate to other areas of computer science. \nRecommended reading \n",
        "Jurafsky, D. & Martin, J. (2023). Speech and language processing. Prentice Hall (3rd ed. draft, \nonline). \n \nAssessment - Part II Students \nAssignment 1 - 10% of marks \nAssignment 2 - 60% of marks \nAssignment 3 - 30% of marks \nCoursework \nSubmit work for 3 assignments as part of practical sessions on information extraction. \n \nThese include an annotation exercise, a feature-based classifier along with code repository and \ndocumentation, and a 4,000-word report on results and analysis from an extended information \nextraction experiment. \n \nPractical work \nStudents will build a natural language processing system which will be trained and evaluated on \nsupplied data. The system will be built from existing components, but students will be expected \nto compare approaches and some programming will be required for this. \n \nAssessment - Part III and MPhil Students \nAssessment will be based on the practicals: \n \nFirst practical exercise (10%, ticked) \nSecond practical exercise (25%, code repository or notebook with documentation) \nFinal report (65%, 4,000 words, excluding references) \nFurther Information \nAlthough the lectures don't assume any exposure to linguistics, the course will be easier to \nfollow if students have some understanding of basic linguistic concepts. The following may be \nuseful for this: The Internet Grammar of English \n \nCurrent Cambridge undergraduate students who are continuing onto Part III or the MPhil in \nAdvanced Computer Science may only take this module if they did NOT take it as a Unit of \nAssessment in Part II. \n \nStudents from other departments may attend the lectures for this module if space allows. \nHowever students wanting to take it for credit will need to make arrangements for assessment \nwithin their own department. \n \nThis module is shared with Part II of the Computer Science Tripos. Assessment will be adjusted \nfor the two groups of students to be at an appropriate level for whichever course the student is \nenrolled on. Further information about assessment and practicals will follow at the first lecture.\n",
        "PRACTICAL RESEARCH IN HUMAN-CENTRED AI \n \nAims \nThis is an advanced course in human-computer interaction, with a specialist focus on intelligent \nuser interfaces and interaction with machine-learning and artificial intelligence technologies. The \nformat will be largely Practical, with students carrying out a mini-project involving empirical \nresearch investigation. These studies will investigate human interaction with some kind of \nmodel-based system for planning, decision-making, automation etc. Possible study formats \nmight include: System evaluation, Field observation, Hypothesis testing experiment, Design \nintervention or Corpus analysis, following set examples from recent research publications. \nProject work will be formally evaluated through a report and presentation. \n \nLectures \n(note that Lectures 2-7 also include one hour class discussion of practical work) \n        \u2022 Current research themes in intelligent user interfaces \n        \u2022 Program synthesis \n        \u2022 Mixed initiative interaction \n        \u2022 Interpretability / explainable AI \n        \u2022 Labelling as a fundamental problem \n        \u2022 Machine learning risks and bias \n        \u2022 Visualisation and visual analytics \n        \u2022 Student research presentations \n \nObjectives \nBy the end of the course students should: \n        \u2022 be familiar with current state of the art in intelligent interactive systems \n        \u2022 understand the human factors that are most critical in the design of such systems \n        \u2022 be able to evaluate evidence for and against the utility of novel systems \n        \u2022 have experience of conducting user studies meeting the quality criteria of this field \n        \u2022 be able to write up and present user research in a professional manner \n \nClass Size \nThis module can accommodate upto 20 Part II, Part III and MPhil students. \n \nRecommended reading \nBrad A. Myers and Richard McDaniel (2000). Demonstrational Interfaces: Sometimes You Need \na Little Intelligence, Sometimes You Need a Lot. \n \nAlan Blackwell (2024). Moral Codes: Designing alternatives to AI \n \nAssessment - Part II Students \nThe format will be largely practical, with students carrying out an individual mini-project involving \nempirical research investigation. \n \n",
        "Assignment 1: six incremental submissions which together contribute 20% to the final module \nmark. \n \nAssignment 2: Final report - 80% of the final module mark \n  \n \nAssessment - MPhil / Part III Students \nThere will be a minor assessment component (20%) in which students compile a reflective diary \nthroughout the term, reporting on the weekly sessions. Diary entries should include citations to \nany key references, notes of possible further reading, summary of key points, questions relevant \nto the personal project, and points of interest noted in relation to the work of other students. \n \nThe major assessment component (80%) will involve a report on research findings, in the style \nof a submission to the ACM CHI or IUI conferences. This work will be submitted incrementally \nthrough the term, in order that feedback can be provided before final assessment of the full \nreport. Phased submissions will cover the following aspects of the empirical study: \n \nResearch question \nMethod \nLiterature Review \nIntroduction \nResults \nDiscussion / Conclusion \nFeedback on each phased submission will include an indicative mark for guidance, but with the \nunderstanding that the final grade will be based on the final delivered report, and that this may \ngo up (or possibly down), depending on how well the student responds to earlier feedback. \n \nReflective diary entries will also be assessed, but graded at a relatively coarse granularity \ncorresponding to the ACS grading bands, and with minimal written feedback. Informal and \ngeneric feedback will be offered verbally in class, and also potentially supplemented with peer \nassessment. \n \nFurther Information \nCurrent Cambridge undergraduate students who are continuing onto Part III or the MPhil in \nAdvanced Computer Science may only take this module if they did NOT take it as a Unit of \nAssessment in Part II. \n \nThis module is shared with Part II of the Computer Science Tripos. Assessment will be adjusted \nfor the two groups of students to be at an appropriate level for whichever course the student is \nenrolled on. Further information about assessment and practicals will follow at the first lecture.\n",
        "PRINCIPLES OF MACHINE LEARNING SYSTEMS \n \nPrerequisites: It is recommended that students have successfully completed introductory \ncourses, at an undergraduate level, in: 1) operating systems, 2) computer architecture, and 3) \nmachine learning. In addition, this course will heavily focus on deep neural network \nmethodologies and assume familiarity with common neural architectures and related algorithms. \nIf these topics were not covered within the machine learning course taken, students should \nsupplement by reviewing material in a book like: \"Dive into Deep Learning\". Finally, students are \nassumed to be comfortable with programming in Python. \nMoodle, timetable \n \nAims \nThis course will examine the emerging principles and methodologies that underpin scalable and \nefficient machine learning systems. Primarily, the course will focus on an exciting cross-section \nof algorithms and system techniques that are used to support the training and inference of \nmachine learning models under a spectrum of computing systems that range from constrained \nembedded systems up to large-scale distributed systems. It will also touch upon the new \nengineering practices that are developing in support of such systems at scale. When needed to \nappreciate issues of scalability and efficiency, the course will drill down to certain aspects of \ncomputer architecture, systems software and distributed systems and explore how these \ninteract with the usage and deployment of state-of-the-art machine learning. \n \nSyllabus \nTopics covered may include the following, with confirmation a month before the course begins: \n \nSystem Performance Trade-offs \nDistributed Learning Algorithms  \nModel Compression  \nDeep Learning Compilers  \nFrameworks and Run-times  \nScalable Inference Serving  \nDevelopment Practices  \nAutomated Machine Learning  \nFederated Learning  \nPrimarily, topics are covered with conventional lectures. However, where appropriate, material \nwill be delivered through hands-on lab tutorials. Lab tutorials will make use of hardware \nincluding ARM microcontrollers and multi-GPU machines to explore forms of efficient machine \nlearning (any necessary equipment will be provided to students) \n \nAssessment \nEach student will be assessed on 3 labs which will be worth 30% of their grade. They will also \nundertake a written project report which will be worth 70% of the grade. This report will detail an \ninvestigation into a particular aspect of machine learning systems, this report will be made \navailable publicly.\n \n",
        "PROOF ASSISTANTS \nAims \nThis module introduces students to interactive theorem proving using Isabelle and Coq. It \nincludes techniques for specifying formal models of software and hardware systems and for \nderiving properties of these models. \n \n  \n \nSyllabus \nIntroduction to proof assistants and logic. \nReasoning in predicate logic and typed set theory. \nReasoning in dependent type theory. \nInductive definitions and recursive functions: modelling them in logic, reasoning about them. \nModelling operational semantics definitions and proving properties. \n  \n \nObjectives \nOn completion of this module, students should: \n \npossess good skills in the use of Isabelle and Coq; \nbe able to specify inductive definitions and perform proofs by induction; \nbe able to formalise and reason about a variety of specifications in a proof assistant. \nCoursework \nPractical sessions will allow students to develop skills. Some of the exercises will serve as the \nbasis for assessment. \n \nAssessment \nAssessment will be based on two marked assignments (due in weeks 5 and 8) with students \nperforming small formalisation and verification projects in a proof assistant together with a \nwrite-up explaining their work (word limit 2,500 per project for the write-up). Each of the \nassignments will be worth 100 marks, distributed as follows: \n \n50 marks for completing basic formalisation and verification tasks assessing grasp of the \nmaterial taught in the lecture. Students will submit their work as theory files for either the \nIsabelle or Coq proof assistant, and they will be assessed for correctness and completeness of \nthe specifications and proofs. \n20 marks for completing designated more challenging tasks, requiring the use of advanced \ntechniques or creative proof strategies. \n30 marks for a clear write-up explaining the design decisions made during the formalisation and \nthe strategy used for the proofs, where 10 of these marks will be reserved for write-ups of \nexceptional quality, e.g. demonstrating particular insight. \nThe main tasks in the assignments will be designed to assess the student's proficiency with the \nbasic material and techniques taught in the lectures and the practical sessions, while the more \nchallenging tasks will give exceptional students the opportunity to earn distinction marks \n",
        " \nRecommended reading \nNipkow, T., Klein, G. (2014). Concrete Semantics with Isabelle/HOL. (The first part of this book, \n\u201cProgramming and Proving in Isabelle/HOL\u201d, comes with the Isabelle distribution.)  \n \nNipkow, T., Paulson, L.C. and Wenzel, M. (2002). A proof assistant for higher-order logic. \nSpringer LNCS 2283.  \n \nThe Software Foundations series of books, in particular the first two: \n \nPierce, B., Azevedo de Amorim, A., Casinghino, C., Gaboardi, M., Greenberg, M., Hri\u0163cu, C., \nSj\u00f6berg, V., Yorgey, B. (2023). Logical Foundations  \nPierce, B., Azevedo de Amorim, A., Casinghino, C., Gaboardi, M., Greenberg, M., Hri\u0163cu, C., \nSj\u00f6berg, V., Tolmach, A., Yorgey, B. (2023). Programming Language Foundations  \nChlipala, A. (2022). Certified programming with dependent types: a pragmatic introduction to the \nCoq proof assistant. MIT Press.  \n \nSergey, I. (2014). Programs and Proofs: Mechanizing Mathematics with Dependent Types.  \n \nAll of these are freely available online.\n \n",
        "QUANTUM ALGORITHMS AND COMPLEXITY \n \nAims \nThis module is a research-focused introduction to the theory of quantum computing. The aim is \nto prepare the students to conduct research in quantum algorithms and quantum complexity \ntheory. \n \nSyllabus \nTopics will vary from year to year, based on developments in cutting edge research. \nRepresentative topics include: \n \nQuantum algorithms: \n \nQuantum learning theory \nQuantum property testing \nShadow tomography \nQuantum walks \nQuantum state/unitary synthesis. \nStructure vs randomness in quantum algorithms \nQuantum complexity theory: \n \nThe quantum PCP conjecture \nEntanglement and MIP \nState complexity, AdS/CFT, and quantum gravity \nQuantum locally testable codes \nPseudorandom states and unitaries \nQuantum zero-knowledge proofs \nObjectives \nOn completion of this module, students should: \n \nbe familiar with contemporary quantum algorithms \ndevelop an understanding of the power and limitations of quantum computation \nconduct research in quantum algorithms and complexity theory \nAssessment \nMini research project (70%) \nPresentation (20%) \nAttendance and participation (10%) \nTimelines for the assignment submissions:  \n \nsubmit questions/observations on a weekly basis (i.e., Weeks 2-8) \ndeliver talks in Weeks 5-8 \nsubmit their mini-project at the end of term \nRecommended reading material and resources \n\u201cQuantum Computing Since Democritus\u201d by Scott Aaronson \n",
        " \n\u201cIntroduction to Quantum Information Science\u201d by Scott Aaronson \n \n\u201cQuantum Computing: Lecture Notes\u201d by Ronald de Wolf \n \n\u201cQuantum Computation and Quantum Information\u201d by Nielsen and Chuang\n \n",
        "UNDERSTANDING QUANTUM ARCHITECTURE \nPrerequisites: Requires introductory linear algebra - concepts such as eigenvalues, Hermitian \nmatrices, unitary matrices. Taking the Part II Quantum Computing course (or similar) is helpful \nbut not required. Familiarity with computer architecture and compilers is helpful. \nMoodle, timetable \n \nAims \nThis course covers the architecture of a practical-scale quantum computer. We will examine the \nresource requirements of practical quantum applications, understand the different layers of the \nquantum stack, the techniques used in these layers and examine how these layers come \ntogether to enable practical quantum advantage over classical computing. \n \nSyllabus \nThe course has two parts: a series of lectures to cover important aspects of the quantum stack \nand a set of student presentations. The following are a list of representative topics: \n \n- Basics of quantum computing \n- The fault-tolerant quantum stack \n- Compilation \n- Instruction sets \n- Implementations of quantum error correction \n- Implementation of magic state distillation \n- Resource estimation \n \nStudent presentations will be based on a reading list of important papers in quantum \narchitecture.  \n  \n \nObjectives \nAt the end of the course, students will have a broad understanding of the quantum computing \nstack. They will understand how major qubit technologies work, design challenges in real \nquantum hardware, how quantum applications are mapped to a system and the importance of \nquantum error correction for scalability.  \n \nAssessment \nSeminar presentation: 20% \nRead one paper from a provided reading list \nPrepare a 20 minute presentation on it + 10 minutes of answering questions. \nThe student presentation should be similar to a conference presentation - convey the problem, \nwhat are prior solutions, what did the paper do, what are the results, what are future directions \nFor the 20% of the score, the score will be split as 15% and 5%. 15% based on how well the \nstudent understands and explains the paper to the rest of the audience. 5% based on the Q&A \nsession. \n \n",
        "  \n \nCourse project: 80% (split across a proposal, mid-term report, final report) \nA. Research proposal - at least 500 words (10% of total) \nB. Mid-term report - at least 1000 words (including aspects like the research problem, literature \nreview, what questions will be evaluated, any early ideas or methods (20% of total)  \nC. Final report - up to 4 pages double column in a conference paper style with 3000-4000 \nwords. In addition to the mid term report, should include aspects like results and future \ndirections. (50% of the total) \nD. Report on contributions (in case of group work by 2 students) - 1 paragraph on individual \ncontribution of the student. 1 paragraph on teammate's contribution. \nStudents may work alone in the project, in which case they need only components A-C. \nStudents may work in pairs of two, but they should in addition individually submit D. The \ninstructor may conduct a short viva in case contributions from both team members are not clear \nor imbalanced. \nRecommended reading \nNielsen M.A., Chuang I.L. (2010). Quantum Computation and Quantum Information. Cambridge \nUniversity Press. \n \nMermin N.D. (2007). Quantum Computer Science: An Introduction. Cambridge University Press. \n \nAssessing requirements to scale to practical quantum advantage (2022) Beverland et al.\n",
        "8th Semester \nADVANCED TOPICS IN CATEGORY THEORY \n \n \nTeaching \nThe teaching style will be lecture-based, but supported by a practical component where \nstudents will learn to use a proof assistant for higher category theory, and build a small portfolio \nof proofs. Towards the end of the course we will explore some of the exciting computer science \nresearch literature on monoidal and higher categories, and students will choose a paper and \npresent it to the class. \n \nAims \nThe module will introduce advanced topics in category theory. The aim is to train students to \nengage and start modern research on the mathematical foundations of higher categories, the \ngraphical calculus, monoids and representations, type theories, and their applications in \ntheoretical computer science, both classical and quantum. \n \nObjectives \nOn completion of this module, students should: \n \nBe familiar with the techniques of compositional category theory. \nHave a strong understanding of basic categorical semantic models. \nBegun exploring current research in monoidal categories and higher structures. \nSyllabus \nPart 1, lecture course: \nThe first part of the course introduces concepts from monoidal categories and higher categories, \nand explores their application in computer science. \n\u2010 Monoidal categories and the graphical calculus \n\u2010 The proof assistant homotopy.io \n\u2010 Coherence theorems and higher category theory \n\u2010 Linearity, superposition, duality, quantum entanglement \n\u2010 Monoids, Frobenius algebras and bialgebras \n\u2010 Type theory for higher category theory \n \nPart 2, exploring the research frontier: \nIn the second part of the course, students choose a research paper to study, and give a \npresentation to the class. \nThere is a nice varied literature related to the topics of the course, and the lecturer will supply a \nlist of suggested papers.  \n \nClasses \nThere will be three exercise sheets for homework, with accompanying classes by a teaching \nassistant to go over them. \n \n",
        "Assessment \nProblem sheets (50%) \nClass presentation (20%) \nPractical portfolio (30%) \nReading List \nChris Heunen and Jamie Vicary, \u201cCategory for Quantum Theory: An Introduction\u201d, Oxford \nUniversity Press\n \n",
        "ADVANCED TOPICS IN COMPUTER SYSTEMS \n \nAims \nThis module will attempt to provide an overview of \u201csystems\u201d research. This is a very broad field \nwhich has existed for over 50 years and which has historically included areas such as operating \nsystems, database systems, file systems, distributed systems and networking, to name but a \nfew. The course will thus necessarily cover only a tiny subset of the field. \n \nMany good ideas in systems research are the result of discussing and debating previous work. \nA primary aim of this course therefore will be to educate students in the art of critical thinking: \nthe ability to argue for and/or against a particular approach or idea. This will be done by having \nstudents read and critique a set of papers each week. In addition, each week will include \npresentations from a number of participants which aim to advocate or criticise each piece of \nwork. \n \nSyllabus \nThe syllabus for this course will vary from year to year so as to cover a mixture of older and \nmore contemporary systems papers. Contemporary papers will be generally selected from the \npast 5 years, primarily drawn from high quality conferences such as SOSP, OSDI, ASPLOS, \nFAST, NSDI and EuroSys. Example topics might include: \n \nSystems Research and System Design \nOS Structure and Virtual Memory \nVirtualisation \nConsensus \nScheduling \nPrivacy \nData Intensive Computing \nBugs \nThe reading each week will involve a load equivalent to 3 full length papers. Students will be \nexpected to read these in detail and prepare a written summary and review. In addition, each \nweek will contain one or more short presentations by students for each paper. The types of \npresentation will include: \n \nOverview: a balanced presentation of the paper, covering both positive and negative aspects. \nAdvocacy: a positive spin on the paper, aiming to convince others of its value. \nCriticism: a negative take on the paper, focusing on its weak spots and omissions. \nThese presentation roles will be assigned in advance, regardless of the soi disant absolute merit \nof the paper or the preference of the student. Furthermore, all students \u2013 regardless of any \nassigned presentation role in a given week \u2013 will be expected to participate in the class by \nasking questions and generally entering into the debate. \n \nObjectives \n",
        "On completion of this module students should have a broad understanding of some key papers \nand concepts in computer systems research, as well as an appreciation of how to argue for or \nagainst any particular idea. \n \nCoursework and practical work \nCoursework will be the production of the weekly paper reviews. Practical work will be presenting \npapers as appropriate, as well as ongoing participation in the class. \n \nAssessment \nAssessment consists of: \n \nOne essay per week for 7 weeks (10% each) \nPresentation (20%) \nParticipation in class over the term (10%) \nRecommended reading \nMost of the reading for this course will be in the form of the selected papers each week. \nHowever, the following may be useful background reading to refresh your knowledge from \nundergraduate courses: \n \nSilberschatz, A., Peterson, J.L. and Galvin, P.C. (2005). Operating systems concepts. \nAddison-Wesley (7th ed.). \n \nTanenbaum, A.S. (2008). Modern Operating Systems. Prentice-Hall (3rd ed.). \n \nBacon, J. and Harris, T. (2003). Operating systems. Addison-Wesley (3rd ed.). \n \nAnderson, T. and Dahlin, M. (2014). Operating Systems: Principles and Practice. Recursive \nBooks (2nd ed.). \n \nHennessy, J. and Patterson, D. (2006). Computer architecture: a quantitative approach. Elsevier \n(4th ed.). ISBN 978-0-12-370490-0. \n \nKleppmann M (2016) Designing Data-Intensive Applications, O'Reilly (1st ed.)\n \n",
        "ADVANCED TOPICS IN MACHINE LEARNING \nAims \nThis course explores current research topics in machine learning in sufficient depth that, at the \nend of the course, participants will be in a position to contribute to research on their chosen \ntopics. Each topic will be introduced with a lecture which, building on the material covered in the \nprerequisite courses, will make the current research literature accessible. Each lecture will be \nfollowed by up to three sessions which will typically be run as a reading group with student \npresentations on recent papers from the literature followed by a discussion, or a practical, or \nsimilar. \n \nStructure \nEach student will attend 3 topics and each topic's sessions will be spread over 5 contact hours. \nStudents will be expected to undertake readings for their selected topics. There will be some \ngroup work. \n \nThere will be a briefing session in Michaelmas term. \n \nSyllabus \nStudents choose five topics in preferential order from a list to be published in Michaelmas term. \nThey will be assigned to three topics out of their list. Students are assessed on one of these \ntopics which may not necessarily be their first choice topic. \n \nThe topics to be offered in 2024-25 are yet to be decided but to give an indicative idea of the \ntypes of topics, the ones offered in 2023-24 were: \n \nImitation learning  Prof A. Vlachos \nThe Future of Large Language Models: data architectures ethics Dr M. Tomalin \nPhysics and Geometry in Machine Learning Dr C. Mishra \nDiffusion Models and SDEs Francisco Vargas, Dr C. H. Ek \nExplainable Artificial Intelligence M. Espinosa, Z. Shams, Prof M. Jamnik \nUnconventional approaches to AI Dr S. Banerjee \nNarratives in Artificial Intelligence and Machine Learning Prof N. Lawrence \nMultimodal Machine Learning K. Hemker, N. Simidjievski, Prof M. Jamnik \nDeep Reinforcement Learning S. Morad, Dr C. H. Ek \nAutomation in Proof Assistants A. Jiang, W. Li, Prof M. Jamnik \nOn completion of this module, students should: \n \nbe in a strong position to contribute to the research topics covered; \nunderstand the fundamental methods (algorithms, data analysis, specific tasks) underlying each \ntopic; \nand be familiar with recent research papers and advances in the field. \nCoursework \nStudents will typically work in groups to give a presentation on assigned papers. Alternatively, a \ntopic may include practical sessions. Each topic will typically consist of one preliminary lecture \n",
        "followed by 3 reading and discussion sessions, or several lectures followed by a practical \nsession. A typical topic can accommodate up to 9 students presenting papers. There will be at \nleast 10 minutes general discussion per session. \n \nFull coursework details will be published by October. \n \nAssessment \nCoursework will be marked by the topic leaders and second marked by the module conveners. \n \nParticipation in all assigned topics, 10% \nPresentation or practical work or similar (for one of the chosen topics), 20% \nTopic coursework (for one of the chosen topics), 70% \nIndividual topic coursework will be published late Michaelmas term. \n \nAssessment criteria for topic coursework will follow project assessment criteria here: \nhttps://www.cl.cam.ac.uk/teaching/exams/acs_project_marking.pdf \n \nPlease note that students will be assessed on one of their three chosen topics but this may not \nbe their first choice. \n \nRecommended reading \nTo be confirmed by each topic convenor.\n \n",
        "COMPUTING FOR COLLECTIVE INTELLIGENCE \nPrerequisites: Familiarity with core machine learning paradigms - Basic calculus (analytical \nskills) - Basic discretre optimization (combinatorics) \nMoodle, timetable \n \nObjectives \nThere is a substantial body of academic work demonstrating that complex life is built by \ncooperation across scales: collections of genes cooperate to produce organisms, cells \ncooperate to produce multi-cellular organisms and multicellular animals cooperate to form \ncomplex social groups. Arguably, all intelligence is collective intelligence. Yet, to-date, many \nartificially intelligent agents (both embodied and virtual), are generally not conceived from the \nground up to interact with other intelligent agents (be it machines or humans). The canonical AI \nproblem is that of a monolithic and solitary machine confronting a non-social environment. This \ncourse aims to balance this trend by (i) equipping students with conceptual and practical \nknowledge on collective intelligence from a computational standpoint, and (ii) by conveying \nvarious computational paradigms by which collective intelligence can be modelled as well as \nsynthesized. \n \nAssessment \nThe assessment will be based on a reading-group paper presentation (individual or in pairs), \naccompanied by a 2-page summary, and a technical position paper (individual work) handed in \nafter the course. The position paper will be assessed based on its technical correctness, \nstrength or arguments, \nand clarity of research vision, and will be accompanied by a brief 5-minute pre-recorded talk that \nsummarizes key arguments (any slides used are also submitted as part of the project). \n \nPaper presentation and 1-page summary: 25% \nTechnical position paper: 75% (4000 word limit) \nRecommended reading \nSwarm Intelligence : From Natural to Artificial Systems, Bonabeau et al (1999) \nJoined-Up Thinking, Hannah Critchlow, (2024) \nSupercooperators, Martin Nowak (2011) \nA Course on Cooperative Game Theory, Chakravarty et al., (2015) \nGoverning the Commons: The Evolution of Institutions for Collective Action. Ostrom (1990).  \n \n",
        "CRYPTOGRAPHY AND PROTOCOL ENGINEERING \nAims \nWe all use cryptographic protocols every day: whenever we access an https:// website or send a \nmessage via WhatsApp or Signal, for example. In this module, students will get hands-on \nexperience of how those protocols are implemented. Students start by writing their own secure \nmessaging protocol from scratch, and then move on to more advanced examples of \ncryptographic protocols for security and privacy, such as private information retrieval (searching \nfor data without revealing what you\u2019re searching for). \n \nSyllabus \nThis is a practical course in which the first half of each of the 2-hour weekly sessions is a lecture \nthat introduces a topic, while the second half is lab time during which the students can work on \ntheir implementations, discuss the topics in small groups, and get help from the lecturers. The \nmodule is structured into three blocks as follows:  \n \nCryptographic primitives (2 weeks). Using common building blocks such as symmetric ciphers \nand hash functions. Implementing selected aspects of elliptic curve Diffie-Hellman and digital \nsignatures. Protocol security, Dolev-Yao model, side-channel attacks.  \nSecure communication (3 weeks). Authenticated key exchange (e.g. SIGMA, TLS), \npassword-authenticated key exchange (e.g. SPAKE2), forward secrecy, post-compromise \nsecurity, double ratchet, the Signal protocol.  \nPrivate database lookups (3 weeks). Lattice-based post-quantum cryptography, learning with \nerrors, homomorphic encryption, private information retrieval.  \nIn each block, students will be given a description of the algorithms and protocols covered (e.g. \na research paper or an RFC standards document), and a code template that the students \nshould use for their implementation. The implementation language will probably be Python (to \nbe confirmed after the practical materials have been developed ). Students will also be given a \nset of test cases to check their code, and will have the opportunity to test their implementation \ncommunicating with other students\u2019 implementations. Finally, for each block, students will write \nand submit a lab report explaining their approach and the findings from their implementation. \n \nObjectives \nOn completion of this module, students should: \n \nUnderstand that cryptography is not magic! The mathematics can look intimidating, but this \nmodule will show students that they needn\u2019t be afraid of cryptography. \nGain appreciation for the complexity and careful implementation of real-world cryptography \nlibraries ,and understand why one should prefer vetted implementations. \nBe familiar with the mathematical notation and concepts commonly used in descriptions of \ncryptographic protocols, and be able to understand and implement research papers and RFCs \nin this field. \nBe comfortable with cryptographic primitives commonly used in protocols, and able to correctly \nuse software libraries that implement them. \n",
        "Have gained hands-on experience of attacks on cryptographic protocols, and an appreciation \nfor the difficulty of making these protocols correct. \nHave been exposed to ideas and techniques from recent research, which may be useful for their \nown future research. \nHave practised technical writing through lab reports. \nAssessment \nStudents will be provided with code skeletons in one or two different programming languages \n(probably Python, maybe Rust) to avoid them struggling with setup issues. For each \nassignment, students are required to produce a working implementation of the protocols \ndiscussed in the course, using the programming language and skeleton provided. Here, \n\u201cworking\u201d means that the algorithms are functionally correct, but they are not production-quality \n(in particular, no side-channel countermeasures such as constant-time algorithms are required). \nFor some primitives (e.g. symmetric ciphers and hash functions) existing libraries should be \nused, whereas other cryptographic algorithms will be implemented from scratch. Students \nshould submit their code as a Docker container that can be run against specified test cases.  \n \nAfter each of the three blocks, students will be asked to submit a lab report of approximately \n2,000 words along with their code for that block. In the lab report, the students should explain \nhow their implementation works and why it is correct, as well as any findings from the work (e.g. \nany limitations or trade-offs they found with the protocols). The report structure and marking \nscheme will be specified in advance. The lab reports provide an opportunity for students to \nprovide critical insights and creativity. For instance, they might compare their implementation \nwith existing real-world implementations which provide additional features and guarantees.  \n \nEach of the three lab reports (along with the associated code) will be marked, forming the basis \nof assessment, and feedback on the reports will be given to the students before the next \nsubmission is due, so that students can take the feedback on board for their next submission. \nOf the three reports, the worst mark will be discarded, and the other two reports each contribute \n50% of the final mark. As such, students might decide to submit only two reports.  \n \nStudents may discuss their work with others, but the implementations and lab reports must be \nindividual work. \n \nRecommended reading \nTextbook: Jonathan Katz and Yehuda Lindell. Introduction to Modern Cryptography (3rd edition). \nCRC Press, 2020. \n \nThe textbook covers prerequisite knowledge on cryptographic primitives, such as ciphers, \nMACs, hash functions, and signatures. This book is also used in the Part II Cryptography \ncourse. For the protocols we cover in this course, we are not aware of a suitable textbook; \ninstead we will use research papers, lecture slides, and RFCs as resources, which will be \nprovided at the start of the module.\n \n",
        "DISTRIBUTED LEDGER TECHNOLOGIES: FOUNDATIONS AND APPLICATIONS \n \nAims \nThis reading group course examines foundations and current research into distributed ledger \n(blockchain) technologies and their applications. Students will read, review, and present seminal \nresearch papers in this area. Once completed, students should be able to integrate blockchain \ntechnologies into their own research and gain familiarity with a range of research skills. \n \nLectures \nIntroduction \nConsensus protocols \nBitcoin and its variants \nEthereum, smart contracts, and other permissionless DLTs \nHybrid and permissioned DLTs \nApplications \nLearning objectives \nThere are two broad objectives: to acquire familiarity with a body of work in the area of \ndistributed ledgers and to learn some specific research skills: \n \nHow to read a paper \nHow to review a paper \nHow to analyze a paper\u2019s strengths and weaknesses \nWritten and oral presentation skills \nAssessment \nYou are expected to read all assigned papers and submit paper reviews each week. Each \nreview must either follow the provided review form [PDF] [Latex source]. Each \u201creview\u201d is worth \n5% of your total mark, and is marked out of 100 with 60 a passing grade. Marks will be awarded \nand penalties for late submission applied according to ACS Assessment Guidelines.  \n \nOne paper review for the first week, then two paper reviews each week for 6 weeks (13 reviews, \n5% each) 65% (approx 600 words per review) \nSummative essay 25% (max 3000 words) \nPresentation 5% \n100% attendance in class 5% (2 marks deducted per missed class) \nRecommended Reading \nNarayanan, A. , Bonneau, J., Felten, E., Miller, A. and Goldfeder, S. (2016). Bitcoin and \nCryptocurrency Technologies: A Comprehensive Introduction. Princeton University Press. \n(2016 Draft available here: \nhttps://d28rh4a8wq0iu5.cloudfront.net/bitcointech/readings/princeton_bitcoin_book.pdf)\n \n",
        "EXPLAINABLE ARTIFICIAL INTELLIGENCE \nPrerequisites: A solid background in statistics, calculus and linear algebra. We strongly \nrecommend some experience with machine learning and deep neural networks (to the level of \nthe first chapters of Goodfellow et al.\u2019s \u201cDeep Learning\u201d). Students are expected to be \ncomfortable reading and writing Python code for the module\u2019s practical sessions. \nMoodle, timetable \n \nAims \nThe recent palpable introduction of Artificial Intelligence (AI) models to everyday \nconsumer-facing products, services, and tools brings forth several new technical challenges and \nethical considerations. Amongst these is the fact that most of these models are driven by Deep \nNeural Networks (DNNs), models that, although extremely expressive and useful, are \nnotoriously complex and opaque. This \u201cblack-box\u201d nature of DNNs limits their ability to be \nsuccessfully deployed in critical scenarios such as those in healthcare and law. Explainable \nArtificial Intelligence (XAI) is a fast-moving subfield of AI that aims to circumvent this crucial \nlimitation of DNNs by either  \n \n(i) constructing human-understandable explanations for their predictions,  \n \nor (ii) designing novel neural architectures that are interpretable by construction.  \n \nIn this module, we will introduce the key ideas behind XAI methods and discuss some of the \nimportant application areas of these methods (e.g., healthcare, scientific discovery, debugging, \nmodel auditing, etc.). We will approach this by focusing on the nature of what constitutes an \nexplanation, and discussing different ways in which explanations can be constructed or learnt to \nbe generated as a by-product of a model. The main aim of this module is to introduce students \nto several commonly used approaches in XAI, both theoretically in lectures and through \nhands-on exercises in practicals, while also bringing recent promising directions within this field \nto their attention. We hope that, by the end of this module, students will be able to directly \ncontribute to XAI research and will understand how methods discussed in this module may be \npowerful tools for their own work and research. \n \nSyllabus \nOverview and taxonomy of XAI (why is explainability needed, definition of terms, taxonomy of \nthe XAI space, etc.)  \nPerturbation-based feature attribution (e.g., LIME, Anchors, SHAP, etc.) \nPropagation-based feature attribution methods (e.g., Relevance Propagation, Saliency \nmethods, etc.) \nConcept-based explainability (Net2Vec, T-CAV, ACE, etc.) \nInterpretable architectures (CBMs and variants, Concept Whitening, SENNs, etc.) \nNeurosymbolic methods (DeepProbLog, Neural Reasoners, etc.) \nSample-based Explanations (Influence functions, ProtoPNets, etc.) \nCounterfactual explanations \nProposed Schedule \n",
        "The 16 hours of lectures across 8 weeks will be divided as follows:  \nWeek 1: 1h lecture + 1h lecture  \nWeek 2: 1h lecture + 1h reading group & presentations  \nWeek 3: 1h lecture + 1h reading group & presentations  \nWeek 4: 2h practical  \nWeek 5: 1h lecture + 1h reading group & presentations  \nWeek 6: 2h practical  \nWeek 7: 1h lecture + 1h reading group & presentations  \nWeek 8: 1h reading group & presentation + 1h reading group & presentations \n \nIn weeks where lectures are planned, one-hour lecture slots will intercalate with one hour of \nstudent paper presentations. During each paper presentation session, three students will \npresent a paper related to the topic covered in the earlier lecture that week for about 10 minutes \neach + 5 minutes of questions. At the end of all paper presentations, there will be a discussion \non all the papers. The order of student presentations will be allocated randomly during the first \nweek so that students know in advance when they are expected to present. For the sake of \nfairness, we will release the paper to be presented by each student a week before their \npresentation slot. \n \nObjectives \nBy the end of this module, students should be able to: \n \nRecognise and identify key concepts in XAI together with their connection to related subfields in \nAI such as fairness, accountability, and trustworthy AI. \nUnderstand how to use, design, and deploy model-agnostic perturbation methods such as \nLIME, Anchors, and RISE. In particular, students should understand the connection between \nfeature importance and cooperative game theory, and its uses in methods such as SHAP. \nIdentify the uses and limitations of propagation-based feature importance methods such as \nSaliency, SmoothGrad, GradCAM, and Integrated Gradients. Students should be able to \nimplement each of these methods on their own and connect the theoretical ideas behind them \nto practical code, exploiting modern frameworks\u2019 auto-differentiation. \nUnderstand what concept learning is and what limitations it overcomes compared to traditional \nfeature-based methods. Specifically, students should understand how probing a DNN\u2019s latent \nspace may be exploited for learning useful concepts for explainability. \nReason about the key components of inherently interpretable architectures and neuro-symbolic \nmethods and understand how interpretable neural networks can be designed from first \nprinciples. \nElaborate on what sample-based explanations are and how influence functions and prototypical \narchitectures such as ProtoPNet can be used to construct such explanations. \nExplain what counterfactual explanations are, how they are related to causality, and under which \nconditions they may be useful. \nUpon completion of this module, students will have the technical background and tools to use \nXAI as part of their own research or partake in XAI research itself. Moreover, we hope that by \ndetailing a clear timeline of how this relatively young subfield has developed, students may be \n",
        "able to better understand what are some fundamental open questions in this area and what are \nsome promising directions that are currently actively being explored. \n  \n \nAssessment \n(10%) student presentation: each student will be randomly assigned a presentation slot at the \nbeginning of the course. We will then distribute a paper for them to present in their slot a week \nbefore the presentation\u2019s scheduled time. Students are expected to prepare a 10-minute \npresentation of their assigned paper where they will present the motivation of their assigned \nwork and discuss the main methodology and findings reported in that paper. We will encourage \nstudents to focus on fully communicating the intuition of the work in their assigned papers and \ntry and connect it with ideas that we have previously discussed in previous lectures. All students \nnot presenting each week will be asked to submit one question pertaining to each paper \npresented that week before the paper presentations. \n \n(20%) practical exercises: We will run two practical sessions where students will be asked to \nperform a series of exercises that require them to use concepts we have introduced in lecture \nup to that point. For each practical session, we will prepare a colab notebook to guide the \nstudent through exercises and we will ask the students to submit their solutions through this \ncolab notebook. We expect students to complete about \u2154 of the exercises in the practical class \nand complete the rest at home as homework. Each practical will be worth 10%. \n \n(70%) mini-project: At the end of week 3, we will hand out a list of papers for students to select \ntheir mini-projects from. Each mini-project will consist of a student selecting a paper from our list \nand reimplementing and expanding the key idea in the paper. We encourage students to be as \ncreative as they want with how they drive their mini-project once the paper has been selected. \nFor example, they can reimplement the technique in the paper and combine it with \nmethodologies from other works we discussed in lecture, or they can apply their paper\u2019s \nmethodology to a new domain, datasets, or setup, where the technique may offer interesting \nand potentially novel insights. We will ask all students to submit a report in a workshop format of \nup to 4,000 words. This report, due roughly a week after Lent term ends, should describe their \nmethodology, experiments, and results. A crucial aspect of this report involves explaining the \nrationale behind different choices in methodology and experiments, as well as elaborating on \nthe choices made and hypotheses tested throughout their mini-project (potentially showing a \ndeep understanding of the work they are basing their mini-project on). To aid students with \nselecting their projects and making progress on them, we will hold regular office hours when \nstudents can come to discuss their progress and questions with us. \n \nRecommended reading \nTextbooks \n \n* Christoph Molnar, \u201cInterpretable Machine Learning\u201d. (2022): \nhttps://christophm.github.io/interpretable-ml-book/ \n \n",
        "Online Courses and Tutorial \n \n* Su-in Lee and Ian Cover, \u201cCSEP 590B Explainable AI\u201d University of Washington* Explaining \nMachine Learning Predictions: State-of-the-art, Challenges, and Opportunities \n \n* On Explainable AI: From Theory to Motivation, Industrial Applications, XAI Coding & \nEngineering Practices - AAAI 2022 Turorial \n \nSurvey papers \n \n* Arrieta, Alejandro Barredo, et al. \"Explainable Artificial Intelligence (XAI): Concepts, \ntaxonomies, opportunities and challenges toward responsible AI.\" Information fusion 58 (2020): \n82-115. \n \n* Rudin, Cynthia, et al. \"Interpretable machine learning: Fundamental principles and 10 grand \nchallenges.\" Statistics Surveys 16 (2022): 1-85.\n \n",
        "FEDERATED LEARNING: THEORY AND PRACTICE \nPrerequisites: It is strongly recommended that students have previously (and successfully) \ncompleted an undergraduate machine learning course - or have equivalent experience through \nopen-source material (e.g., lectures or similar). An example course would be: Part1B \"Artificial \nIntelligence\". Students should feel comfortable with SGD and optimization methods used to train \ncurrent popular neural networks and simple forms of neural network architectures. \nMoodle, timetable \n \nObjectives \nThis course aims to extend the machine learning knowledge available to students in Part I (or \npresent in typical undergraduate degrees in other universities), and allow them to understand \nhow these concepts can manifest in a decentralized setting. The course will consider both \ntheoretical (e.g., decentralized optimization) and practical (e.g., networking efficiency) aspects \nthat combine to define this growing area of machine learning.  \n \nAt the end of the course students should: \n \nUnderstand popular methods used in federated learning \nBe able to construct and scale a simple federated system \nHave gained an appreciation of the core limitations to existing methods, and the approaches \navailable to cope with these issues \nDeveloped an intuition for related technologies like differential privacy and secure aggregation, \nand are able to use them within typical federated settings  \nCan reason about the privacy and security issues with federated systems \nLectures \nCourse Overview. Introduction to Federated Learning. \nDecentralized Optimization. \nStatistical and Systems Heterogeneity. \nVariations of Federated Aggregation. \nSecure Aggregation. \nDifferential Privacy within Federated Systems. \nExtensions to Federated Analytics. \nApplications to Speech, Video, Images and Robotics. \nLab sessions \nFederating a Centralized ML Classifier. \nBehaviour under Heterogeneity. \nScaling a Federated Implementation. \nExploring Privacy with Federated Settings \nRecommended Reading \nReadings will be assigned for each lecture. Readings will be taken from either research papers, \ntutorials, source code or blogs that provide more comprehensive treatment of taught concepts. \n \nAssessment - Part II Students \n",
        "Four labs are performed during the course, and students receive 10% of their total grade for \nwork done as part of each lab. (For a total of 40% of the total grade from lab work alone). Labs \nwill primarily provide hands-on teaching opportunities, that are then utilized within the lab \nassignment which is completed outside of the lab contact time. MPhil and Part III students will \nbe given additional questions to answer within their version of the lab assignment which will \ndiffer from the assignment given to Part II CST students. \n \nThe remainder of the course grade (60%) will be given based on a hands-on project that applies \nthe concepts taught in lectures and labs. This hands-on project will be assessed based on upon \na combination of source code, related documentation and brief 8-minute pre-recorded talk that \nsummarizes key project elements (any slides used are also submitted as part of the project). \nPlease note, that in the case of Part II CST students, the talk itself is not examinable -- as such \nwill be made optional to those students. \n \nA range of possible practical projects will be described and offered to students to select from, or \nalternatively students may propose their own. MPhil and Part III students will select from a \nproject pool that is separate from those offered to Part II CST students. MPhil and Part III \nprojects will contain a greater emphasis on a research element, and the pre-recorded talks \nprovided by this student group will focus on this research contribution. The project will be \nassessed on the level of student understanding demonstrated, the degree of difficulty, \ncorrectness of implementation -- and for Part III/MPhil students the additional criteria of the \nquality and execution of the research methodology, and depth and quality of results analysis. \n \nThis project can be done individually or in groups -- although individual projects will be strongly \nencouraged. It will be required the project is performed using a code repository that also will \ncontain all documentation -- access to this repository will be shared with course staff (e.g., \nlecturer and TAs). Where needed, marks assigned to students within a group will be \ndifferentiated using this repository as an input. Furthermore if groups are formed, members must \nbe either entirely from Part III/MPhil students or Part II CST, i.e., these two student groups \nshould not mix to form a project group. \n \nProjects will be made available publicly. A maximum word count for written contributions for the \nproject will be enforced.  \n \n  \nAssessment - MPhil / Part III Students \n50% final mini-project. This hands-on project will be assessed based on upon a combination of \nsource code, related documentation and brief 8-minute pre-recorded talk that summarizes key \nproject elements (any slides used are also submitted as part of the project). \n \n30% midterm lab report (a written report of a bit more depth of thought than required in a lab, \nthe report provides the results of experiments -- with the skills to perform these experiments \ncoming from lab sessions),  \n \n",
        "20% for two individual hands-on labs. Labs will primarily provide hands-on teaching \nopportunities, that are then utilized within the lab assignment which is completed outside of the \nlab contact time. There will also be additional questions to answer within their version of the lab \nassignment. \n \nFor the mini-project, a range of possible practical projects will be described and offered to \nstudents to select from, or alternatively students may propose their own. The project will be \nassessed on the level of student understanding demonstrated, the degree of difficulty, \ncorrectness of implementation, the quality and execution of the research methodology, and \ndepth and quality of results analysis. \n \nThe project can be done individually or in groups -- although individual projects will be strongly \nencouraged. It will be required the project is performed using a code repository that also will \ncontain all documentation -- access to this repository will be shared with course staff (e.g., \nlecturer and TAs). Where needed, marks assigned to students within a group will be \ndifferentiated using this repository as an input. Furthermore if groups are formed, members must \nbe either entirely from Part III or entirely from MPhil students, i.e., these two student groups \nshould not mix to form a project group. \n \nProjects will be made available publicly. A maximum word count for written contributions for the \nproject will be enforced.   \n \nAdvanced Material sessions - MPhil / Part III Students \nThese sessions are mandatory for the MPhil and Part III students. Part II CST students may \nattend if they wish \n \nTopics and announced the first week of class. Selected to extend beyond topics covered in \nlectures and labs. Content is a mixture of sessions run in the lecture room the are either (1) \npresentations by guest lectures by an invited domain expert or (2) class-wide discussions \nregarding one or more related academic papers. Paper discussions will require students to read \nthe paper ahead of the lecture, and a brief discussion primer presentation will be given students \nbefore discussions begin.  \n \nThis module is shared with Part II of the Computer Science Tripos. Assessment will be adjusted \nfor the two groups of students to be at an appropriate level for whichever course the student is \nenrolled on. Further information about assessment and practicals will follow at the first lecture.\n",
        "GEOMETRIC DEEP LEARNING \n \nPrerequisites: Experience with machine learning and deep neural networks is recommended. \nKnowledge of concepts from graph theory and group theory is useful, although the relevant \nparts will be explicitly retaught. \nMoodle, timetable \n \nAims and Objectives \nMost of the patterns we see in nature can be elegantly reasoned about using spatial \nsymmetries\u2014transformations that leave underlying objects unchanged. This observation has \nhad direct implications for the development of modern deep learning architectures that are \nseemingly able to escape the \u201ccurse of dimensionality\u201d and fit complex high-dimensional tasks \nfrom noisy real-world data. Beyond this, one of the most generic symmetries\u2014the \npermutation\u2014will prove remarkably powerful in building models that reason over graph \nstructured-data, which is an excellent abstraction to reason about naturally-occurring, \nirregularly-structured data. Prominent examples include molecules (represented as graphs of \natoms and bonds, with three-dimensional coordinates provided), social networks and \ntransportation networks. Several already-impacted application areas include traffic forecasting, \ndrug discovery, social network analysis and recommender systems. The module will provide the \nstudents the capability to analyse irregularly- and nontrivially-structured data in an effective way, \nand position geometric deep learning in a proper context with related fields. The main aim of the \ncourse is to enable students to make direct contributions to the field, thoroughly assimilate the \nkey concepts in the area, and draw relevant connections to various other fields (such as NLP, \nFourier Analysis and Probabilistic Graphical Models). We assume only a basic background in \nmachine learning with deep neural networks. \n \nLearning outcomes \nThe framework of geometric deep learning, and its key building blocks: symmetries, \nrepresentations, invariance and equivariance \nFundamentals of processing data on graphs, as well as impactful application areas for graph \nrepresentation learning \nTheoretical principles of graph machine learning: permutation invariance and equivariance \nThe three \"flavours\" of spatial graph neural networks (GNNs) (convolutional, attentional, \nmessage passing) and their relative merits. The Transformer architecture as a special case. \nAttaching symmetries to graphs: CNNs on images, spheres and manifolds, Geometric Graphs \nand E(n)-equivariant GNNs \nRelevant connections of geometric deep learning to various other fields (such as NLP, Fourier \nAnalysis and Probabilistic Graphical Models) \nLectures \nThe lectures will cover the following topics: \n \nLearning with invariances and symmetries: geometric deep learning. Foundations of group \ntheory and representation theory. \n",
        "Why study data on graphs? Success stories: drug screening, travel time estimation, \nrecommender systems. Fundamentals of graph data processing: network science, spectral \nclustering, node embeddings. \nPermutation invariance and equivariance on sets and graphs. The principal tasks of node, edge \nand graph classification. Neural networks for point clouds: Deep Sets, PointNet; universal \napproximation properties. \nThe three flavours of spatial GNNs: convolutional, attentional, message passing. Prominent \nexamples: GCN, SGC, ChebyNets, MoNet, GAT, GATv2, IN, MPNN, GraphNets. Tradeoffs of \nusing different GNN variants. \nGraph Rewiring: how to apply GNNs when there is no graph? Links to natural language \nprocessing---Transformers as a special case of attentional GNNs. Representative \nmethodologies for graph rewiring: GDC, SDRF, EGP, DGCNN. \nExpressive power of graph neural networks: the Weisfeiler-Lehman hierarchy. GINs as a \nmaximally expressive GNN. Links between GNNs and graph algorithms: neural algorithmic \nreasoning. \nCombining spatial symmetries with GNNs: E(n)-equivariant GNNs, TFNs, SE(3)-Transformer. A \ndeep dive into AlphaFold 2. \nWorked examples: Circulant matrices on grids, the discrete Fourier transform, and convolutional \nnetworks on spheres. Graph Fourier transform and the Laplacian eigenbasis. \nPracticals \nThe practical is designed to complement the knowledge learnt in lectures and teach students to \nderive additional important results and architectures not directly shown in lectures. The practical \nwill be given as a series of individual exercises (each either code implementation or \nproof/derivation). Each of these exercises can be individually assessed based on a specified \nmark budget. \n \nPossible practical topics include the study of higher-order GNNs and equivariant message \npassing. \n \nAssessment \n(60%) Group Mini-project (writeup) at the end of the course. The mini projects can either be \nself-proposed, or the students can express their preference for one of the provided topics, the \nlist of which will be announced at the start of term. The projects will consist of implementing \nand/or extending graph representation learning models in the literature, applying them to \npublicly available datasets. Students will undertake the project in pairs and submit a joint \nwriteup limited to 4,000 words (in line with other modules); appendix of work logs to be included \nbut ungraded; \n \n(10%) Short presentation and viva: students will give a short presentation to explain their \nindividual contribution to the mini-project and there will be a short viva following. \n \n(30%) Practical work completion. Completing the exercises specified in the practical to a \nsatisfactory standard. The practical assessor should be satisfied that the student derived their \nanswers using insight gained from the course; coupled with original thought, not by simple \n",
        "copy-pasting of relevant related work. The students would submit code and a short report, which \nwould then be marked in line with the predetermined mark budget for each practical item. \nThe students will learn how to run advanced architectures on GPU but no specific need for \ndedicated GPU resources. Practicals will be made possible to do on CPU; if required, students \ncan use GPUs on publicly available free services (such as Colab) for their mini-project work. \n \nReferences \nThe course will be based on the following literature: \n \n\"Geometric Deep Learning: Grids, Graphs, Groups, Geodesics, and Gauges\", by Michael \nBronstein, Joan Bruna, Taco Cohen and Petar Veli\u010dkovi\u0107 \n\"Graph Representation Learning\", by Will Hamilton \n\"Deep Learning\", by Ian Goodfellow, Yoshua Bengio and Aaron Courville.\n \n",
        "MOBILE HEALTH \nAims \nThe course aims to explore how wearable and mobile systems sensors can be used to gather \ndata relevant to understand health, how the data can be analysed with advanced signal \nprocessing and machine learning and the performance of these systems in terms of diagnostics \nand disease progression detection. \n \nSyllabus \nCourse Overview. Introduction to Mobile Health. Evaluation metrics and methodology. Basics of \nSignal Processing. \nInertial Measurement Units, Human Activity Recognition (HAR) and Gait Analysis and Machine \nLearning for IMU data. \nRadios, Bluetooth, GPS and Cellular. Epidemiology and contact tracing, Social interaction \nsensing and applications. Location tracking in health monitoring and public health. \nAudio Signal Processing. Voice and Speech Analysis: concepts and data analysis. Body \nSounds analysis. \nPhotoplethysmogram and Light sensing for health (heart and sleep) \nContactless and wireless behaviour and physiological monitoring \nGenerative Models for Wearable Health Data \nTopical Guest Lectures \nObjectives \nThe course aims to explore how wearable and mobile systems sensors can be used to gather \ndata relevant to understand health, how the data can be analysed with advanced signal \nprocessing and machine learning and the performance of these systems in terms of diagnostics \nand disease progression detection. \n \nRoughly, each lecture contains a theory part about the working of \u201csensor signals\u201d or \u201cdata \nanalysis methods\u201d and an application part which contextualises the concepts. \n \nAt the end of the course students should: Understand how mobile/wearable sensors capture \ndata and their working. Understand different approaches to acquiring and analysing sensor data \nfrom different types of sensors. Understand the concept of signal processing applied to time \nseries data and their practical application in health. Be able to extract sensor data and analyse it \nwith basic signal processing and machine learning techniques. Be aware of the different health \napplications of the various sensor techniques. The course will also touch on privacy and ethics \nimplications of the approaches developed in an orthogonal fashion. \n \nRecommended Reading \nPlease see Course Materials for recommended reading for each session. \n \nAssessment - Part II students \nTwo assignments will be based on two datasets which will be provided to the students: \n \n",
        "Assignment 1 (shared for Part II and Part III/MPhil): this will be based on a dataset and will be \nworth 40% of the final mark. The task of the assessment will be to perform pre-processing and \nbasic data analysis in a \"colab\" and an answer sheet of no more than 1000 words. \n \nAssignment 2 (Part II): This assignment (worth 60% of the final mark) will be a fuller analysis of \na dataset focusing on machine learning algorithms and metrics. Discussion and interpretation of \nthe findings will be reported in a colab and a report of no more than 1200 words. \n \nAssessment  - Part III and MPhil students \nTwo assignments will be based on datasets which will be provided to the students: \n \nAssignment 1: this will be based on a dataset and will be worth 40% of the final mark. The task \nof the assessment will be to perform pre-processing and basic data analysis in a \"colab\" and an \nanswer sheet of no more than 1000 words. \n \nAssignment 2: The second assignment (worth 60% of the final mark) will be based on multiple \ndatasets. The students will be asked to compare and contract ML algorithms/solutions (trained \nand tested on the different data) and discuss the findings and interpretation in terms of health \ncontext. This will be in the form of a colab and a report of 1800 words. \n \nFurther Information \nCurrent Cambridge undergraduate students who are continuing onto Part III or the MPhil in \nAdvanced Computer Science may only take this module if they did NOT take it as a Unit of \nAssessment in Part II. \n \nThis module is shared with Part II of the Computer Science Tripos. Assessment will be adjusted \nfor the two groups of students to be at an appropriate level for whichever course the student is \nenrolled on. Further information about assessment and practicals will follow at the first lecture.\n",
        "MULTICORE SEMANTICS AND PROGRAMMING \nPrerequisites: Some familiarity with discrete mathematics (sets, partial orders, etc.) and with \nsequential Java programming will be assumed. Experience with operational semantics and with \nsome concurrent programming would be helpful. \nMoodle, timetable \n \nAims \nIn recent years multiprocessors have become ubiquitous, but building reliable concurrent \nsystems with good performance remains very challenging. The aim of this module is to \nintroduce some of the theory and the practice of concurrent programming, from hardware \nmemory models and the design of high-level programming languages to the correctness and \nperformance properties of concurrent algorithms. \n \nLectures \nPart 1: Introduction and relaxed-memory concurrency [Professor P. Sewell] \n \nIntroduction. Sequential consistency, atomicity, basic concurrent problems. [1 block] \nConcurrency on real multiprocessors: the relaxed memory model(s) for x86, ARM, and IBM \nPower, and theoretical tools for reasoning about x86-TSO programs. [2 blocks] \nHigh-level languages. An introduction to C/C++11 and Java shared-memory concurrency. [1 \nblock] \nPart 2: Concurrent algorithms [Dr T. Harris] \n \nConcurrent programming. Simple algorithms (readers/writers, stacks, queues) and correctness \ncriteria (linearisability and progress properties). Advanced synchronisation patterns (e.g. some \nof the following: optimistic and lazy list algorithms, hash tables, double-checked locking, RCU, \nhazard pointers), with discussion of performance and on the interaction between algorithm \ndesign and the underlying relaxed memory models. [3 blocks] \nResearch topics, likely to include one hour on transactional memory and one guest lecture. [1 \nblock] \nObjectives \nBy the end of the course students should: \n \nhave a good understanding of the semantics of concurrent programs, both at the multprocessor \nlevel and the C/Java programming language level; \nhave a good understanding of some key concurrent algorithms, with practical experience. \nAssessment - Part II Students \nTwo assignments each worth 50% \n \nRecommended reading \nHerlihy, M. and Shavit, N. (2008). The art of multiprocessor programming. Morgan Kaufmann. \n \nCoursework \nCoursework will consist of assessed exercises. \n",
        " \nPractical work \nPart 2 of the course will include a practical exercise sheet.  The practical exercises involve \nbuilding concurrent data structures and measuring their performance.  The work can be \ncompleted in C, Java, or similar languages. \n \nAssessment - Part III and MPhil Students \nAssignment 1, for 50% of the overall grade. Written coursework comprising a series of questions \non hardware and software relaxed-memory concurrency semantics. \nAssignment 2, for 50% of the overall grade. Written coursework comprising a series of questions \non the design of mutual exclusion locks and shared-memory data structures. \nFurther Information \nCurrent Cambridge undergraduate students who are continuing onto Part III or the MPhil in \nAdvanced Computer Science may only take this module if they did NOT take it as a Unit of \nAssessment in Part II. \n \nThis module is shared with Part II of the Computer Science Tripos. Assessment will be adjusted \nfor the two groups of students to be at an appropriate level for whichever course the student is \nenrolled on. Further information about assessment and practicals will follow at the first lecture.\n",
        "REINFORCEMENT LEARNING \n \nAims \nThe aim of this module is to introduce students to the foundations of the field of reinforcement \nlearning (RL), discuss state-of-the-art RL methods, incentivise students to produce critical \nanalysis of recent methods, and prompt students to propose novel solutions that address \nshortcomings of existing methods. If judged by the headlines, RL has seen an unprecedented \nsuccess in recent years. However, the vast majority of RL methods still have shortcomings that \nmight not be apparent at first glance. The aim of the course is to inspire students by \ncommunicating the promising aspects of RL, but ensure that students develop the ability to \nproduce a critical analysis of current RL limitations. \n \nObjectives \nStudents will learn the following technical concepts: \n \nfundamental RL terminology and mathematical formalism; a brief history of RL and its \nconnection to neuroscience and biological systems \nRL methods for discrete action spaces, e.g. deep Q-learning and large-scale Monte Carlo Tree \nSearch \nmethods for exploration, modelling uncertainty, and partial observability for RL \nmodern policy gradient and actor-critic methods \nconcepts needed to construct model-based RL and Model Predictive Control methods \napproaches to make RL data-efficient and ways to enable simulation-to-reality transfer \nexamples of fine-tuning foundation models and large language models (LLMs) with human \nfeedback; safe RL concepts; examples of using RL for safety validation \nexamples of using RL for scientific discovery \nStudents will also gain experience with analysing RL methods to uncover their strengths and \nshortcomings, as well as proposing extensions to improve performance. \n \nFinally, students will gain skills needed to create and deliver a successful presentation in a \nformat similar to that of conference presentations. \n \nWith all of the above, students who take part in this module will be well-prepared to start \nconducting research in the field of reinforcement learning. \n  \n \nSyllabus \nTopic 1: Introduction and Fundamentals \n \nOverview of RL: foundational ideas, history, and books; connection to neuroscience and \nbiological systems, recent industrial applications and research demonstrations \nMathematical fundamentals: Markov decision processes, Bellman equations, policy and value \niteration, temporal difference learning \nTopic 2: RL in Discrete Action Spaces \n",
        " \nQ-learning, function approximation and deep Q-learning; nonstationarity in RL and its \nimplications for deep learning; example applications (video games; initial example: Atari) \nMonte Carlo Tree Search; example applications (AlphaGo) \nTopic 3: Exploration, Uncertainty, Partial Observability \n \nMulti-armed bandits, Bayesian optimisation, regret analysis \nPartially observable Markov decision process; belief, memory, and sequence modelling \n(probabilistic methods, recurrent networks, transformers) \nTopic 4: Policy Gradient and Actor-critic Methods for Continuous Action Spaces \n \nImportance sampling, policy gradient theorem, actor-critic methods (SPG, DDPG) \nProximal policy optimisation; example applications \nTopic 5: Model-based RL and Model Predictive Control \n \nLearning dynamics models (graph networks, stochastic processes, diffusion models, \nphysics-based models, ensembles); planning with learned models \nModel predictive control; example applications (real-time control) \nTopic 6: Data-efficient RL and Simulation-to-reality Transfer \n \nData-efficient learning with probabilistic methods from real data (e.g. policy search in robotics), \nreal-to-sim inference and differentiable simulation, data-efficient simulation-to-reality transfer \nRL for physical systems (successful examples in locomotion, open problems in contact-rich \nmanipulation, applications to logistics, energy, and transport systems); examples of RL for \nhealthcare. \nTopic 7: RL with Human Feedback ; Safe RL and RL for Validation \n \nFine-tuning large language models (LLMs) and other foundation models with human feedback \n(TRLX,RL4LMs, a light-weight overview of RLHF) \nA review of SafeRL, example: optimising commercial HVAC systems using policy improvement \nwith constraints; improving safety using RL for validation: examples in autonomous driving and \nautonomous flying and aircraft collision avoidance \nTopic 8: RL for Scientific Discovery; Student Presentations \n \nExamples of RL for molecular design and drug discovery, active learning for synthesising new \nmaterials, RL for nuclear fusion experiments \nStudent presentations (based on essays and mini-projects) for other topics in RL, e.g. \nmulti-agent RL, hierarchical RL, RL for hyperparameter optimisation and NN architecture \nsearch, RL for multi-task transfer, lifelong RL, RL in biological systems, etc. \nAssessment \nThe assessment for this module consists of: \n \nEssay and mini-project (60%)  \n",
        "Students will choose a concrete RL method from the literature, then complete a 2-part essay \ndescribed below: \nEssay Part 1 (1500 words, 20%): Introduce a formal description of the method and explain how \nthe method extended the state-of-the-art at the time of its publication \nEssay Part 2 or a mini-project (2500 words, 40%): Provide a critical analysis of the chosen RL \nmethod. \nPresentation of the essay and mini-project results (15%)  \nstudents will be expected to create and record a 15-minute video presentation of the analysis \ndescribed in their essay and mini-project. \nShort test of RL theory fundamentals (15%)  \nto test the understanding of RL fundamentals (~15 minutes, in-class, closed book) \nParticipation in seminar discussions (10%)  \ntake an active part in the seminars by asking clarifying questions and mentioning related works. \nRecommended Reading \nBooks \n \n[S&B] Reinforcement Learning: An Introduction (second print edition). Richard S. Sutton, \nAndrew G. Barto. [Available from the book\u2019s website as a free PDF updated in 2022] \n \n[CZ] Algorithms for Reinforcement Learning. Csaba Szepesvari. [Available from the book\u2019s \nwebsite as a free PDF updated in 2019] \n \n[MK] Algorithms for Decision Making. Mykel J. Kochenderfer Tim A. WheelerKyle H. Wray. \n[Available from the book\u2019s website as a free PDF updated in 2023] \n \n[DB] Reinforcement Learning and Optimal Control. Dimitri Bertsekas. [Available from the book\u2019s \nwebsite as a free PDF updated in 2023] \n \nPresentation guidelines \n \n[KN] Ten simple rules for effective presentation slides. Kristen M.Naegle. PLoS computational \nbiology 17, no. 12 (2021).\n \n",
        "THEORIES OF SOCIO-DIGITAL DESIGN FOR HUMAN-CENTRED AI \n \nPrerequisites: This course is appropriate to any ACS student, and will assist in the development \nof critical thinking, argument and long-form writing skills. Prior experience of essay-based \nhumanities subjects at university or secondary school will be beneficial, but not required. \nMoodle, timetable \n \nAims \nThis module is a theoretically-oriented advanced introduction to the broad field of \nhuman-computer interaction, extending to consider topics such as intelligent user interfaces, \ncritical and speculative design, participatory design, cognitive models of users, human-centred, \nas well as more-than-human-centred design, and others. The course will not address purely \nengineering approaches to the development of user interfaces (unless there is a clear \ntheoretical question being addressed), but allow students to effectively link the political, social, \nand ethical considerations in HCI to specific technical and conceptual design challenges. The \nmodule builds on the Practical Research in Human-Centred AI course offered in Michaelmas \n(developed with researchers at Microsoft Research Cambridge) and a collaboration with the \nLeverhulme Centre for the Future of Intelligence at Cambridge. Participants may include visitors \nfrom these groups and/or interdisciplinary research students and academic guests from other \nUniversity departments, including Cambridge Digital Humanities. \n \nSyllabus \nThe syllabus will remain broadly within the area of human-computer interaction, including \ntheories of design practice and the social contexts of technology use. Individual seminar topics \nwill be selected in response to contemporary and recent research developments, in consultation \nwith members of the class and visiting contributors. \n \nRepresentative topics in the next year are likely to include: \n \nResponsible AI and HCI \nIntersectional design for social justice \nParticipatory design and co-design \nSustainable AI and more-than-human-centred design \nUsefulness and usability of ethical design toolkits \nThe EU AI Act in design practice \nAI and interface design for transparency \nDesigning otherwise: on anarchist HCI \n  \nObjectives \nOn completion of this module, students should have developed facility in discussing and \ncritiquing the aims of their research, especially for an audience drawn from other academic \ndisciplines, including the following skills: \n \nTheoretical motivation and defence of a research question. \n",
        "Consideration of a research proposal from one or more alternative theoretical perspectives. \nPotential critique of the theoretical basis for a programme of research. \nCoursework \nIn advance of each seminar, all participants must read the essential reading - generally one \nlong, plus one or two short papers. Further reading is suggested for some seminars. Some \nweeks, instead of shorter papers, students will be asked to explore specific design tools or \nresources. \n \nBefore seminars 2-8, each student will submit a written commentary on the paper, either \ngenerated using a large language model (LLM), or written in the style of an LLM, supplemented \nby a short original observation. \n \nEach member of the class must select one of the 8 seminar themes as the subject for a more \nextended critical review essay. The critical review should be written as an academic research \nessay, not in LLM style. \n \nEach seminar will include an informal design exercise to be carried out in small groups. The \npurpose of these exercises is to reflect on a variety of practical design resources, by applying \nthose resources to concrete case studies. \n \nThroughout the course, students will be expected to keep a \"reflective diary\", briefly reporting \nthemes that arise in discussion, reflections on the design exercises, and ways in which the \nreadings relate to their own research interests. \n  \n \nPractical work \nThere is no practical work element in this course. \n \nAssessment \nWritten critical review of a single discussion text (20%) \nReflective diary submitted at the end of the module (60%) \nIn advance of the session, each student will submit a written commentary on the paper, either \ngenerated using a large language model (LLM), or written in the style of an LLM. A short original \nobservation should be added (20%) \nRecommended reading \nTo be assigned during the module, as discussed above. Most set readings will be available \neither from the ACM Digital Library, or from institutional repositories of the authors. \n \nFurther Information \nStudents from the MPhil in the Ethics of AI, Data and Algorithms, MSt in AI, and MPhil in Digital \nHumanities courses also attend the lectures for this module.\n \n",
        "THEORY OF DEEP LEARNING \nPrerequisites: A strong background in calculus, probability theory and linear algebra, familiarity \nwith differential equations, optimization and information theory is required. Students need to \nhave studied Deep Neural Networks, familiarity with deep learning terminology (e.g. \narchitectures, benchmark problems) as well as deep learning frameworks (pytorch, jax or \nsimilar) is assumed \nMoodle, timetable \n \nAims \nThe objectives of this course is to expose you to one of the most active contemporary research \ndirections within machine learning: the theory of deep learning (DL). While the first wave of \nmodern DL has focussed on empirical breakthroughs and ever more complex techniques, the \nattention is now shifting to building a solid mathematical understanding of why these techniques \nwork so well in the first place. The purpose of this course is to review this recent progress \nthrough a mixture of reading group sessions and invited talks by leading researchers in the \ntopic, and prepare you to embark on a PhD in modern deep learning research. Compared to \ntypical, non-mathematical courses on deep learning, this advanced module will appeal to those \nwho have strong foundations in mathematics and theoretical computer science. In a way, this \ncourse is our answer to the question \u201cWhat should the world\u2019s best computer science students \nknow about deep learning in 2023?\u201d \n \nLearning Outcomes \nThis course should prepare the best students to start a PhD in the theory and mathematics of \ndeep learning, and to start formulating their own hypotheses in this space. You will be \nintroduced to a range of empirical and mathematical tools developed in recent years for the \nstudy of deep learning behaviour, and you will build an awareness of the main open questions \nand current lines of attack. At the end of the course you will: \n \nbe able to explain why classical learning theory is insufficient to describe the phenomenon of \ngeneralization in DL \nbe able to design and interpret empirical studies aimed at understanding generalization \nbe able to explain the role of overparameterization: be able to use deep linear models as a \nmodel to study implicit regularisation of gradient-based learning \nbe able to state PAC-Bayes and Information-theoretic bounds, and apply them to DL \nbe able to explain the connection between Gaussian processes and neural networks, and will \nbe able to study learning dynamics in the neural tangent kernel (NTK) regime. \nbe able to formulate your own hypotheses about DL and choose tools to prove/test them \nleverage your deeper theoretical understanding to produce more robust, rigorous and \nreproducible solutions to practical machine learning problems. \nSyllabus \nEach week we'll have two to four student-lead presentations about a research paper chosen \nfrom a reading list. The reading list loosely follows the weekly breakdown below (but we adapt it \neach year based as this is an active research area): \n \n",
        "Week 1: Introduction to the topic \nWeek 2: Empirical Studies of Deep Learning Phenomena \nWeek 3: Interpolation Regime and \u201cDouble Descent\u201d Phenomena \nWeek 4: Implicit Regularization in Deep Linear Models \nWeek 5: Approximation Theory \nWeek 6: Networks in the Infinite Width Limit \nWeek 7: PAC-Bayes and Information Theoretic Bounds for SGD \nWeek 8: Discussion and Coursework Spotlight Session \n \nAssessment \nStudents will be assessed on the following basis: \n \n20% for presentation/content contributed to the module: Each student will have an opportunity \nto present one of the recommended papers during Weeks 1-7 (30 minute slot including Q&A). \nFor the presentation, students should aim to communicate the core ideas behind the paper, and \nclearly present the results, conclusions, and future directions. Where possible, students are \nencouraged to comment on how the work itself fits into broader research goals. \n10% for active participation (regular attendance and contribution to discussions during the Q&A \nsessions). \n70% for a group project report, with a word limit of 4000. Either (1) an original research \nproposal/report with a hypothesis, review of related literature, and ideally preliminary findings, \nor, (2) reproduction and ideally extension of an existing relevant paper. \nCoursework reports are marked in line with general ACS guidelines, reports receiving top marks \nwill have have demonstrable research value (contain an original research idea, extension of \nexisting work, or a thorough reproduction effort which is valuable to the research community). \nAdditionally, some projects will be suggested during the first weeks of the course, although \nstudents are encouraged to come up with their own ideas. Students may be required to \nparticipate in group projects, with groups of size 2-3 (the class groups will be separated). For \nany given project, individual contributions would be noted for assessment though a viva \ncomponent. \nRelationship with related modules \nThis course can be considered as an advanced follow-up to the Part IIB course on Deep Neural \nNetworks. That course introduces some high level concepts that this course significantly \nexpands on. \n \nThis module complements L48: Machine Learning in the Physical World and L46: Principles of \nMachine Learning Systems, which focus on applications and hardware/systems aspects of ML \nrespectively. \n \nRecommended reading \nPNAS Colloquium on the Science of Deep Learning \nMathematics of Machine Learning book by Marc Deisenroth, Aldo Faisal and Cheng Soon Ong. \nProbabilistic Machine Learning: An Introduction book by Kevin Murphy \nMatus Telgarsky's lecture notes on deep learning theory  \n",
        "These are in addition to the papers which will be discussed in the lectures.\n \n",
        "UNDERSTANDING NETWORKED-SYSTEMS PERFORMANCE \nPrerequisites: A student must possess knowledge comparable to the undergraduate subjects on \nUnix Tools, C/C++ programming and Computer Networks. Optionally; a student will be \nadvantaged by doing an introduction to Computer Systems Modelling; such as the Part 2 \nComputer Systems Modelling subject as well as having a background in measurement \nmethodologies such as that of L50: Introduction to networking and systems measurements \nprovided in the ACS/Part III curriculum. \nMoodle, timetable \n \nAims \nThis is a practical course, actively building and extending software tools to observe the detailed \nbehaviour of transaction-oriented datacenter-like software. Students will observe and \nunderstand sources of user-facing tail latency, including that stemming \nfrom resource contention, cross-program interference, bad software locking, and simple design \nerrors. \nA 2-hour weekly hybrid, lecture/lab format permits students continuous monitored progress with \ncomplexity of tasks building naturally upon the previous weeks learning. \n \nObjectives \nUpon successful completion of the course, students will be able to: \n \nMake order-of-magnitude estimates of software, hardware, and I/O speeds \nMake valid measurements of actual software, hardware, and I/O speeds \nCreate observation facilities, including logs and dashboards, as part of a software system \ndesign \nCreate tracing facilities to fully observe the execution of complex software \nTime-align traces across multiple computers \nDisplay dense tracing information in meaningful ways \nReason about the sources of real-time and transaction delays, including cross-program \nresource interference, remote procedure call delays, and software locking surprises \nFix programs based on the above reasoning, making their response times faster and more \nrobust \n \nSyllabus \nMeasurement \n   Week 1 Intro, Measuring CPU time, rdtsc, Measuring memory access times, Measuring disks, \n   Week 2 gettimeofday, logs Measuring networks, Remote Procedure Calls, Multi-threads, locks \nObservation \n   Week 3 RPC, logs, displaying traces, interference \n   Week 4 Antagonist programs, Logging, dashboards, profiling. \nKUtrace \n   Week 5 Kernel patches, hello world, post-processing \n   Week 6 KUtrace multi-CPU time display \n   Week 7 Client-server KUtrace, with antagonists and interference \n",
        "   Week 8 Other trace mysteries \n \n \nAssessment \nThis is a Lab-based module; assessment is based upon reports covering guided laboratory work \nperformed each week. Assessment will be via two submissions: \n \n20% assignment deadline week 4 based upon Lab work from Weeks 1-4; word target 1000, limit \n2000 \n80% assignment deadline week 8 based upon lab work from weeks 5-8; word target 2000, limit \n4000 \nThe intent of the first assessment point is to provide rich feedback to students based upon a \n20% assignment permitting focussed and improved work to be executedfor the final assignment. \n \nAll work in this module is expected to be the effort of the student. Enough equipment is \nprovisioned to allow each student an independent set of apparatus. While classmembers may \nfind sharing operational experience valuable all assessment is based upon a students sole \nsubmission based upon. their own experiments and findings. \n \nReading Material \nCore text \n \nRichard L. Sites, Understanding Software Dynamics, Addison-Wesley Professional Computing \nSeries, 2022 \nFurther reading: papers and presentations that you might find interesting. None are required \nreading \u2013 they are well-written and/or informative. \n \nJohn K. Ousterhout et al., A trace-driven analysis of the UNIX 4.2 BSD file system ACM \nSIGOPS Operating Systems Review, Vol. 19, No. 5, Dec, 1985 \nhttps://dl.acm.org/doi/pdf/10.1145/323627.323631 \nLuiz Andr\u00b4e Barroso, Jimmy Clidaras, Urs H\u00a8olzle, The Datacenter as a Computer: An \nIntroduction to the Design of Warehouse-Scale Machines, Second Edition \nJohn L. Hennessy, David A. Patterson, Computer Architecture, A Quantitative Approach 5th \nEdition \nGeorge Varghese, Network Algorithmics. Morgan Kaufmann \nActually keeping datacenter software up and running \u2013 how Google runs production systems \nSite Reliability Engineering Edited by Betsy Beyer, Chris Jones, Jennifer Petoff and Niall \nRichard Murphy free PDF: https://landing.google.com/sre/book.html \nHow NOT to run datacenters (27 minute video, funny/sad) dotScale 2014 - Robert Kennedy - \nLife in the Trenches of healthcare.gov https://www.youtube.com/watch?v=GLQyj-kBRdo \nAn extended (optional) reading list will also be provided. \n \n"
    ],
    "semesters_-2289514641991506068": [
        "1st Semester \n \nDATABASES \n \nAims \nThis course introduces basic concepts for database systems as seen from the perspective of \napplication designers. That is, the focus is on the abstractions supported by database \nmanagement systems and not on how those abstractions are implemented. \n \nThe database world is currently undergoing swift and dramatic transformations largely driven by \nInternet-oriented applications and services. Today many more options are available to database \napplication developers than in the past and so it is becoming increasingly difficult to sort fact \nfrom fiction. The course attempts to cut through the fog with a practical approach that \nemphasises engineering tradeoffs that underpin these recent developments and also guide our \nselection of \u201cthe right tool for the job.\u201d \n \nThis course covers three approaches. First, the traditional mainstay of the database industry -- \nthe relational approach -- is described with emphasis on eliminating logical redundancy in data. \nThen two representatives of recent trends are presented -- graph-oriented and \ndocument-oriented databases. The lectures are supported with two Help and Tick sessions, \nwhere students gain hands-on experience and guidance with the Assessed Exercises (ticks). \n \nLectures \nL1 Introduction. What is a database system? What is a data model? Typical DBMS \nconfigurations and variations (fast queries or reliable update). In-core, in secondary store or \ndistributed. \nL2 Conceptual modelling. Entities, relations, E/R diagrams and implementation-independent \nmodelling, weak entities, cardinality. \nL3+4 The relational database model. Implementing E/R models with relational tables. Relational \nalgebra and SQL. Basic query primitives. Update anomalies caused by redundancy. Minimising \nredundancy with normalised schemas. \nL5 Transactions. On-Line Transaction Processing. On-line Analytical Processing. Reliability, \nthroughput, normal forms, ACID, BASE, eventual consistency. \nL6 Documents and semi-structured data. The NoSQL, schema-free movement. XML/JSON. \nKey/value stores. Embracing data redundancy: representing data for fast, application-specific \naccess. Path queries (if time permits). \nL7 Further SQL. Multisets, NULL values, aggregates, transitive closure, expressibility (nested \nqueries and recursive SQL). \nL8 Graph databases. Optimised for processing enormous numbers of nodes and edges. \nImplementing E/R models in a graph-oriented database. Comparison of the presented models. \nObjectives \nAt the end of the course students should \n\n \nbe able to design entity-relationship diagrams to represent simple database application \nscenarios \nknow how to convert entity-relationship diagrams to relational- and graph-oriented \nimplementations \nunderstand the fundamental tradeoff between the ease of updating data and the response time \nof complex queries \nunderstand that no single data architecture can be used to meet all data management \nrequirements \nbe familiar with recent trends in the database area. \nRecommended reading \nLemahieu, W., Broucke, S. van den and Baesens, B. (2018) Principles of database \nmanagement. Cambridge University Press.\n \n\nDIGITAL ELECTRONICS \n \nAims \nThe aims of this course are to present the principles of combinational and sequential digital logic \ndesign and optimisation at a gate level. The use of n and p channel MOSFETs for building logic \ngates is also introduced. \n \nTopics \nIntroduction. Semiconductors to computers. Logic variables. Examples of simple logic. Logic \ngates. Boolean algebra. De Morgan\u2019s theorem. \nLogic minimisation. Truth tables and normal forms. Karnaugh maps. Quine-McCluskey method. \nBinary adders. Half adder, full adder, ripple carry adder, fast carry generation. \nCombinational logic design: further considerations. Multilevel logic. Gate propagation delay. An \nintroduction to timing diagrams. Hazards and hazard elimination. Other ways to implement \ncombinational logic. \nIntroduction to practical classes. Prototyping box. Breadboard and Dual in line (DIL) packages. \nWiring. \nSequential logic. Memory elements. RS latch. Transparent D latch. Master-slave D flip-flop. T \nand JK flip-flops. Setup and hold times. \nSequential logic. Counters: Ripple and synchronous. Shift registers. System timing - setup time \nconstraint, clock skew, metastability. \nSynchronous State Machines. Moore and Mealy finite state machines (FSMs). Reset and self \nstarting. State transition diagrams. Elimination of redundant states - row matching and state \nequivalence/implication table. \nFurther state machines. State assignment: sequential, sliding, shift register, one hot. \nImplementation of FSMs. \nIntroduction to microprocessors. Microarchitecture, fetch, register access, memory access, \nbranching, execution time. \nElectronics, Devices and Circuits. Current and voltage, conductors/insulators/semiconductors, \nresistance, basic circuit theory, the potential divider. Solving non-linear circuits. P-N junction \n(forward and reverse bias), N and p channel MOSFETs (operation and characteristics) and \nn-MOSFET logic, e.g., n-MOSFET inverter. Power consumption and switching time problems \nproblems in n-MOSFET logic. CMOS logic (NOT, NAND and NOR gates), logic families, noise \nmargin. \nObjectives \nAt the end of the course students should \n \nunderstand the relationships between combination logic and boolean algebra, and between \nsequential logic and finite state machines; \nbe able to design and minimise combinational logic; \nappreciate tradeoffs in complexity and speed of combinational designs; \nunderstand how state can be stored in a digital logic circuit; \nknow how to design a simple finite state machine from a specification and be able to implement \nthis in gates and edge triggered flip-flops; \n\nunderstand how to use MOSFETs to build digital logic circuits. \nRecommended reading \n* Harris, D.M. and Harris, S.L. (2013). Digital design and computer architecture. Morgan \nKaufmann (2nd ed.). The first edition is still relevant. \nKatz, R.H. (2004). Contemporary logic design. Benjamin/Cummings. The 1994 edition is more \nthan sufficient. \nHayes, J.P. (1993). Introduction to digital logic design. Addison-Wesley. \n \nBooks for reference: \n \nHorowitz, P. and Hill, W. (1989). The art of electronics. Cambridge University Press (2nd ed.) \n(more analog). \nWeste, N.H.E. and Harris, D. (2005). CMOS VLSI Design - a circuits and systems perspective. \nAddison-Wesley (3rd ed.). \nMead, C. and Conway, L. (1980). Introduction to VLSI systems. Addison-Wesley. \nCrowe, J. and Hayes-Gill, B. (1998). Introduction to digital electronics. Butterworth-Heinemann. \nGibson, J.R. (1992). Electronic logic circuits. Butterworth-Heinemann.\n \n\nDISCRETE MATHEMATICS \n \nAims \nThe course aims to introduce the mathematics of discrete structures, showing it as an essential \ntool for computer science that can be clever and beautiful. \n \nLectures \nProof [5 lectures]. \nProofs in practice and mathematical jargon. Mathematical statements: implication, bi-implication, \nuniversal quantification, conjunction, existential quantification, disjunction, negation. Logical \ndeduction: proof strategies and patterns, scratch work, logical equivalences. Proof by \ncontradiction. Divisibility and congruences. Fermat\u2019s Little Theorem. \n \nNumbers [5 lectures]. \nNumber systems: natural numbers, integers, rationals, modular integers. The Division Theorem \nand Algorithm. Modular arithmetic. Sets: membership and comprehension. The greatest \ncommon divisor, and Euclid\u2019s Algorithm and Theorem. The Extended Euclid\u2019s Algorithm and \nmultiplicative inverses in modular arithmetic. The Diffie-Hellman cryptographic method. \nMathematical induction: Binomial Theorem, Pascal\u2019s Triangle, Fundamental Theorem of \nArithmetic, Euclid\u2019s infinity of primes. \n \nSets [9 lectures]. \nExtensionality Axiom: subsets and supersets. Separation Principle: Russell\u2019s Paradox, the \nempty set. Powerset Axiom: the powerset Boolean algebra, Venn and Hasse diagrams. Pairing \nAxiom: singletons, ordered pairs, products. Union axiom: big unions, big intersections, disjoint \nunions. Relations: composition, matrices, directed graphs, preorders and partial orders. Partial \nand (total) functions. Bijections: sections and retractions. Equivalence relations and set \npartitions. Calculus of bijections: characteristic (or indicator) functions. Finite cardinality and \ncounting. Infinity axiom. Surjections. Enumerable and countable sets. Axiom of choice. \nInjections. Images: direct and inverse images. Replacement Axiom: set-indexed constructions. \nSet cardinality: Cantor-Schoeder-Bernstein Theorem, unbounded cardinality, diagonalisation, \nfixed-points. Foundation Axiom. \n \nFormal languages and automata [5 lectures]. \nIntroduction to inductive definitions using rules and proof by rule induction. Abstract syntax \ntrees. Regular expressions and their algebra. Finite automata and regular languages: Kleene\u2019s \ntheorem and the Pumping Lemma. \n \nObjectives \nOn completing the course, students should be able to \n \nprove and disprove mathematical statements using a variety of techniques; \napply the mathematical principle of induction; \nknow the basics of modular arithmetic and appreciate its role in cryptography; \n\nunderstand and use the language of set theory in applications to computer science; \ndefine sets inductively using rules and prove properties about them; \nconvert between regular expressions and finite automata; \nuse the Pumping Lemma to prove that a language is not regular. \nRecommended reading \nBiggs, N.L. (2002). Discrete mathematics. Oxford University Press (Second Edition). \nDavenport, H. (2008). The higher arithmetic: an introduction to the theory of numbers. \nCambridge University Press. \nHammack, R. (2013). Book of proof. Privately published (Second edition). Available at: \nhttp://www.people.vcu.edu/ rhammack/BookOfProof/index.html \nHouston, K. (2009). How to think like a mathematician: a companion to undergraduate \nmathematics. Cambridge University Press. \nKozen, D.C. (1997). Automata and computability. Springer. \nLehman, E.; Leighton, F.T.; Meyer, A.R. (2014). Mathematics for computer science. Available \non-line. \nVelleman, D.J. (2006). How to prove it: a structured approach. Cambridge University Press \n(Second Edition).\n \n\nFOUNDATIONS OF COMPUTER SCIENCE \nAims \nThe main aim of this course is to present the basic principles of programming. As the \nintroductory course of the Computer Science Tripos, it caters for students from all backgrounds. \nTo those who have had no programming experience, it will be comprehensible; to those \nexperienced in languages such as C, it will attempt to correct any bad habits that they have \nlearnt. \n \nA further aim is to introduce the principles of data structures and algorithms. The course will \nemphasise the algorithmic side of programming, focusing on problem-solving rather than on \nhardware-level bits and bytes. Accordingly it will present basic algorithms for sorting, searching, \netc., and discuss their efficiency using O-notation. Worked examples (such as polynomial \narithmetic) will demonstrate how algorithmic ideas can be used to build efficient applications. \n \nThe course will use a functional language (OCaml). OCaml is particularly appropriate for \ninexperienced programmers, since a faulty program cannot crash and OCaml\u2019s unobtrusive type \nsystem captures many program faults before execution. The course will present the elements of \nfunctional programming, such as curried and higher-order functions. But it will also introduce \ntraditional (procedural) programming, such as assignments, arrays and references. \n \nLectures \nIntroduction to Programming. The role of abstraction and representation. Introduction to integer \nand floating-point arithmetic. Declaring functions. Decisions and booleans. Example: integer \nexponentiation. \nRecursion and Efficiency. Examples: Exponentiation and summing integers. Iteration versus \nrecursion. Examples of growth rates. Dominance and O-Notation. The costs of some \nrepresentative functions. Cost estimation. \nLists. Basic list operations. Append. Na\u00efve versus efficient functions for length and reverse. \nStrings. \nMore on lists. The utilities take and drop. Pattern-matching: zip, unzip. A word on polymorphism. \nThe \u201cmaking change\u201d example. \nSorting. A random number generator. Insertion sort, mergesort, quicksort. Their efficiency. \nDatatypes and trees. Pattern-matching and case expressions. Exceptions. Binary tree traversal \n(conversion to lists): preorder, inorder, postorder. \nDictionaries and functional arrays. Functional arrays. Dictionaries: association lists (slow) versus \nbinary search trees. Problems with unbalanced trees. \nFunctions as values. Nameless functions. Currying. The \u201capply to all\u201d functional, map. \nExamples: The predicate functionals filter and exists. \nSequences, or lazy lists. Non-strict functions such as IF. Call-by-need versus call-by-name. Lazy \nlists. Their implementation in OCaml. Applications, for example Newton-Raphson square roots. \nQueues and search strategies. Depth-first search and its limitations. Breadth-first search (BFS). \nImplementing BFS using lists. An efficient representation of queues. Importance of efficient data \nrepresentation. \n\nElements of procedural programming. Address versus contents. Assignment versus binding. \nOwn variables. Arrays, mutable or not. Introduction to linked lists. \nObjectives \nAt the end of the course, students should \n \nbe able to write simple OCaml programs; \nunderstand the fundamentals of using a data structure to represent some mathematical \nabstraction; \nbe able to estimate the efficiency of simple algorithms, using the notions of average-case, \nworse-case and amortised costs; \nknow the comparative advantages of insertion sort, quick sort and merge sort; \nunderstand binary search and binary search trees; \nknow how to use currying and higher-order functions; \nunderstand how OCaml combines imperative and functional programming in a single language. \nRecommended reading \nJohn Whitington. OCaml from the Very Beginning. (http://ocaml-book.com). \n \nOkasaki, C. (1998). Purely functional data structures. Cambridge University Press.\n \n\nHARDWARE PRACTICAL CLASSES \nThe Hardware Practical Classes accompany the Digital Electronics series of lectures. The aim \nof the Practical Classes is to enable students to get hands-on experience of designing, building, \nand testing and debugging of digital electronic circuits. The labs will take place in the Intel Lab. \nlocated in the William Gates Building (WGB). \n \nThe Digital Electronics lecture series occupies 12 lectures in the first 4 weeks of the Michaelmas \nTerm, while the Practical Classes occupy the latter 6 weeks of the Michaelmas Term and the \nfirst 6 weeks of the Lent Term. If required, extra sessions will be available for any students \nneeding to 'catch-up' on missed sessions or to complete any remaining practical work. \n \nThe Practical Classes take the form of 4 workshops, specifically: \n \nWorkshop 1 \u2013 Electronic Die; \nWorkshop 2 \u2013 Shaft Position Encoder; \nWorkshop 3 \u2013 Debouncing a Switch; \nWorkshop 4 \u2013 Framestore for an LED Array. \nIn general, the workshops require some preparatory work to be done prior to the practical \nsession. These tasks are highlighted at the beginning of each Worksheet. Typically this involves \npreparing a design that you will then build, test and modify during the practical class. Note that \ninsufficient preparation prior to the practical classes may compromise effective use of your time \nin the Intel lab. \n \nIn the Practical Classes you will usually work on your own, and you are expected to complete \none Workshop in each of your scheduled sessions. Demonstrators are available during the \nsessions to assist you with any queries or problems you may have. \n \nImportant: Remember to get your work ticked.\n \n\nINTRODUCTION TO GRAPHICS \nAims \nTo introduce the necessary background, the basic algorithms, and the applications of computer \ngraphics and graphics hardware. \n \nLectures \nBackground. What is an image? Resolution and quantisation. Storage of images in memory. [1 \nlecture] \nRendering. Perspective. Reflection of light from surfaces and shading. Geometric models. Ray \ntracing. [2 lectures] \nGraphics pipeline. Polygonal mesh models. Transformations using matrices in 2D and 3D. \nHomogeneous coordinates. Projection: orthographic and perspective. [1 lecture] \nGraphics hardware and modern OpenGL. GPU rendering. GPU frameworks and APIs. Vertex \nprocessing. Rasterisation. Fragment processing. Working with meshes and textures. Z-buffer. \nDouble-buffering and frame synchronization. [2 lectures] \n  \nHuman vision, colour and tone mapping. Perception of colour. Tone mapping. Colour spaces. [2 \nlectures] \nObjectives \nBy the end of the course students should be able to: \n \nunderstand and apply in practice basic concepts of ray-tracing: ray-object intersection, \nreflections, refraction, shadow rays, distributed ray-tracing, direct and indirect illumination; \ndescribe and explain the following algorithms: z-buffer, texture mapping, double buffering, \nmip-map, and normal-mapping; \nuse matrices and homogeneous coordinates to represent and perform 2D and 3D \ntransformations; understand and use 3D to 2D projection, the viewing volume, and 3D clipping; \nimplement OpenGL code for rendering of polygonal objects, control camera and lighting, work \nwith vertex and fragment shaders; \ndescribe a number of colour spaces and their relative merits. \nexplain the need for tone mapping and colour processing in rendering pipeline. \nRecommended reading \n* Shirley, P. and Marschner, S. (2009). Fundamentals of Computer Graphics. CRC Press (3rd \ned.). \n \nFoley, J.D., van Dam, A., Feiner, S.K. and Hughes, J.F. (1990). Computer graphics: principles \nand practice. Addison-Wesley (2nd ed.). \n \nKessenich, J.M., Sellers, G. and Shreiner, D (2016). OpenGL Programming Guide: The Official \nGuide to Learning OpenGL, Version 4.5 with SPIR-V, [seventh edition and later]\n \n\nOBJECT-ORIENTED PROGRAMMING \n \nAims \nThe goal of this course is to provide students with an understanding of Object-Oriented \nProgramming. Concepts are demonstrated in multiple languages, but the primary language is \nJava. \n \nLecture syllabus \nTypes, Objects and Classes Moving from functional to imperative. Functions, methods. Control \nflow. values, variables and types. Primitive Types. Classes as custom types. Objects vs \nClasses. Class definition, constructors. Static data and methods. \nDesigning Classes Identifying classes. UML class diagrams. Modularity. Encapsulation/data \nhiding. Immutability. Access modifiers. Parameterised types (Generics). \nPointers, References and Memory Pointers and references. Reference types in Java. The call \nstack. The heap. Iteration and recursion. Pass-by-value and pass-by-reference. \nInheritance Inheritance. Casting. Shadowing. Overloading. Overriding. Abstract Methods and \nClasses. \nPolymorphism and Multiple Inheritance Polymorphism in ML and Java. Multiple inheritance. \nInterfaces in Java. \nLifecycle of an Object Constructors and chaining. Destructors. Finalizers. Garbage Collection: \nreference counting, tracing. \nJava Collections and Object Comparison Java Collection interface. Key classes. Collections \nclass. Iteration options and the use of Iterator. Comparing primitives and objects. Operator \noverloading. \nError Handling Types of errors. Limitations of return values. Deferred error handling. Exceptions. \nCustom exceptions. Checked vs unchecked. Inappropriate use of exceptions. Assertions. \nDesignLanguage evolution Need for languages to evolve. Generics in Java. Type erasure. \nIntroduction to Java 8: Lambda functions, functions as values, method references, streams. \nDesign Patterns Introduction to design patterns. Open-closed principle. Examples of Singleton, \nDecorator, State, Composite, Strategy, Observer. [2 lectures] \nObjectives \nAt the end of the course students should \n \nProvide an overview of key OOP concepts that are transferable in mainstream programming \nlanguages; \nGain an understanding of software development approaches adopted in the industry including \nmaintainability, testing, and software design patterns; \nIncrease programming familiarity with Java; \nRecommended reading \n1. Real-World Software Development: A Project-Driven Guide to Fundamentals in Java [Urma et \nal.] \n2. Modern Java in Action (2nd edition) [Urma et al.]. \n3. Java How to Program: early objects [Deitel et al.]\n \n\nOCAML PRACTICAL CLASSES \n \nThe Role of Tickers \nA ticking session is a short (5 minute) one-to-one session with a ticker conducted on Thursday \nafternoons. The role of the ticker is twofold: \n \nto check you understand your solution (and didn\u2019t just have some lucky guesses or copy code \nfrom elsewhere) \nto give you general feedback on ways to improve your code. \nTo those ends a Ticker will typically ask you questions related to the core material of the tick and \ndiscuss your code directly. \n \nDeadline Extensions \nDeadline extensions can be granted for illness or similar reasons. To obtain an extension please \nemail Jon Ludlam in the first instance, clearly stating your justification for an extension and \nCCing your Director of Studies, who will need to support your request.\n \n\nSCIENTIFIC COMPUTING \n \nAims \nThis course is a hands-on introduction to using computers to investigate scientific models and \ndata. \n \nSyllabus \nPython notebooks. Overview of the Python programming language. Use of notebooks for \nscientific computing. \nNumerical computation. Writing fast vectorized code in numpy. Optimization and fitting. \nSimulation. \nWorking with data. Data import. Common ways to summarize and plot data, for univariate and \nmultivariate analysis. \nObjectives \nAt the end of the course students should \n \nbe able to import data, plot it, and summarize it appropriately \nbe able to write fast vectorized code for scientific / data work\n \n\n",
        "2nd Semester \nALGORITHMS 1 \nAims \nThe aim of this course is to provide an introduction to computer algorithms and data structures, \nwith an emphasis on foundational material. \n \nLectures \nSorting. Review of complexity and O-notation. Trivial sorting algorithms of quadratic complexity. \nReview of merge sort and quicksort, understanding their memory behaviour on statically \nallocated arrays. Heapsort. Stability. Other sorting methods including sorting in linear time. \nMedian and order statistics. [Ref: CLRS3 chapters 1, 2, 3, 6, 7, 8, 9] [about 4 lectures] \nStrategies for algorithm design. Dynamic programming, divide and conquer, greedy algorithms \nand other useful paradigms. [Ref: CLRS3 chapters 4, 15, 16] [about 3 lectures] \nData structures. Elementary data structures: pointers, objects, stacks, queues, lists, trees. \nBinary search trees. Red-black trees. B-trees. Hash tables. Priority queues and heaps. [Ref: \nCLRS3 chapters 6, 10, 11, 12, 13, 18] [about 5 lectures]. \nObjectives \nAt the end of the course students should: \n \nhave a thorough understanding of several classical algorithms and data structures; \nbe able to analyse the space and time efficiency of most algorithms; \nhave a good understanding of how a smart choice of data structures may be used to increase \nthe efficiency of particular algorithms; \nbe able to design new algorithms or modify existing ones for new applications and reason about \nthe efficiency of the result. \nRecommended reading \n* Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest and Clifford Stein Introduction to \nAlgorithms, Fourth Edition ISBN 9780262046305 Published: April 5, 2022. \n \nSedgewick, R., Wayne, K. (2011). Algorithms. Addison-Wesley. ISBN 978-0-321-57351-3. \n \nKleinberg, J. and Tardos, \u00c9. (2006). Algorithm design. Addison-Wesley. ISBN \n978-0-321-29535-4. \n \nKnuth, D.A. (2011). The Art of Computer Programming. Addison-Wesley. ISBN \n978-0-321-75104-1.\n \n\nALGORITHMS 2 \n \nAims \nThe aim of this course is to provide an introduction to computer algorithms and data structures, \nwith an emphasis on foundational material. \n \nLectures \nGraphs and path-finding algorithms. Graph representations. Breadth-first and depth-first search. \nSingle-source shortest paths: Bellman-Ford and Dijkstra algorithms. All-pairs shortest paths: \ndynamic programming and Johnson\u2019s algorithms. [About 4 lectures] \nGraphs and subgraphs. Maximum flow: Ford-Fulkerson method, Max-flow min-cut theorem. \nMatchings in bipartite graphs. Minimum spanning tree: Kruskal and Prim algorithms. Topological \nsort. [About 4 lectures] \nAdvanced data structures. Binomial heap. Amortized analysis: aggregate analysis, potential \nmethod. Fibonacci heaps. Disjoint sets. [About 4 lectures] \nObjectives \nAt the end of the course students should: \n \nhave a thorough understanding of several classical algorithms and data structures; \nbe able to analyse the space and time efficiency of most algorithms; \nhave a good understanding of how a smart choice of data structures may be used to increase \nthe efficiency of particular algorithms; \nbe able to design new algorithms or modify existing ones for new applications and reason about \nthe efficiency of the result. \nRecommended reading \n* Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest and Clifford Stein Introduction to \nAlgorithms, Fourth Edition ISBN 9780262046305 Published: April 5, 2022. \n \nSedgewick, R., Wayne, K. (2011). Algorithms. Addison-Wesley. ISBN 978-0-321-57351-3. \n \nKleinberg, J. and Tardos, \u00c9. (2006). Algorithm design. Addison-Wesley. ISBN \n978-0-321-29535-4. \n \nKnuth, D.A. (2011). The Art of Computer Programming. Addison-Wesley. ISBN \n978-0-321-75104-1.\n \n\nMACHINE LEARNING AND REAL-WORLD DATA \n \nAims \nThis course introduces students to machine learning algorithms as used in real-world \napplications, and to the experimental methodology necessary to perform statistical analysis of \nlarge-scale data from unpredictable processes. Students will perform 3 extended practicals, as \nfollows: \n \nStatistical classification: Determining movie review sentiment using Naive Bayes; \nSequence Analysis: Hidden Markov Modelling and its application to a task from biology \n(predicting protein interactions with a cell membrane); \nAnalysis of social networks, including detection of cliques and central nodes. \nSyllabus \nTopic One: Statistical Classification \nIntroduction to sentiment classification. \nNaive Bayes parameter estimation. \nStatistical laws of language. \nStatistical tests for classification tasks. \nCross-validation and test sets. \nUncertainty and human agreement. \nTopic Two: Sequence Analysis  \nHidden Markov Models (HMM) and HMM training. \nThe Viterbi algorithm. \nUsing an HMM in a biological application. \nTopic Three: Social Networks  \nProperties of networks: Degree, Diameter. \nBetweenness Centrality. \nClustering using betweenness centrality. \nObjectives \nBy the end of the course students should be able to: \n \nunderstand and program two simple supervised machine learning algorithms; \nuse these algorithms in statistically valid experiments, including the design of baselines, \nevaluation metrics, statistical testing of results, and provision against overtraining; \nvisualise the connectivity and centrality in large networks; \nuse clustering (i.e., a type of unsupervised machine learning) for detection of cliques in \nunstructured networks. \nRecommended reading \nJurafsky, D. and Martin, J. (2008). Speech and language processing. Prentice Hall. \n \nEasley, D. and Kleinberg, J. (2010). Networks, crowds, and markets: reasoning about a highly \nconnected world. Cambridge University Press.\n \n\nOPERATING SYSTEMS \n \nAims \nThe overall aim of this course is to provide a general understanding of the structure and key \nfunctions of the operating system. Case studies will be used to illustrate and reinforce \nfundamental concepts. \n \nLectures \nIntroduction to operating systems. Elementary computer organisaton. Abstract view of an \noperating system. Key concepts: layering, multiplexing, performance, caching, buffering, policies \nversus mechanisms. OS evolution: multi-programming, time-sharing. Booting. [1 lecture] \nProtection. Dual-mode operation. Protecting CPU, memory, I/O, storage. Kernels, micro-kernels, \nand modules. System calls. Virtual machines and containers. Subjects and objects. \nAuthentication. Access matrix: ACLs and capabilities. Covert channels. [1 lecture] \nProcesses. Job/process concepts. Process lifecycle. Process management. Context switching. \nInter-process communication. [1 lecture] \nScheduling. CPU-I/O burst cycle. Scheduling basics: preemption and non-preemption. \nScheduling algorithms: FCFS, SJF, SRTF, Priority, and Round Robin. Multilevel scheduling. [2 \nlectures] \nMemory management. Processes in memory. Logical versus physical addresses. Partitions: \nstatic versus dynamic, free space management, external fragmentation. Segmented memory. \nPaged memory: concepts, internal fragmentation, page tables. Demand paging/segmentation. \nPage replacement strategies: FIFO, OPT, and LRU with approximations including NRU, LFU, \nMFU, MRU. Working set schemes. Thrashing. [3 lectures] \nI/O subsystem. General structure. Polled mode versus interrupt-driven I/O. Programmed I/O \n(PIO) versus Direct Memory Access (DMA). Application I/O interface: block and character \ndevices, buffering, blocking, non-blocking, asynchronous, and vectored I/O. Other issues: \ncaching, scheduling, spooling, performance. [1 lecture] \nFile management. File concept. Directory and storage services. File names and metadata. \nDirectory name-space: hierarchies, DAGs, hard and soft links. File operations. Access control. \nExistence and concurrency control. [1 lecture] \nUnix case study. History. General structure. Unix file system: file abstraction, directories, mount \npoints, implementation details. Processes: memory image, lifecycle, start of day. The shell: \nbasic operation, commands, standard I/O, redirection, pipes, signals. Character and block I/O. \nProcess scheduling. [2 lectures] \nObjectives \nAt the end of the course students should be able to: \n \ndescribe the general structure and purpose of an operating system; \nexplain the concepts of process, address space, and file; \ncompare and contrast various CPU scheduling algorithms; \ncompare and contrast mechanisms involved in memory and storage management; \ncompare and contrast polled, interrupt-driven, and DMA-based access to I/O devices. \nRecommended reading \n\n* Silberschatz, A., Galvin, P.C., and Gagne, G. (2018). Operating systems concepts. Wiley (10th \ned.). Prior editions, at least back to 8th ed., should also be acceptable. \nAnderson, T. and Dahlin, M. (2014). Operating Systems: Principles and Practice. Recursive \nBooks (2nd ed.). \nBacon, J. and Harris, T. (2003). Operating systems. Addison-Wesley (3rd ed.). Currently \nappears out-of-print. \nLeffler, S. (1989). The design and implementation of the 4.3BSD Unix operating system. \nAddison-Wesley. \nMcKusick, M.K., Neville-Neil, G.N. and Watson, R.N.M. (2014) The Design and Implementation \nof the FreeBSD Operating System. Pearson Education. (2nd ed.).\n \n\nINTERACTION DESIGN \nAims \nThe aim of this course is to provide an introduction to interaction design, with an emphasis on \nunderstanding and experiencing the user-centred design process, from conducting user \nresearch and requirements development to implementation and evaluation, while understanding \nthe background to human-computer interaction. \n \nLectures \nCourse overview and user research methods. Introduction to the course and practicals. \nUser-Centred Design. User research methods. \nKeeping usera in mind. User research data analysis. Identifying users and stakeholders. \nRepresenting user goals and activities. Identifying and establishing requirements. \nDesign and prototyping. Methods for exploring the design space. Prototyping and different kinds \nof prototypes. \nVisual and interaction design. Memory, perception, attention, and their implications for \ninteraction design. Modalities of interaction, interaction design patterns, information architecture, \nand their implications for interaction design. \nEvaluation. Practical methods for evaluating designs. Evaluation methods without users. \nEvaluation methods with users. \nCase studies from industry and research. Guest lectures (the topics of these lectures are \nsubject to change). \nObjectives \nBy the end of the course students should \n \nhave a thorough understanding of the user-centred design process and be able to apply it to \ninteraction design; \nbe able to design new user interfaces that are informed by principles of good design, and the \nprinciples of human visual perception, cognition and communication; \nbe able to prototype and implement interactive user interfaces with a strong emphasis on users, \nusability and appearance; \nbe able to evaluate existing or new user interfaces using multiple techniques; \nbe able to compare and contrast different design techniques and to critique their applicability to \nnew domains. \nRecommended reading \n* Preece, J., Rogers, Y. and Sharp, H. (2015). Interaction design. Wiley (4th ed.).\n \n\nINTRODUCTION TO PROBABILITY \n \nAims \nThis course provides an elementary introduction to probability and statistics with applications. \nProbability theory and the related field of statistical inference provide the foundations for \nanalysing and making sense of data. The focus of this course is to introduce the language and \ncore concepts of probability theory. The course will also cover some applications of statistical \ninference and algorithms in order to equip students with the ability to represent problems using \nthese concepts and analyse the data within these principles. \n \nLectures \nPart 1 - Introduction to Probability \n \nIntroduction. Counting/Combinatorics (revision), Probability Space, Axioms, Union Bound. \nConditional probability. Conditional Probabilities and Independence, Bayes\u2019Theorem, Partition \nTheorem \nPart 2 - Discrete Random Variables \n \nRandom variables. Definition of a Random Variable, Probability Mass Function, Cumulative \nDistribution, Expectation. \nProbability distributions. Definition and Properties of Expectation, Variance, different ways of \ncomputing them, Examples of important Distributions (Bernoulli, Binomial, Geometric, Poisson), \nPrimer on Continuous Distributions including Normal and Exponential Distributions. \nMultivariate distributions. Multiple Random Variables, Joint and Marginal Distributions, \nIndependence of Random Variables, Covariance. \nPart 3 - Moments and Limit Theorems \n \nIntroduction. Law of Average, Useful inequalities (Markov and Chebyshef), Weak Law of Large \nNumbers (including Proof using Chebyshef\u2019s inequality), Examples. \nMoments and Central Limit Theorem. Introduction to Moments of Random Variables, Central \nLimit Theorem (Proof using Moment Generating functions), Example. \nPart 4 - Applications/Statistics \n \nStatistics. Classical Parameter Estimation (Maximum-Likelihood-Estimation), bias, sample \nmean, sample variance), Examples (Collision-Sampling, Estimating Population Size). \nAlgorithms. Online Algorithms (Secretary Problem, Odd\u2019s Algorithm). \nObjectives \nAt the end of the course students should \n \nunderstand the basic principles of probability spaces and random variables \nbe able to formulate problems using concepts from probability theory and compute or estimate \nprobabilities \nbe familiar with more advanced concepts such as moments, limit theorems and applications \nsuch as parameter estimation \n\nRecommended reading \n* Ross, S.M. (2014). A First course in probability. Pearson (9th ed.). \n \nBertsekas, D.P. and Tsitsiklis, J.N. (2008). Introduction to probability. Athena Scientific. \n \nGrimmett, G. and Welsh, D. (2014). Probability: an Introduction. Oxford University Press (2nd \ned.). \n \nDekking, F.M., et. al. (2005) A modern introduction to probability and statistics. Springer.\n\nSOFTWARE AND SECURITY ENGINEERING \n \nAims \nThis course aims to introduce students to software and security engineering, and in particular to \nthe problems of building large systems, safety-critical systems and systems that must withstand \nattack by capable opponents. Case histories of failure are used to illustrate what can go wrong, \nwhile current software and security engineering practice is studied as a guide to how failures \ncan be avoided. \n \nLectures \n1. What is a security policy or a safety case? Definitions and examples; one-way flows for both \nconfidentiality and safety properties; separation of duties. Top-down and bottom-up analysis \nmethods. What architecture can do, versus benefits of decoupling policy from mechanism. \n \n2. Examples of safety and security policies. Safety and security usability; the pyramid of harms. \nPredicting and mitigating user errors. The prevention of fraud and error in accounting systems; \nthe safety usability of medical devices. \n \n3. Attitudes to risk: expected  utility, prospect theory, framing, status quo bias. Authority, \nconformity and gender; mental models, affordances and defaults. The characteristics of human \nmemory; forgetting passwords versus guessing them. \n \n4. Security protocols; how to enforce policy using  structured human interaction, cryptography or \nboth. Middleperson attacks.The role of verification and its limitations. \n \n5. Attacks on TLS, from rogue CAs through side channels to Heartbleed. Other types of \nsoftware bugs: syntactic, timing, concurrency, code injection, buffer overflows. Defensive \nprogramming: secure coding, contracts. Fuzzing. \n \n6. The software crisis. Examples of large-scale project failure, such as the London Ambulance \nService system and the NHS National Programme for IT. Intrinsic difficulties with complex \nsoftware. \n \n7. Software engineering as the management of complexity. The software life cycle; requirements \nanalysis methods; modular design; the role of prototyping; the waterfall, spiral and agile models. \n \n8. The economics of software as a Service (SaaS); the impact SaaS has on software \nengineering. Continuous integration, release engineering, behavioural analytics and experiment \nframeworks, rearchitecting systems while in operation. \n \n9. Critical systems: safety as an emergent system property. Examples of catastrophic failure: \nfrom Therac-25 to the Boeing 737Max. The problems of managing redundancy. The overall \nprocess of safety engineering. \n \n\n10. Managing the development of critical systems: tools and methods, individual versus group \nproductivity, economics of testing and agile development, measuring outcomes versus process, \nthe technical and human aspects of management, post-market surveillance and coordinated \ndisclosure. The sustainability of products with software components. \n \nAt the end of the course students should know how writing programs with tough assurance \ntargets, in large teams, or both, differs from the programming exercises they have engaged in \nso far. They should understand the different models of software development described in the \ncourse as well as the value of various development and management tools. They should \nunderstand the development life cycle and its basic economics. They should understand the \nvarious types of bugs, vulnerabilities and hazards, how to find them, and how to avoid \nintroducing them. Finally, they should be prepared for the organizational aspects of their Part IB \ngroup project. \n \nRecommended reading \nAnderson, R. (Third Edition 2020). Security engineering (Part 1 and Chapters 27-28). Wiley. \nAvailable at: http://www.cl.cam.ac.uk/users/rja14/book.html\n \n\n",
        "3rd Semester \nCONCURRENT AND DISTRIBUTED SYSTEMS \n \nAims \nThis course considers two closely related topics, Concurrent Systems and Distributed Systems, \nover 16 lectures. The aim of the first half of the course is to introduce concurrency control \nconcepts and their implications for system design and implementation. The aims of the latter \nhalf of the course are to study the fundamental characteristics of distributed systems, including \ntheir models and architectures; the implications for software design; some of the techniques that \nhave been used to build them; and the resulting details of good distributed algorithms and \napplications. \n \nLectures: Concurrency \nIntroduction to concurrency, threads, and mutual exclusion. Introduction to concurrent systems; \nthreads; interleaving; preemption; parallelism; execution orderings; processes and threads; \nkernel vs. user threads; M:N threads; atomicity; mutual exclusion; and mutual exclusion locks \n(mutexes). \nAutomata Composition. Synchronous and asynchronous parallelism; sequential consistency; \nrendezvous. Safety, liveness and deadlock; the Dining Philosophers; Hardware foundations for \natomicity: test-and-set, load-linked/store-conditional and fence instructions. Lamport bakery \nalgorithm. \nCommon design patterns: semaphores, producer-consumer, and MRSW. Locks and invariants; \nsemaphores; condition synchronisation; N-resource allocation; two-party and generalised \nproducer-consumer; Multi-Reader, Single-Write (MRSW) locks. \nCCR, monitors, and concurrency in practice. Conditional critical regions (CCR); monitors; \ncondition variables; signal-wait vs. signal-continue semantics; concurrency in practice (kernels, \npthreads, Java, Cilk, OpenMP). \nDeadlock and liveness guarantees Offline vs. online; model checking; resource allocation \ngraphs; lock order checking; deadlock prevention, avoidance, detection, and recovery; livelock; \npriority inversion; auto parallelisation. \nConcurrency without shared data; transactions. Active objects; message passing; tuple spaces; \nCSP; and actor models. Composite operations; transactions; ACID; isolation; and serialisability. \nFurther transactions History graphs; good and bad schedules; isolation vs. strict isolation; \n2-phase locking; rollback; timestamp ordering (TSO); and optimistic concurrency control (OCC). \nCrash recovery, lock-free programming, and transactional memory. Write-ahead logging, \ncheckpoints, and recovery. Lock-free programming. Hardware and software transactional \nmemories. \n  \n \nLectures: Distributed Systems \nIntroduction to distributed systems; RPC. Avantages and challenges of distributed systems; \nunbounded delay and partial failure; network protocols; transparency; client-server systems; \nremote procedure call (RPC); marshalling; interface definition languages (IDLs). \n\nSystem models and faults. Synchronous, partially synchronous, and asynchronous network \nmodels; crash-stop, crash-recovery, and Byzantine faults; failures, faults, and fault tolerance; \ntwo generals problem. \nTime, clocks, and ordering of events. Physical clocks; leap seconds; UTC; clock synchronisation \nand drift; Network Time Protocol (NTP). Causality; happens-before relation. \nLogical time; Lamport clocks; vector clocks. Broadcast (FIFO, causal, total order); gossip \nprotocols. \nReplication. Quorums; idempotence; replica consistency; read-after-write consistency. State \nmachine replication; leader-based replication. \nConsensus and total order broadcast. FLP result; leader election; the Raft consensus algorithm. \nReplica consistency. Two-phase commit; relationship between 2PC and consensus; \nlinearizability; ABD algorithm; eventual consistency; CAP theorem. \nCase studies. Conflict-free Replicated Data Types (CRDTs); collaborative text editing. Google's \nSpanner; TrueTime. \nObjectives \nAt the end of Concurrent Systems portion of the course, students should: \n \nunderstand the need for concurrency control in operating systems and applications, both mutual \nexclusion and condition synchronisation; \nunderstand how multi-threading can be supported and the implications of different approaches; \nbe familiar with the support offered by various programming languages for concurrency control \nand be able to judge the scope, performance implications and possible applications of the \nvarious approaches; \nbe aware that dynamic resource allocation can lead to deadlock; \nunderstand the concept of transaction; the properties of transactions, how they can be \nimplemented, and how their performance can be optimised based on optimistic assumptions; \nunderstand how the persistence properties of transactions are addressed through logging; and \nhave a high-level understanding of the evolution of software use of concurrency in the \noperating-system kernel case study. \nAt the end of the Distributed Systems portion of the course, students should: \n \nunderstand the difference between shared-memory concurrency and distributed systems; \nunderstand the fundamental properties of distributed systems and their implications for system \ndesign; \nunderstand notions of time, including logical clocks, vector clocks, and physical time \nsynchronisation; \nbe familiar with various approaches to data and service replication, as well as the concept of \nreplica consistency; \nunderstand the effects of large scale on the provision of fundamental services and the tradeoffs \narising from scale; \nappreciate the implications of individual node and network communications failures on \ndistributed computation; \nbe aware of a variety of programming models and abstractions for distributed systems, such as \nRPC, middleware, and total order broadcast; \n\nbe familiar with a range of distributed algorithms, such as consensus, causal broadcast, and \ntwo-phase commit. \nRecommended reading \nModern Operating Systems (free PDF available online) by Andrew S Tanenbaum, Herbert Bos \nJava Concurrency in Practice' (2006 but still highly relevant) by Brian Goetz \nKleppmann, M. (2017). Designing data-intensive applications. O\u2019Reilly. \nTanenbaum, A.S. and van Steen, M. (2017). Distributed systems, 3rd edition. available online. \nCachin, C., Guerraoui, R. and Rodrigues, L. (2011) Introduction to Reliable and Secure \nDistributed Programming. Springer (2nd edition).\n \n\nDATA SCIENCE \n \nAims \nThis course introduces fundamental tools for describing and reasoning about data. There are \ntwo themes: designing probability models to describe systems; and drawing conclusions based \non data generated by such systems. \n \nLectures \nSpecifying and fitting probability models. Random variables. Maximum likelihood estimation. \nGenerative and supervised models. Goodness of fit. \nFeature spaces. Vector spaces, bases, inner products, projection. Linear models. Model fitting \nas projection. Design of features. \nHandling probability models. Handling pdf and cdf. Bayes\u2019s rule. Monte Carlo estimation. \nEmpirical distribution. \nInference. Bayesianism. Frequentist confidence intervals, hypothesis testing. Bootstrap \nresampling. \nRandom processes. Markov chains. Stationarity, and drift analysis. Processes with memory. \nLearning a random process. \nObjectives \nAt the end of the course students should \n \nbe able to formulate basic probabilistic models, including discrete time Markov chains and linear \nmodels \nbe familiar with common random variables and their uses, and with the use of empirical \ndistributions rather than formulae \nunderstand different types of inference about noisy data, including model fitting, hypothesis \ntesting, and making predictions \nunderstand the fundamental properties of inner product spaces and orthonormal systems, and \ntheir application to modelling \nRecommended reading \n* F.M. Dekking, C. Kraaikamp, H.P. Lopuha\u00e4, L.E. Meester (2005). A modern introduction to \nprobability and statistics: understanding why and how. Springer. \n \nS.M. Ross (2002). Probability models for computer science. Harcourt / Academic Press. \n \nM. Mitzenmacher and E. Upfal (2005). Probability and computing: randomized algorithms and \nprobabilistic analysis. Cambridge University Press.\n \n\nECAD AND ARCHITECTURE PRACTICAL CLASSES \nAims \nThe aims of this course are to enable students to apply the concepts learned in the Computer \nDesign course. In particular a web based tutor is used to introduce the SystemVerilog hardware \ndescription language, while the remaining practical classes will then allow students to implement \nthe design of components in this language. \n \nPractical Classes \nWeb tutor The first class uses a web based tutor to rapidly teach the SystemVerilog language. \nFPGA design flow Test driven hardware development for FPGA including an embedded \nprocessor and peripherals [3 classes] \nEmbedded system implementation Embedded system implementation on FPGA [3-4 classes] \nObjectives \nGain experience in electronic computer aided design (ECAD) through learning a design-flow for \nfield programmable gate arrays (FPGAs). \nLearn how to interface to peripherals like a touch screen. \nLearn how to debug hardware and software systems in simulation. \nUnderstand how to construct and program a heterogeneous embedded system. \nRecommended reading \n* Harris, D.M. and Harris, S.L. (2007). Digital design and computer architecture: from gates to \nprocessors. Morgan Kaufmann. \n \nPointers to sources of more specialist information are included on the associated course web \npage. \n \nIntroduction \nThe ECAD and Architecture Laboratory sessions are a companion to the Introduction to \nComputer Architecture course. The objective is to provide experience of hardware design for \nFPGA including use of a small embedded processor. It covers hardware design in \nSystemVerilog, embedded software design in RISC-V assembly and C, and use of FPGA tools. \n \nPrerequisites \nThis course assumes familiarity with the material from Part IA Digital Electronics, although we \nwill use automated tools to perform many of the steps you might have done there by hand (for \nexample, logic minimisation). \n \nWe will be using a Linux command-line environment. We provide scripts and guidance on what \nyou need, however you may find it useful to review Unix Tools, in particular basic navigation \nsuch as ls, cd, mkdir, cp, mv. We'll be using the GCC compiler and Makefiles, described in parts \n31-32. We will also be using git for revision control and submission of work for ticking - you \nshould review at least parts 23-25 and 29. \n \nPracticalities \n\nThe course runs over the 8 weeks of Michaelmas term. The course has been designed so you \ncan do most of the work at home or at any time you wish. You should be able to run all the \nnecessary tools on your own machine, through the use of virtual machines and Docker \ncontainers. We have a number of routes to do this in case some of them aren't suitable for the \nmachine you have. \n \nWe're running the course in a hybrid mode this year. We will provide online help sessions via \nMicrosoft Teams as well as in-person lab sessions on Fridays and Tuesdays 2-5pm during term. \n \nThe core components of the course, being the RISC-V architecture and software and ticked \nexercise, can be done entirely online, without needing access to hardware. They can be \nundertaken with any of the platforms we support, including MCS Linux (on the Intel Lab \nworkstations and remotely) if your own machine isn't suitable. We expect everyone to complete \nthese parts. \n \nThe rest of the course is optional. The hardware simulation and FPGA compilation can be done \non your machine without access to hardware, if you are able to run the virtual machine. \n \nThe parts that require access to an FPGA board will need the ability to run the virtual machine \nand you be able to collect and return an FPGA board from the Department. That process will be \ndescribed when you come to those parts. There is an optional starred tick available for \ncompletion of this part, which will need assessment by a demonstrator in person. \n \nWe'll do our best to support all the different platforms and variations, but please bear with us - \nit's quite possible we'll come across a problem we haven't seen before! \n \nWe have provided four routes you can use different tools to complete the course. Some routes \nnecessarily exclude some material but this material is optional. \n \nAssessment \nIn a similar way to other practical courses, this course carries one 'tick'. Everyone should expect \nto receive this tick, and those who do not should expect to lose marks on their final exams. \n \nThis year we have adopted the 'chime' autoticker used by Further Java. You will check out the \ninitial files as a git repository from the chime server, commit your work, and push the repository \nback to submit it. You can then press a button to run tests against your code and the autoticker \nwill tell you whether you have passed. Additionally you may be selected for a (real/video) \ninterview with a ticker to discuss your code and understanding of the material. \n \nTicking procedures will only be available during weeks 1-8 of Michaelmas term. The deadline for \nsubmitting Tick 1 is 5pm on the Tuesday of week 6 (19 November 2024). After passing the \nautoticker you may be asked for a tick interview with a demonstrator (online or in person). You \ncan gain Tick 1* during the timetabled lab sessions, assuming demonstrators are available. \n \n\nIf you do not submit a successful Tick 1 to the autoticker by 5pm on the Tuesday of week 6 you \ncan self-certify an extension. If you need more time than this you will need to ask your Directory \nof Studies to organise a further extension. The hard deadline by which all ticks must be \ncompleted is noon on Friday 31 January 2025 (the Head of Department Notices is the definitive \ndocument). Extensions beyond this date due to exceptional circumstances require a formal \napplication from your college to the Examiners. \n \nScheduling \nWhile the full course contains 8 exercises, there is not a tight binding to doing one exercise per \nweek. Students should expect to spend about three hours per week on the course, however it is \nrecommended that you make a start on the next exercise if you have time in hand. \n \nDemonstrator help will be focused on the Tuesday and Friday 2-5pm slots during term the lab \nwould normally operate in, so you may find it helpful to work in these times as that will provide \nthe quickest response to questions. \n \nCollaboration \nWhen we have done these labs in person, we have often found they worked well when doing \nthem collaboratively. While your work must be your own, students can work together and help \neach other, and demonstrators contribute advice and experience with the tools. \n \nFor the timetabled sessions, which are Tuesdays and Fridays 2-5pm UK time during term, there \nwill be demonstrators available in the lab and at the same time we'll use the ECAD group on \nMicrosoft Teams. With this we can provide online audio/video help, screensharing and (for the \nWindows and possibly Mac Teams apps) optional remote control of your development \nenvironment. You will be added to the Team in week 1 - if you believe you have been missed \nplease post in the Moodle forum. In Teams there are a number of Helpdesk Channels (A-C) - if \nyou need help during lab times, post in Help Centre and a demonstrator can ask you to go to a \nspecific channel. There are also Student Breakout Channels (D-F) which are available for \nstudents to chat amongst themselves. \n \nFor help outside of timetabled sessions we've set up a Moodle forum where you can post \nquestions - we'll keep an eye on this at other times, but students are encouraged to use it to \nsupport each other. \n \nStudents are of course free to use other platforms to help each other. If you find anything useful \nthat we might use in future, please let us know! \n \nIf you are having difficulty accessing both Moodle and Teams, please email theo.markettos at \ncl.cam.ac.uk. \n \nFeedback \nWe revise the course each year, which may cause new bugs in the code or the notes. The \ncreators (Theo Markettos and Simon Moore) appreciate constructive feedback. \n\n \nCredits \nThis course was written by Theo Markettos and Simon Moore, with portions developed by \nRobert Eady, Jonas Fiala, Paul Fox, Vlad Gavrila, Brian Jones and Roy Spliet. We would like to \nthank Altera, Intel and Terasic for funding and other contributions.\n \n\nECONOMICS, LAW AND ETHICS \n \nAims \nThis course aims to give students an introduction to some basic concepts in economics, law and \nethics. \n \nLectures \nClassical economics and consumer theory. Prices and markets; Pareto efficiency; preferences; \nutility; supply and demand; the marginalist revolution; elasticity; the welfare theorems; \ntransaction costs. \nInformation economics. The discriminating monopolist; marginal costs; effects of technology on \nsupply and demand; competition and information; lock in; real and virtual networks; Metcalfe\u2019s \nlaw; the dominant firm model; price discrimination; bundling; income distribution. \nMarket failure and behavioural economics. Market failure: the business cycle; recession and \ntechnology; tragedy of the commons; externalities; monopoly rents; asymmetric information: the \nmarket for lemons; adverse selection; moral hazard; signalling. Behavioural economics: \nbounded rationality, heuristics and biases; nudge theory; the power of defaults; agency effects. \nAuction theory and game theory. Auction theory: types of auctions; strategic equivalence; the \nrevenue equivalence theorem; the winner\u2019s curse; problems with real auctions; mechanism \ndesign and the combinatorial auction; applicability of auction mechanisms in computer science; \nadvertising auctions. Game theory: the choice between cooperation and conflict; strategic \nforms; dominant strategy equilibrium; Nash equilibrium; the prisoners\u2019 dilemma; evolution of \nstrategies; stag hunt; volunteer\u2019s dilemma; chicken; iterated games; hawk-dove; application to \ncomputer science. \nPrinciples of law. Criminal and civil law; contract law; choice of law and jurisdiction; arbitration; \ntort; negligence; defamation; intellectual property rights. \nLaw and the Internet. Computer evidence; the General Data Protection Regulation; UK laws that \nspecifically affect the Internet; e-commerce regulations; privacy and electronic communications. \nPhilosophies of ethics. Authority, intuitionist, egoist and deontological theories; utilitarian and \nRawlsian models; morality; insights from evolutionary psychology, neurology, and experimental \nethics; professional codes of ethics; research ethics. \nContemporary ethical issues. The Internet and social policy; current debates on privacy, \nsurveillance, and censorship; responsible vulnerability disclosure; algorithmic bias; predictive \npolicing; gamification and engagement; targeted political advertising; environmental impacts. \nObjectives \nOn completion of this course, students should be able to: \n \nReflect on and discuss professional, economic, social, environmental, moral and ethical issues \nrelating to computer science \nDefine and explain economic and legal terminology and arguments \nApply the philosophies and theories covered to computer science problems and scenarios \nReflect on the main constraints that market, legislation and ethics place on firms dealing in \ninformation goods and services \nRecommended reading \n\n* Shapiro, C. & Varian, H. (1998). Information rules. Harvard Business School Press. \nHare, S. (2022). Technology is not neutral: A short guide to technology ethics. London \nPublishing Partnership. \n \nFurther reading: \nSmith, A. (1776). An inquiry into the nature and causes of the wealth of nations, available at    \nhttp://www.econlib.org/library/Smith/smWN.html \nThaler, R.H. (2016). Misbehaving. Penguin. \nGalbraith, J.K. (1991). A history of economics. Penguin. \nPoundstone, W. (1992). Prisoner\u2019s dilemma. Anchor Books. \nPinker, S (2011). The Better Angels of our Nature. Penguin. \nAnderson, R. (2008). Security engineering (Chapter 7). Wiley. \nVarian, H. (1999). Intermediate microeconomics - a modern approach. Norton. \nNuffield Council on Bioethics (2015) The collection, linking and use of data in biomedical \nresearch and health care.\n \n\nFURTHER GRAPHICS \n \nAims \nThe course introduces fundamental concepts of modern graphics pipelines. \n \nLectures \nThe order and content of lectures is provisional and subject to change. \n \nGeometry Representations. Parametric surfaces, implicit surfaces, meshes, point-set surfaces, \ngeometry processing pipeline. [1 lecture] \nDiscrete Differential Geometry. Surface normal and curvature, Laplace-Beltrami operator, heat \ndiffusion. [1 lecture] \nGeometry Processing.  Parametrization, filtering, 3D capture. [1 lecture] \nAnimation I. Animation types, animation pipeline, rigging/skinning, character animation. [1 \nlecture] \nAnimation II. Blending transformations, pose-space animation, controllers. [1 lecture] \nThe Rendering Equation. Radiosity, reflection models, BRDFs, local vs. global illumination. [1 \nlecture] \nDistributed Ray Tracing. Quadrature, importance sampling, recursive ray tracing. [1 lecture] \nInverse Rendering. Differentiable rendering, inverse problems. [1 lecture] \nObjectives \nOn completing the course, students should be able to \n \nunderstand and use fundamental 3D geometry/scene representations and operations; \nlearn animation techniques and controls; \nlearn how light transport is modeled and simulated in rendering; \nunderstand how graphics and rendering can be used to solve computer perception problems. \nRecommended reading \nStudents should expect to refer to one or more of these books, but should not find it necessary \nto purchase any of them. \n \nShirley, P. and Marschner, S. (2009). Fundamentals of Computer Graphics. CRC Press (3rd \ned.). \nWatt, A. (2000). 3D Computer Graphics. Addison-Wesley (3rd ed). \nHughes, van Dam, McGuire, Skalar, Foley, Feiner and Akeley (2013). Computer Graphics: \nPrinciples and Practice. Addison-Wesley (3rd edition) \nAkenine-M\u00f6ller, et. al. (2018). Real-time rendering. CRC Press (4th ed.).\n \n\nFURTHER JAVA \n \nAims \nThe goal of this course is to provide students with the ability to understand the advanced \nprogramming features available in the Java programming language, completing the coverage of \nthe language started in the Programming in Java course. The course is designed to \naccommodate students with diverse programming backgrounds; consequently Java is taught \nfrom first principles in a practical class setting where students can work at their own pace from a \ncourse handbook. \n \nObjectives \n \nAt the end of the course students should \n \nunderstand different mechanisms for communication between distributed applications and be \nable to evaluate their trade-offs; \nbe able to use Java generics and annotations to improve software usability, readability and \nsafety; \nunderstand and be able to exploit the Java class-loading mechansim; \nunderstand and be able to use concurrency control correctly; \nbe able to implement a vector clock algorithm and the happens-before relation. \nRecommended reading \n* Goetz, B. (2006). Java concurrency in practice. Addison-Wesley. \nGosling, J., Joy, B., Steele, G., Bracha, G. and Buckley, A. (2014). The Java language \nspecification, Java SE 8 Edition. Addison-Wesley. \nhttp://docs.oracle.com/javase/specs/jls/se8/html/\n \n\nGROUP PROJECTS \nExample Risk Report \nThe risk report should focus on the risks to your project succeeding on time (and not, say, on \ngeneral health and safety points). It is only 50 words. Example: \n \n\"Identified risks: \n * Training data requires access authorisation that has not been approved, and we don\u2019t know if \nclient can do this \n * Two group members have unreliable network connections and may miss meetings \n * One group member is in timezone UTC+8, and will have to attend meetings late at night \n * Bug reports on Github for a library we plan to use suggest maintenance updates will be \nnecessary for recent Android release\"\n \n\nINTRODUCTION TO COMPUTER ARCHITECTURE \n \nAims \nThe aims of this course are to introduce a hardware description language (SystemVerilog) and \ncomputer architecture concepts in order to design computer systems. The parallel ECAD+Arch \npractical classes will allow students to apply the concepts taught in lectures. \n \nLectures \nPart 1 - Gates to processors  \n \nTechnology trends and design challenges. Current technology, technology trends, ECAD trends, \nchallenges. [1 lecture] \nDigital system design. Practicalities of mapping SystemVerilog descriptions of hardware \n(including a processor) onto an FPGA board. Tips and pitfalls when generating larger modular \ndesigns. [1 lecture] \nEight great ideas in computer architecture. [1 lecture] \nReduced instruction set computers and RISC-V. Introduction to the RISC-V processor design. [1 \nlecture] \nExecutable and synthesisable models. [1 lecture] \nPipelining. [2 lectures] \nMemory hierarchy and caching. Caching, etc. [1 lecture] \nSupport for operating systems. Memory protection, exceptions, interrupts, etc. [1 lecture] \nOther instruction set architectures. CISC, stack, accumulator. [1 lecture] \nPart 2  \n \nOverview of Systems-on-Chip (SoCs) and DRAM. [1 lecture] High-level SoCs, DRAM storage \nand accessing. \nMulticore Processors. [2 lectures] Communication, cache coherence, barriers and \nsynchronisation primitives. \nGraphics processing units (GPUs) [2 lectures] Basic GPU architecture and programming. \nFuture Directions [1 lecture] Where is computer architecture heading? \nObjectives \nAt the end of the course students should \n \nbe able to read assembler given a guide to the instruction set and be able to write short pieces \nof assembler if given an instruction set or asked to invent an instruction set; \nunderstand the differences between RISC and CISC assembler; \nunderstand what facilities a processor provides to support operating systems, from memory \nmanagement to software interrupts; \nunderstand memory hierarchy including different cache structures and coherency needed for \nmulticore systems; \nunderstand how to implement a processor in SystemVerilog; \nappreciate the use of pipelining in processor design; \nhave an appreciation of control structures used in processor design; \n\nhave an appreciation of system-on-chips and their components; \nunderstand how DRAM stores data; \nunderstand how multicore processors communicate; \nunderstand how GPUs work and have an appreciation of how to program them. \nRecommended reading \n* Patterson, D.A. and Hennessy, J.L. (2017). Computer organization and design: The \nhardware/software interface RISC-V edition. Morgan Kaufmann. ISBN 978-0-12-812275-4. \n \nRecommended further reading: \n \nHarris, D.M. and Harris, S.L. (2012). Digital design and computer architecture. Morgan \nKaufmann. ISBN 978-0-12-394424-5. \nHennessy, J. and Patterson, D. (2006). Computer architecture: a quantitative approach. Elsevier \n(4th ed.). ISBN 978-0-12-370490-0. (Older versions of the book are also still generally relevant.) \n \nPointers to sources of more specialist information are included in the lecture notes and on the \nassociated course web page\n \n\nPROGRAMMING IN C AND C++ \n \nAims \nThis course aims \n \nto provide a solid introduction to programming in C and to provide an overview of the principles \nand constraints that affect the way in which the C programming language has been designed \nand is used; \nto introduce the key additional features of C++. \n  \n \nLectures \nIntroduction to the C language. Background and goals of C. Types, expressions, control flow \nand strings. \n(continued) Functions. Multiple compilation units. Scope. Segments. Incremental compilation. \nPreprocessor. \n(continued) Pointers and pointer arithmetic. Function pointers. Structures and Unions. \n(continued) Const and volatile qualifiers. Typedefs. Standard input/output. Heap allocation. \nMiscellaneous Features, Hints and Tips. \nC semantics and tools. Undefined vs implementation-defined behaviour. Buffer and integer \noverflows. ASan, MSan, UBsan, Valgrind checkers. \nMemory allocation, data structures and aliasing. Malloc/free, tree and DAG examples and their \ndeallocation using a memory arena. \nFurther memory management. Reference Counting and Garbage Collection \nMemory hierarchy and cache optimization. Data structure layouts. Intrusive lists. Array-of-structs \nvs struct-of-arrays representations. Loop blocking. \nDebugging. Using print statements, assertions and an interactive debugger. Unit testing and \nregression. \nIntroduction to C++. Goals of C++. Differences between C and C++. References versus \npointers. Overloaded functions. \nObjects in C++. Classes and structs. Destructors. Resource Acquisition is Initialisation. Operator \noverloading. Virtual functions. Casts. Multiple inheritance. Virtual base classes. \nOther C++ concepts. Templates and meta-programming. \nObjectives \nAt the end of the course students should \n \nbe able to read and write C programs; \nunderstand the interaction between C programs and the host operating system; \nbe familiar with the structure of C program execution in machine memory; \nunderstand the potential dangers of writing programs in C; \nunderstand the object model and main additional features of C++. \nRecommended reading \n* Kernighan, B.W. and Ritchie, D.M. (1988). The C programming language. Prentice Hall (2nd \ned.).\n \n\nSEMANTICS OF PROGRAMMING LANGUAGES \n \nAims \nThe aim of this course is to introduce the structural, operational approach to programming \nlanguage semantics. It will show how to specify the meaning of typical programming language \nconstructs, in the context of language design, and how to reason formally about semantic \nproperties of programs. \n \nLectures \nIntroduction. Transition systems. The idea of structural operational semantics. Transition \nsemantics of a simple imperative language. Language design options. [2 lectures] \nTypes. Introduction to formal type systems. Typing for the simple imperative language. \nStatements of desirable properties. [2 lectures] \nInduction. Review of mathematical induction. Abstract syntax trees and structural induction. \nRule-based inductive definitions and proofs. Proofs of type safety properties. [2 lectures] \nFunctions. Call-by-name and call-by-value function application, semantics and typing. Local \nrecursive definitions. [2 lectures] \nData. Semantics and typing for products, sums, records, references. [1 lecture] \nSubtyping. Record subtyping and simple object encoding. [1 lecture] \nSemantic equivalence. Semantic equivalence of phrases in a simple imperative language, \nincluding the congruence property. Examples of equivalence and non-equivalence. [1 lecture] \nConcurrency. Shared variable interleaving. Semantics for simple mutexes; a serializability \nproperty. [1 lecture] \nObjectives \nAt the end of the course students should \n \nbe familiar with rule-based presentations of the operational semantics and type systems for \nsome simple imperative, functional and interactive program constructs; \nbe able to prove properties of an operational semantics using various forms of induction \n(mathematical, structural, and rule-based); \nbe familiar with some operationally-based notions of semantic equivalence of program phrases \nand their basic properties. \nRecommended reading \n* Pierce, B.C. (2002). Types and programming languages. MIT Press. \nHennessy, M. (1990). The semantics of programming languages. Wiley. Out of print, but \navailable on the web at \nhttp://www.cs.tcd.ie/matthew.hennessy/splexternal2015/resources/sembookWiley.pdf \nWinskel, G. (1993). The formal semantics of programming languages. MIT Press.\n \n\nUNIX TOOLS \n \nAims \nThis video-lecture course gives students who have already basic Unix/Linux experience some \nadditional practical software-engineering knowledge: how to use the shell and related tools as \nan efficient working environment, how to automate routine tasks, and how version-control and \nautomated-build tools can help to avoid confusion and accidents, especially when working in \nteams. These are essential skills, both in industrial software development and student projects. \n \nLectures \nUnix concepts. Brief review of Unix history and design philosophy, documentation, terminals, \ninter-process communication mechanisms and conventions, shell, command-line arguments, \nenvironment variables, file descriptors. \nShell concepts. Program invocation, redirection, pipes, file-system navigation, argument \nexpansion, quoting, job control, signals, process groups, variables, locale, history and alias \nfunctions, security considerations. \nScripting. Plain-text formats, executables, #!, shell control structures and functions. Startup \nscripts. \nText, file and networking tools. sed, grep, chmod, find, ssh, rsync, tar, zip, etc. \nVersion control. diff, patch, RCS, Subversion, git. \nSoftware development tools. C compiler, linker, debugger, make. \nPerl. Introduction to a powerful scripting and text-manipulation language. [2 lectures] \nObjectives \nAt the end of the course students should \n \nbe confident in performing routine user tasks on a POSIX system, understand command-line \nuser-interface conventions and know how to find more detailed documentation; \nappreciate how simple tools can be combined to perform a large variety of tasks; \nbe familiar with the most common tools, file formats and configuration practices; \nbe able to understand, write, and maintain shell scripts and makefiles; \nappreciate how using a version-control system and fully automated build processes help to \nmaintain reproducibility and audit trails during software development; \nknow enough about basic development tools to be able to install, modify and debug C source \ncode; \nhave understood the main concepts of, and gained initial experience in, writing Perl scripts \n(excluding the facilities for object-oriented programming). \nRecommended reading \nRobbins, A. (2005). Unix in a nutshell. O\u2019Reilly (4th ed.). \nSchwartz, R.L., Foy, B.D. and Phoenix, T. (2011). Learning Perl. O\u2019Reilly (6th ed.).\n \n\n",
        "4th Semester \n \nCOMPILER CONSTRUCTION \n \nAims \nThis course aims to cover the main concepts associated with implementing compilers for \nprogramming languages. We use a running example called SLANG (a Small LANGuage) \ninspired by the languages described in 1B Semantics. A toy compiler (written in ML) is provided, \nand students are encouraged to extend it in various ways. \n \nLectures \nOverview of compiler structure The spectrum of interpreters and compilers; compile-time and \nrun-time. Compilation as a sequence of translations from higher-level to lower-level intermediate \nlanguages, where each translation preserves semantics. The structure of a simple compiler: \nlexical analysis and syntax analysis, type checking, intermediate representations, optimisations, \ncode generation. Overview of run-time data structures: stack and heap. Virtual machines. [1 \nlecture] \nLexical analysis and syntax analysis. Lexical analysis based on regular expressions and finite \nstate automata. Using LEX-tools. How does LEX work? Parsing based on context-free \ngrammars and push-down automata. Grammar ambiguity, left- and right-associativity and \noperator precedence. Using YACC-like tools. How does YACC work? LL(k) and LR(k) parsing \ntheory. [3 lectures] \nCompiler Correctness Recursive functions can be transformed into iterative functions using the \nContinuation-Passing Style (CPS) transformation. CPS applied to a (recursive) SLANG \ninterpreter to derive, in a step-by-step manner, a correct stack-based compiler. [3 lectures] \nData structures, procedures/functions Representing tuples, arrays, references. Procedures and \nfunctions: calling conventions, nested structure, non-local variables. Functions as first-class \nvalues represented as closures. Simple optimisations: inline expansion, constant folding, \nelimination of tail recursion, peephole optimisation. [5 lectures] \nAdvanced topics Run-time memory management (garbage collection). Static and dynamic \nlinking. Objects and inheritance; implementation of method dispatch. Try-catch exception \nmechanisms. Compiling a compiler via bootstrapping. [4 lectures] \nObjectives \nAt the end of the course students should understand the overall structure of a compiler, and will \nknow significant details of a number of important techniques commonly used. They will be \naware of the way in which language features raise challenges for compiler builders. \n \nRecommended reading \n* Aho, A.V., Sethi, R. and Ullman, J.D. (2007). Compilers: principles, techniques and tools. \nAddison-Wesley (2nd ed.). \nMogensen, T. \u00c6. (2011). Introduction to compiler design. Springer. http://www.diku.dk/ \ntorbenm/Basics.\n \n\nCOMPUTATION THEORY \n \nAims \nThe aim of this course is to introduce several apparently different formalisations of the informal \nnotion of algorithm; to show that they are equivalent; and to use them to demonstrate that there \nare uncomputable functions and algorithmically undecidable problems. \n \nLectures \nIntroduction: algorithmically undecidable problems. Decision problems. The informal notion of \nalgorithm, or effective procedure. Examples of algorithmically undecidable problems. [1 lecture] \nRegister machines. Definition and examples; graphical notation. Register machine computable \nfunctions. Doing arithmetic with register machines. [1 lecture] \nUniversal register machine. Natural number encoding of pairs and lists. Coding register machine \nprograms as numbers. Specification and implementation of a universal register machine. [2 \nlectures] \nUndecidability of the halting problem. Statement and proof. Example of an uncomputable partial \nfunction. Decidable sets of numbers; examples of undecidable sets of numbers. [1 lecture] \nTuring machines. Informal description. Definition and examples. Turing computable functions. \nEquivalence of register machine computability and Turing computability. The Church-Turing \nThesis. [2 lectures] \nPrimitive and partial recursive functions. Definition and examples. Existence of a recursive, but \nnot primitive recursive function. A partial function is partial recursive if and only if it is \ncomputable. [2 lectures] \nLambda-Calculus. Alpha and beta conversion. Normalization. Encoding data. Writing recursive \nfunctions in the lambda-calculus. The relationship between computable functions and \nlambda-definable functions. [3 lectures] \nObjectives \nAt the end of the course students should \n \nbe familiar with the register machine, Turing machine and lambda-calculus models of \ncomputability; \nunderstand the notion of coding programs as data, and of a universal machine; \nbe able to use diagonalisation to prove the undecidability of the Halting Problem; \nunderstand the mathematical notion of partial recursive function and its relationship to \ncomputability. \nRecommended reading \n* Hopcroft, J.E., Motwani, R. and Ullman, J.D. (2001). Introduction to automata theory, \nlanguages, and computation. Addison-Wesley (2nd ed.). \n* Hindley, J.R. and Seldin, J.P. (2008). Lambda-calculus and combinators, an introduction. \nCambridge University Press (2nd ed.). \nCutland, N.J. (1980). Computability: an introduction to recursive function theory. Cambridge \nUniversity Press. \nDavis, M.D., Sigal, R. and Weyuker, E.J. (1994). Computability, complexity and languages. \nAcademic Press (2nd ed.). \n\nSudkamp, T.A. (2005). Languages and machines. Addison-Wesley (3rd ed.).\n \n\nCOMPUTER NETWORKING \n \nAims \nThe aim of this course is to introduce key concepts and principles of computer networks. The \ncourse will use a top-down approach to study the Internet and its protocol stack. Instances of \narchitecture, protocol, application-examples will include email, web and media-streaming. We \nwill cover communications services (e.g., TCP/IP) required to support such network \napplications. The implementation and deployment of communications services in practical \nnetworks: including wired and wireless LAN environments, will be followed by a discussion of \nissues of network-management. Throughout the course, the Internet\u2019s architecture and \nprotocols will be used as the primary examples to illustrate the fundamental principles of \ncomputer networking. \n \nLectures \nIntroduction. Overview of networking using the Internet as an example. LANs and WANs. OSI \nreference model, Internet TCP/IP Protocol Stack. Circuit-switching, packet-switching, Internet \nstructure, networking delays and packet loss. [3 lectures] \nLink layer and local area networks. Link layer services, error detection and correction, Multiple \nAccess Protocols, link layer addressing, Ethernet, hubs and switches, Point-to-Point Protocol. [2 \nlectures] \nWireless and mobile networks. Wireless links and network characteristics, Wi-Fi: IEEE 802.11 \nwireless LANs. [1 lecture] \nNetwork layer addressing. Network layer services, IP, IP addressing, IPv4, DHCP, NAT, ICMP, \nIPv6. [3 lectures] \nNetwork layer routing. Routing and forwarding, routing algorithms, routing in the Internet, \nmulticast. [3 lectures] \nTransport layer. Service models, multiplexing/demultiplexing, connection-less transport (UDP), \nprinciples of reliable data transfer, connection-oriented transport (TCP), TCP congestion control, \nTCP variants. [6 lectures] \nApplication layer. Client/server paradigm, WWW, HTTP, Domain Name System, P2P. [1.5 \nlectures] \nMultimedia networking. Networked multimedia applications, multimedia delivery requirements, \nmultimedia protocols (SIP), content distribution networks. [0.5 lecture] \nObjectives \nAt the end of the course students should \n \nbe able to analyse a communication system by separating out the different functions provided \nby the network; \nunderstand that there are fundamental limits to any communications system; \nunderstand the general principles behind multiplexing, addressing, routing, reliable transmission \nand other stateful protocols as well as specific examples of each; \nunderstand what FEC is; \nbe able to compare communications systems in how they solve similar problems; \n\nhave an informed view of both the internal workings of the Internet and of a number of common \nInternet applications and protocols. \nRecommended reading \n* Peterson, L.L. and Davie, B.S. (2011). Computer networks: a systems approach. Morgan \nKaufmann (5th ed.). ISBN 9780123850591 \nKurose, J.F. and Ross, K.W. (2009). Computer networking: a top-down approach. \nAddison-Wesley (5th ed.). \nComer, D. and Stevens, D. (2005). Internetworking with TCP-IP, vol. 1 and 2. Prentice Hall (5th \ned.). \nStevens, W.R., Fenner, B. and Rudoff, A.M. (2003). UNIX network programming, Vol.I: The \nsockets networking API. Prentice Hall (3rd ed.).\n \n\nFURTHER HUMAN\u2013COMPUTER INTERACTION \n \nAims \nThis aim of this course is to provide an introduction to the theoretical foundations of Human \nComputer Interaction, and an understanding of how these can be applied to the design of \ncomplex technologies. \n \nLectures \nTheory driven approaches to HCI. What is a theory in HCI? Why take a theory driven approach \nto HCI? \nDesign of visual displays. Segmentation and variables of the display plane. Modes of \ncorrespondence. \nGoal-oriented interaction. Using cognitive theories of planning, learning and understanding to \nunderstand user behaviour, and what they find hard. \nDesigning smart systems. Using statistical methods to anticipate user needs and actions with \nBayesian strategies. \nDesigning efficient systems. Measuring and optimising human performance through quantitative \nexperimental methods. \nDesigning meaningful systems. Qualitative research methods to understand social context and \nrequirements of user experience. \nEvaluating interactive system designs. Approaches to evaluation in systems research and \nengineering, including Part II Projects. \nDesigning complex systems. Worked case studies of applying the theories to a hard HCI \nproblem. Research directions in HCI. \nObjectives \nAt the end of the course students should be able to apply theories of human performance and \ncognition to system design, including selection of appropriate techniques to analyse, observe \nand improve the usability of a wide range of technologies. \n \nRecommended reading \n* Preece, J., Sharp, H. and Rogers, Y. (2015). Interaction design: beyond human-computer \ninteraction. Wiley (Currently in 4th edition, but earlier editions will suffice). \n \nFurther reading: \n \nCarroll, J.M. (ed.) (2003). HCI models, theories and frameworks: toward a multi-disciplinary \nscience. Morgan Kaufmann.\n \n\nLOGIC AND PROOF \n \nAims \nThis course will teach logic, especially the predicate calculus. It will present the basic principles \nand definitions, then describe a variety of different formalisms and algorithms that can be used \nto solve problems in logic. Putting logic into the context of Computer Science, the course will \nshow how the programming language Prolog arises from the automatic proof method known as \nresolution. It will introduce topics that are important in mechanical verification, such as binary \ndecision diagrams (BDDs), SAT solvers and modal logic. \n \nLectures \nIntroduction to logic. Schematic statements. Interpretations and validity. Logical consequence. \nInference. \nPropositional logic. Basic syntax and semantics. Equivalences. Normal forms. Tautology \nchecking using CNF. \nThe sequent calculus. A simple (Hilbert-style) proof system. Natural deduction systems. \nSequent calculus rules. Sample proofs. \nFirst order logic. Basic syntax. Quantifiers. Semantics (truth definition). \nFormal reasoning in FOL. Free versus bound variables. Substitution. Equivalences for \nquantifiers. Sequent calculus rules. Examples. \nClausal proof methods. Clause form. A SAT-solving procedure. The resolution rule. Examples. \nRefinements. \nSkolem functions, Unification and Herbrand\u2019s theorem. Prenex normal form. Skolemisation. \nMost general unifiers. A unification algorithm. Herbrand models and their properties. \nResolution theorem-proving and Prolog. Binary resolution. Factorisation. Example of Prolog \nexecution. Proof by model elimination. \nSatisfiability Modulo Theories. Decision problems and procedures. How SMT solvers work. \nBinary decision diagrams. General concepts. Fast canonical form algorithm. Optimisations. \nApplications. \nModal logics. Possible worlds semantics. Truth and validity. A Hilbert-style proof system. \nSequent calculus rules. \nTableaux methods. Simplifying the sequent calculus. Examples. Adding unification. \nSkolemisation. The world\u2019s smallest theorem prover? \nObjectives \nAt the end of the course students should \n \nbe able to manipulate logical formulas accurately; \nbe able to perform proofs using the presented formal calculi; \nbe able to construct a small BDD; \nunderstand the relationships among the various calculi, e.g. SAT solving, resolution and Prolog; \nunderstand the concept of a decision procedure and the basic principles of \u201csatisfiability modulo \ntheories\u201d. \nbe able to apply the unification algorithm and to describe its uses. \nRecommended reading \n\n* Huth, M. and Ryan, M. (2004). Logic in computer science: modelling and reasoning about \nsystems. Cambridge University Press (2nd ed.). \nBen-Ari, M. (2001). Mathematical logic for computer science. Springer (2nd ed.).\n \n\nPROLOG \n \nAims \nThe aim of this course is to introduce programming in the Prolog language. Prolog encourages \na different programming style to Java or ML and particular focus is placed on programming to \nsolve real problems that are suited to this style. Practical experimentation with the language is \nstrongly encouraged. \n \nLectures \nIntroduction to Prolog. The structure of a Prolog program and how to use the Prolog interpreter. \nUnification. Some simple programs. \nArithmetic and lists. Prolog\u2019s support for evaluating arithmetic expressions and lists. The space \ncomplexity of program evaluation discussed with reference to last-call optimisation. \nBacktracking, cut, and negation. The cut operator for controlling backtracking. Negation as \nfailure and its uses. \nSearch and cut. Prolog\u2019s search method for solving problems. Graph searching exploiting \nProlog\u2019s built-in search mechanisms. \nDifference structures. Difference lists: introduction and application to example programs. \nBuilding on Prolog. How particular limitations of Prolog programs can be addressed by \ntechniques such as Constraint Logic Programming (CLP) and tabled resolution. \nObjectives \nAt the end of the course students should \n \nbe able to write programs in Prolog using techniques such as accumulators and difference \nstructures; \nknow how to model the backtracking behaviour of program execution; \nappreciate the unique perspective Prolog gives to problem solving and algorithm design; \nunderstand how larger programs can be created using the basic programming techniques used \nin this course. \nRecommended reading \n* Bratko, I. (2001). PROLOG programming for artificial intelligence. Addison-Wesley (3rd or 4th \ned.). \nSterling, L. and Shapiro, E. (1994). The art of Prolog. MIT Press (2nd ed.). \n \nFurther reading: \n \nO\u2019Keefe, R. (1990). The craft of Prolog. MIT Press. [This book is beyond the scope of this \ncourse, but it is very instructive. If you understand its contents, you\u2019re more than prepared for \nthe examination.]\n \n\nARTIFICIAL INTELLIGENCE \n \nPrerequisites: Algorithms 1, Algorithms 2. In addition the course requires some mathematics, in \nparticular some use of vectors and some calculus. Part IA Natural Sciences Mathematics or \nequivalent and Discrete Mathematics are likely to be helpful although not essential. Similarly, \nelements of Machine Learning and Real World Data, Foundations of Data Science, Logic and \nProof, Prolog and Complexity Theory are likely to be useful. This course is a prerequisite for the \nPart II courses Machine Learning and Bayesian Inference and Natural Language Processing. \nThis course is a prerequisite for: Natural Language Processing \nExam: Paper 7 Question 1, 2 \nPast exam questions, Moodle, timetable \n \nAims \nThe aim of this course is to provide an introduction to some fundamental issues and algorithms \nin artificial intelligence (AI). The course approaches AI from an algorithmic, computer \nscience-centric perspective; relatively little reference is made to the complementary \nperspectives developed within psychology, neuroscience or elsewhere. The course aims to \nprovide some fundamental tools and algorithms required to produce AI systems able to exhibit \nlimited human-like abilities, particularly in the form of problem solving by search, game-playing, \nrepresenting and reasoning with knowledge, planning, and learning. \n \nLectures \nIntroduction. Alternate ways of thinking about AI. Agents as a unifying view of AI systems. [1 \nlecture] \nSearch I. Search as a fundamental paradigm for intelligent problem-solving. Simple, uninformed \nsearch algorithms. Tree search and graph search. [1 lecture] \nSearch II. More sophisticated heuristic search algorithms. The A* algorithm and its properties. \nImproving memory efficiency: the IDA* and recursive best first search algorithms. Local search \nand gradient descent. [1 lecture] \nGame-playing. Search in an adversarial environment. The minimax algorithm and its \nshortcomings. Improving minimax using alpha-beta pruning. [1 lecture] \nConstraint satisfaction problems (CSPs). Standardising search problems to a common format. \nThe backtracking algorithm for CSPs. Heuristics for improving the search for a solution. Forward \nchecking, constraint propagation and arc consistency. [1 lecture] \nBackjumping in CSPs. Backtracking, backjumping using Gaschnig\u2019s algorithm, graph-based \nbackjumping. [1 lecture] \nKnowledge representation and reasoning I. How can we represent and deal with commonsense \nknowledge and other forms of knowledge? Semantic networks, frames and rules. How can we \nuse inference in conjunction with a knowledge representation scheme to perform reasoning \nabout the world and thereby to solve problems? Inheritance, forward and backward chaining. [1 \nlecture] \nKnowledge representation and reasoning II. Knowledge representation and reasoning using first \norder logic. The frame, qualification and ramification problems. The situation calculus. [1 lecture] \n\nPlanning I. Methods for planning in advance how to solve a problem. The STRIPS language. \nAchieving preconditions, backtracking and fixing threats by promotion or demotion: the \npartial-order planning algorithm. [1 lecture] \nPlanning II. Incorporating heuristics into partial-order planning. Planning graphs. The \nGRAPHPLAN algorithm. Planning using propositional logic. Planning as a constraint satisfaction \nproblem. [1 lecture] \nNeural Networks I. A brief introduction to supervised learning from examples. Learning as fitting \na curve to data. The perceptron. Learning by gradient descent. [1 lecture] \nNeural Networks II. Multilayer perceptrons and the backpropagation algorithm. [1 lecture] \nObjectives \nAt the end of the course students should: \n \nappreciate the distinction between the popular view of the field and the actual research results; \nappreciate the fact that the computational complexity of most AI problems requires us regularly \nto deal with approximate techniques; \nbe able to design basic problem solving methods based on AI-based search, knowledge \nrepresentation, reasoning, planning, and learning algorithms. \nRecommended reading \nThe recommended text is: \n \n* Russell, S. and Norvig, P. (2010). Artificial intelligence: a modern approach. Prentice Hall (3rd \ned.). \n \nThere are many good books available on artificial intelligence; one alternative is: \n \nPoole, D. L. and Mackworth, A. K. (2017). Artificial intelligence: foundations of computational \nagents. Cambridge University Press (2nd ed.). \n \nFor some of the material you might find it useful to consult more specialised texts, in particular: \n \nDechter, R. (2003). Constraint processing. Morgan Kaufmann. \n \nCawsey, A. (1998). The essence of artificial intelligence. Prentice Hall. \n \nGhallab, M., Nau, D. and Traverso, P. (2004). Automated planning: theory and practice. Morgan \nKaufmann. \n \nBishop, C.M. (2006). Pattern recognition and machine learning. Springer. \n \nBrachman, R.J and Levesque, H.J. (2004). Knowledge Representation and Reasoning. Morgan \nKaufmann.\n \n\nCOMPLEXITY THEORY \n \nAims \nThe aim of the course is to introduce the theory of computational complexity. The course will \nexplain measures of the complexity of problems and of algorithms, based on time and space \nused on abstract models. Important complexity classes will be defined, and the notion of \ncompleteness established through a thorough study of NP-completeness. Applications to \ncryptography will be considered. \n \nSyllabus \nIntroduction to Complexity Theory. Decidability and complexity; lower and upper bounds; the \nChurch-Turing hypothesis. \nTime complexity. Time complexity classes; polynomial time computation and the class P. \nNon-determinism. Non-deterministic machines; the class NP; the problem 3SAT. \nNP-completeness. Reductions and completeness; the Cook-Levin theorem; graph-theoretic \nproblems. \nMore NP-complete problems. 3-colourability; scheduling, matching, set covering, and knapsack.  \ncoNP. Complement classes. NP \u2229 coNP; primality and factorisation. \nCryptography. One-way functions; the class UP. \nSpace complexity. Space complexity classes; Savitch\u2019s theorem. \nHierarchy theorems. Time and space hierarchy theorems. \nRandomised algorithms. The class BPP; derandomisation; error-correcting codes, \ncommunication complexity. \nQuantum Complexity. The classes BQP and QMA. \nInteractive Proofs. IP=PSPACE; Zero Knowledge Proofs; Probabilistically Checkable Proofs \n(PCPs).  \n  \nObjectives \nAt the end of the course students should \n \nbe able to analyse practical problems and classify them according to their complexity; \nbe familiar with the phenomenon of NP-completeness, and be able to identify problems that are \nNP-complete; \nbe aware of a variety of complexity classes and their interrelationships; \nunderstand the role of complexity analysis in cryptography. \nRecommended reading \nA Survey of P vs. NP by Scott Aaronson. \nComputational Complexity: A Modern Approach by Arora and Barak \nComputational Complexity: A Conceptual Perspective by Oded Goldreich \nMathematics and Computation by Avi Wigderson\n \n\nCYBERSECURITY \n \nAims \nIn today\u2019s digital society, computer security is vital for commercial competitiveness, national \nsecurity and privacy of individuals. Protection against both criminal and state-sponsored attacks \nwill need a large cohort of skilled individuals with an understanding of the principles of security \nand with practical experience of the application of these principles. We want you to become one \nof them. In this adversarial game, to defeat the bad guys, the good guys have to be at least as \nskilled at them. Therefore this course has a strong foundation of becoming proficient at actual \nattacks. A theoretical understanding is of course necessary, but without some practice the bad \nguys will run circles around the good guys. In 12 hours I can\u2019t bring you from zero to the stage \nwhere you can invent new attacks and countermeasures, but at least by practicing and \ndissecting common attacks (akin to \u201cstudying the classics\u201d) I\u2019ll give you a feeling for the kind of \nadversarial thinking that is required in this field. Large portions of this course are hands-on: you \nwill need to acquire the relevant skills rather than just reading about it. The recommended \ncourse textbook will be especially helpful to those with no prior low-level hacking experience. \n \nLectures \nIntroduction. \nThe adversarial nature of security; thinking like the attacker; confidentiality, integrity and \navailability; systems-level thinking; Trusted Computing Base; role of cryptography. \n \nFundamentals of access control (chapter 1). \nDiscretionary vs mandatory access control; discretionary access control in POSIX; file \npermissions, file ownership, groups, permission bits. \n \nSoftware security (spanning 4 book chapters) \nSetuid (chapter 2): privileged programs, attack surfaces, exploitable vulnerabilities, privilege \nescalation from regular user to root. \nBuffer overflow (chapter 4): stack memory layout, exploiting a buffer overflow vulnerability, \nguessing unknown addresses, payload, countermeasures and counter-countermeasures. \nReturn to libc and return-oriented programming (chapter 5): exploiting a non-executable stack, \nchaining function calls, chaining ROP gadgets. \nSQL injection (chapter 14): vulnerability and its exploitation, countermeasures, input filtering, \nprepared statements. \n \nAuthentication. \nSomething you know, have, are; passwords, their dominance, their shortcomings and the many \nattempts at replacing them; password cracking; tokens; biometrics; taxonomy of Single Sign-On \nsystems; password managers. \n \nWeb and internet security (spanning 3 book chapters). \nCross-Site Request Forgery (chapter 12): why cross-site requests, CSRF attacks on HTTP GET \nand HTTP POST, countermeasures. \n\nCross-Site Scripting (chapter 13): non-persistent vs persistent XSS, javascript, self-propagating \nXSS worm, countermeasures; \n \nHuman factors. \nUsers are not the enemy; Compliance budget; Prospect theory; 7 principles for systems \nsecurity. \n \nAdditional topics. \nViruses, worms and quines; lockpicking; privilege escalation in physical locks; conclusions. \n \nTo complete the course successfully, students must acquire the practical ability to carry out (as \nopposed to just describing) the exploits mentioned in the syllabus, given a vulnerable system. \nThe low-level hands-on portions of the course are supported by the very helpful course \ntextbook. Those who choose not to study on the recommended textbook will be severely \ndisadvantaged. \n \nRecommended textbook: Wenliang Du. Computer Security: A Hands-on Approach. 3rd Edition. \nISBN: 978-17330039-5-7. Published: 1 May 2022. \n \nhttps://www.handsonsecurity.net. Some chapters freely available online. \n \n \nOptional additional books (not substitutes): \n \nRoss Anderson, Security Engineering 3rd Ed, Wiley, 2020. ISBN: 978-1-119-64278-7. \nhttps://www.cl.cam.ac.uk/~rja14/book.html. Some chapters freely available online. \n \nPaul van Oorschot, Computer Security and the Internet, Springer, 2020. ISBN: \n978-3-030-33648-6 (hardcopy), 978-3-030-33649-3 (eBook). \nhttps://people.scs.carleton.ca/~paulv/toolsjewels.html. All chapters freely available online.\n\nFORMAL MODELS OF LANGUAGE \n \nAims \nThis course studies formal models of language and considers how they might be relevant to the \nprocessing and acquisition of natural languages. The course will extend knowledge of formal \nlanguage theory; introduce several new grammars; and use concepts from information theory to \ndescribe natural language. \n \nLectures \nNatural language and the Chomsky hierarchy 1. Recap classes of language. Closure properties \nof language classes. Recap pumping lemma for regular languages. Discussion of relevance (or \nnot) to natural languages (example embedded clauses in English). \nNatural language and the Chomsky hierarchy 2. Pumping lemma for context free languages. \nDiscussion of relevance (or not) to natural languages (example Swiss-German cross serial \ndependancies). Properties of minimally context sensitive languages. Introduction to tree \nadjoining grammars. \nLanguage processing and context free grammar parsing 1. Recap of context free grammar \nparsing. Language processing predictions based on top down parsing models (example Yngve\u2019s \nlanguage processing predictions). Language processing predictions based on probabilistic \nparsing (example Halle\u2019s language processing predictions). \nLanguage processing and context free grammar parsing 2. Introduction to context free grammar \nequivalent dependancy grammars. Language processing predictions based on Shift-Reduce \nparsing (examples prosodic look-ahead parsers, Parsey McParseface). \nGrammar induction of language classes. Introduction to grammar induction. Discussion of \nrelevance (or not) to natural language acquisition. Gold\u2019s theorem. Introduction to context free \ngrammar equivalent categorial grammars and their learnable classes. \nNatural language and information theory 1. Entropy and natural language typology. Uniform \ninformation density as a predictor for language processing. \nNatural language and information theory 2. Noisy channel encoding as a model for spelling \nerror, translation and language processing. \nVector space models and word vectors. Introduction to word vectors (example Word2Vec). Word \nvectors as predictors for semantic language processing. \nObjectives \nAt the end of the course students should \n \nunderstand how known natural languages relate to formal languages in the Chomsky hierarchy; \nhave knowledge of several context free grammars equivalents; \nunderstand how we might make predictions about language processing and language \nacquisition from formal models; \nknow how to use information theoretic concepts to describe aspects of natural language. \nRecommended reading \n* Jurafsky, D. and Martin, J. (2008). Speech and language processing. Prentice Hall. \nManning, C.D. and Schutze, H. (1999) Foundations of statistical natural language processing. \nMIT Press. \n\nRuslan, M. (2003) The Oxford handbook of computational linguistics. Oxford University Press. \nClark, A., Fox, C. and Lappin, S. (2010) The handbook of computational linguistics and natural \nlanguage processing. Wiley-Blackwell. \nKozen, D. (1997) Automata and computibility. Springer.\n \n\n",
        "5th Semester \nBIOINFORMATICS \nAims \nThis course focuses on algorithms used in Bioinformatics and System Biology. Most of the \nalgorithms are general and can be applied in other fields on multidimensional and noisy data. All \nthe necessary biological terms and concepts useful for the course and the examination will be \ngiven in the lectures. The most important software implementing the described algorithms will be \ndemonstrated. \n \nLectures \nIntroduction to biological data: Bioinformatics as an interesting field in computer science. \nComputing and storing information with DNA (including Adleman\u2019s experiment). \nDynamic programming. Longest common subsequence, DNA global and local alignment, linear \nspace alignment, Nussinov algorithm for RNA, heuristics for multiple alignment. (Vol. 1, chapter \n5) \nSequence database search. Blast. (see notes and textbooks) \nGenome sequencing. De Bruijn graph. (Vol. 1, chapter 3) \nPhylogeny. Distance based algorithms (UPGMA, Neighbour-Joining). Parsimony-based \nalgorithms. Examples in Computer Science. (Vol. 2, chapter 7) \nClustering. Hard and soft K-means clustering, use of Expectation Maximization in clustering, \nHierarchical clustering, Markov clustering algorithm. (Vol. 2, chapter 8) \nGenomics Pattern Matching. Suffix Tree String Compression and the Burrows-Wheeler \nTransform. (Vol. 2, chapter 9) \nHidden Markov Models. The Viterbi algorithm, profile HMMs for sequence alignment, classifying \nproteins with profile HMMs, soft decoding problem, Baum-Welch learning. (Vol. 2, chapter 10) \nObjectives \nAt the end of this course students should \n \nunderstand Bioinformatics terminology; \nhave mastered the most important algorithms in the field; \nbe able to work with bioinformaticians and biologists; \nbe able to find data and literature in repositories. \nRecommended reading \n* Compeau, P. and Pevzner, P.A. (2015). Bioinformatics algorithms: an active learning approach. \nActive Learning Publishers. \nDurbin, R., Eddy, S., Krough, A. and Mitchison, G. (1998). Biological sequence analysis: \nprobabilistic models of proteins and nucleic acids. Cambridge University Press. \nJones, N.C. and Pevzner, P.A. (2004). An introduction to bioinformatics algorithms. MIT Press. \nFelsenstein, J. (2003). Inferring phylogenies. Sinauer Associates.\n \n\nBUSINESS STUDIES \n \nAims \nHow to start and run a computer company; the aims of this course are to introduce students to \nall the things that go to making a successful project or product other than just the programming. \nThe course will survey some of the issues that students are likely to encounter in the world of \ncommerce and that need to be considered when setting up a new computer company. \n \nSee also Business Seminars in the Easter Term. \n \nLectures \nSo you\u2019ve got an idea? Introduction. Why are you doing it and what is it? Types of company. \nMarket analysis. The business plan. \nMoney and tools for its management. Introduction to accounting: profit and loss, cash flow, \nbalance sheet, budgets. Sources of finance. Stocks and shares. Options and futures. \nSetting up: legal aspects. Company formation. Brief introduction to business law; duties of \ndirectors. Shares, stock options, profit share schemes and the like. Intellectual Property Rights, \npatents, trademarks and copyright. Company culture and management theory. \nPeople. Motivating factors. Groups and teams. Ego. Hiring and firing: employment law. \nInterviews. Meeting techniques. \nProject planning and management. Role of a manager. PERT and GANTT charts, and critical \npath analysis. Estimation techniques. Monitoring. \nQuality, maintenance and documentation. Development cycle. Productization. Plan for quality. \nPlan for maintenance. Plan for documentation. \nMarketing and selling. Sales and marketing are different. Marketing; channels; marketing \ncommunications. Stages in selling. Control and commissions. \nGrowth and exit routes. New markets: horizontal and vertical expansion. Problems of growth; \nsecond system effects. Management structures. Communication. Exit routes: acquisition, \nfloatation, MBO or liquidation. Futures: some emerging ideas for new computer businesses. \nSummary. Conclusion: now you do it! \nObjectives \nAt the end of the course students should \n \nbe able to write and analyse a business plan; \nknow how to construct PERT and GANTT diagrams and perform critical path analysis; \nappreciate the differences between profitability and cash flow, and have some notion of budget \nestimation; \nhave an outline view of company formation, share structure, capital raising, growth and exit \nroutes; \nhave been introduced to concepts of team formation and management; \nknow about quality documentation and productization processes; \nunderstand the rudiments of marketing and the sales process. \nRecommended reading \n\nLang, J. (2001). The high-tech entrepreneur\u2019s handbook: how to start and run a high-tech \ncompany. FT.COM/Prentice Hall. \n \nStudents will be expected to be able to use Microsoft Excel and Microsoft Project. \n \nFor additional reading on a lecture-by-lecture basis, please see the course website. \n \nStudents are strongly recommended to enter the CU Entrepreneurs Business Ideas Competition \nhttp://www.cue.org.uk/\n \n\nDENOTATIONAL SEMANTICS \n \nAims \nThe aims of this course are to introduce domain theory and denotational semantics, and to \nshow how they provide a mathematical basis for reasoning about the behaviour of programming \nlanguages. \n \nLectures \nIntroduction.The denotational approach to the semantics of programming \nlanguages.Recursively defined objects as limits of successive approximations. \nLeast fixed points.Complete partial orders (cpos) and least elements.Continuous functions and \nleast fixed points. \nConstructions on domains.Flat domains.Product domains. Function domains. \nScott induction.Chain-closed and admissible subsets of cpos and domains.Scott\u2019s fixed-point \ninduction principle. \nPCF.The Scott-Plotkin language PCF.Evaluation. Contextual equivalence. \nDenotational semantics of PCF.Denotation of types and terms. Compositionality. Soundness \nwith respect to evaluation. [2 lectures]. \nRelating denotational and operational semantics.Formal approximation relation and its \nfundamental property.Computational adequacy of the PCF denotational semantics with respect \nto evaluation. Extensionality properties of contextual equivalence. [2 lectures]. \nFull abstraction.Failure of full abstraction for the domain model. PCF with parallel or. \nObjectives \nAt the end of the course students should \n \nbe familiar with basic domain theory: cpos, continuous functions, admissible subsets, least fixed \npoints, basic constructions on domains; \nbe able to give denotational semantics to simple programming languages with simple types; \nbe able to apply denotational semantics; in particular, to understand the use of least fixed points \nto model recursive programs and be able to reason about least fixed points and simple \nrecursive programs using fixed point induction; \nunderstand the issues concerning the relation between denotational and operational semantics, \nadequacy and full abstraction, especially with respect to the language PCF. \nRecommended reading \nWinskel, G. (1993). The formal semantics of programming languages: an introduction. MIT \nPress. \nGunter, C. (1992). Semantics of programming languages: structures and techniques. MIT Press. \nTennent, R. (1991). Semantics of programming languages. Prentice Hall.\n \n\nINFORMATION THEORY \n \nAims \nThis course introduces the principles and applications of information theory: how information is \nmeasured in terms of probability and various entropies, how these are used to calculate the \ncapacity of communication channels, with or without noise, and to measure how much random \nvariables reveal about each other. Coding schemes including error correcting codes are studied \nalong with data compression, spectral analysis, transforms, and wavelet coding. Applications of \ninformation theory are reviewed, from astrophysics to pattern recognition. \n \nLectures \nInformation, probability, uncertainty, and surprise. How concepts of randomness and uncertainty \nare related to information. How the metrics of information are grounded in the rules of \nprobability. Shannon Information. Weighing problems and other examples. \nEntropy of discrete variables. Definition and link to Shannon information. Joint entropy, Mutual \ninformation. Visual depictions of the relationships between entropy types. Why entropy gives \nfundamental measures of information content. \nSource coding theorem and data compression; prefix, variable-, and fixed-length codes. \nInformation rates; Asymptotic equipartition principle; Symbol codes; Huffman codes and the \nprefix property. Binary symmetric channels. Capacity of a noiseless discrete channel. Stream \ncodes. \nNoisy discrete channel coding. Joint distributions; mutual information; Conditional Entropy; \nError-correcting codes; Capacity of a discrete channel. Noisy channel coding theorem. \nEntropy of continuous variables. Differential entropy; Mutual information; Channel Capacity; \nGaussian channels. \nEntropy for comparing probability distributions and for machine learning. Relative entropy/KL \ndivergence; cross-entropy; use as loss function. \nApplications of information theory in other sciences.  \nObjectives \nAt the end of the course students should be able to \n \ncalculate the information content of a random variable from its probability distribution; \nrelate the joint, conditional, and marginal entropies of variables in terms of their coupled \nprobabilities; \ndefine channel capacities and properties using Shannon\u2019s Theorems; \nconstruct efficient codes for data on imperfect communication channels; \ngeneralize the discrete concepts to continuous signals on continuous channels; \nunderstand encoding and communication schemes in terms of the spectral properties of signals \nand channels; \ndescribe compression schemes, and efficient coding using wavelets and other representations \nfor data. \nRecommended reading \n  Mackay's Information Theory, inference and Learning Algorithms \n \n\n Stone's Information Theory: A Tutorial Introduction.\n \n\nLATEX AND JULIA \n \nAims \nIntroduction to two widely-used languages for typesetting dissertations and scientific \npublications, for prototyping numerical algorithms and to visualize results. \n \nLectures \nLATEX. Workflow example, syntax, typesetting conventions, non-ASCII characters, document \nstructure, packages, mathematical typesetting, graphics and figures, cross references, build \ntools. \nJulia. Tools for technical computing and visualization. The Array type and its operators, 2D/3D \nplotting, functions and methods, notebooks, packages, vectorized audio demonstration. \nObjectives \nStudents should be able to avoid the most common LATEX mistakes, to prototype simple image \nand signal-processing algorithms in Julia, and to visualize the results. \n \nRecommended reading \n* Lamport, L. (1994). LATEX \u2013 a documentation preparation system user\u2019s guide and reference \nmanual. Addison-Wesley (2nd ed.). \nMittelbach, F., et al. (2023). The LATEX companion. Addison-Wesley (3rd ed.).\n \n\nPRINCIPLES OF COMMUNICATIONS \n \nAims \nThis course aims to provide a detailed understanding of the underlying principles for how \ncommunications systems operate. Practical examples (from wired and wireless \ncommunications, the Internet, and other communications systems) are used to illustrate the \nprinciples. \n \nLectures \nIntroduction. Course overview. Abstraction, layering. Review of structure of real networks, links, \nend systems and switching systems. [1 lecture] \nRouting. Central versus Distributed Routing Policy Routing. Multicast Routing Circuit Routing [6 \nlectures] \nFlow control and resource optimisation. 1[Control theory] is a branch of engineering familiar to \npeople building dynamic machines. It can be applied to network traffic. Stemming the flood, at \nsource, sink, or in between? Optimisation as a model of networkand user. TCP in the wild. [3 \nlectures] \nPacket Scheduling. Design choices for scheduling and queue management algorithms for \npacket forwarding, and fairness. [2 lectures] \nThe big picture for managing traffic. Economics and policy are relevant to networks in many \nways. Optimisation and game theory are both relevant topics discussed here. [2 lectures] \nSystem Structures and Summary. Abstraction, layering. The structure of real networks, links, \nend systems and switching. [2 lectures] \n1 Control theory was not taught and will not be the subject of any exam question. \n \nObjectives \nAt the end of the course students should be able to explain the underlying design and behaviour \nof protocols and networks, including capacity, topology, control and use. Several specific \nmathematical approaches are covered (control theory, optimisation). \n \nRecommended reading \n* Keshav, S. (2012). Mathematical Foundations of Computer Networking. Addison Wesley. ISBN \n9780321792105 \nBackground reading: \nKeshav, S. (1997). An engineering approach to computer networking. Addison-Wesley (1st ed.). \nISBN 0201634422 \nStevens, W.R. (1994). TCP/IP illustrated, vol. 1: the protocols. Addison-Wesley (1st ed.). ISBN \n0201633469\n \n\nTYPES \n \nAims \nThe aim of this course is to show by example how type systems for programming languages can \nbe defined and their properties developed, using techniques that were introduced in the Part IB \ncourse on Semantics of Programming Languages. The emphasis is on type systems for \nfunctional languages and their connection to constructive logic. \n \nLectures \nIntroduction. The role of type systems in programming languages. Review of rule-based \nformalisation of type systems. [1 lecture] \nPropositions as types. The Curry-Howard correspondence between intuitionistic propositional \ncalculus and simply-typed lambda calculus. Inductive types and iteration. Consistency and \ntermination. [2 lectures] \nPolymorphic lambda calculus (PLC). PLC syntax and reduction semantics. Examples of \ndatatypes definable in the polymorphic lambda calculus. Type inference. [3 lectures] \nMonads and effects. Explicit versus implicit effects. Using monadic types to control effects. \nReferences and polymorphism. Recursion and looping. [2 lectures] \nContinuations and classical logic. First-class continuations and control operators. Continuations \nas Curry-Howard for classical logic. Continuation-passing style. [2 lectures] \nDependent types. Dependent function types. Indexed datatypes. Equality types and combining \nproofs with programming. [2 lectures] \nObjectives \nAt the end of the course students should \n \nbe able to use a rule-based specification of a type system to carry out type checking and type \ninference; \nunderstand by example the Curry-Howard correspondence between type systems and logics; \nunderstand how types can be used to control side-effects in programming; \nappreciate the expressive power of parametric polymorphism and dependent types. \nRecommended reading \n* Pierce, B.C. (2002). Types and programming languages. MIT Press. \nPierce, B. C. (Ed.) (2005). Advanced Topics in Types and Programming Languages. MIT Press. \nGirard, J-Y. (tr. Taylor, P. and Lafont, Y.) (1989). Proofs and types. Cambridge University Press.\n\nADVANCED DATA SCIENCE \n \nPracticals \nThe module will have a practical session for each of the three stages of the pipeline. Each of the \nparts will be tied into an aspect of the final project. \n \nObjectives \nAt the end of the course students will be familiar with the purpose of data science, how it differs \nfrom the closely related fields of machine learning, statistics and artificial intelligence and what a \ntypical data analysis pipeline looks like in practice. As well as emphasising the importance of \nanalysis methods we will introduce a formalism for organising how data science is done in \npractice and what the different aspects the data scientist faces when giving data-driven answers \nto questions of interest. \n \nRecommended reading \nSimon Rogers et al. (2016). A First Course in Machine Learning, Second Edition. [] Chapman \nand Hall/CRC, nil \nShai Shalev-Shwartz et al. (2014). Understanding Machine Learning: From Theory to \nAlgorithms. New York, NY, USA: Cambridge  University Press \nChristopher M. Bishop (2006). Pattern Recognition and Machine Learning (Information Science \nand Statistics). Secaucus, NJ, USA: Springer-Verlag New York, Inc. \nAssessment \nPractical There will be four practicals contributing 20% to the final module mark. Data science is \na topic where there is rarely a single \ncorrect answer therefore the important thing is that an informed attempt has been made to \naddress the questions that can be supported and motivated. \nReport The course will be concluded by an individual report covering the material in the course \nthrough \npractical exercise working with real data. This report will make up 80% of the mark for the \ncourse. \n \n \n\nAFFECTIVE ARTIFICIAL INTELLIGENCE \n \nSynopsis \nAffective Artificial Intelligence (Affective AI) aims to imbue machines with social and emotional \nintelligence (EQ). More specifically, Affective AI aims to create artificially intelligent systems and \nmachines that can recognize, interpret, process, and simulate human social signals and \nbehaviours, expressions, and emotions, to enhance human-AI interaction and communication. \n\\r\\n\\r\\nTo achieve this goal, Affective AI draws upon various scientific disciplines, including \nmachine learning, computer vision, speech / natural language / signal processing, psychology \nand cognitive science, and ethics and social sciences. \n \n  \nBackground & Aims \nAffective Artificial Intelligence (Affective AI) aims to imbue machines with social and emotional \nintelligence (EQ). More specifically, Affective AI aims to create artificially intelligent systems and \nmachines that can recognize, interpret, process, and simulate human social signals and \nbehaviours, expressions, and emotions, to enhance human-AI interaction and communication.  \n \nTo achieve this goal, Affective AI draws upon various scientific disciplines, including machine \nlearning, computer vision, speech / natural language / signal processing, psychology and \ncognitive science, and ethics and social sciences. \n \nAffective AI has direct applications in and implications for the design of innovative interactive \ntechnology (e.g., interaction with chat bots, virtual agents, robots), single and multi-user smart \nenvironments ( e.g., in-car/ virtual / augmented / mixed reality, serious games), public speaking \nand cognitive training, and clinical and biomedical studies (e.g., autism, depression, pain). \n \nThe aim of this module is to impart knowledge and ability needed to make informed choices of \nmodels, data, and machine learning techniques for sensing, recognition, and generation of \naffective and social behaviour (e.g., smile, frown, head nodding/shaking, \nagreement/disagreement) in order to create Affectively intelligent AI systems, with a \nconsideration for various ethical issues (e.g., privacy, bias) arising from the real-world \ndeployment of these systems. \n \n  \n \nSyllabus \nThe following list provides a representative list of topics: \n \nIntroduction, definitions, and overview \nTheories from various disciplines \nSensing from multiple modalities (e.g., vision, audio, bio signals, text) \nData acquisition and annotation \nSignal processing / feature extraction \n\nLearning / prediction / recognition and evaluation \nBehaviour synthesis / generation (e.g., for embodied agents / robots) \nAdvanced topics and ethical considerations (e.g., bias and fairness) \nCross-disciplinary applications (via seminar presentations and discussions) \nGuest lectures (diverse topics \u2013 changes each year) \nHands-on research and programming work (i.e., mini project & report)  \n \n  \n \nObjectives \nOn completion of this module, students will: \n \nunderstand and demonstrate knowledge in key characteristics of affectively intelligent AI, which \ninclude: \nRecognition: How to equip Affective AI systems with capabilities of analysing facial expressions, \nvocal intonations, gestures, and other physiological signals to infer human affective states? \nGeneration: How to enable affectively intelligent AI simulate expressions of emotions in \nmachines, allowing them to respond in a more human-like manner, such as virtual agents and \nhumanoid robots, showing empathy or sympathy? \nAdaptation and Personalization: How to enable affectively intelligent AI adapt system responses \nbased on users' affective states and/or needs, or tailor interactions and experiences to individual \nusers based on their expressivity / emotional profiles, personalities, and past interactions? \nEmpathetic Communication: How to design Affective AI systems to communicate with users in a \nway that demonstrates empathy, understanding, and sensitivity to their affective states and \nneeds? \nEthical and societal considerations: What are the various human differences, ethical guidelines, \nand societal impacts that need to be considered when designing and deploying Affective AI to \nensure that these systems respect users' privacy, autonomy, and well-being? \ncomprehend and apply (appropriate) methods for collection, analysis, representation, and \nevaluation of human affective and communicative behavioural data. \nenhance programming skills for creating and implementing (components of) Affective AI \nsystems. \ndemonstrate critical thinking, analysis and synthesis while deciding on 'when' and 'how' to \nincorporate human affect and social signals in a specific AI system context and gain practical \nexperience in proposing and justifying computational solution(s) of suitable nature and scope. \n  \n \nAssessment - Part II Students \nSeminar presentation: 20% \nParticipating in Q&A and discussions: 10% \nMid-term mini project report and presentation (as a team of 2): 10%  \nFinal mini project report & code (as a team of 2): 60% \n \n\nCATEGORY THEORY \n \nAims \nCategory theory provides a unified treatment of mathematical properties and constructions that \ncan be expressed in terms of 'morphisms' between structures. It gives a precise framework for \ncomparing one branch of mathematics (organized as a category) with another and for the \ntransfer of problems in one area to another. Since its origins in the 1940s motivated by \nconnections between algebra and geometry, category theory has been applied to diverse fields, \nincluding computer science, logic and linguistics. This course introduces the basic notions of \ncategory theory: adjunction, natural transformation, functor and category. We will use category \ntheory to organize and develop the kinds of structure that arise in models and semantics for \nlogics and programming languages. \n \nSyllabus \nIntroduction; some history. Definition of category. The category of sets and functions. \nCommutative diagrams. Examples of categories: preorders and monotone functions; monoids \nand monoid homomorphisms; a preorder as a category; a monoid as a category. Definition of \nisomorphism. Informal notion of a 'category-theoretic' property. \nTerminal objects. The opposite of a category and the duality principle. Initial objects. Free \nmonoids as initial objects. \nBinary products and coproducts. Cartesian categories. \nExponential objects: in the category of sets and in general. Cartesian closed categories: \ndefinition and examples. \nIntuitionistic Propositional Logic (IPL) in Natural Deduction style. Semantics of IPL in a cartesian \nclosed preorder. \nSimply Typed Lambda Calculus (STLC). The typing relation. Semantics of STLC types and \nterms in a cartesian closed category (ccc). The internal language of a ccc. The \nCurry-Howard-Lawvere correspondence. \nFunctors. Contravariance. Identity and composition for functors. \nSize: small categories and locally small categories. The category of small categories. Finite \nproducts of categories. \nNatural transformations. Functor categories. The category of small categories is cartesian \nclosed. \nHom functors. Natural isomorphisms. Adjunctions. Examples of adjoint functors. Theorem \ncharacterising the existence of right (respectively left) adjoints in terms of a universal property. \nDependent types. Dependent product sets and dependent function sets as adjoint functors. \nEquivalence of categories. Example: the category of I-indexed sets and functions is equivalent \nto the slice category Set/I. \nPresheaves. The Yoneda Lemma. Categories of presheaves are cartesian closed. \nMonads. Modelling notions of computation as monads. Moggi's computational lambda calculus. \nObjectives \nOn completion of this module, students should: \n \n\nbe familiar with some of the basic notions of category theory and its connections with logic and \nprogramming language semantics \nAssessment \na graded exercise sheet (25% of the final mark), and \na take-home test (75%) \nRecommended reading \nAwodey, S. (2010). Category theory. Oxford University Press (2nd ed.). \n \nCrole, R. L. (1994). Categories for types. Cambridge University Press. \n \nLambek, J. and Scott, P. J. (1986). Introduction to higher order categorical logic. Cambridge \nUniversity Press. \n \nPitts, A. M. (2000). Categorical Logic. Chapter 2 of S. Abramsky, D. M. Gabbay and T. S. E. \nMaibaum (Eds) Handbook of Logic in Computer Science, Volume 5. Oxford University Press. \n(Draft copy available here.) \n \nClass Size \nThis module can accommodate upto 15 Part II students plus 15 MPhil / Part III students.\n\nDIGITAL SIGNAL PROCESSING \n \nAims \nThis course teaches basic signal-processing principles necessary to understand many modern \nhigh-tech systems, with examples from audio processing, image coding, radio communication, \nradar, and software-defined radio. Students will gain practical experience from numerical \nexperiments in programming assignments (in Julia, MATLAB or NumPy). \n \nLectures \nSignals and systems. Discrete sequences and systems: types and properties. Amplitude, \nphase, frequency, modulation, decibels, root-mean square. Linear time-invariant systems, \nconvolution. Some examples from electronics, optics and acoustics. \nPhasors. Eigen functions of linear time-invariant systems. Review of complex arithmetic. \nPhasors as orthogonal base functions. \nFourier transform. Forms and properties of the Fourier transform. Convolution theorem. Rect \nand sinc. \nDirac\u2019s delta function. Fourier representation of sine waves, impulse combs in the time and \nfrequency domain. Amplitude-modulation in the frequency domain. \nDiscrete sequences and spectra. Sampling of continuous signals, periodic signals, aliasing, \ninterpolation, sampling and reconstruction, sample-rate conversion, oversampling, spectral \ninversion. \nDiscrete Fourier transform. Continuous versus discrete Fourier transform, symmetry, linearity, \nFFT, real-valued FFT, FFT-based convolution, zero padding, FFT-based resampling, \ndeconvolution exercise. \nSpectral estimation. Short-time Fourier transform, leakage and scalloping phenomena, \nwindowing, zero padding. Audio and voice examples. DTFM exercise. \nFinite impulse-response filters. Properties of filters, implementation forms, window-based FIR \ndesign, use of frequency-inversion to obtain high-pass filters, use of modulation to obtain \nband-pass filters. \nInfinite impulse-response filters. Sequences as polynomials, z-transform, zeros and poles, some \nanalog IIR design techniques (Butterworth, Chebyshev I/II, elliptic filters, second-order cascade \nform). \nBand-pass signals. Band-pass sampling and reconstruction, IQ up and down conversion, \nsuperheterodyne receivers, software-defined radio front-ends, IQ representation of AM and FM \nsignals and their demodulation. \nDigital communication. Pulse-amplitude modulation. Matched-filter detector. Pulse shapes, \ninter-symbol interference, equalization. IQ representation of ASK, BSK, PSK, QAM and FSK \nsignals. [2 hours] \nRandom sequences and noise. Random variables, stationary and ergodic processes, \nautocorrelation, cross-correlation, deterministic cross-correlation sequences, filtered random \nsequences, white noise, periodic averaging. \nCorrelation coding. Entropy, delta coding, linear prediction, dependence versus correlation, \nrandom vectors, covariance, decorrelation, matrix diagonalization, eigen decomposition, \n\nKarhunen-Lo\u00e8ve transform, principal component analysis. Relation to orthogonal transform \ncoding using fixed basis vectors, such as DCT. \nLossy versus lossless compression. What information is discarded by human senses and can \nbe eliminated by encoders? Perceptual scales, audio masking, spatial resolution, colour \ncoordinates, some demonstration experiments. \nQuantization, image coding standards. Uniform and logarithmic quantization, A/\u00b5-law coding, \ndithering, JPEG. \nObjectives \napply basic properties of time-invariant linear systems; \nunderstand sampling, aliasing, convolution, filtering, the pitfalls of spectral estimation; \nexplain the above in time and frequency domain representations; \nuse filter-design software; \nvisualize and discuss digital filters in the z-domain; \nuse the FFT for convolution, deconvolution, filtering; \nimplement, apply and evaluate simple DSP applications; \nfamiliarity with a number of signal-processing concepts used in digital communication systems \nRecommended reading \nLyons, R.G. (2010). Understanding digital signal processing. Prentice Hall (3rd ed.). \nOppenheim, A.V. and Schafer, R.W. (2007). Discrete-time digital signal processing. Prentice \nHall (3rd ed.). \nStein, J. (2000). Digital signal processing \u2013 a computer science perspective. Wiley. \n \nClass size \nThis module can accommodate a maximum of 24 students (16 Part II students and 8 MPhil \nstudents) \n \nAssessment - Part II students \nThree homework programming assignments, each comprising 20% of the mark \nWritten test, comprising 40% of the total mark.\n \n\nMACHINE VISUAL PERCEPTION \n \nAims \nThis course aims at introducing the theoretical foundations and practical techniques for machine \nperception, the capability of computers to interpret data resulting from sensor measurements. \nThe course will teach the fundamentals and modern techniques of machine perception, i.e. \nreconstructing the real world starting from sensor measurements with a focus on machine \nperception for visual data. The topics covered will be image/geometry representations for \nmachine perception, semantic segmentation, object detection and recognition, geometry \ncapture, appearance modeling and acquisition, motion detection and estimation, \nhuman-in-the-loop machine perception, select topics in applied machine perception. \n \nMachine perception/computer vision is a rapidly expanding area of research with real-world \napplications. An understanding of machine perception is also important for robotics, interactive \ngraphics (especially AR/VR), applied machine learning, and several other fields and industries. \nThis course will provide a fundamental understanding of and practical experience with the \nrelevant techniques. \n \nLearning outcomes \nStudents will understand the theoretical underpinnings of the modern machine perception \ntechniques for reconstructing models of reality starting from an incomplete and imperfect view of \nreality. \nStudents will be able to apply machine perception theory to solve practical problems, e.g. \nclassification of images, geometry capture. \nStudents will gain an understanding of which machine perception techniques are appropriate for \ndifferent tasks and scenarios. \nStudents will have hands-on experience with some of these techniques via developing a \nfunctional machine perception system in their projects. \nStudents will have practical experience with the current prominent machine perception \nframeworks. \nSyllabus \nThe fundamentals of machine learning for machine perception \nDeep neural networks and frameworks for machine perception \nSemantic segmentation of objects and humans \nObject detection and recognition \nMotion estimation, tracking and recognition \n3D geometry capture \nAppearance modeling and acquisition \nSelect topics in applied machine perception \nAssessment \nA practical exercise, worth 20% of the mark. This will cover the basics and theory of machine \nperception and some of the practical techniques the students will likely use in their projects. This \nis individual work. No GPU hours will be needed for the practical work. \nA machine perception project worth 80% of the marks: \n\nCourse projects will be selected by the students following the possible project themes proposed \nby the lecturer, and will be checked by the lecturer for appropriateness.  \nThe students will form groups of 2-3 to design, implement, report, and present a project to tackle \na given task in machine perception. \nThe final mark will be composed of an implementation/report mark (60%) and a presentation \nmark (20%). Each team member will be evaluated based on her/his contribution. \nEach project will have extensions to be completed only by the ACS students. Each student will \nwrite a different part of the report, whose author will be clearly marked. Each student will further \nsummarise her/his contributions to the project in the same report. \nRecommended Reading List \nComputer Vision: Algorithms and Applications, Richard Szeliski, Springer, 2010. \nDeep Learning, Ian Goodfellow, Yoshua Bengio, and Aaron Courville, MIT Press, 2016. \nMachine Learning and Visual Perception, Baochang Zhang, De Gruyter, 2020.\n \n\nNATURAL LANGUAGE PROCESSING \n \nAims \nThis course introduces the fundamental techniques of natural language processing. It aims to \nexplain the potential and the main limitations of these techniques. Some current research issues \nare introduced and some current and potential applications discussed and evaluated. Students \nwill also be introduced to practical experimentation in natural language processing. \n \nLectures \nOverview. Brief history of NLP research, some current applications, components of NLP \nsystems. \nMorphology and Finite State Techniques. Morphology in different languages, importance of \nmorphological analysis in NLP, finite-state techniques in NLP. \nPart-of-Speech Tagging and Log-Linear Models. Lexical categories, word tagging, corpora and \nannotations, empirical evaluation. \nPhrase Structure and Structure Prediction. Phrase structures, structured prediction, context-free \ngrammars, weights and probabilities. Some limitations of context-free grammars. \nDependency Parsing. Dependency structure, grammar-free parsing, incremental processing.  \nGradient Descent and Neural Nets. Parameter optimisation by gradient descent. Non-linear \nfunctions with neural network layers. Log-linear model as softmax layer. Current findings of \nNeural NLP. \nWord representations. Representing words with vectors, count-based and prediction-based \napproaches, similarity metrics. \nRecurrent Neural Networks. Modelling sequences, parameter sharing in recurrent neural \nnetworks, neural language models, word prediction. \nCompositional Semantics. Logical representations, compositional semantics, lambda calculus, \ninference and robust entailment. \nLexical Semantics. Semantic relations, WordNet, word senses. \nDiscourse. Discourse relations, anaphora resolution, summarization. \nNatural Language Generation. Challenges of natural language generation (NLG), tasks in NLG, \nsurface realisation. \nPractical and assignments. Students will build a natural language processing system which will \nbe trained and evaluated on supplied data. The system will be built from existing components, \nbut students will be expected to compare approaches and some programming will be required \nfor this. Several assignments will be set during the practicals for assessment. \nObjectives \nBy the end of the course students should: \n \nbe able to discuss the current and likely future performance of several NLP applications; \nbe able to describe briefly a fundamental technique for processing language for several \nsubtasks, such as morphological processing, parsing, word sense disambiguation etc.; \nunderstand how these techniques draw on and relate to other areas of computer science. \nRecommended reading \n\nJurafsky, D. & Martin, J. (2023). Speech and language processing. Prentice Hall (3rd ed. draft, \nonline). \n \nAssessment - Part II Students \nAssignment 1 - 10% of marks \nAssignment 2 - 60% of marks \nAssignment 3 - 30% of marks\n \n\nPRACTICAL RESEARCH IN HUMAN-CENTRED AI \n \nAims \nThis is an advanced course in human-computer interaction, with a specialist focus on intelligent \nuser interfaces and interaction with machine-learning and artificial intelligence technologies. The \nformat will be largely Practical, with students carrying out a mini-project involving empirical \nresearch investigation. These studies will investigate human interaction with some kind of \nmodel-based system for planning, decision-making, automation etc. Possible study formats \nmight include: System evaluation, Field observation, Hypothesis testing experiment, Design \nintervention or Corpus analysis, following set examples from recent research publications. \nProject work will be formally evaluated through a report and presentation. \n \nLectures \n(note that Lectures 2-7 also include one hour class discussion of practical work) \n        \u2022 Current research themes in intelligent user interfaces \n        \u2022 Program synthesis \n        \u2022 Mixed initiative interaction \n        \u2022 Interpretability / explainable AI \n        \u2022 Labelling as a fundamental problem \n        \u2022 Machine learning risks and bias \n        \u2022 Visualisation and visual analytics \n        \u2022 Student research presentations \n \nObjectives \nBy the end of the course students should: \n        \u2022 be familiar with current state of the art in intelligent interactive systems \n        \u2022 understand the human factors that are most critical in the design of such systems \n        \u2022 be able to evaluate evidence for and against the utility of novel systems \n        \u2022 have experience of conducting user studies meeting the quality criteria of this field \n        \u2022 be able to write up and present user research in a professional manner \n \nClass Size \nThis module can accommodate upto 20 Part II, Part III and MPhil students. \n \nRecommended reading \nBrad A. Myers and Richard McDaniel (2000). Demonstrational Interfaces: Sometimes You Need \na Little Intelligence, Sometimes You Need a Lot. \n \nAlan Blackwell (2024). Moral Codes: Designing alternatives to AI \n \nAssessment - Part II Students \nThe format will be largely practical, with students carrying out an individual mini-project involving \nempirical research investigation. \n \n\nAssignment 1: six incremental submissions which together contribute 20% to the final module \nmark. \n \nAssignment 2: Final report - 80% of the final module mark \n \n\n",
        "6th Semester \nADVANCED COMPUTER ARCHITECTURE \nAims \nThis course examines the techniques and underlying principles that are used to design \nhigh-performance computers and processors. Particular emphasis is placed on understanding \nthe trade-offs involved when making design decisions at the architectural level. A range of \nprocessor architectures are explored and contrasted. In each case we examine their merits and \nlimitations and how ultimately the ability to scale performance is restricted. \n \nLectures \nIntroduction. The impact of technology scaling and market trends. \nFundamentals of Computer Design. Amdahl\u2019s law, energy/performance trade-offs, ISA design. \nAdvanced pipelining. Pipeline hazards; exceptions; optimal pipeline depth; branch prediction; \nthe branch target buffer [2 lectures] \nSuperscalar techniques. Instruction-Level Parallelism (ILP); superscalar processor architecture \n[2 lectures] \nSoftware approaches to exploiting ILP. VLIW architectures; local and global instruction \nscheduling techniques; predicated instructions and support for speculative compiler \noptimisations. \nMultithreaded processors. Coarse-grained, fine-grained, simultaneous multithreading \nThe memory hierarchy. Caches; programming for caches; prefetching [2 lectures] \nVector processors. Vector machines; short vector/SIMD instruction set extensions; stream \nprocessing \nChip multiprocessors. The communication model; memory consistency models; false sharing; \nmultiprocessor memory hierarchies; cache coherence protocols; synchronization [2 lectures] \nOn-chip interconnection networks. Bus-based interconnects; on-chip packet switched networks \nSpecial-purpose architectures. Converging approaches to computer design \nObjectives \nAt the end of the course students should \n \nunderstand what determines processor design goals; \nappreciate what constrains the design process and how architectural trade-offs are made within \nthese constraints; \nbe able to describe the architecture and operation of pipelined and superscalar processors, \nincluding techniques such as branch prediction, register renaming and out-of-order execution; \nhave an understanding of vector, multithreaded and multi-core processor architectures; \nfor the architectures discussed, understand what ultimately limits their performance and \napplication domain. \nRecommended reading \n* Hennessy, J. and Patterson, D. (2012). Computer architecture: a quantitative approach. \nElsevier (5th ed.) ISBN 9780123838728. (the 3rd and 4th editions are also good)\n \n\nCRYPTOGRAPHY \n \nAims \nThis course provides an overview of basic modern cryptographic techniques and covers \nessential concepts that users of cryptographic standards need to understand to achieve their \nintended security goals. \n \nLectures \nCryptography. Overview, private vs. public-key ciphers, MACs vs. signatures, certificates, \ncapabilities of adversary, Kerckhoffs\u2019 principle. \nClassic ciphers. Attacks on substitution and transposition ciphers, Vigen\u00e9re. Perfect secrecy: \none-time pads. \nPrivate-key encryption. Stream ciphers, pseudo-random generators, attacking \nlinear-congruential RNGs and LFSRs. Semantic security definitions, oracle queries, advantage, \ncomputational security, concrete-security proofs. \nBlock ciphers. Pseudo-random functions and permutations. Birthday problem, random \nmappings. Feistel/Luby-Rackoff structure, DES, TDES, AES. \nChosen-plaintext attack security. Security with multiple encryptions, randomized encryption. \nModes of operation: ECB, CBC, OFB, CNT. \nMessage authenticity. Malleability, MACs, existential unforgeability, CBC-MAC, ECBC-MAC, \nCMAC, birthday attacks, Carter-Wegman one-time MAC. \nAuthenticated encryption. Chosen-ciphertext attack security, ciphertext integrity, \nencrypt-and-authenticate, authenticate-then-encrypt, encrypt-then-authenticate, padding oracle \nexample, GCM. \nSecure hash functions. One-way functions, collision resistance, padding, Merkle-Damg\u00e5rd \nconstruction, sponge function, duplex construct, entropy pool, SHA standards. \nApplications of secure hash functions. HMAC, stream authentication, Merkle tree, commitment \nprotocols, block chains, Bitcoin. \nKey distribution problem. Needham-Schroeder protocol, Kerberos, hardware-security modules, \npublic-key encryption schemes, CPA and CCA security for asymmetric encryption. \nNumber theory, finite groups and fields. Modular arithmetic, Euclid\u2019s algorithm, inversion, \ngroups, rings, fields, GF(2n), subgroup order, cyclic groups, Euler\u2019s theorem, Chinese \nremainder theorem, modular roots, quadratic residues, modular exponentiation, easy and \ndifficult problems. [2 lectures] \nDiscrete logarithm problem. Baby-step-giant-step algorithm, computational and decision \nDiffie-Hellman problem, DH key exchange, ElGamal encryption, hybrid cryptography, Schnorr \ngroups, elliptic-curve systems, key sizes. [2 lectures] \nTrapdoor permutations. Security definition, turning one into a public-key encryption scheme, \nRSA, attacks on \u201ctextbook\u201d RSA, RSA as a trapdoor permutation, optimal asymmetric \nencryption padding, common factor attacks. \nDigital signatures. One-time signatures, RSA signatures, Schnorr identification scheme, \nElGamal signatures, DSA, PS3 hack, certificates, PKI. \nObjectives \nBy the end of the course students should \n\n \nbe familiar with commonly used standardized cryptographic building blocks; \nbe able to match application requirements with concrete security definitions and identify their \nabsence in naive schemes; \nunderstand various adversarial capabilities and basic attack algorithms and how they affect key \nsizes; \nunderstand and compare the finite groups most commonly used with discrete-logarithm \nschemes; \nunderstand the basic number theory underlying the most common public-key schemes, and \nsome efficient implementation techniques. \nRecommended reading \nKatz, J., Lindell, Y. (2015). Introduction to modern cryptography. Chapman and Hall/CRC (2nd \ned.).\n \n\nE-COMMERCE \n \nAims \nThis course aims to give students an outline of the issues involved in setting up an e-commerce \nsite. \n \nLectures \nThe history of electronic commerce. Mail order; EDI; web-based businesses, credit card \nprocessing, PKI, identity and other hot topics. \nNetwork economics. Real and virtual networks, supply-side versus demand-side scale \neconomies, Metcalfe\u2019s law, the dominant firm model, the differentiated pricing model Data \nProtection Act, Distance Selling regulations, business models. \nWeb site design. Stock and price control; domain names, common mistakes, dynamic pages, \ntransition diagrams, content management systems, multiple targets. \nWeb site implementation. Merchant systems, system design and sizing, enterprise integration, \npayment mechanisms, CRM and help desks. Personalisation and internationalisation. \nThe law and electronic commerce. Contract and tort; copyright; binding actions; liabilities and \nremedies. Legislation: RIP; Data Protection; EU Directives on Distance Selling and Electronic \nSignatures. \nPutting it into practice. Search engine interaction, driving and analysing traffic; dynamic pricing \nmodels. Integration with traditional media. Logs and audit, data mining modelling the user. \ncollaborative filtering and affinity marketing brand value, building communities, typical behaviour. \nFinance. How business plans are put together. Funding Internet ventures; the recent hysteria; \nmaximising shareholder value. Future trends. \nUK and International Internet Regulation. Data Protection Act and US Privacy laws; HIPAA, \nSarbanes-Oxley, Security Breach Disclosure, RIP Act 2000, Electronic Communications Act \n2000, Patriot Act, Privacy Directives, data retention; specific issues: deep linking, Inlining, brand \nmisuse, phishing. \nObjectives \nAt the end of the course students should know how to apply their computer science skills to the \nconduct of e-commerce with some understanding of the legal, security, commercial, economic, \nmarketing and infrastructure issues involved. \n \nRecommended reading \nShapiro, C. and Varian, H. (1998). Information rules. Harvard Business School Press. \n \nAdditional reading: \n \nStandage, T. (1999). The Victorian Internet. Phoenix Press. Klemperer, P. (2004). Auctions: \ntheory and practice. Princeton Paperback ISBN 0-691-11925-2.\n \n\nMACHINE LEARNING AND BAYESIAN INFERENCE \nAims \nThe Part 1B course Artificial Intelligence introduced simple neural networks for supervised \nlearning, and logic-based methods for knowledge representation and reasoning. This course \nhas two aims. First, to provide a rigorous introduction to machine learning, moving beyond the \nsupervised case and ultimately presenting state-of-the-art methods. Second, to provide an \nintroduction to the wider area of probabilistic methods for representing and reasoning with \nknowledge. \n \nLectures \nIntroduction to learning and inference. Supervised, unsupervised, semi-supervised and \nreinforcement learning. Bayesian inference in general. What the naive Bayes method actually \ndoes. Review of backpropagation. Other kinds of learning and inference. [1 lecture] \nHow to classify optimally. Treating learning probabilistically. Bayesian decision theory and Bayes \noptimal classification. Likelihood functions and priors. Bayes theorem as applied to supervised \nlearning. The maximum likelihood and maximum a posteriori hypotheses. What does this teach \nus about the backpropagation algorithm? [2 lectures] \nLinear classifiers I. Supervised learning via error minimization. Iterative reweighted least \nsquares. The maximum margin classifier. [2 lectures] \nGaussian processes. Learning and inference for regression using Gaussian process models. [2 \nlectures] \nSupport vector machines (SVMs). The kernel trick. Problem formulation. Constrained \noptimization and the dual problem. SVM algorithm. [2 lectures] \nPractical issues. Hyperparameters. Measuring performance. Cross-validation. Experimental \nmethods. [1 lecture] \nLinear classifiers II. The Bayesian approach to neural networks. [1 lecture] \nUnsupervised learning I. The k-means algorithm. Clustering as a maximum likelihood problem. \n[1 lecture] \nUnsupervised learning II. The EM algorithm and its application to clustering. [1 lecture] \nBayesian networks I. Representing uncertain knowledge using Bayesian networks. Conditional \nindependence. Exact inference in Bayesian networks. [2 lectures] \nBayesian networks II. Markov random fields. Approximate inference. Markov chain Monte Carlo \nmethods. [1 lecture] \nObjectives \nAt the end of this course students should: \n \nUnderstand how learning and inference can be captured within a probabilistic framework, and \nknow how probability theory can be applied in practice as a means of handling uncertainty in AI \nsystems. \nUnderstand several algorithms for machine learning and apply those methods in practice with \nproper regard for good experimental practice. \nRecommended reading \nIf you are going to buy a single book for this course I recommend: \n \n\n* Bishop, C.M. (2006). Pattern recognition and machine learning. Springer. \n \nThe course text for Artificial Intelligence I: \n \nRussell, S. and Norvig, P. (2010). Artificial intelligence: a modern approach. Prentice Hall (3rd \ned.). \n \ncovers some relevant material but often in insufficient detail. Similarly: \n \nMitchell, T.M. (1997). Machine Learning. McGraw-Hill. \n \ngives a gentle introduction to some of the course material, but only an introduction. \n \nRecently a few new books have appeared that cover a lot of relevant ground well. For example: \n \nBarber, D. (2012). Bayesian Reasoning and Machine Learning. Cambridge University Press. \nFlach, P. (2012). Machine Learning: The Art and Science of Algorithms that Make Sense of \nData. Cambridge University Press. \nMurphy, K.P. (2012). Machine Learning: A Probabilistic Perspective. MIT Press.\n \n\nOPTIMISING COMPILERS \n \nAims \nThe aims of this course are to introduce the principles of program optimisation and related \nissues in decompilation. The course will cover optimisations of programs at the abstract syntax, \nflowgraph and target-code level. It will also examine how related techniques can be used in the \nprocess of decompilation. \n \nLectures \nIntroduction and motivation. Outline of an optimising compiler. Optimisation partitioned: analysis \nshows a property holds which enables a transformation. The flow graph; representation of \nprogramming concepts including argument and result passing. The phase-order problem. \nKinds of optimisation. Local optimisation: peephole optimisation, instruction scheduling. Global \noptimisation: common sub-expressions, code motion. Interprocedural optimisation. The call \ngraph. \nClassical dataflow analysis. Graph algorithms, live and avail sets. Register allocation by register \ncolouring. Common sub-expression elimination. Spilling to memory; treatment of \nCSE-introduced temporaries. Data flow anomalies. Static Single Assignment (SSA) form. \nHigher-level optimisations. Abstract interpretation, Strictness analysis. Constraint-based \nanalysis, Control flow analysis for lambda-calculus. Rule-based inference of program properties, \nTypes and effect systems. Points-to and alias analysis. \nTarget-dependent optimisations. Instruction selection. Instruction scheduling and its phase-order \nproblem. \nDecompilation. Legal/ethical issues. Some basic ideas, control flow and type reconstruction. \nObjectives \nAt the end of the course students should \n \nbe able to explain program analyses as dataflow equations on a flowgraph; \nknow various techniques for high-level optimisation of programs at the abstract syntax level; \nunderstand how code may be re-scheduled to improve execution speed; \nknow the basic ideas of decompilation. \nRecommended reading \n* Nielson, F., Nielson, H.R. and Hankin, C.L. (1999). Principles of program analysis. Springer. \nGood on part A and part B. \nAppel, A. (1997). Modern compiler implementation in Java/C/ML (3 editions). \nMuchnick, S. (1997). Advanced compiler design and implementation. Morgan Kaufmann. \nWilhelm, R. (1995). Compiler design. Addison-Wesley. \nAho, A.V., Sethi, R. and Ullman, J.D. (2007). Compilers: principles, techniques and tools. \nAddison-Wesley (2nd ed.).\n \n\nQUANTUM COMPUTING \n \nAims \nThe principal aim of the course is to introduce students to the basics of the quantum model of \ncomputation. The model will be used to study algorithms for searching, factorisation and \nquantum chemistry as well as other important topics in quantum information such as \ncryptography and super-dense coding. Issues in the complexity of computation will also be \nexplored. A second aim of the course is to introduce student to near-term quantum computing. \nTo this end, error-correction and adiabatic quantum computing are studied. \n \nLectures \nBits and qubits. Introduction to quantum states and measurements with motivating examples. \nComparison with discrete classical states. \nLinear algebra. Review of linear algebra: vector spaces, linear operators, Dirac notation, the \ntensor product. \nThe postulates of quantum mechanics. Postulates of quantum mechanics, incl. evolution and \nmeasurement. \nImportant concepts in quantum mechanics. Entanglement, distinguishing orthogonal and \nnon-orthogonal quantum states, no-cloning and no signalling. \nThe quantum circuit model. The circuit model of quantum computation. Quantum gates and \ncircuits. Universality of the quantum circuit model, and efficient simulation of arbitrary two-qubit \ngates with a standard universal set of gates. \nSome applications of quantum information. Applications of quantum information (other than \nquantum computation): quantum key distribution, superdense coding and quantum teleportation. \nDeutsch-Jozsa algorithm. Introducing Deutsch\u2019s problem and Deutsch\u2019s algorithm leading onto \nits generalisation, the Deutsch-Jozsa algorithm. \nQuantum search. Grover\u2019s search algorithm: analysis and lower bounds. \nQuantum Fourier Transform and Quantum Phase Estimation. Definition of the Quantum Fourier \nTransform (QFT), and efficient representation thereof as a quantum circuit. Application of the \nQFT to enable Quantum Phase Estimation (QPE). \nApplication 1 of QFT / QPE: Factoring. Shor\u2019s algorithm: reduction of factoring to period finding \nand then using the QFT for period finding. \nApplication 2 of QFT / QPE: Quantum Chemistry. Efficient simulation of quantum systems, and \napplications to real-world problems in quantum chemistry. \nQuantum complexity. Quantum complexity classes and their relationship to classical complexity. \nComparison with probabilistic computation. \nQuantum error correction. Introducing the concept of quantum error correction required for the \nfollowing lecture on fault-tolerance. \nFault tolerant quantum computing. Elements of fault tolerant computing; the threshold theorem \nfor efficient suppression of errors. \nAdiabatic quantum computing and quantum optimisation. The quantum adiabatic theorem, and \nadiabatic optimisation. Quantum annealing and D-Wave. \nCase studies in near-term quantum computation. Examples of state-of-the-art quantum \nalgorithms and computers, including superconducting and networked quantum computers. \n\nObjectives \nAt the end of the course students should: \n \nunderstand the quantum model of computation and the basic principles of quantum mechanics; \nbe familiar with basic quantum algorithms and their analysis; \nbe familiar with basic quantum protocols such as teleportation and superdense coding; \nsee how the quantum model relates to classical models of deterministic and probabilistic \ncomputation. \nappreciate the importance of efficient error-suppression if quantum computation is to yield an \nadvantage over classical computation. \ngain a general understanding of the important topics in near-term quantum computing, including \nadiabatic quantum computing. \nRecommended reading \nBooks: \nNielsen M.A., Chuang I.L. (2010). Quantum Computation and Quantum Information. Cambridge \nUniversity Press. \nMermin N.D. (2007). Quantum Computer Science: An Introduction. Cambridge University Press. \nMcGeoch, C. (2014). Adiabatic Quantum Computation and Quantum Annealing Theory and \nPractice. Morgan and Claypool. https://ieeexplore.ieee.org/document/7055969 \n \nPapers: \n \nBraunstein S.L. (2003). Quantum computation tutorial. Available at: \nhttps://www-users.cs.york.ac.uk/~schmuel/comp/comp_best.pdf \nAharonov D., Quantum computation [arXiv:quant-ph/9812037] \nAlbash T., Adiabatic Quantum Computing https://arxiv.org/pdf/1611.04471.pdf \nMcCardle S. et al, Quantum computational chemistry https://arxiv.org/abs/1808.10402 \n \nOther lecture notes: \n \nAndrew Childs (University of Maryland): http://cs.umd.edu/~amchilds/qa/ \n \n\nRANDOMISED ALGORITHMS \n \nAims: \nThe aim of this course is to introduce advanced techniques in the design and analysis \nalgorithms, with a strong focus on randomised algorithms. It covers essential tools and ideas \nfrom probability, optimisation and graph theory, and develops this knowledge through a variety \nof examples of algorithms and processes. This course will demonstrate that randomness is not \nonly an important design technique which often leads to simpler and more elegant algorithms, \nbut may also be essential in settings where time and space are restricted.   \n \nLectures: \nIntroduction. Course Overview. Why Randomised Algorithms? A first Randomised Algorithm for \nthe MAX-CUT problem. [1 Lecture]  \n \nConcentration Inequalities. Moment-Generating Functions and Chernoff Bounds. Extension: \nMethod of Bounded Independent Differences. Applications: Balls-into-Bins, Quick-Sort and Load \nBalancing. [approx. 2 Lectures] \n \nMarkov Chains and Mixing Times. Random Walks on Graphs. Application: Randomised \nAlgorithm for the 2-SAT problem. Mixing Times of Markov Chains. Application: Load Balancing \non Networks. [approx. 2 Lectures] \n \nLinear Programming and Applications. Definitions and Applications. Formulating Linear \nPrograms. The Simplex Algorithm. Finding Initial Solutions. How to use Linear Programs and \nBranch & Bound to Solve a Classical TSP instance. [approx. 3 Lectures] \n \nRandomised Approximation Algorithms. Randomised Approximation Schemes. Linearity of \nExpectations, Derandomisation. Deterministic and Randomised Rounding of Linear Programs. \nApplications: MAX3-SAT problem, Weighted Vertex Cover, Weighted Set Cover, MAX-SAT. \n[approx. 2 Lectures] \n \nSpectral Graph Theory and Clustering. Eigenvalues of Graphs and Matrices: Relations between \nEigenvalues and Graph Properties, Spectral Graph Drawing. Spectral Clustering: Conductance, \nCheeger's Inequality. Spectral Partitioning Algorithm [approx. 2 Lectures] \n \nObjectives: \nBy the end of the course students should be able to: \n \nlearn how to use randomness in the design of algorithms, in particular, approximation \nalgorithms; \nlearn the basics of linear programming, integer programming and randomised rounding; \napply randomisation to various problems coming from optimisation, machine learning and data \nscience; \nuse results from probability theory to analyse the performance of randomised algorithms. \n\nRecommended reading \n* Michael Mitzenmacher and Eli Upfal. Probability and Computing: Randomized Algorithms and \nProbabilistic Analysis., Cambridge University Press, 2nd edition. \n \n* David P. Williamson and David B. Shmoys. The Design of Approximation Algorithms, \nCambridge University Press, 2011 \n \n* Cormen, T.H., Leiserson, C.D., Rivest, R.L. and Stein, C. (2009). Introduction to Algorithms. \nMIT Press (3rd ed.). ISBN 978-0-262-53305-8 \n \n \n\nCLOUD COMPUTING \nAims \nThis module aims to teach students the fundamentals of Cloud Computing covering topics such \nas virtualization, data centres, cloud resource management, cloud storage and popular cloud \napplications including batch and data stream processing. Emphasis is given on the different \nbackend technologies to build and run efficient clouds and the way clouds are used by \napplications to realise computing on demand. The course will include practical tutorials on cloud \ninfrastructure technologies. Students will be assessed via a Cloud-based coursework project. \n \nLectures \nIntroduction to Cloud Computing \nData centres \nVirtualization I \nVirtualization II \nMapReduce \nMapReduce advanced \nResource management for virtualized data centres \nCloud storage \nCloud-based data stream processing \nObjectives \nBy the end of the course students should: \n \nunderstand how modern clouds operate and provide computing on demand; \nunderstand about cloud availability, performance, scalability and cost; \nknow about cloud infrastructure technologies including virtualization, data centres, resource \nmanagement and storage; \nknow how popular applications such as batch and data stream processing run efficiently on \nclouds; \nAssessment \nAssignment 1, worth 55% of the final mark. Develop batch processing applications running on a \ncluster assessed through automatic testing, code inspection and a report. \nAssignment 2, worth 30% of the final mark: Design and develop a framework for running the \nbatch processing applications from Assignment 1 on a cluster according to certain resource \nallocation policies. This assigment will be assessed by an expert, testing, code inspection and a \nreport. \nAssignment 3, worth 15% of the final mark: Write two individual project reports describing your \nwork for assignments 1 and 2. Write two sets of scripts to manage and run your code. Reports \nand scripts will be submitted with each assignment. \nRecommended reading \nMarinescu, D.C. Cloud Computing, Theory and Practice. Morgan Kaufmann. \nBarham, P., et. al. (2003). \u201cXen and the Art of Virtualization\u201d. In Proceedings of SOSP 2003. \nCharkasova, L., Gupta, D. and Vahdat, A. (2007). \u201cComparison of the Three CPU Schedulers in \nXen\u201d. In SIGMETRICS 2007. \n\nDean, J. and Ghemawat, S. (2004). \u201cMapReduce: Simplified Data Processing on Large \nClusters\u201d. In Proceedings of OSDI 2004. \nZaharia, M, et al. (2008). \u201cImproving MapReduce Performance in Heterogeneous \nEnvironments\u201d. In Proceedings of OSDI 2008. \nHindman, A., et al. (2011). \u201cMesos: A Platform for Fine-Grained Resource Sharing in Data \nCenter\u201d. In Proceedings of NSDI 2011. \nSchwarzkopf, M., et al. (2013). \u201cOmega: Flexible, Scalable Schedulers for Large Compute \nClusters\u201d. In EuroSys 2013. \nGhemawat, S. (2003). \u201cThe Google File System\u201d. In Proceedings of SOSP 2003. \nChang, F. (2006). \u201cBigtable: A Distributed Storage System for Structured Data\u201d. In Proceedings \nof OSDI 2006. \nFernandez, R.C., et al. (2013). \u201cIntegrating Scale Out and Fault Tolerance in Stream Processing \nusing Operator State Management\u201d. In SIGMOD 2013.\n \n\nCOMPUTER SYSTEMS MODELLING \n \nAims \n \nThe aims of this course are to introduce the concepts and principles of mathematical modelling \n \nand simulation, with particular emphasis on using queuing theory and control theory for \nunderstanding the behaviour of computer and communications systems. \n \nSyllabus \n \n1.     Overview of computer systems modeling using both analytic techniques and simulation. \n \n2.     Introduction to discrete event simulation. \n \na.     Simulation techniques \n \nb.     Random number generation methods \n \nc.     Statistical aspects: confidence intervals, stopping criteria \n \nd.     Variance reduction techniques. \n \n3.     Stochastic processes (builds on starred material in Foundations of Data Science) \n \na.     Discrete and continuous stochastic processes. \n \nb.     Markov processes and Chapman-Kolmogorov equations. \n \nc.     Discrete time Markov chains. \n \nd.     Ergodicity and the stationary distribution. \n \ne.     Continuous time Markov chains. \n \nf.      Birth-death processes, flow balance equations. \n \ng.     The Poisson process. \n \n4.     Queueing theory \n \na.     The M/M/1 queue in detail. \n \n\nb.     The equilibrium distribution with conditions for existence and common performance \nmetrics. \n \nc.     Extensions of the M/M/1 queue: the M/M/k queue, the M/M/infinity queue. \n \nd.     Queueing networks. Jacksonian networks. \n \ne.     The M/G/1 queue. \n \n5.     Signals, systems, and transforms \n \na.     Discrete- and continuous-time convolution. \n \nb.     Signals. The complex exponential signal. \n \nc.     Linear Time-Invariant Systems. Modeling practical systems as an LTI system. \n \nd.     Fourier and Laplace transforms. \n \n6.     Control theory. \n \na.     Controlled systems. Modeling controlled systems. \n \nb.     State variables. The transfer function model. \n \nc.     First-order and second-order systems. \n \nd.     Feedback control. PID control. \n \ne.     Stability. BIBO stability. Lyapunov stability. \n \nf.      Introduction to Model Predictive Control. \n \nObjectives \n \nAt the end of the course students should \n \n\u00b7       Be aware of different approaches to modeling a computer system; their pros and cons \n \n\u00b7       Be aware of the issues in building a simulation of a computer system and analysing the \nresults obtained \n \n\u00b7       Understand the concept of a stochastic process and how they arise in practice \n \n\n\u00b7       Be able to build simple Markov models and understand the critical modelling assumptions \n \n\u00b7       Be able to solve simple birth-death processes \n \n\u00b7       Understand and use M/M/1 queues to model computer systems \n \n\u00b7       Be able to model a computer system as a linear time-invariant system \n \n\u00b7       Understand the dynamics of a second-order controlled system \n \n\u00b7       Design a PID control for an LTI system \n \n\u00b7       Understand what is meant by BIBO and Lyapunov stability \n \nAssessment \n \nThere will be one programming assignment to build a discrete event simulator and using it to \nsimulate an M/M/1 and an M/D/1 queue (worth 15% of the final marks). There will also be two \none-hour in-person assessments for the course on: Stochastic processes and Queueing theory \n(40%) and Signals, Systems, Transforms, and Control Theory (45%). \n \nReference books \n \nKeshav, S. (2012)*. Mathematical Foundations of Computer Networking. Addison-Wesley. A \ncustomized PDF will be made available to class participants. \n \nKleinrock, L. (1975). Queueing systems, vol. 1. Theory. Wiley. \n \nKraniauskas, Peter. Transforms in signals and systems. Addison-Wesley Longman, 1992. \n \nJain, R. (1991). The art of computer systems performance analysis. Wiley. \n \n \n\nCOMPUTING EDUCATION \n \nAims \nThis module considers various aspects of the teaching and learning of computer science in \nschool, including practical experience, and provides an introduction to the field of computing \neducation research. It is offered by the Raspberry Pi Computing Education Research Centre, \npart of the Department of Computer Science and Technology, in conjunction with local schools \nin the Cambridge area. Students will have the opportunity to observe, plan and deliver a lesson, \nand explore the teaching of computing in a secondary education setting. The lesson taught will \nbe on a topic within the computing/computer science curriculum in England but will be \nnegotiated with the teacher mentoring the school placement. In the sessions in the Department, \nstudents will be introduced to elements of pedagogy and assessment alongside current issues \nin computing education. The course can also serve as a useful introduction to research in \ncomputing education for those interested in this area. Assessment will include lesson planning, \npresentations, an in-person viva and a 3000-word report which will include evidence of \nengagement with relevant research underpinning the lessons and activities undertaken. An \ninformation session relating to the module will be available in the preceding Easter Term. The \narrangement of school placements and DBS checks will be carried out in Michaelmas Term, so \nstudents should be prepared to engage in these preparatory activities. \n \nLectures \nThis is not a traditional lecture course and the term will be structured as follows: \n \nWeek 1: Introduction to computing education & visit to school placement \nWeek 2: In school (observation, supporting teacher) \nWeek 3: In school (observation, supporting teacher) \nWeek 4: In school (co-teach / small group teaching) \nWeek 5: Computing pedagogy and assessment (school half-term) \nWeek 6: In school (co-teach / small group teaching) \nWeek 7: In school (teach part/whole lesson) \nWeek 8: Presentations \nObjectives \nBy the end of the course students should: \n \ngain experience of computing education in practice \ndemonstrate an ability to communicate an aspect of computer science clearly and effectively \nbe able to plan and design a teaching activity/lesson with consideration of the audience \nbe able to thoroughly evaluate the design and implementation of a teaching/activity lesson \nbe able to demonstrate an understanding of relevant theories of learning and pedagogy from the \nliterature and how they apply to the learning/teaching of the topic under consideration \ngain an understanding of the breadth and depth of the field of computing education research \nClass Size \nThis module can accommodate up to 10 Part II students. \n \n\nRecommended reading \nBrown, N. C., Sentance, S., Crick, T., & Humphreys, S. (2014). Restart: The resurgence of \ncomputer science in UK schools. ACM Transactions on Computing Education (TOCE), 14(2), \n1-22. https://doi.org/10.1145/2602484 \n \nSentance, S., Barendsen, E., Howard, N. R., & Schulte, C. (Eds.). (2023). Computer science \neducation: Perspectives on teaching and learning in school. Bloomsbury Publishing. \n \nGrover, S. (Ed). 2020. Computer Science in K-12: An A-to-Z Handbook on Teaching \nProgramming. Edfinity. https://www.shuchigrover.com/atozk12cs/ \n \nRaspberry Pi Foundation (2022). Hello World Big Book of Pedagogy. \nhttps://www.raspberrypi.org/hello-world/issues/the-big-book-of-computing-pedagogy \n \nAssessment \nWeek 2: Reflections on an observed lesson (10%, graded out of 20) \nWeek 4: Submission of a lesson plan outline (10% graded out of 20) \nWeek 8: Delivery of an oral presentation relating to the lesson delivery (10% graded out of 20) \nAfter course completion: 3000 word report (60%), graded out of 20 \nViva with assessor (10%) (pass/fail) \nFurther Information \nWe will find placements for students in secondary schools within cycling or bus distance from \nthe Department/centre of Cambridge. There will be a timetabled slot for the course that students \ncan use to go to their school placement, but students may need to be flexible and liaise with the \nschool to find the best time for working with a specific class. \n \n \n\nDEEP NEURAL NETWORKS \n \nObjectives \nYou will gain detailed knowledge of \n \nCurrent understanding of generalization in neural networks vs classical statistical models. \nOptimization procedures for neural network models such as stochastic gradient descent and \nADAM. \nAutomatic differentiation and at least one software framework (PyTorch, TensorFolow) as well as \nan overview of other software approaches. \nArchitectures that are deployed to deal with different data types such as images or sequences \nincluding a. convolutional networks b. recurrent networks and c. transformers. \nIn addition you will gain knowledge of more advanced topics reflecting recent research in \nmachine learning. These change from year to year, examples include: \n \nApproaches to unsupervised learning including autoencoders and self-supervised learning.. \nTechniques for deploying models in low data regimes such as transfer learning and \nmeta-learning. \nTechniques for propagating uncertainty such as Bayesian neural networks. \nReinforcement learning and its applications to robotics or games. \nGraph Neural Networks and Geometric Deep Learning. \nTeaching Style \nThe start of the course will focus on the latest undertanding of current theory of neural networks, \ncontrasting with previous classical understandings of generalization performance. Then we will \nmove to practical examples of network architectures and deployment. We will end with more \nadvanced topics reflecting current research, mostly delivered by guest lecturers from the \nDepartment and beyond. \n \nSchedule \nWeek 1 \nTwo lectures: Generalization and Approximation with Neural Networks \n \nWeek 2 \nTwo lectures: Optimization: Stochastic Gradient Descent and ADAM \n \nWeek 3 \nTwo lectures: Hardware Acceleration and Convolutional Neural Networks \n \nWeek 4 \nTwo lectures: Vanishing Gradients, Recurrent and Residual Networks, Sequence Modeling. \n \nWeek 5 \nTwo lectures: Attention, The Transformer Architecture, Large Language Models \n \n\nWeek 6-8 \nLectures and guest lectures from the special topics below. \n \nSpecial topics \nSelf-Supervised Learning, AutoEncoders, Generative Modeling of Images \nHardware Implementations \nReinforcement learning \nTransfer learning and meta-learning \nUncertainty and Bayesian Neural Networks \nGraph Neural Networks \nAssessment \nAssessment involves solving a number of exercises in a jupyter notebook environment. \nFamiliarity with the python programming language is assumed. The exercises rely on the \npytorch deep learning framework, the syntax of which we don\u2019t cover in detail in the lectures, \nstudents are referred to documentation and tutorials to learn this component of the assessment. \n \nAssignment 1 \n \n30% of the total marks, with guided exercises. \n \nAssignment 2 \n \n70% of the total marks, a combination of guided exercises and a more exploratory mini-project.\n\nEXTENDED REALITY \n \nSyllabus & Schedule \nWeek 1 \nIntroduction \nCourse schedule and concept \nThe history of AR/VR/XR \nApplications of AR/VR/XR \nOverview of the XR pipeline \u2013 3 main components of XR \nDisplay technologies and rendering \nMachine perception and input interfaces \nContent generation \u2013 geometry, appearance, motion \nOverview of the AR/VR/XR frameworks \nPractical 1 \nProject structure \nProject themes \nIntroduction to the XR programming platform \nWeek 2 \n3D scene representations and rendering \nObject representations with polygonal meshes \nEfficient rendering via rasterization \nPractical XR rendering of textured geometries \nTracking and pose estimation for XR \nRigid transforms \nCamera pose estimation \nBody/ hand tracking \nPractical 2 \nProject draft proposal due \nProject feedback \nRendering and displaying a first object \nWeek 3 \n3D scene capture and understanding \nDepth estimation \n3D geometry capture \nAppearance acquisition \nLighting estimation and relighting \nLighting models for 3D scenes \nEstimating lighting in a scene \nRelighting rendered models \nPractical 3 \nAcquiring and utilizing depth from sensors \nRelighting a displayed object \nWeek 4 \nPhysically-based simulation of rigid objects \n\nRigid-body dynamics \nCollision detection \nTime integration \nPhysically-based simulation of non-rigid volumetric phenomena \nParticle systems \nBasic fluid dynamics \nPractical 4 \nPractical due \nProject prototype due \nProject check-in \nSimulating and rendering a simple object \nWeek 5 \nAdvanced topics in XR I: AR/VR displays technologies \nMicro LED-based displays \nWaveguide-based displays \nNext generation displays \nWeek 6 \nAdvanced topics in XR II \u2013 Mobile AR \nComputer vision on limited hardware \nUX/UI challenges \nContent challenges \nProject due \n  \n \nAssessment \nA practical exercise, worth 20% of the mark. This will cover the basics of AR/VR development \nand some of the practical techniques the students will use in their projects. This is individual \nwork. No mobile device or GPU hours are needed. \nAn AR/VR project worth 80% of the marks: \nCourse projects will be designed by the students following the possible project themes proposed \nby the lecturer and will be checked by the lecturer for appropriateness. \nThe students will form groups of 2-3 to design, implement, report, and present the course \nproject. \nThe final mark will be an implementation mark (40%), a report mark (20%), and a \npresentation/demo mark (20%). Each team member will be evaluated based on their \ncontribution. \nEach project will have extensions to be completed only by the ACS students. Each student will \nwrite a different part of the report, whose author will be clearly marked. Each student will further \nsummarise her/his contributions to the project in the same report. \nNote: Submission is by one submission per group but work by each individual must be clearly \nlabelled.\n \n\nFEDERATED LEARNING: THEORY AND PRACTICE \n \nObjectives \nThis course aims to extend the machine learning knowledge available to students in Part I (or \npresent in typical undergraduate degrees in other universities), and allow them to understand \nhow these concepts can manifest in a decentralized setting. The course will consider both \ntheoretical (e.g., decentralized optimization) and practical (e.g., networking efficiency) aspects \nthat combine to define this growing area of machine learning.  \n \nAt the end of the course students should: \n \nUnderstand popular methods used in federated learning \nBe able to construct and scale a simple federated system \nHave gained an appreciation of the core limitations to existing methods, and the approaches \navailable to cope with these issues \nDeveloped an intuition for related technologies like differential privacy and secure aggregation, \nand are able to use them within typical federated settings  \nCan reason about the privacy and security issues with federated systems \nLectures \nCourse Overview. Introduction to Federated Learning. \nDecentralized Optimization. \nStatistical and Systems Heterogeneity. \nVariations of Federated Aggregation. \nSecure Aggregation. \nDifferential Privacy within Federated Systems. \nExtensions to Federated Analytics. \nApplications to Speech, Video, Images and Robotics. \nLab sessions \nFederating a Centralized ML Classifier. \nBehaviour under Heterogeneity. \nScaling a Federated Implementation. \nExploring Privacy with Federated Settings \nRecommended Reading \nReadings will be assigned for each lecture. Readings will be taken from either research papers, \ntutorials, source code or blogs that provide more comprehensive treatment of taught concepts. \n \nAssessment - Part II Students \nFour labs are performed during the course, and students receive 10% of their total grade for \nwork done as part of each lab. (For a total of 40% of the total grade from lab work alone). Labs \nwill primarily provide hands-on teaching opportunities, that are then utilized within the lab \nassignment which is completed outside of the lab contact time. MPhil and Part III students will \nbe given additional questions to answer within their version of the lab assignment which will \ndiffer from the assignment given to Part II CST students. \n \n\nThe remainder of the course grade (60%) will be given based on a hands-on project that applies \nthe concepts taught in lectures and labs. This hands-on project will be assessed based on upon \na combination of source code, related documentation and brief 8-minute pre-recorded talk that \nsummarizes key project elements (any slides used are also submitted as part of the project). \nPlease note, that in the case of Part II CST students, the talk itself is not examinable -- as such \nwill be made optional to those students. \n \nA range of possible practical projects will be described and offered to students to select from, or \nalternatively students may propose their own. MPhil and Part III students will select from a \nproject pool that is separate from those offered to Part II CST students. MPhil and Part III \nprojects will contain a greater emphasis on a research element, and the pre-recorded talks \nprovided by this student group will focus on this research contribution. The project will be \nassessed on the level of student understanding demonstrated, the degree of difficulty, \ncorrectness of implementation -- and for Part III/MPhil students the additional criteria of the \nquality and execution of the research methodology, and depth and quality of results analysis. \n \nThis project can be done individually or in groups -- although individual projects will be strongly \nencouraged. It will be required the project is performed using a code repository that also will \ncontain all documentation -- access to this repository will be shared with course staff (e.g., \nlecturer and TAs). Where needed, marks assigned to students within a group will be \ndifferentiated using this repository as an input. Furthermore if groups are formed, members must \nbe either entirely from Part III/MPhil students or Part II CST, i.e., these two student groups \nshould not mix to form a project group. \n \nProjects will be made available publicly. A maximum word count for written contributions for the \nproject will be enforced.\n \n\nMOBILE HEALTH \n \nAims \nThe course aims to explore how wearable and mobile systems sensors can be used to gather \ndata relevant to understand health, how the data can be analysed with advanced signal \nprocessing and machine learning and the performance of these systems in terms of diagnostics \nand disease progression detection. \n \nSyllabus \nCourse Overview. Introduction to Mobile Health. Evaluation metrics and methodology. Basics of \nSignal Processing. \nInertial Measurement Units, Human Activity Recognition (HAR) and Gait Analysis and Machine \nLearning for IMU data. \nRadios, Bluetooth, GPS and Cellular. Epidemiology and contact tracing, Social interaction \nsensing and applications. Location tracking in health monitoring and public health. \nAudio Signal Processing. Voice and Speech Analysis: concepts and data analysis. Body \nSounds analysis. \nPhotoplethysmogram and Light sensing for health (heart and sleep) \nContactless and wireless behaviour and physiological monitoring \nGenerative Models for Wearable Health Data \nTopical Guest Lectures \nObjectives \nThe course aims to explore how wearable and mobile systems sensors can be used to gather \ndata relevant to understand health, how the data can be analysed with advanced signal \nprocessing and machine learning and the performance of these systems in terms of diagnostics \nand disease progression detection. \n \nRoughly, each lecture contains a theory part about the working of \u201csensor signals\u201d or \u201cdata \nanalysis methods\u201d and an application part which contextualises the concepts. \n \nAt the end of the course students should: Understand how mobile/wearable sensors capture \ndata and their working. Understand different approaches to acquiring and analysing sensor data \nfrom different types of sensors. Understand the concept of signal processing applied to time \nseries data and their practical application in health. Be able to extract sensor data and analyse it \nwith basic signal processing and machine learning techniques. Be aware of the different health \napplications of the various sensor techniques. The course will also touch on privacy and ethics \nimplications of the approaches developed in an orthogonal fashion. \n \nRecommended Reading \nPlease see Course Materials for recommended reading for each session. \n \nAssessment - Part II students \nTwo assignments will be based on two datasets which will be provided to the students: \n \n\nAssignment 1 (shared for Part II and Part III/MPhil): this will be based on a dataset and will be \nworth 40% of the final mark. The task of the assessment will be to perform pre-processing and \nbasic data analysis in a \"colab\" and an answer sheet of no more than 1000 words. \n \nAssignment 2 (Part II): This assignment (worth 60% of the final mark) will be a fuller analysis of \na dataset focusing on machine learning algorithms and metrics. Discussion and interpretation of \nthe findings will be reported in a colab and a report of no more than 1200 words.\n \n\nMULTICORE SEMANTICS AND PROGRAMMING \n \nAims \nIn recent years multiprocessors have become ubiquitous, but building reliable concurrent \nsystems with good performance remains very challenging. The aim of this module is to \nintroduce some of the theory and the practice of concurrent programming, from hardware \nmemory models and the design of high-level programming languages to the correctness and \nperformance properties of concurrent algorithms. \n \nLectures \nPart 1: Introduction and relaxed-memory concurrency [Professor P. Sewell] \n \nIntroduction. Sequential consistency, atomicity, basic concurrent problems. [1 block] \nConcurrency on real multiprocessors: the relaxed memory model(s) for x86, ARM, and IBM \nPower, and theoretical tools for reasoning about x86-TSO programs. [2 blocks] \nHigh-level languages. An introduction to C/C++11 and Java shared-memory concurrency. [1 \nblock] \nPart 2: Concurrent algorithms [Dr T. Harris] \n \nConcurrent programming. Simple algorithms (readers/writers, stacks, queues) and correctness \ncriteria (linearisability and progress properties). Advanced synchronisation patterns (e.g. some \nof the following: optimistic and lazy list algorithms, hash tables, double-checked locking, RCU, \nhazard pointers), with discussion of performance and on the interaction between algorithm \ndesign and the underlying relaxed memory models. [3 blocks] \nResearch topics, likely to include one hour on transactional memory and one guest lecture. [1 \nblock] \nObjectives \nBy the end of the course students should: \n \nhave a good understanding of the semantics of concurrent programs, both at the multprocessor \nlevel and the C/Java programming language level; \nhave a good understanding of some key concurrent algorithms, with practical experience. \nAssessment - Part II Students \nTwo assignments each worth 50% \n \nRecommended reading \nHerlihy, M. and Shavit, N. (2008). The art of multiprocessor programming. Morgan Kaufmann.\n\nBUSINESS STUDIES SEMINARS \nAims \nThis course is a series of seminars by former members and friends of the Laboratory about their \nreal-world experiences of starting and running high technology companies. It is a follow on to \nthe Business Studies course in the Michaelmas Term. It provides practical examples and case \nstudies, and the opportunity to network with and learn from actual entrepreneurs. \n \nLectures \nEight lectures by eight different entrepreneurs. \n \nObjectives \nAt the end of the course students should have a better knowledge of the pleasures and pitfalls \nof starting a high tech company. \n \nRecommended reading \nLang, J. (2001). The high-tech entrepreneur\u2019s handbook: how to start and run a high-tech \ncompany. FT.COM/Prentice Hall. \nMaurya, A. (2012). Running Lean: Iterate from Plan A to a Plan That Works. O\u2019Reilly. \nOsterwalder, A. and Pigneur, Y. (2010). Business Model Generation: A Handbook for \nVisionaires, Game Changers, and Challengers. Wiley. \nKim, W. and Mauborgne, R. (2005). Blue Ocean Strategy. Harvard Business School Press. \n \nSee also the additional reading list on the Business Studies web page.\n \n\nHOARE LOGIC AND MODEL CHECKING \n \nAims \nThe course introduces two verification methods, Hoare Logic and Temporal Logic, and uses \nthem to formally specify and verify imperative programs and systems. \n \nThe first aim is to introduce Hoare logic for a simple imperative language and then to show how \nit can be used to formally specify programs (along with discussion of soundness and \ncompleteness), and also how to use it in a mechanised program verifier. \n \nThe second aim is to introduce model checking: to show how temporal models can be used to \nrepresent systems, how temporal logic can describe the behaviour of systems, and finally to \nintroduce model-checking algorithms to determine whether properties hold, and to find \ncounter-examples. \n \nCurrent research trends also will be outlined. \n \nLectures \nPart 1: Hoare logic. Formal versus informal methods. Specification using preconditions and \npostconditions. \nAxioms and rules of inference. Hoare logic for a simple language with assignments, sequences, \nconditionals and while-loops. Syntax-directedness. \nLoops and invariants. Various examples illustrating loop invariants and how they can be found. \nPartial and total correctness. Hoare logic for proving termination. Variants. \nSemantics and metatheory Mathematical interpretation of Hoare logic. Semantics and \nsoundness of Hoare logic. \nSeparation logic Separation logic as a resource-aware reinterpretation of Hoare logic to deal \nwith aliasing in programs with pointers. \nPart 2: Model checking. Models. Representation of state spaces. Reachable states. \nTemporal logic. Linear and branching time. Temporal operators. Path quantifiers. CTL, LTL, and \nCTL*. \nModel checking. Simple algorithms for verifying that temporal properties hold. \nApplications and more recent developments Simple software and hardware examples. CEGAR \n(counter-example guided abstraction refinement). \nObjectives \nAt the end of the course students should \n \nbe able to prove simple programs correct by hand and implement a simple program verifier; \nbe familiar with the theory and use of separation logic; \nbe able to write simple models and specify them using temporal logic; \nbe familiar with the core ideas of model checking, and be able to implement a simple model \nchecker. \nRecommended reading \n\nHuth, M. and Ryan M. (2004). Logic in Computer Science: Modelling and Reasoning about \nSystems. Cambridge University Press (2nd ed.).\n \n\n",
        "7th Semester \nADVANCED TOPICS IN COMPUTER ARCHITECTURE \nAims \nThis course aims to provide students with an introduction to a range of advanced topics in \ncomputer architecture. It will explore the current and future challenges facing the architects of \nmodern computers. These will also be used to illustrate the many different influences and \ntrade-offs involved in computer architecture. \n \nObjectives \nOn completion of this module students should: \n \nunderstand the challenges of designing and verifying modern microprocessors \nbe familiar with recent research themes and emerging challenges \nappreciate the complex trade-offs at the heart of computer architecture \nSyllabus \nEach seminar will focus on a different topic. The proposed topics are listed below but there may \nbe some minor changes to this: \n \nTrends in computer architecture \nState-of-the-art microprocessor design \nMemory system design \nHardware reliability \nSpecification, verification and test (may be be replaced with a different topic) \nHardware security (2) \nHW accelerators and accelerators for machine learning \nEach two hour seminar will include three student presentations (15mins) questions (5mins) and \na broader discussion of the topics (around 30mins). The last part of the seminar will include a \nshort scene setting lecture (around 20mins) to introduce the following week's topic. \n \nAssessment \nEach week students will compare and contrast two of the main papers and submit a written \nsummary and review in advance of each seminar (except when presenting). \n \nStudents will be expected to give a number of 15 minute presentations. \n \nEssays and presentations will be marked out of 10. After dropping the lowest mark, the \nremaining marks will be scaled to give a final score out of 100. \n \nStudents will give at least one presentation during the course. They will not be required to \nsubmit an essay during the weeks they are presenting. \n \nEach presentation will focus on a single paper from the reading list. Marks will be awarded for \nclarity and the communication of the paper's key ideas, an analysis of the work's strengths and \nweaknesses and the work\u2019s relationship to related work and broader trends and constraints. \n\n \nRecommended prerequisite reading \nPatterson, D. A., Hennessy, J. L. (2017). Computer organization and design: The \nHardware/software interface RISC-V edition Morgan Kaufmann. ISBN 978-0-12-812275-4. \n \nHennessy, J. and Patterson, D. (2012). Computer architecture: a quantitative approach. Elsevier \n(5th ed.) ISBN 9780123838728. (the 3rd and 4th editions are also relevant)\n \n\nADVANCED TOPICS IN PROGRAMMING LANGUAGES \nAims \nThis module explores various topics in programming languages beyond the scope of \nundergraduate courses. It aims to introduce students to ideas, results and techniques found in \nthe literature and prepare them for research in the field. \n \nSyllabus and structure \nThe module consists of eight two-hour seminars, each on a particular topic. Topics will vary from \nyear to year, but may include, for example, \n \nAbstract interpretation \nVerified software \nMetaprogramming \nBehavioural types \nProgram synthesis \nVerified compilation \nPartial evaluation \nGarbage collection \nDependent types \nAutomatic differentiation \nDelimited continuations \nModule systems \nThere will be three papers assigned for each topic, which students are expected to read before \nthe seminar. \nEach seminar will include three 20 minute student presentations (15 minutes + 5 minutes \nquestions), time for general discussion of the topic, and a brief overview lecture for the following \nweek\u2019s topic. \nBefore each seminar, except in weeks in which they give presentations, students will submit a \nshort essay about two of the papers. \n \n \nObjectives \nOn completion of this module, students should \n \nbe able to identify some major themes in programming language research \nbe familiar with some classic papers and recent advances \nhave an understanding of techniques used in the field \n \nAssessment \nAssessment consists of: \n \nPresentation of one of the papers from the reading list (typically once or twice in total for each \nstudent, depending on class numbers) \nOne essay per week (except on the first week and on presentation weeks) \n\nAll essays and presentations carry equal numbers of marks. \n \nEssay marks are awarded for understanding, for insight and analysis, and for writing quality. \nEssays should be around 1500 words (with a lower limit of 1450 and upper limit of 1650). \nPresentation marks are awarded for clarity, for effective communication, and for selection and \norganisation of topics. \n \nThere will be seven submissions (essays or presentations) in total and, as in other courses, the \nlowest mark for each student will be disregarded when computing the final mark. \n \nMarking, deadlines and extensions will be handled in accordance with the MPhil Assessment \nGuidelines. \n \nRecommended reading material and resources \nResearch and survey papers from programming language conferences and journals (e.g. \nPOPL, PLDI, TOPLAS, FTPL) will be assigned each week. General background material may \nbe found in: \n \n\u2022 Types and Programming Languages (Benjamin C. Pierce) \n   The MIT Press \n   ISBN 0-262-16209-1 \n \n\u2022 Advanced Topics in Types and Programming Languages (ed. Benjamin C. Pierce) \n   The MIT Press \n   ISBN 0-262-16228-8 \n \n\u2022 Practical Foundations for Programming Languages (Robert Harper) \n   Cambridge University Press \n   9781107150300\n \n\nAFFECTIVE ARTIFICIAL INTELLIGENCE \n \nSynopsis \nAffective Artificial Intelligence (Affective AI) aims to imbue machines with social and emotional \nintelligence (EQ). More specifically, Affective AI aims to create artificially intelligent systems and \nmachines that can recognize, interpret, process, and simulate human social signals and \nbehaviours, expressions, and emotions, to enhance human-AI interaction and communication. \n\\r\\n\\r\\nTo achieve this goal, Affective AI draws upon various scientific disciplines, including \nmachine learning, computer vision, speech / natural language / signal processing, psychology \nand cognitive science, and ethics and social sciences. \n \n  \nBackground & Aims \nAffective Artificial Intelligence (Affective AI) aims to imbue machines with social and emotional \nintelligence (EQ). More specifically, Affective AI aims to create artificially intelligent systems and \nmachines that can recognize, interpret, process, and simulate human social signals and \nbehaviours, expressions, and emotions, to enhance human-AI interaction and communication.  \n \nTo achieve this goal, Affective AI draws upon various scientific disciplines, including machine \nlearning, computer vision, speech / natural language / signal processing, psychology and \ncognitive science, and ethics and social sciences. \n \nAffective AI has direct applications in and implications for the design of innovative interactive \ntechnology (e.g., interaction with chat bots, virtual agents, robots), single and multi-user smart \nenvironments ( e.g., in-car/ virtual / augmented / mixed reality, serious games), public speaking \nand cognitive training, and clinical and biomedical studies (e.g., autism, depression, pain). \n \nThe aim of this module is to impart knowledge and ability needed to make informed choices of \nmodels, data, and machine learning techniques for sensing, recognition, and generation of \naffective and social behaviour (e.g., smile, frown, head nodding/shaking, \nagreement/disagreement) in order to create Affectively intelligent AI systems, with a \nconsideration for various ethical issues (e.g., privacy, bias) arising from the real-world \ndeployment of these systems. \n \n  \n \nSyllabus \nThe following list provides a representative list of topics: \n \nIntroduction, definitions, and overview \nTheories from various disciplines \nSensing from multiple modalities (e.g., vision, audio, bio signals, text) \nData acquisition and annotation \nSignal processing / feature extraction \n\nLearning / prediction / recognition and evaluation \nBehaviour synthesis / generation (e.g., for embodied agents / robots) \nAdvanced topics and ethical considerations (e.g., bias and fairness) \nCross-disciplinary applications (via seminar presentations and discussions) \nGuest lectures (diverse topics \u2013 changes each year) \nHands-on research and programming work (i.e., mini project & report)  \n \n  \n \nObjectives \nOn completion of this module, students will: \n \nunderstand and demonstrate knowledge in key characteristics of affectively intelligent AI, which \ninclude: \nRecognition: How to equip Affective AI systems with capabilities of analysing facial expressions, \nvocal intonations, gestures, and other physiological signals to infer human affective states? \nGeneration: How to enable affectively intelligent AI simulate expressions of emotions in \nmachines, allowing them to respond in a more human-like manner, such as virtual agents and \nhumanoid robots, showing empathy or sympathy? \nAdaptation and Personalization: How to enable affectively intelligent AI adapt system responses \nbased on users' affective states and/or needs, or tailor interactions and experiences to individual \nusers based on their expressivity / emotional profiles, personalities, and past interactions? \nEmpathetic Communication: How to design Affective AI systems to communicate with users in a \nway that demonstrates empathy, understanding, and sensitivity to their affective states and \nneeds? \nEthical and societal considerations: What are the various human differences, ethical guidelines, \nand societal impacts that need to be considered when designing and deploying Affective AI to \nensure that these systems respect users' privacy, autonomy, and well-being? \ncomprehend and apply (appropriate) methods for collection, analysis, representation, and \nevaluation of human affective and communicative behavioural data. \nenhance programming skills for creating and implementing (components of) Affective AI \nsystems. \ndemonstrate critical thinking, analysis and synthesis while deciding on 'when' and 'how' to \nincorporate human affect and social signals in a specific AI system context and gain practical \nexperience in proposing and justifying computational solution(s) of suitable nature and scope. \n  \n \nAssessment - Part II Students \nSeminar presentation: 20% \nParticipating in Q&A and discussions: 10% \nMid-term mini project report and presentation (as a team of 2): 10%  \nFinal mini project report & code (as a team of 2): 60% \n  \n \n\nAssessment - MPhil / Part III Students \nSeminar presentation: 20% \nParticipating in Q&A and discussions: 10% \nMid-term mini project report and presentation: 10%  \nFinal mini project report & code: 60% \nFurther Information \nCurrent Cambridge undergraduate students who are continuing onto Part III or the MPhil in \nAdvanced Computer Science may only take this module if they did NOT take it as a Unit of \nAssessment in Part II. \n \nThis module is shared with ACS. Assessment will be adjusted for the two groups of students to \nbe at an appropriate level for whichever course the student is enrolled on. More information will \nfollow at the first lecture. \n \nThis module is shared with Part II of the Computer Science Tripos. Assessment will be adjusted \nfor the two groups of students to be at an appropriate level for whichever course the student is \nenrolled on. Further information about assessment and practicals will follow at the first lecture.\n\nCATEGORY THEORY \n \nAims \nCategory theory provides a unified treatment of mathematical properties and constructions that \ncan be expressed in terms of 'morphisms' between structures. It gives a precise framework for \ncomparing one branch of mathematics (organized as a category) with another and for the \ntransfer of problems in one area to another. Since its origins in the 1940s motivated by \nconnections between algebra and geometry, category theory has been applied to diverse fields, \nincluding computer science, logic and linguistics. This course introduces the basic notions of \ncategory theory: adjunction, natural transformation, functor and category. We will use category \ntheory to organize and develop the kinds of structure that arise in models and semantics for \nlogics and programming languages. \n \nSyllabus \nIntroduction; some history. Definition of category. The category of sets and functions. \nCommutative diagrams. Examples of categories: preorders and monotone functions; monoids \nand monoid homomorphisms; a preorder as a category; a monoid as a category. Definition of \nisomorphism. Informal notion of a 'category-theoretic' property. \nTerminal objects. The opposite of a category and the duality principle. Initial objects. Free \nmonoids as initial objects. \nBinary products and coproducts. Cartesian categories. \nExponential objects: in the category of sets and in general. Cartesian closed categories: \ndefinition and examples. \nIntuitionistic Propositional Logic (IPL) in Natural Deduction style. Semantics of IPL in a cartesian \nclosed preorder. \nSimply Typed Lambda Calculus (STLC). The typing relation. Semantics of STLC types and \nterms in a cartesian closed category (ccc). The internal language of a ccc. The \nCurry-Howard-Lawvere correspondence. \nFunctors. Contravariance. Identity and composition for functors. \nSize: small categories and locally small categories. The category of small categories. Finite \nproducts of categories. \nNatural transformations. Functor categories. The category of small categories is cartesian \nclosed. \nHom functors. Natural isomorphisms. Adjunctions. Examples of adjoint functors. Theorem \ncharacterising the existence of right (respectively left) adjoints in terms of a universal property. \nDependent types. Dependent product sets and dependent function sets as adjoint functors. \nEquivalence of categories. Example: the category of I-indexed sets and functions is equivalent \nto the slice category Set/I. \nPresheaves. The Yoneda Lemma. Categories of presheaves are cartesian closed. \nMonads. Modelling notions of computation as monads. Moggi's computational lambda calculus. \nObjectives \nOn completion of this module, students should: \n \n\nbe familiar with some of the basic notions of category theory and its connections with logic and \nprogramming language semantics \nAssessment \na graded exercise sheet (25% of the final mark), and \na take-home test (75%) \nRecommended reading \nAwodey, S. (2010). Category theory. Oxford University Press (2nd ed.). \n \nCrole, R. L. (1994). Categories for types. Cambridge University Press. \n \nLambek, J. and Scott, P. J. (1986). Introduction to higher order categorical logic. Cambridge \nUniversity Press. \n \nPitts, A. M. (2000). Categorical Logic. Chapter 2 of S. Abramsky, D. M. Gabbay and T. S. E. \nMaibaum (Eds) Handbook of Logic in Computer Science, Volume 5. Oxford University Press. \n(Draft copy available here.) \n \nClass Size \nThis module can accommodate upto 15 Part II students plus 15 MPhil / Part III students. \n \nPre-registration evaluation form  \nStudents who are interested in taking this module will be asked to complete a pre-registration \nevaluation form to determine whether they have enough mathematical background to take the \nmodule.  \n \nThis will take place during the week Monday 12 August - Friday 16 August.  \n \nCoursework \nThe coursework in this module will consist of a graded exercise sheet. \n \nFurther Information \nCurrent Cambridge undergraduate students who are continuing onto Part III or the MPhil in \nAdvanced Computer Science may only take this module if they did NOT take 'Catgeory Theory' \nas a Unit of Assessment in Part II. \n \nThis module is shared with Part II of the Computer Science Tripos. Assessment will be adjusted \nfor the two groups of students to be at an appropriate level for whichever course the student is \nenrolled on. Further information about assessment and practicals will follow at the first lecture.\n\nDIGITAL MONEY AND DECENTRALISED FINANCE \n \nAims \nOur society is evolving towards digital payments and the elimination of physical cash. Digital \nalternatives to cash require trade-offs between various properties including unforgeability, \ntraceability, divisibility, transferability without intermediaries, privacy, redeemability, control over \nthe money supply and many more, often in conflict with each other. Since the 1980s, \ncryptographers have proposed a variety of clever technical solutions addressing some of these \nissues but it is only with the appearance of Bitcoin, blockchain and a plethora of copycat \ncryptocurrencies that a new asset class has now emerged, worth in excess of two trillion dollars \n(although plagued by extreme volatility). Meanwhile, most major countries have been planning \nthe introduction of Central Bank Digital Currencies in an attempt to retain control. This \nseminar-style interactive module encourages the students to understand what's happening, \nwhat the underlying problems and opportunities are (technical, social and economic) and where \nwe should go from here. We are at a historical turning point where an enterprising candidate \nmight disrupt the status quo and truly make a difference. \n \nSyllabus \nUnderstanding money: from gold standard to digital cash \nIntro to the Decentralised Finance (DeFi) ecosystem: Bitcoin, blockchain, wallets, smart \ncontracts \nStablecoins and Central Bank Digital Currencies \nDecentralised Autonomous Organisations and Automated Market Makers \nYield Farming, Perpetual Futures and Maximal Extractable Value \nWeb3, Non Fungible Tokens \nCryptocurrency crashes; crimes facilitated by cryptocurrencies \nNew areas of development. Where from here? \nThe module is structured as a reading club with a high degree of interactivity. Aside from the first \nand last session, which are treated differently, the regular two-hour sessions have the following \nstructure (not necessarily in this order). \n \nTwo half-hour debates on each of two papers \nTwo rapid-fire presentations on open questions previously assigned by the lecturers. \nHalf an hour of presentation by the lecturers. \nAbout ten minutes of buffer that can account for switchover time or be absorbed into the \nlecturers' presentation. \nThe first session is scene-setting by the lecturers, with no student presentations. The last \nsession has no paper debates, only one rapid-fire presentation on an open question from each \nof the students. \n \nObjectives \nOn completion of this module, students will gain an in-depth understanding of digital money \nalternatives as well as many modern applications and components of Decentralised Finance. By \ncomparing DeFi with current processes in traditional banking, students will be able to discuss \n\nthe merits and limitations of these new technologies as well as to identify potential new areas of \ndevelopment. \n \nThrough having to write, present and defend very brief extended abstracts, the students will \nimprove their communication skills and in particular the ability to distil and convey the most \nimportant points forcefully and concisely. \n \n \nAssessment \nThe module is an interactive reading club. Each student produces four output triplets throughout \nthe course. Each output triplet consists of three parts: a 1200-word essay, a 200-word extended \nabstract and a 7-minute presentation. \n \nThe essay and abstract must be submitted in advance, using the LaTeX templates provided. \nThe abstract must consist of full sentences, not bullet points, and must fit on a single slide. \nDuring the presentation, the slide with the abstract will be projected on screen as the only visual \naid; the essay will not be accessible to either presenter or audience. Reading out the slide \nverbatim will obviously not count as a great presentation. Presenters must be able to expand \nand defend their ideas live, and answer questions from lecturers and audience. \n \nThe essays, the abstracts and the presentations are graded separately, each out of 10, and \nassigned weights of 60%, 20%, 20%, respectively, within the triplet. \n \nIn a regular 2-hour session (all but the first and last) there will be two paper slots and two \ninvestigation slots, plus a lecturer slot, described next. \n \nEach paper slot (30 min) involves a \u201csupporter\u201d and a \u201cdissenter\u201d for the paper, each \ncontributing one output triplet, and then a panel Q&A where all the other participants quiz the \nsupporter and dissenter. \n \nEach investigation slot (10 min) involves a single student contributing an output triplet on a \ngiven theme. \n \nRoles, papers and themes are assigned by the lecturers the previous week. \n \nIn the first session there will be no student presentations, only lecturer-led exploration. In the \nlast session there will be 12 investigation slots and no paper slots. \n \nOverall, each student will be supporter once, dissenter once and investigator twice, for a total of \n4 output triplets. The output triplets of the last session will be weighed twice as much as the \nothers, i.e. 40% each, while the others 20% each. \n \nAttendance is required at all sessions. Missing a session in which the candidate was due to \npresent will result in the loss of the corresponding presentation marks, while the written work will \n\nstill have to be submitted and will be marked as usual. Missing a session in which the candidate \nwas not due to present will result in the loss of 3%. \n \nRecommended reading \nThere isn't a textbook covering all the material in this course. The following textbook, also \nadopted by R47 Distributed Ledger Technologies (also freely available online), is a thorough \nintroduction to Bitcoin and Blockchain, but we will complement it with research papers and \nindustry white papers for each lecture. \n \nNarayanan, A. , Bonneau, J., Felten, E., Miller, A. and Goldfeder, S. (2016). Bitcoin and \nCryptocurrency Technologies: A Comprehensive Introduction. Princeton University Press. \n(2016 Draft available here: \nhttps://d28rh4a8wq0iu5.cloudfront.net/bitcointech/readings/princeton_bitcoin_book.pdf)\n \n\nDIGITAL SIGNAL PROCESSING \n \nAims \nThis course teaches basic signal-processing principles necessary to understand many modern \nhigh-tech systems, with examples from audio processing, image coding, radio communication, \nradar, and software-defined radio. Students will gain practical experience from numerical \nexperiments in programming assignments (in Julia, MATLAB or NumPy). \n \nLectures \nSignals and systems. Discrete sequences and systems: types and properties. Amplitude, \nphase, frequency, modulation, decibels, root-mean square. Linear time-invariant systems, \nconvolution. Some examples from electronics, optics and acoustics. \nPhasors. Eigen functions of linear time-invariant systems. Review of complex arithmetic. \nPhasors as orthogonal base functions. \nFourier transform. Forms and properties of the Fourier transform. Convolution theorem. Rect \nand sinc. \nDirac\u2019s delta function. Fourier representation of sine waves, impulse combs in the time and \nfrequency domain. Amplitude-modulation in the frequency domain. \nDiscrete sequences and spectra. Sampling of continuous signals, periodic signals, aliasing, \ninterpolation, sampling and reconstruction, sample-rate conversion, oversampling, spectral \ninversion. \nDiscrete Fourier transform. Continuous versus discrete Fourier transform, symmetry, linearity, \nFFT, real-valued FFT, FFT-based convolution, zero padding, FFT-based resampling, \ndeconvolution exercise. \nSpectral estimation. Short-time Fourier transform, leakage and scalloping phenomena, \nwindowing, zero padding. Audio and voice examples. DTFM exercise. \nFinite impulse-response filters. Properties of filters, implementation forms, window-based FIR \ndesign, use of frequency-inversion to obtain high-pass filters, use of modulation to obtain \nband-pass filters. \nInfinite impulse-response filters. Sequences as polynomials, z-transform, zeros and poles, some \nanalog IIR design techniques (Butterworth, Chebyshev I/II, elliptic filters, second-order cascade \nform). \nBand-pass signals. Band-pass sampling and reconstruction, IQ up and down conversion, \nsuperheterodyne receivers, software-defined radio front-ends, IQ representation of AM and FM \nsignals and their demodulation. \nDigital communication. Pulse-amplitude modulation. Matched-filter detector. Pulse shapes, \ninter-symbol interference, equalization. IQ representation of ASK, BSK, PSK, QAM and FSK \nsignals. [2 hours] \nRandom sequences and noise. Random variables, stationary and ergodic processes, \nautocorrelation, cross-correlation, deterministic cross-correlation sequences, filtered random \nsequences, white noise, periodic averaging. \nCorrelation coding. Entropy, delta coding, linear prediction, dependence versus correlation, \nrandom vectors, covariance, decorrelation, matrix diagonalization, eigen decomposition, \n\nKarhunen-Lo\u00e8ve transform, principal component analysis. Relation to orthogonal transform \ncoding using fixed basis vectors, such as DCT. \nLossy versus lossless compression. What information is discarded by human senses and can \nbe eliminated by encoders? Perceptual scales, audio masking, spatial resolution, colour \ncoordinates, some demonstration experiments. \nQuantization, image coding standards. Uniform and logarithmic quantization, A/\u00b5-law coding, \ndithering, JPEG. \nObjectives \napply basic properties of time-invariant linear systems; \nunderstand sampling, aliasing, convolution, filtering, the pitfalls of spectral estimation; \nexplain the above in time and frequency domain representations; \nuse filter-design software; \nvisualize and discuss digital filters in the z-domain; \nuse the FFT for convolution, deconvolution, filtering; \nimplement, apply and evaluate simple DSP applications; \nfamiliarity with a number of signal-processing concepts used in digital communication systems \nRecommended reading \nLyons, R.G. (2010). Understanding digital signal processing. Prentice Hall (3rd ed.). \nOppenheim, A.V. and Schafer, R.W. (2007). Discrete-time digital signal processing. Prentice \nHall (3rd ed.). \nStein, J. (2000). Digital signal processing \u2013 a computer science perspective. Wiley. \n \nClass size \nThis module can accommodate a maximum of 24 students (16 Part II students and 8 MPhil \nstudents) \n \nAssessment - Part II students \nThree homework programming assignments, each comprising 20% of the mark \nWritten test, comprising 40% of the total mark. \nAssessment - Part III and MPhil students \nThree homework programming exercises, each comprising 10% of the mark. \nWritten test, comprising 20% of the total mark. \nIndividual implementation project with 8-page written report, topic agreed with lecturer, \ncomprising 50% of the mark. \nFurther Information \nCurrent Cambridge undergraduate students who are continuing onto Part III or the MPhil in \nAdvanced Computer Science may only take this module if they did NOT take it as a Unit of \nAssessment in Part II. \n \nThis module is shared with Part II of the Computer Science Tripos. Assessment will be adjusted \nfor the two groups of students to be at an appropriate level for whichever course the student is \nenrolled on. Further information about assessment and practicals will follow at the first lecture.\n\nINTRODUCTION TO COMPUTATIONAL SEMANTICS \n \nAims \nThis is a lecture-style course that introduces students to various aspects of the semantics of \nNatural Languages (mainly English): \n \nLexical Semantics, with an emphasis on theory and phenomenology  \nCompositional Semantics \nDiscourse and pragmatics-related aspects of semantics \nObjectives \nGive an operational definition of what is meant by \u201cmeaning\u201d (for instance, above and beyond \nsyntax); \nName the types of phenomena in language that require semantic consideration, in terms of \nlexical, compositional and discourse/pragmatic aspects, in other words, argue why semantics is \nimportant; \nDemonstrate an understanding of the basics of various semantic representations, including \nlogic-based and graph-based semantic representations, their properties, how they are used and \nwhy they are important, and how they are different from syntactic representations; \nKnow how such semantic representations are derived during or after parsing, and how they can \nbe analysed and mapped to surface strings; \nUnderstand applications of semantic representations e.g. reasoning, validation, and methods \nhow these are approached. \nWhen designing NL tasks that clearly require semantic processing (e.g. knowledge-based QA), \nto be aware of and reuse semantic representations and algorithms when designing the task, \nrather than reinventing the wheel. \nPractical advantages of this course for NLP students \nKnowledge of underlying semantic effects helps improve NLP evaluation, for instance by \nproviding more meaningful error analysis. You will be able to link particular errors to design \ndecisions inside your system. \nYou will learn methods for better benchmarking of your system, whatever the task may be. \nSupervised ML systems (in particular black-box systems such as Deep Learning) are only as \nclever as the datasets they are based on. In this course, you will learn to design datasets so that \nthey are harder to trick without real understanding, or critique existing datasets. \nYou will be able to design tests for ML systems that better pinpoint which aspects of language \nan end-to-end system has \u201cunderstood\u201d. \nYou will learn to detect ambiguity and ill-formed semantics in human-human communication. \nThis can serve to write more clearly and logically. \nYou will learn about decomposing complex semantics-reliant tasks sensibly so that you can \nreuse the techniques underlying semantic analyzers in a modular way. In this way, rather than \nbeing forced to treat complex tasks in an end-to-end manner, you will be able to profit from \npartial explanations and a better error analysis already built into the system. \nSyllabus \nOverview of the course \nEvents and semantic role labelling \n\nReferentiality and coreference resolution \nTruth-conditional semantics \nGraph-based meaning representation \nCompositional semantics \nContext-free Graph Rewriting \nSurface realisation \nNegation and psychological approach to semantics \nDynamic semantics \nGricean pragmatics \nVector space models \nCross-modality \nAcquisition of semantics \nDiachronic change of semantics \nSummary of the course \n  \n \nAssessment \n5 take-home exercises worth 20% each: \n \nTake-home assignment 1 is given in Lecture 4 and due is Lecture 6 \n \nStudents are given 10 English sentences and asked to provide their semantic analysis \naccording to truth-conditions. Students will be expected to return 10 logical expressions as their \nanswers. No word limit. Assessment criteria: correctness of the 10 logical expressions; 2 points \nfor each logical expression. \n \nTake-home assignment 2 is given in Lecture 7 and due is Lecture 9  \n \nStudents are given 10 English sentences and asked to provide their syntactico-semantic \nderivations according to the compositionality principle. Students will be expected to return \nderivation graphs as their answers. No word limit. Assessment criteria: correctness of the 10 \nderivation graphs; 2 points for each derivation. \n \nTake-home assignment 3 is given in Lecture 11 and due is Lecture 13  \n \nAll students are assigned with a paper on modelling common ground in dialogue system. \nStudents will receive related but different papers. Each student will write a review of their \nassigned paper, including a comprehensive summary and their own thoughts. Word limit: 1000 \nwords. Assessment criteria: 15 points on whether a student understands the paper correctly; 5 \npoints on whether a student is able to think critically. \n \nTake-home assignment 4 is given in Lecture 13 and due is Lecture 15 \n \n\nAll students are assigned with a paper on language-vision interaction. Students will receive \nrelated but different papers. Each student will write a review of their assigned paper, including a \ncomprehensive summary and their own thoughts. Word limit: 1000 words. Assessment criteria: \n15 points on whether a student understands the paper correctly; 5 points on whether a student \nis able to think critically. \n \nTake-home assignment 5 is given in Lecture 14 and due is two weeks later. \n \nContent: All students are assigned with a paper on bootstrapping language acquisition. All \nstudents will receive the same paper. Each student will write a review of the paper, including a \ncomprehensive summary and their own thoughts. Word limit: 1000 words. Assessment criteria: \n15 points on whether a student understands the paper correctly; 5 points on whether a student \nis able to think critically.\n \n\nINTRODUCTION TO NATURAL LANGUAGE SYNTAX AND PARSING \n \nAims \nThis module aims to provide a brief introduction to linguistics for computer scientists and then \ngoes on to cover some of the core tasks in natural language processing (NLP), focussing on \nstatistical parsing of sentences to yield syntactic representations. We will look at how to \nevaluate parsers and see how well state-of-the-art tools perform given current techniques. \n \nSyllabus \nLinguistics for NLP focusing on morphology and syntax \nGrammars and representations \nStatistical and neural parsing \nTreebanks and evaluation \nObjectives \nOn completion of this module, students should: \n \nunderstand the basic properties of human languages and be familiar with descriptive and \ntheoretical frameworks for handling these properties; \nunderstand the design of tools for NLP tasks such as parsing and be able to apply them to text \nand evaluate their performance; \nPractical work \nWeek 1-7: There will be non-assessed practical exercises between sessions and during \nsessions. \nWeek 8: Download and apply two parsers to a designated text. Evaluate the performance of the \ntools quantitatively and qualitatively. \nAssessment \nThere will be one presentation worth 10% of the final mark; presentation topics will be allocated \nat the start of term. \nAn assessed practical report of not more 8 pages in ACL conference format. It will contribute \n90% of the final mark. \nRecommended reading \nJurafsky, D. and Martin, J. (2008). Speech and Language Processing. Prentice-Hall (2nd ed.). \n(See also 3rd ed. available online.)\n \n\nINTRODUCTION TO NETWORKING AND SYSTEMS MEASUREMENTS \n \nAims \nSystems research refers to the study of a broad range of behaviours arising from complex \nsystem design, including: resource sharing and scheduling; interactions between hardware and \nsoftware; network topology, protocol and device design and implementation; low-level operating \nsystems; Interconnect, storage and more. This module will: \n \nTeach performance characteristics and performance measurement methodology and practice \nthrough profiling experiments; \nExpose students to real-world systems artefacts evident through different measurement tools; \nDevelop scientific writing skills through a series of laboratory reports; \nProvide research skills for characterization and modelling of systems and networks using \nmeasurements. \nPrerequisites \nIt is strongly recommended that students have previously (and successfully) completed an \nundergraduate networking course -- or have equivalent experience through project or \nopen-source work. \n \nSyllabus \nIntroduction to performance measurements, performance characteristics [1 lecture] \nPerformance measurements tools and techniques [2 lectures + 2 lab sessions] \nReproducible experiments [1 lecture + 1 lab session] \nCommon pitfalls in measurements [1 lecture] \nDevice and system characterisation [1 lecture + 2 lab sessions] \nObjectives \nOn completion of this module, students should: \n \nDescribe the objectives of measurements, and what they can achieve; \nCharacterise and model a system using measurements; \nPerform reproducible measurements experiments; \nEvaluate the performance of a system using measurements; \nOperate measurements tools and be aware of their limitations; \nDetect anomalies in the network and avoid common measurements pitfalls; \nWrite system-style performance evaluations. \nPractical work \nFive 2-hour in-classroom labs will ask students to develop and use skills in performance \nmeasurements as applied to real-world systems and networking artefacts. Results from these \nlabs (and follow-up work by students outside of the classroom) will by the primary input to lab \nreports. \n \nThe first three labs will provide an introduction and hands on experience with measurement \ntools and measurements methodologies, while the last two labs will focus on practical \nmeasurements and evaluation of specific platforms. Students may find it useful to work in pairs \n\nwithin the lab, but must prepare lab reports independently. The module lecturer will give a short \nintroductory at the start of each lab, and instructors will be on-hand throughout labs to provide \nassistance. \n \nLab participation is not directly included in the final mark, but lab work is a key input to lab \nreports that are assessed. Guided lab experiments resulting in practical write ups. \n \nAssessment \nEach student will write two lab reports. The first lab report will summarise the experiments done \nin the first three labs (20%). The second will be a lab report (5000 words) summarising the \nevaluation of a device or a system (80%). \n \nRecommended reading \nThe following list provides some background to the course materials, but is not mandatory. A \nreading list, including research papers, will be provided in the course materials. \n \nGeorge Varghese. Network algorithmics. Chapman and Hall/CRC, 2010. \nMark Crovella and Balachander Krishnamurthy. Internet measurement: infrastructure, traffic and \napplications. John Wiley and Sons, Inc., 2006. \nBrendan Gregg. Systems Performance: Enterprise and the Cloud, Prentice Hall Press, Upper \nSaddle River, NJ, USA, October 2013. \nRaj Jain, The Art of Computer Systems Performance Analysis: Techniques for Experimental \nDesign, Measurement, Simulation, and Modeling, Wiley - Interscience, New York, NY, USA, \nApril 1991.\n \n\nLARGE-SCALE DATA PROCESSING AND OPTIMISATION \n \nAims \nThis module provides an introduction to large-scale data processing, optimisation, and the \nimpact on computer system's architecture. Large-scale distributed applications with high volume \ndata processing such as training of machine learning will grow ever more in importance. \nSupporting the design and implementation of robust, secure, and heterogeneous large-scale \ndistributed systems is essential. To deal with distributed systems with a large and complex \nparameter space, tuning and optimising computer systems is becoming an important and \ncomplex task, which also deals with the characteristics of input data and algorithms used in the \napplications. Algorithm designers are often unaware of the constraints imposed by systems and \nthe best way to consider these when designing algorithms with massive volume of data. On the \nother hand, computer systems often miss advances in algorithm design that can be used to cut \ndown processing time and scale up systems in terms of the size of the problem they can \naddress. Integrating machine learning approaches (e.g. Bayesian Optimisation, Reinforcement \nLearning) for system optimisation will be explored in this course. \n \nSyllabus \nThis course provides perspectives on large-scale data processing, including data-flow \nprogramming, graph data processing, probabilistic programming and computer system \noptimisation, especially using machine learning approaches, thus providing a solid basis to work \non the next generation of distributed systems. \n \nThe module consists of 8 sessions, with 5 sessions on specific aspects of large-scale data \nprocessing research. Each session discusses 3-4 papers, led by the assigned students. One \nsession is a hands-on tutorial on on dataflow programming fundamentals. The first session \nadvises on how to read/review a paper together with a brief introduction on different \nperspectives in large-scale data \nprocessing and optimisation. The last session is dedicated to the student presentation of \nopensource project studies. \n \nIntroduction to large-scale data processing and optimisation \nData flow programming: Map/Reduce to TensorFlow \nLarge-scale graph data processing: storage, processing model and parallel processing \nDataflow programming hands-on tutorial \nProbabilistic Programming \nOptimisations in ML Compiler \nOptimisations of Computer systems using ML \nPresentation of Open Source Project Study \nObjectives \nOn completion of this module, students should: \n \nUnderstand key concepts of scalable data processing approaches in future computer systems. \n\nObtain a clear understanding of building distributed systems using data centric programming \nand large-scale data processing. \nUnderstand a large and complex parameter space in computer system's optimisation and \napplicability of Machine Learning approach. \nCoursework \nReading Club: \nThe preparation for the reading club will involve 1-3 papers every week. At each session, \naround 3-4 papers are selected under the given topic, and the students present their review \nwork. \nHands-on tutorial session of data flow programming including writing an application of \nprocessing streaming in Twitter data and/or Deep Neural Networks using Google TensorFlow \nusing cluster computing. \nReports \nThe following three reports are required, which could be extended from the assignment of the \nreading club, within the scope of data centric systems. \n \nReview report on a full length paper (max 1800 words) \nDescribe the contribution of the paper in depth with criticisms \nCrystallise the significant novelty in contrast to other related work \nSuggestions for future work \nSurvey report on sub-topic in large-scale data processing and optimisation (max 2000 words) \nPick up to 5 papers as core papers in the survey scope \nRead the above and expand reading through related work \nComprehend the view and finish an own survey paper \nProject study and exploration of a prototype (max 2500 words) \nWhat is the significance of the project in the research domain? \nCompare with similar and succeeding projects \nDemonstrate the project by exploring its prototype \nReports 1 should be handed in by the end of 5th week; Report 2 in the 8th week and Report 3 \non the first day of Lent Term. \n \nAssessment \nThe final grade for the course will be provided as a percentage, and the assessment will consist \nof two parts: \n \n25%: for reading club (participation, presentation) \n75%: for the three reports: \n15%: Intensive review report \n25%: Survey report \n35%: Project study \nRecommended reading \nM. Abadi et al. TensorFlow: A System for Large-Scale Machine Learning, OSDI, 2016. \nD. Aken et al.: Automatic Database Management System Tuning Through Large-scale Machine \nLearning, SIGMOD, 2017. \n\nJ. Ansel et al. Opentuner: an extensible framework for program autotuning. PACT, 2014. \nV. Dalibard, M. Schaarschmidt, E. Yoneki. BOAT: Building Auto-Tuners with Structured Bayesian \nOptimization, WWW, 2017. \nJ. Dean et al. Large scale distributed deep networks. NIPS, 2012. \nG. Malewicz, M. Austern, A. Bik, J. Dehnert, I. Horn, N. Leiser, G. Czajkowski. Pregel: A System \nfor Large-Scale Graph Processing, SIGMOD, 2010. \nA. Mirhoseini et al. Device Placement Optimization with Reinforcement Learning, ICML, 2017. \nD. Murray, F. McSherry, R. Isaacs, M. Isard, P. Barham, M. Abadi. Naiad: A Timely Dataflow \nSystem, SOSP, 2013. \nM. Schaarschmidt, S. Mika, K. Fricke and E. Yoneki: RLgraph: Modular Computation Graphs for \nDeep Reinforcement Learning, SysML, 2019. \nZ. Jia, O. Padon, J. Thomas, T. Warszawski, M. Zaharia,  A. Aiken: TASO: Optimizing Deep \nLearning Computation with Automated Generation of Graph Substitutions: SOSP, 2019. \nH. Mao et al.: Park: An Open Platform for Learning-Augmented Computer Systems, \nOpenReview, 2019. \nJ. Shao, et al.: Tensor Program Optimization with Probabilistic Programs, NeurIPS, 2022.  \nA complete list can be found on the course material web page. See also 2023-2024 course \nmaterial on the previous course Large-Scale Data Processing and Optimisation.\n \n\nMACHINE LEARNING AND THE PHYSICAL WORLD \nAims \nThe module \u201cMachine Learning and the Physical World\u201d is focused on machine learning \nsystems that interact directly with the real world. Building artificial systems that interact with the \nphysical world have significantly different challenges compared to the purely digital domain. In \nthe real world data is scares, often uncertain and decisions can have costly and irreversible \nconsequences. However, we also have the benefit of centuries of scientific knowledge that we \ncan draw from. This module will provide the methodological background to machine learning \napplied in this scenario. We will study how we can build models with a principled treatment of \nuncertainty, allowing us to leverage prior knowledge and provide decisions that can be \ninterrogated. \n \nThere are three principle points about machine learning in the real world that will concern us. \n \nWe often have a mechanistic understanding of the real world which we should be able to \nbootstrap to make decisions. For example, equations from physics or an understanding of \neconomics. \nReal world decisions have consequences which may have costs, and often these cost functions \nneed to be assimilated into our machine learning system. \nThe real world is surprising, it does things that you do not expect and accounting for these \nchallenges requires us to build more robust and or interpretable systems. \nDecision making in the real world hasn\u2019t begun only with the advent of machine learning \ntechnologies. There are other domains which take these areas seriously, physics, environmental \nscientists, econometricians, statisticians, operational researchers. This course identifies how \nmachine learning can contribute and become a tool within these fields. It will equip you with an \nunderstanding of methodologies based on uncertainty and decision making functions for \ndelivering on these challenges. \n \nObjectives \nYou will gain detailed knowledge of \n \nsurrogate models and uncertainty \nsurrogate-based optimization \nsensitivity analysis \nexperimental design \nYou will gain knowledge of \n \ncounterfactual analysis \nsurrogate-based quadrature \nSchedule \nWeek 1: Introduction to the unit and foundation of probabilistic modelling \n \nWeek 2: Gaussian processes and probablistic inference \n \n\nWeek 3: Simulation and Sequential decision making under uncertainty \n \nWeek 4: Emulation and Experimental Design \n \nWeek 5: Sensitivity Analysis and Multifidelity Modelling \n \nWeek 6-8: Case studies of applications and Projects \n \nPractical work \nDuring the first five weeks of the unit we will provide a weekly worksheet that will focus on \nimplementation and practical exploration of the material covered in the lectures. The worksheets \nwill allow you to build up a these methods without relying on extensive external libraries. You are \nfree to use any programming language of choice however we highly recommended the use of \n=Python=. \n \nAssessment \nTwo individual tasks (15% each) \nGroup Project (70%). Each group will work on an application of uncertainty that covers the \nmaterial of the first 5 weeks of lectures in the unit. Each group will submit a report which will \nform the basis of the assessment. In addition to the report each group will also attend a short \noral examination based on the material covered both in the report and the taught material. \nRecommended Reading \nRasmussen, C. E. and Williams, C. K. I. (2006). Gaussian Processes for Machine Learning. MIT \nPress \n \nBishop, C. (2006). Pattern recognition and machine learning. Springer. \nhttps://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-an\nd-Machine-Learning-2006.pdf \n \nLaplace, P. S. (1902). A Philosophical Essay on Probabilities. John Wiley & Sons. \nhttps://archive.org/details/philosophicaless00lapliala\n \n\nMACHINE VISUAL PERCEPTION \nAims \nThis course aims at introducing the theoretical foundations and practical techniques for machine \nperception, the capability of computers to interpret data resulting from sensor measurements. \nThe course will teach the fundamentals and modern techniques of machine perception, i.e. \nreconstructing the real world starting from sensor measurements with a focus on machine \nperception for visual data. The topics covered will be image/geometry representations for \nmachine perception, semantic segmentation, object detection and recognition, geometry \ncapture, appearance modeling and acquisition, motion detection and estimation, \nhuman-in-the-loop machine perception, select topics in applied machine perception. \n \nMachine perception/computer vision is a rapidly expanding area of research with real-world \napplications. An understanding of machine perception is also important for robotics, interactive \ngraphics (especially AR/VR), applied machine learning, and several other fields and industries. \nThis course will provide a fundamental understanding of and practical experience with the \nrelevant techniques. \n \nLearning outcomes \nStudents will understand the theoretical underpinnings of the modern machine perception \ntechniques for reconstructing models of reality starting from an incomplete and imperfect view of \nreality. \nStudents will be able to apply machine perception theory to solve practical problems, e.g. \nclassification of images, geometry capture. \nStudents will gain an understanding of which machine perception techniques are appropriate for \ndifferent tasks and scenarios. \nStudents will have hands-on experience with some of these techniques via developing a \nfunctional machine perception system in their projects. \nStudents will have practical experience with the current prominent machine perception \nframeworks. \nSyllabus \nThe fundamentals of machine learning for machine perception \nDeep neural networks and frameworks for machine perception \nSemantic segmentation of objects and humans \nObject detection and recognition \nMotion estimation, tracking and recognition \n3D geometry capture \nAppearance modeling and acquisition \nSelect topics in applied machine perception \nAssessment \nA practical exercise, worth 20% of the mark. This will cover the basics and theory of machine \nperception and some of the practical techniques the students will likely use in their projects. This \nis individual work. No GPU hours will be needed for the practical work. \nA machine perception project worth 80% of the marks: \n\nCourse projects will be selected by the students following the possible project themes proposed \nby the lecturer, and will be checked by the lecturer for appropriateness.  \nThe students will form groups of 2-3 to design, implement, report, and present a project to tackle \na given task in machine perception. \nThe final mark will be composed of an implementation/report mark (60%) and a presentation \nmark (20%). Each team member will be evaluated based on her/his contribution. \nEach project will have extensions to be completed only by the ACS students. Each student will \nwrite a different part of the report, whose author will be clearly marked. Each student will further \nsummarise her/his contributions to the project in the same report. \nRecommended Reading List \nComputer Vision: Algorithms and Applications, Richard Szeliski, Springer, 2010. \nDeep Learning, Ian Goodfellow, Yoshua Bengio, and Aaron Courville, MIT Press, 2016. \nMachine Learning and Visual Perception, Baochang Zhang, De Gruyter, 2020. \nFurther Information \nCurrent Cambridge undergraduate students who are continuing onto Part III or the MPhil in \nAdvanced Computer Science may only take this module if they did NOT take it as a Unit of \nAssessment in Part II. \n \nThis module is shared with Part II of the Computer Science Tripos. Assessment will be adjusted \nfor the two groups of students to be at an appropriate level for whichever course the student is \nenrolled on. Further information about assessment and practicals will follow at the first lecture.\n\nMOBILE, WEARABLE SYSTEMS AND MACHINE LEARNING \n \nAims \nThis module aims to introduce the latest research advancements in mobile systems and mobile \ndata machine learning, spanning a range of domains including systems, data gathering, \nanalytics and machine learning, on device machine learning and applications such as health, \ntransportation, behavior monitoring, cyber-physical systems, autonomous vehicles, drones. The \ncourse will cover current and seminal research papers in the area of research. \n \nSyllabus \nThe course will consist of one introductory lecture, seven two-hour, and one three-hour, \nsessions covering a variety of topics roughly including the following material (some variation in \nthe topics might happen from year to year): \n \nSystem, Energy and Security \nBackscatter Communication, Battery Free and Energy Harvesting Devices \nNew Sensing Modalities \nMachine Learning on Wearable Data \nOn Device Machine Learning \nMobile and Wearable Health \nMobile and Wearable Systems of Sustainability \nEach week, three class participants will be assigned to introduce assigned three papers via \n20-minute presentations, conference-style and highlighting critically its features. Each \npresentation will be followed by 10 minutes of questions. This will be followed by 10 minutes of \ngeneral discussion. Slides will be used for presentation. \n \nStudents will give one or more presentations each term. Each student will submit a paper review \neach week for one of the three papers presented except for the week they will be presenting \nslides. Each review will follow a template and be up to 1,000 words. Each review will receive a \nmaximum of 10 points. As a result, each student will produce 6-7 reviews and at least one \npresentation, probably two. All participants are expected to attend and participate in every class; \nthe instructor must be notified of any absences in advance. \n \nObjectives \nOn completion of this module students should have an understanding of the recent key research \nin mobile and sensor systems and mobile analytics as well as an improved critical thinking over \nresearch papers. \n \nAssessment \nAggregate mark for 7 assignments from 6-7 reports and 1-2 presentations. Each report or \npresentation will contribute one seventh of the course mark. \nA tick for presence and participation to each class will also be awarded. \nRecommended reading \n\nReadings from the most recent conferences such as AAAI, ACM KDD, ACM MobiCom, ACM \nMobiSys, ACM SenSys, ACM UbiComp, ICLR, ICML Neurips, and WWW pertinent to mobile \nsystems and data.\n \n\nNETWORK ARCHITECTURES \nAims \nThis module aims to provide the world with more network architects. The 2011-2012 version \nwas oriented around the evolution of IP to support new services like multicast, mobility, \nmultihoming, pub/sub and, in general, data oriented networking. The course is a paper reading \nwhich puts the onus on the student to do the work. \n \nSyllabus \nIPng [2 lectures, Jon Crowcroft] \nNew Architectures [2 lectures, Jon Crowcroft] \nMulticast [2 lectures, Jon Crowcroft] \nContent Distribution and Content Centric Networks [2 lectures, Jon Crowcroft] \nResource Pooling [2 lectures, Jon Crowcroft] \nGreen Networking [2 lectures, Jon Crowcroft] \nAlternative Router Implementions [2 lectures, Jon Crowcroft] \nData Center Networks [2 Lectures, Jon Crowcroft] \nObjectives \nOn completion of this module, students should be able to: \n \ncontribute to new network system designs; \nengineer evolutionary changes in network systems; \nidentify and repair architectural design flaws in networked systems; \nsee that there are no perfect solutions (aside from academic ones) for routing, addressing, \nnaming; \nunderstand tradeoffs in modularisation and other pressures on clean software systems \nimplementation, and see how the world is changing the proper choices in protocol layering, or \nnon layered or cross-layered. \nCoursework \nAssessment is through three graded essays (each chosen individually from a number of \nsuggested or student-chosen topics), as follows: \n \nAnalysis of two different architectures for a particular scenario in terms of cost/performance \ntradeoffs for some functionality and design dimension, for example: \nATM \u2013 e.g. for hardware versus software tradeoff \nIP \u2013 e.g. for mobility, multi-homing, multicast, multipath \n3GPP \u2013 e.g. for plain complexity versus complicatedness \nA discursive essay on a specific communications systems component, in a particular context, \nsuch as ad hoc routing, or wireless sensor networks. \nA bespoke network design for a narrow, well specified specialised target scenario, for example: \nA customer baggage tracking network for an airport. \nin-flight entertainment system. \nin-car network for monitoring and control. \ninter-car sensor/control network for automatic highways. \nPractical work \n\nThis course does not feature any implementation work due to time constraints. \n \nAssessment \nThree 1,200-word essays (worth 25% each), and \nan annotated bibliography (25%). \nRecommended reading \nPre-course reading: \n \nKeshav, S. (1997). An engineering approach to computer networking. Addison-Wesley (1st ed.). \nISBN 0201634422 \nPeterson, L.L. and Davie, B.S. (2007). Computer networks: a systems approach. Morgan \nKaufmann (4th ed.). \n \nDesign patterns: \n \nDay, John (2007). Patterns in network architecture: a return to fundamentals. Prentice Hall. \n \nExample systems: \n \nKrishnamurthy, B. and Rexford, J. (2001). Web protocols and practice: HTTP/1.1, Networking \nprotocols, caching, and traffic measurement. Addison-Wesley. \n \nEconomics and networks: \n \nFrank, Robert H. (2008). The economic naturalist: why economics explains almost everything. \n \nPapers: \n \nCertainly, a collection of papers (see ACM CCR which publishes notable network researchers' \nfavourite ten papers every 6 months or so).\n \n\nOVERVIEW OF NATURAL LANGUAGE PROCESSING \n \nAims \nThis course introduces the fundamental techniques of natural language processing. It aims to \nexplain the potential and the main limitations of these techniques. Some current research issues \nare introduced and some current and potential applications discussed and evaluated. Students \nwill also be introduced to practical experimentation in natural language processing. \n \nLectures \nOverview. Brief history of NLP research, some current applications, components of NLP \nsystems. \nMorphology and Finite State Techniques. Morphology in different languages, importance of \nmorphological analysis in NLP, finite-state techniques in NLP. \nPart-of-Speech Tagging and Log-Linear Models. Lexical categories, word tagging, corpora and \nannotations, empirical evaluation. \nPhrase Structure and Structure Prediction. Phrase structures, structured prediction, context-free \ngrammars, weights and probabilities. Some limitations of context-free grammars. \nDependency Parsing. Dependency structure, grammar-free parsing, incremental processing.  \nGradient Descent and Neural Nets. Parameter optimisation by gradient descent. Non-linear \nfunctions with neural network layers. Log-linear model as softmax layer. Current findings of \nNeural NLP. \nWord representations. Representing words with vectors, count-based and prediction-based \napproaches, similarity metrics. \nRecurrent Neural Networks. Modelling sequences, parameter sharing in recurrent neural \nnetworks, neural language models, word prediction. \nCompositional Semantics. Logical representations, compositional semantics, lambda calculus, \ninference and robust entailment. \nLexical Semantics. Semantic relations, WordNet, word senses. \nDiscourse. Discourse relations, anaphora resolution, summarization. \nNatural Language Generation. Challenges of natural language generation (NLG), tasks in NLG, \nsurface realisation. \nPractical and assignments. Students will build a natural language processing system which will \nbe trained and evaluated on supplied data. The system will be built from existing components, \nbut students will be expected to compare approaches and some programming will be required \nfor this. Several assignments will be set during the practicals for assessment. \nObjectives \nBy the end of the course students should: \n \nbe able to discuss the current and likely future performance of several NLP applications; \nbe able to describe briefly a fundamental technique for processing language for several \nsubtasks, such as morphological processing, parsing, word sense disambiguation etc.; \nunderstand how these techniques draw on and relate to other areas of computer science. \nRecommended reading \n\nJurafsky, D. & Martin, J. (2023). Speech and language processing. Prentice Hall (3rd ed. draft, \nonline). \n \nAssessment - Part II Students \nAssignment 1 - 10% of marks \nAssignment 2 - 60% of marks \nAssignment 3 - 30% of marks \nCoursework \nSubmit work for 3 assignments as part of practical sessions on information extraction. \n \nThese include an annotation exercise, a feature-based classifier along with code repository and \ndocumentation, and a 4,000-word report on results and analysis from an extended information \nextraction experiment. \n \nPractical work \nStudents will build a natural language processing system which will be trained and evaluated on \nsupplied data. The system will be built from existing components, but students will be expected \nto compare approaches and some programming will be required for this. \n \nAssessment - Part III and MPhil Students \nAssessment will be based on the practicals: \n \nFirst practical exercise (10%, ticked) \nSecond practical exercise (25%, code repository or notebook with documentation) \nFinal report (65%, 4,000 words, excluding references) \nFurther Information \nAlthough the lectures don't assume any exposure to linguistics, the course will be easier to \nfollow if students have some understanding of basic linguistic concepts. The following may be \nuseful for this: The Internet Grammar of English \n \nCurrent Cambridge undergraduate students who are continuing onto Part III or the MPhil in \nAdvanced Computer Science may only take this module if they did NOT take it as a Unit of \nAssessment in Part II. \n \nStudents from other departments may attend the lectures for this module if space allows. \nHowever students wanting to take it for credit will need to make arrangements for assessment \nwithin their own department. \n \nThis module is shared with Part II of the Computer Science Tripos. Assessment will be adjusted \nfor the two groups of students to be at an appropriate level for whichever course the student is \nenrolled on. Further information about assessment and practicals will follow at the first lecture.\n\nPRACTICAL RESEARCH IN HUMAN-CENTRED AI \n \nAims \nThis is an advanced course in human-computer interaction, with a specialist focus on intelligent \nuser interfaces and interaction with machine-learning and artificial intelligence technologies. The \nformat will be largely Practical, with students carrying out a mini-project involving empirical \nresearch investigation. These studies will investigate human interaction with some kind of \nmodel-based system for planning, decision-making, automation etc. Possible study formats \nmight include: System evaluation, Field observation, Hypothesis testing experiment, Design \nintervention or Corpus analysis, following set examples from recent research publications. \nProject work will be formally evaluated through a report and presentation. \n \nLectures \n(note that Lectures 2-7 also include one hour class discussion of practical work) \n        \u2022 Current research themes in intelligent user interfaces \n        \u2022 Program synthesis \n        \u2022 Mixed initiative interaction \n        \u2022 Interpretability / explainable AI \n        \u2022 Labelling as a fundamental problem \n        \u2022 Machine learning risks and bias \n        \u2022 Visualisation and visual analytics \n        \u2022 Student research presentations \n \nObjectives \nBy the end of the course students should: \n        \u2022 be familiar with current state of the art in intelligent interactive systems \n        \u2022 understand the human factors that are most critical in the design of such systems \n        \u2022 be able to evaluate evidence for and against the utility of novel systems \n        \u2022 have experience of conducting user studies meeting the quality criteria of this field \n        \u2022 be able to write up and present user research in a professional manner \n \nClass Size \nThis module can accommodate upto 20 Part II, Part III and MPhil students. \n \nRecommended reading \nBrad A. Myers and Richard McDaniel (2000). Demonstrational Interfaces: Sometimes You Need \na Little Intelligence, Sometimes You Need a Lot. \n \nAlan Blackwell (2024). Moral Codes: Designing alternatives to AI \n \nAssessment - Part II Students \nThe format will be largely practical, with students carrying out an individual mini-project involving \nempirical research investigation. \n \n\nAssignment 1: six incremental submissions which together contribute 20% to the final module \nmark. \n \nAssignment 2: Final report - 80% of the final module mark \n  \n \nAssessment - MPhil / Part III Students \nThere will be a minor assessment component (20%) in which students compile a reflective diary \nthroughout the term, reporting on the weekly sessions. Diary entries should include citations to \nany key references, notes of possible further reading, summary of key points, questions relevant \nto the personal project, and points of interest noted in relation to the work of other students. \n \nThe major assessment component (80%) will involve a report on research findings, in the style \nof a submission to the ACM CHI or IUI conferences. This work will be submitted incrementally \nthrough the term, in order that feedback can be provided before final assessment of the full \nreport. Phased submissions will cover the following aspects of the empirical study: \n \nResearch question \nMethod \nLiterature Review \nIntroduction \nResults \nDiscussion / Conclusion \nFeedback on each phased submission will include an indicative mark for guidance, but with the \nunderstanding that the final grade will be based on the final delivered report, and that this may \ngo up (or possibly down), depending on how well the student responds to earlier feedback. \n \nReflective diary entries will also be assessed, but graded at a relatively coarse granularity \ncorresponding to the ACS grading bands, and with minimal written feedback. Informal and \ngeneric feedback will be offered verbally in class, and also potentially supplemented with peer \nassessment. \n \nFurther Information \nCurrent Cambridge undergraduate students who are continuing onto Part III or the MPhil in \nAdvanced Computer Science may only take this module if they did NOT take it as a Unit of \nAssessment in Part II. \n \nThis module is shared with Part II of the Computer Science Tripos. Assessment will be adjusted \nfor the two groups of students to be at an appropriate level for whichever course the student is \nenrolled on. Further information about assessment and practicals will follow at the first lecture.\n\nPRINCIPLES OF MACHINE LEARNING SYSTEMS \n \nPrerequisites: It is recommended that students have successfully completed introductory \ncourses, at an undergraduate level, in: 1) operating systems, 2) computer architecture, and 3) \nmachine learning. In addition, this course will heavily focus on deep neural network \nmethodologies and assume familiarity with common neural architectures and related algorithms. \nIf these topics were not covered within the machine learning course taken, students should \nsupplement by reviewing material in a book like: \"Dive into Deep Learning\". Finally, students are \nassumed to be comfortable with programming in Python. \nMoodle, timetable \n \nAims \nThis course will examine the emerging principles and methodologies that underpin scalable and \nefficient machine learning systems. Primarily, the course will focus on an exciting cross-section \nof algorithms and system techniques that are used to support the training and inference of \nmachine learning models under a spectrum of computing systems that range from constrained \nembedded systems up to large-scale distributed systems. It will also touch upon the new \nengineering practices that are developing in support of such systems at scale. When needed to \nappreciate issues of scalability and efficiency, the course will drill down to certain aspects of \ncomputer architecture, systems software and distributed systems and explore how these \ninteract with the usage and deployment of state-of-the-art machine learning. \n \nSyllabus \nTopics covered may include the following, with confirmation a month before the course begins: \n \nSystem Performance Trade-offs \nDistributed Learning Algorithms  \nModel Compression  \nDeep Learning Compilers  \nFrameworks and Run-times  \nScalable Inference Serving  \nDevelopment Practices  \nAutomated Machine Learning  \nFederated Learning  \nPrimarily, topics are covered with conventional lectures. However, where appropriate, material \nwill be delivered through hands-on lab tutorials. Lab tutorials will make use of hardware \nincluding ARM microcontrollers and multi-GPU machines to explore forms of efficient machine \nlearning (any necessary equipment will be provided to students) \n \nAssessment \nEach student will be assessed on 3 labs which will be worth 30% of their grade. They will also \nundertake a written project report which will be worth 70% of the grade. This report will detail an \ninvestigation into a particular aspect of machine learning systems, this report will be made \navailable publicly.\n \n\nPROOF ASSISTANTS \nAims \nThis module introduces students to interactive theorem proving using Isabelle and Coq. It \nincludes techniques for specifying formal models of software and hardware systems and for \nderiving properties of these models. \n \n  \n \nSyllabus \nIntroduction to proof assistants and logic. \nReasoning in predicate logic and typed set theory. \nReasoning in dependent type theory. \nInductive definitions and recursive functions: modelling them in logic, reasoning about them. \nModelling operational semantics definitions and proving properties. \n  \n \nObjectives \nOn completion of this module, students should: \n \npossess good skills in the use of Isabelle and Coq; \nbe able to specify inductive definitions and perform proofs by induction; \nbe able to formalise and reason about a variety of specifications in a proof assistant. \nCoursework \nPractical sessions will allow students to develop skills. Some of the exercises will serve as the \nbasis for assessment. \n \nAssessment \nAssessment will be based on two marked assignments (due in weeks 5 and 8) with students \nperforming small formalisation and verification projects in a proof assistant together with a \nwrite-up explaining their work (word limit 2,500 per project for the write-up). Each of the \nassignments will be worth 100 marks, distributed as follows: \n \n50 marks for completing basic formalisation and verification tasks assessing grasp of the \nmaterial taught in the lecture. Students will submit their work as theory files for either the \nIsabelle or Coq proof assistant, and they will be assessed for correctness and completeness of \nthe specifications and proofs. \n20 marks for completing designated more challenging tasks, requiring the use of advanced \ntechniques or creative proof strategies. \n30 marks for a clear write-up explaining the design decisions made during the formalisation and \nthe strategy used for the proofs, where 10 of these marks will be reserved for write-ups of \nexceptional quality, e.g. demonstrating particular insight. \nThe main tasks in the assignments will be designed to assess the student's proficiency with the \nbasic material and techniques taught in the lectures and the practical sessions, while the more \nchallenging tasks will give exceptional students the opportunity to earn distinction marks \n\n \nRecommended reading \nNipkow, T., Klein, G. (2014). Concrete Semantics with Isabelle/HOL. (The first part of this book, \n\u201cProgramming and Proving in Isabelle/HOL\u201d, comes with the Isabelle distribution.)  \n \nNipkow, T., Paulson, L.C. and Wenzel, M. (2002). A proof assistant for higher-order logic. \nSpringer LNCS 2283.  \n \nThe Software Foundations series of books, in particular the first two: \n \nPierce, B., Azevedo de Amorim, A., Casinghino, C., Gaboardi, M., Greenberg, M., Hri\u0163cu, C., \nSj\u00f6berg, V., Yorgey, B. (2023). Logical Foundations  \nPierce, B., Azevedo de Amorim, A., Casinghino, C., Gaboardi, M., Greenberg, M., Hri\u0163cu, C., \nSj\u00f6berg, V., Tolmach, A., Yorgey, B. (2023). Programming Language Foundations  \nChlipala, A. (2022). Certified programming with dependent types: a pragmatic introduction to the \nCoq proof assistant. MIT Press.  \n \nSergey, I. (2014). Programs and Proofs: Mechanizing Mathematics with Dependent Types.  \n \nAll of these are freely available online.\n \n\nQUANTUM ALGORITHMS AND COMPLEXITY \n \nAims \nThis module is a research-focused introduction to the theory of quantum computing. The aim is \nto prepare the students to conduct research in quantum algorithms and quantum complexity \ntheory. \n \nSyllabus \nTopics will vary from year to year, based on developments in cutting edge research. \nRepresentative topics include: \n \nQuantum algorithms: \n \nQuantum learning theory \nQuantum property testing \nShadow tomography \nQuantum walks \nQuantum state/unitary synthesis. \nStructure vs randomness in quantum algorithms \nQuantum complexity theory: \n \nThe quantum PCP conjecture \nEntanglement and MIP \nState complexity, AdS/CFT, and quantum gravity \nQuantum locally testable codes \nPseudorandom states and unitaries \nQuantum zero-knowledge proofs \nObjectives \nOn completion of this module, students should: \n \nbe familiar with contemporary quantum algorithms \ndevelop an understanding of the power and limitations of quantum computation \nconduct research in quantum algorithms and complexity theory \nAssessment \nMini research project (70%) \nPresentation (20%) \nAttendance and participation (10%) \nTimelines for the assignment submissions:  \n \nsubmit questions/observations on a weekly basis (i.e., Weeks 2-8) \ndeliver talks in Weeks 5-8 \nsubmit their mini-project at the end of term \nRecommended reading material and resources \n\u201cQuantum Computing Since Democritus\u201d by Scott Aaronson \n\n \n\u201cIntroduction to Quantum Information Science\u201d by Scott Aaronson \n \n\u201cQuantum Computing: Lecture Notes\u201d by Ronald de Wolf \n \n\u201cQuantum Computation and Quantum Information\u201d by Nielsen and Chuang\n \n\nUNDERSTANDING QUANTUM ARCHITECTURE \nPrerequisites: Requires introductory linear algebra - concepts such as eigenvalues, Hermitian \nmatrices, unitary matrices. Taking the Part II Quantum Computing course (or similar) is helpful \nbut not required. Familiarity with computer architecture and compilers is helpful. \nMoodle, timetable \n \nAims \nThis course covers the architecture of a practical-scale quantum computer. We will examine the \nresource requirements of practical quantum applications, understand the different layers of the \nquantum stack, the techniques used in these layers and examine how these layers come \ntogether to enable practical quantum advantage over classical computing. \n \nSyllabus \nThe course has two parts: a series of lectures to cover important aspects of the quantum stack \nand a set of student presentations. The following are a list of representative topics: \n \n- Basics of quantum computing \n- The fault-tolerant quantum stack \n- Compilation \n- Instruction sets \n- Implementations of quantum error correction \n- Implementation of magic state distillation \n- Resource estimation \n \nStudent presentations will be based on a reading list of important papers in quantum \narchitecture.  \n  \n \nObjectives \nAt the end of the course, students will have a broad understanding of the quantum computing \nstack. They will understand how major qubit technologies work, design challenges in real \nquantum hardware, how quantum applications are mapped to a system and the importance of \nquantum error correction for scalability.  \n \nAssessment \nSeminar presentation: 20% \nRead one paper from a provided reading list \nPrepare a 20 minute presentation on it + 10 minutes of answering questions. \nThe student presentation should be similar to a conference presentation - convey the problem, \nwhat are prior solutions, what did the paper do, what are the results, what are future directions \nFor the 20% of the score, the score will be split as 15% and 5%. 15% based on how well the \nstudent understands and explains the paper to the rest of the audience. 5% based on the Q&A \nsession. \n \n\n  \n \nCourse project: 80% (split across a proposal, mid-term report, final report) \nA. Research proposal - at least 500 words (10% of total) \nB. Mid-term report - at least 1000 words (including aspects like the research problem, literature \nreview, what questions will be evaluated, any early ideas or methods (20% of total)  \nC. Final report - up to 4 pages double column in a conference paper style with 3000-4000 \nwords. In addition to the mid term report, should include aspects like results and future \ndirections. (50% of the total) \nD. Report on contributions (in case of group work by 2 students) - 1 paragraph on individual \ncontribution of the student. 1 paragraph on teammate's contribution. \nStudents may work alone in the project, in which case they need only components A-C. \nStudents may work in pairs of two, but they should in addition individually submit D. The \ninstructor may conduct a short viva in case contributions from both team members are not clear \nor imbalanced. \nRecommended reading \nNielsen M.A., Chuang I.L. (2010). Quantum Computation and Quantum Information. Cambridge \nUniversity Press. \n \nMermin N.D. (2007). Quantum Computer Science: An Introduction. Cambridge University Press. \n \nAssessing requirements to scale to practical quantum advantage (2022) Beverland et al.\n\n",
        "8th Semester \nADVANCED TOPICS IN CATEGORY THEORY \n \n \nTeaching \nThe teaching style will be lecture-based, but supported by a practical component where \nstudents will learn to use a proof assistant for higher category theory, and build a small portfolio \nof proofs. Towards the end of the course we will explore some of the exciting computer science \nresearch literature on monoidal and higher categories, and students will choose a paper and \npresent it to the class. \n \nAims \nThe module will introduce advanced topics in category theory. The aim is to train students to \nengage and start modern research on the mathematical foundations of higher categories, the \ngraphical calculus, monoids and representations, type theories, and their applications in \ntheoretical computer science, both classical and quantum. \n \nObjectives \nOn completion of this module, students should: \n \nBe familiar with the techniques of compositional category theory. \nHave a strong understanding of basic categorical semantic models. \nBegun exploring current research in monoidal categories and higher structures. \nSyllabus \nPart 1, lecture course: \nThe first part of the course introduces concepts from monoidal categories and higher categories, \nand explores their application in computer science. \n\u2010 Monoidal categories and the graphical calculus \n\u2010 The proof assistant homotopy.io \n\u2010 Coherence theorems and higher category theory \n\u2010 Linearity, superposition, duality, quantum entanglement \n\u2010 Monoids, Frobenius algebras and bialgebras \n\u2010 Type theory for higher category theory \n \nPart 2, exploring the research frontier: \nIn the second part of the course, students choose a research paper to study, and give a \npresentation to the class. \nThere is a nice varied literature related to the topics of the course, and the lecturer will supply a \nlist of suggested papers.  \n \nClasses \nThere will be three exercise sheets for homework, with accompanying classes by a teaching \nassistant to go over them. \n \n\nAssessment \nProblem sheets (50%) \nClass presentation (20%) \nPractical portfolio (30%) \nReading List \nChris Heunen and Jamie Vicary, \u201cCategory for Quantum Theory: An Introduction\u201d, Oxford \nUniversity Press\n \n\nADVANCED TOPICS IN COMPUTER SYSTEMS \n \nAims \nThis module will attempt to provide an overview of \u201csystems\u201d research. This is a very broad field \nwhich has existed for over 50 years and which has historically included areas such as operating \nsystems, database systems, file systems, distributed systems and networking, to name but a \nfew. The course will thus necessarily cover only a tiny subset of the field. \n \nMany good ideas in systems research are the result of discussing and debating previous work. \nA primary aim of this course therefore will be to educate students in the art of critical thinking: \nthe ability to argue for and/or against a particular approach or idea. This will be done by having \nstudents read and critique a set of papers each week. In addition, each week will include \npresentations from a number of participants which aim to advocate or criticise each piece of \nwork. \n \nSyllabus \nThe syllabus for this course will vary from year to year so as to cover a mixture of older and \nmore contemporary systems papers. Contemporary papers will be generally selected from the \npast 5 years, primarily drawn from high quality conferences such as SOSP, OSDI, ASPLOS, \nFAST, NSDI and EuroSys. Example topics might include: \n \nSystems Research and System Design \nOS Structure and Virtual Memory \nVirtualisation \nConsensus \nScheduling \nPrivacy \nData Intensive Computing \nBugs \nThe reading each week will involve a load equivalent to 3 full length papers. Students will be \nexpected to read these in detail and prepare a written summary and review. In addition, each \nweek will contain one or more short presentations by students for each paper. The types of \npresentation will include: \n \nOverview: a balanced presentation of the paper, covering both positive and negative aspects. \nAdvocacy: a positive spin on the paper, aiming to convince others of its value. \nCriticism: a negative take on the paper, focusing on its weak spots and omissions. \nThese presentation roles will be assigned in advance, regardless of the soi disant absolute merit \nof the paper or the preference of the student. Furthermore, all students \u2013 regardless of any \nassigned presentation role in a given week \u2013 will be expected to participate in the class by \nasking questions and generally entering into the debate. \n \nObjectives \n\nOn completion of this module students should have a broad understanding of some key papers \nand concepts in computer systems research, as well as an appreciation of how to argue for or \nagainst any particular idea. \n \nCoursework and practical work \nCoursework will be the production of the weekly paper reviews. Practical work will be presenting \npapers as appropriate, as well as ongoing participation in the class. \n \nAssessment \nAssessment consists of: \n \nOne essay per week for 7 weeks (10% each) \nPresentation (20%) \nParticipation in class over the term (10%) \nRecommended reading \nMost of the reading for this course will be in the form of the selected papers each week. \nHowever, the following may be useful background reading to refresh your knowledge from \nundergraduate courses: \n \nSilberschatz, A., Peterson, J.L. and Galvin, P.C. (2005). Operating systems concepts. \nAddison-Wesley (7th ed.). \n \nTanenbaum, A.S. (2008). Modern Operating Systems. Prentice-Hall (3rd ed.). \n \nBacon, J. and Harris, T. (2003). Operating systems. Addison-Wesley (3rd ed.). \n \nAnderson, T. and Dahlin, M. (2014). Operating Systems: Principles and Practice. Recursive \nBooks (2nd ed.). \n \nHennessy, J. and Patterson, D. (2006). Computer architecture: a quantitative approach. Elsevier \n(4th ed.). ISBN 978-0-12-370490-0. \n \nKleppmann M (2016) Designing Data-Intensive Applications, O'Reilly (1st ed.)\n \n\nADVANCED TOPICS IN MACHINE LEARNING \nAims \nThis course explores current research topics in machine learning in sufficient depth that, at the \nend of the course, participants will be in a position to contribute to research on their chosen \ntopics. Each topic will be introduced with a lecture which, building on the material covered in the \nprerequisite courses, will make the current research literature accessible. Each lecture will be \nfollowed by up to three sessions which will typically be run as a reading group with student \npresentations on recent papers from the literature followed by a discussion, or a practical, or \nsimilar. \n \nStructure \nEach student will attend 3 topics and each topic's sessions will be spread over 5 contact hours. \nStudents will be expected to undertake readings for their selected topics. There will be some \ngroup work. \n \nThere will be a briefing session in Michaelmas term. \n \nSyllabus \nStudents choose five topics in preferential order from a list to be published in Michaelmas term. \nThey will be assigned to three topics out of their list. Students are assessed on one of these \ntopics which may not necessarily be their first choice topic. \n \nThe topics to be offered in 2024-25 are yet to be decided but to give an indicative idea of the \ntypes of topics, the ones offered in 2023-24 were: \n \nImitation learning  Prof A. Vlachos \nThe Future of Large Language Models: data architectures ethics Dr M. Tomalin \nPhysics and Geometry in Machine Learning Dr C. Mishra \nDiffusion Models and SDEs Francisco Vargas, Dr C. H. Ek \nExplainable Artificial Intelligence M. Espinosa, Z. Shams, Prof M. Jamnik \nUnconventional approaches to AI Dr S. Banerjee \nNarratives in Artificial Intelligence and Machine Learning Prof N. Lawrence \nMultimodal Machine Learning K. Hemker, N. Simidjievski, Prof M. Jamnik \nDeep Reinforcement Learning S. Morad, Dr C. H. Ek \nAutomation in Proof Assistants A. Jiang, W. Li, Prof M. Jamnik \nOn completion of this module, students should: \n \nbe in a strong position to contribute to the research topics covered; \nunderstand the fundamental methods (algorithms, data analysis, specific tasks) underlying each \ntopic; \nand be familiar with recent research papers and advances in the field. \nCoursework \nStudents will typically work in groups to give a presentation on assigned papers. Alternatively, a \ntopic may include practical sessions. Each topic will typically consist of one preliminary lecture \n\nfollowed by 3 reading and discussion sessions, or several lectures followed by a practical \nsession. A typical topic can accommodate up to 9 students presenting papers. There will be at \nleast 10 minutes general discussion per session. \n \nFull coursework details will be published by October. \n \nAssessment \nCoursework will be marked by the topic leaders and second marked by the module conveners. \n \nParticipation in all assigned topics, 10% \nPresentation or practical work or similar (for one of the chosen topics), 20% \nTopic coursework (for one of the chosen topics), 70% \nIndividual topic coursework will be published late Michaelmas term. \n \nAssessment criteria for topic coursework will follow project assessment criteria here: \nhttps://www.cl.cam.ac.uk/teaching/exams/acs_project_marking.pdf \n \nPlease note that students will be assessed on one of their three chosen topics but this may not \nbe their first choice. \n \nRecommended reading \nTo be confirmed by each topic convenor.\n \n\nCOMPUTING FOR COLLECTIVE INTELLIGENCE \nPrerequisites: Familiarity with core machine learning paradigms - Basic calculus (analytical \nskills) - Basic discretre optimization (combinatorics) \nMoodle, timetable \n \nObjectives \nThere is a substantial body of academic work demonstrating that complex life is built by \ncooperation across scales: collections of genes cooperate to produce organisms, cells \ncooperate to produce multi-cellular organisms and multicellular animals cooperate to form \ncomplex social groups. Arguably, all intelligence is collective intelligence. Yet, to-date, many \nartificially intelligent agents (both embodied and virtual), are generally not conceived from the \nground up to interact with other intelligent agents (be it machines or humans). The canonical AI \nproblem is that of a monolithic and solitary machine confronting a non-social environment. This \ncourse aims to balance this trend by (i) equipping students with conceptual and practical \nknowledge on collective intelligence from a computational standpoint, and (ii) by conveying \nvarious computational paradigms by which collective intelligence can be modelled as well as \nsynthesized. \n \nAssessment \nThe assessment will be based on a reading-group paper presentation (individual or in pairs), \naccompanied by a 2-page summary, and a technical position paper (individual work) handed in \nafter the course. The position paper will be assessed based on its technical correctness, \nstrength or arguments, \nand clarity of research vision, and will be accompanied by a brief 5-minute pre-recorded talk that \nsummarizes key arguments (any slides used are also submitted as part of the project). \n \nPaper presentation and 1-page summary: 25% \nTechnical position paper: 75% (4000 word limit) \nRecommended reading \nSwarm Intelligence : From Natural to Artificial Systems, Bonabeau et al (1999) \nJoined-Up Thinking, Hannah Critchlow, (2024) \nSupercooperators, Martin Nowak (2011) \nA Course on Cooperative Game Theory, Chakravarty et al., (2015) \nGoverning the Commons: The Evolution of Institutions for Collective Action. Ostrom (1990).  \n \n\nCRYPTOGRAPHY AND PROTOCOL ENGINEERING \nAims \nWe all use cryptographic protocols every day: whenever we access an https:// website or send a \nmessage via WhatsApp or Signal, for example. In this module, students will get hands-on \nexperience of how those protocols are implemented. Students start by writing their own secure \nmessaging protocol from scratch, and then move on to more advanced examples of \ncryptographic protocols for security and privacy, such as private information retrieval (searching \nfor data without revealing what you\u2019re searching for). \n \nSyllabus \nThis is a practical course in which the first half of each of the 2-hour weekly sessions is a lecture \nthat introduces a topic, while the second half is lab time during which the students can work on \ntheir implementations, discuss the topics in small groups, and get help from the lecturers. The \nmodule is structured into three blocks as follows:  \n \nCryptographic primitives (2 weeks). Using common building blocks such as symmetric ciphers \nand hash functions. Implementing selected aspects of elliptic curve Diffie-Hellman and digital \nsignatures. Protocol security, Dolev-Yao model, side-channel attacks.  \nSecure communication (3 weeks). Authenticated key exchange (e.g. SIGMA, TLS), \npassword-authenticated key exchange (e.g. SPAKE2), forward secrecy, post-compromise \nsecurity, double ratchet, the Signal protocol.  \nPrivate database lookups (3 weeks). Lattice-based post-quantum cryptography, learning with \nerrors, homomorphic encryption, private information retrieval.  \nIn each block, students will be given a description of the algorithms and protocols covered (e.g. \na research paper or an RFC standards document), and a code template that the students \nshould use for their implementation. The implementation language will probably be Python (to \nbe confirmed after the practical materials have been developed ). Students will also be given a \nset of test cases to check their code, and will have the opportunity to test their implementation \ncommunicating with other students\u2019 implementations. Finally, for each block, students will write \nand submit a lab report explaining their approach and the findings from their implementation. \n \nObjectives \nOn completion of this module, students should: \n \nUnderstand that cryptography is not magic! The mathematics can look intimidating, but this \nmodule will show students that they needn\u2019t be afraid of cryptography. \nGain appreciation for the complexity and careful implementation of real-world cryptography \nlibraries ,and understand why one should prefer vetted implementations. \nBe familiar with the mathematical notation and concepts commonly used in descriptions of \ncryptographic protocols, and be able to understand and implement research papers and RFCs \nin this field. \nBe comfortable with cryptographic primitives commonly used in protocols, and able to correctly \nuse software libraries that implement them. \n\nHave gained hands-on experience of attacks on cryptographic protocols, and an appreciation \nfor the difficulty of making these protocols correct. \nHave been exposed to ideas and techniques from recent research, which may be useful for their \nown future research. \nHave practised technical writing through lab reports. \nAssessment \nStudents will be provided with code skeletons in one or two different programming languages \n(probably Python, maybe Rust) to avoid them struggling with setup issues. For each \nassignment, students are required to produce a working implementation of the protocols \ndiscussed in the course, using the programming language and skeleton provided. Here, \n\u201cworking\u201d means that the algorithms are functionally correct, but they are not production-quality \n(in particular, no side-channel countermeasures such as constant-time algorithms are required). \nFor some primitives (e.g. symmetric ciphers and hash functions) existing libraries should be \nused, whereas other cryptographic algorithms will be implemented from scratch. Students \nshould submit their code as a Docker container that can be run against specified test cases.  \n \nAfter each of the three blocks, students will be asked to submit a lab report of approximately \n2,000 words along with their code for that block. In the lab report, the students should explain \nhow their implementation works and why it is correct, as well as any findings from the work (e.g. \nany limitations or trade-offs they found with the protocols). The report structure and marking \nscheme will be specified in advance. The lab reports provide an opportunity for students to \nprovide critical insights and creativity. For instance, they might compare their implementation \nwith existing real-world implementations which provide additional features and guarantees.  \n \nEach of the three lab reports (along with the associated code) will be marked, forming the basis \nof assessment, and feedback on the reports will be given to the students before the next \nsubmission is due, so that students can take the feedback on board for their next submission. \nOf the three reports, the worst mark will be discarded, and the other two reports each contribute \n50% of the final mark. As such, students might decide to submit only two reports.  \n \nStudents may discuss their work with others, but the implementations and lab reports must be \nindividual work. \n \nRecommended reading \nTextbook: Jonathan Katz and Yehuda Lindell. Introduction to Modern Cryptography (3rd edition). \nCRC Press, 2020. \n \nThe textbook covers prerequisite knowledge on cryptographic primitives, such as ciphers, \nMACs, hash functions, and signatures. This book is also used in the Part II Cryptography \ncourse. For the protocols we cover in this course, we are not aware of a suitable textbook; \ninstead we will use research papers, lecture slides, and RFCs as resources, which will be \nprovided at the start of the module.\n \n\nDISTRIBUTED LEDGER TECHNOLOGIES: FOUNDATIONS AND APPLICATIONS \n \nAims \nThis reading group course examines foundations and current research into distributed ledger \n(blockchain) technologies and their applications. Students will read, review, and present seminal \nresearch papers in this area. Once completed, students should be able to integrate blockchain \ntechnologies into their own research and gain familiarity with a range of research skills. \n \nLectures \nIntroduction \nConsensus protocols \nBitcoin and its variants \nEthereum, smart contracts, and other permissionless DLTs \nHybrid and permissioned DLTs \nApplications \nLearning objectives \nThere are two broad objectives: to acquire familiarity with a body of work in the area of \ndistributed ledgers and to learn some specific research skills: \n \nHow to read a paper \nHow to review a paper \nHow to analyze a paper\u2019s strengths and weaknesses \nWritten and oral presentation skills \nAssessment \nYou are expected to read all assigned papers and submit paper reviews each week. Each \nreview must either follow the provided review form [PDF] [Latex source]. Each \u201creview\u201d is worth \n5% of your total mark, and is marked out of 100 with 60 a passing grade. Marks will be awarded \nand penalties for late submission applied according to ACS Assessment Guidelines.  \n \nOne paper review for the first week, then two paper reviews each week for 6 weeks (13 reviews, \n5% each) 65% (approx 600 words per review) \nSummative essay 25% (max 3000 words) \nPresentation 5% \n100% attendance in class 5% (2 marks deducted per missed class) \nRecommended Reading \nNarayanan, A. , Bonneau, J., Felten, E., Miller, A. and Goldfeder, S. (2016). Bitcoin and \nCryptocurrency Technologies: A Comprehensive Introduction. Princeton University Press. \n(2016 Draft available here: \nhttps://d28rh4a8wq0iu5.cloudfront.net/bitcointech/readings/princeton_bitcoin_book.pdf)\n \n\nEXPLAINABLE ARTIFICIAL INTELLIGENCE \nPrerequisites: A solid background in statistics, calculus and linear algebra. We strongly \nrecommend some experience with machine learning and deep neural networks (to the level of \nthe first chapters of Goodfellow et al.\u2019s \u201cDeep Learning\u201d). Students are expected to be \ncomfortable reading and writing Python code for the module\u2019s practical sessions. \nMoodle, timetable \n \nAims \nThe recent palpable introduction of Artificial Intelligence (AI) models to everyday \nconsumer-facing products, services, and tools brings forth several new technical challenges and \nethical considerations. Amongst these is the fact that most of these models are driven by Deep \nNeural Networks (DNNs), models that, although extremely expressive and useful, are \nnotoriously complex and opaque. This \u201cblack-box\u201d nature of DNNs limits their ability to be \nsuccessfully deployed in critical scenarios such as those in healthcare and law. Explainable \nArtificial Intelligence (XAI) is a fast-moving subfield of AI that aims to circumvent this crucial \nlimitation of DNNs by either  \n \n(i) constructing human-understandable explanations for their predictions,  \n \nor (ii) designing novel neural architectures that are interpretable by construction.  \n \nIn this module, we will introduce the key ideas behind XAI methods and discuss some of the \nimportant application areas of these methods (e.g., healthcare, scientific discovery, debugging, \nmodel auditing, etc.). We will approach this by focusing on the nature of what constitutes an \nexplanation, and discussing different ways in which explanations can be constructed or learnt to \nbe generated as a by-product of a model. The main aim of this module is to introduce students \nto several commonly used approaches in XAI, both theoretically in lectures and through \nhands-on exercises in practicals, while also bringing recent promising directions within this field \nto their attention. We hope that, by the end of this module, students will be able to directly \ncontribute to XAI research and will understand how methods discussed in this module may be \npowerful tools for their own work and research. \n \nSyllabus \nOverview and taxonomy of XAI (why is explainability needed, definition of terms, taxonomy of \nthe XAI space, etc.)  \nPerturbation-based feature attribution (e.g., LIME, Anchors, SHAP, etc.) \nPropagation-based feature attribution methods (e.g., Relevance Propagation, Saliency \nmethods, etc.) \nConcept-based explainability (Net2Vec, T-CAV, ACE, etc.) \nInterpretable architectures (CBMs and variants, Concept Whitening, SENNs, etc.) \nNeurosymbolic methods (DeepProbLog, Neural Reasoners, etc.) \nSample-based Explanations (Influence functions, ProtoPNets, etc.) \nCounterfactual explanations \nProposed Schedule \n\nThe 16 hours of lectures across 8 weeks will be divided as follows:  \nWeek 1: 1h lecture + 1h lecture  \nWeek 2: 1h lecture + 1h reading group & presentations  \nWeek 3: 1h lecture + 1h reading group & presentations  \nWeek 4: 2h practical  \nWeek 5: 1h lecture + 1h reading group & presentations  \nWeek 6: 2h practical  \nWeek 7: 1h lecture + 1h reading group & presentations  \nWeek 8: 1h reading group & presentation + 1h reading group & presentations \n \nIn weeks where lectures are planned, one-hour lecture slots will intercalate with one hour of \nstudent paper presentations. During each paper presentation session, three students will \npresent a paper related to the topic covered in the earlier lecture that week for about 10 minutes \neach + 5 minutes of questions. At the end of all paper presentations, there will be a discussion \non all the papers. The order of student presentations will be allocated randomly during the first \nweek so that students know in advance when they are expected to present. For the sake of \nfairness, we will release the paper to be presented by each student a week before their \npresentation slot. \n \nObjectives \nBy the end of this module, students should be able to: \n \nRecognise and identify key concepts in XAI together with their connection to related subfields in \nAI such as fairness, accountability, and trustworthy AI. \nUnderstand how to use, design, and deploy model-agnostic perturbation methods such as \nLIME, Anchors, and RISE. In particular, students should understand the connection between \nfeature importance and cooperative game theory, and its uses in methods such as SHAP. \nIdentify the uses and limitations of propagation-based feature importance methods such as \nSaliency, SmoothGrad, GradCAM, and Integrated Gradients. Students should be able to \nimplement each of these methods on their own and connect the theoretical ideas behind them \nto practical code, exploiting modern frameworks\u2019 auto-differentiation. \nUnderstand what concept learning is and what limitations it overcomes compared to traditional \nfeature-based methods. Specifically, students should understand how probing a DNN\u2019s latent \nspace may be exploited for learning useful concepts for explainability. \nReason about the key components of inherently interpretable architectures and neuro-symbolic \nmethods and understand how interpretable neural networks can be designed from first \nprinciples. \nElaborate on what sample-based explanations are and how influence functions and prototypical \narchitectures such as ProtoPNet can be used to construct such explanations. \nExplain what counterfactual explanations are, how they are related to causality, and under which \nconditions they may be useful. \nUpon completion of this module, students will have the technical background and tools to use \nXAI as part of their own research or partake in XAI research itself. Moreover, we hope that by \ndetailing a clear timeline of how this relatively young subfield has developed, students may be \n\nable to better understand what are some fundamental open questions in this area and what are \nsome promising directions that are currently actively being explored. \n  \n \nAssessment \n(10%) student presentation: each student will be randomly assigned a presentation slot at the \nbeginning of the course. We will then distribute a paper for them to present in their slot a week \nbefore the presentation\u2019s scheduled time. Students are expected to prepare a 10-minute \npresentation of their assigned paper where they will present the motivation of their assigned \nwork and discuss the main methodology and findings reported in that paper. We will encourage \nstudents to focus on fully communicating the intuition of the work in their assigned papers and \ntry and connect it with ideas that we have previously discussed in previous lectures. All students \nnot presenting each week will be asked to submit one question pertaining to each paper \npresented that week before the paper presentations. \n \n(20%) practical exercises: We will run two practical sessions where students will be asked to \nperform a series of exercises that require them to use concepts we have introduced in lecture \nup to that point. For each practical session, we will prepare a colab notebook to guide the \nstudent through exercises and we will ask the students to submit their solutions through this \ncolab notebook. We expect students to complete about \u2154 of the exercises in the practical class \nand complete the rest at home as homework. Each practical will be worth 10%. \n \n(70%) mini-project: At the end of week 3, we will hand out a list of papers for students to select \ntheir mini-projects from. Each mini-project will consist of a student selecting a paper from our list \nand reimplementing and expanding the key idea in the paper. We encourage students to be as \ncreative as they want with how they drive their mini-project once the paper has been selected. \nFor example, they can reimplement the technique in the paper and combine it with \nmethodologies from other works we discussed in lecture, or they can apply their paper\u2019s \nmethodology to a new domain, datasets, or setup, where the technique may offer interesting \nand potentially novel insights. We will ask all students to submit a report in a workshop format of \nup to 4,000 words. This report, due roughly a week after Lent term ends, should describe their \nmethodology, experiments, and results. A crucial aspect of this report involves explaining the \nrationale behind different choices in methodology and experiments, as well as elaborating on \nthe choices made and hypotheses tested throughout their mini-project (potentially showing a \ndeep understanding of the work they are basing their mini-project on). To aid students with \nselecting their projects and making progress on them, we will hold regular office hours when \nstudents can come to discuss their progress and questions with us. \n \nRecommended reading \nTextbooks \n \n* Christoph Molnar, \u201cInterpretable Machine Learning\u201d. (2022): \nhttps://christophm.github.io/interpretable-ml-book/ \n \n\nOnline Courses and Tutorial \n \n* Su-in Lee and Ian Cover, \u201cCSEP 590B Explainable AI\u201d University of Washington* Explaining \nMachine Learning Predictions: State-of-the-art, Challenges, and Opportunities \n \n* On Explainable AI: From Theory to Motivation, Industrial Applications, XAI Coding & \nEngineering Practices - AAAI 2022 Turorial \n \nSurvey papers \n \n* Arrieta, Alejandro Barredo, et al. \"Explainable Artificial Intelligence (XAI): Concepts, \ntaxonomies, opportunities and challenges toward responsible AI.\" Information fusion 58 (2020): \n82-115. \n \n* Rudin, Cynthia, et al. \"Interpretable machine learning: Fundamental principles and 10 grand \nchallenges.\" Statistics Surveys 16 (2022): 1-85.\n \n\nFEDERATED LEARNING: THEORY AND PRACTICE \nPrerequisites: It is strongly recommended that students have previously (and successfully) \ncompleted an undergraduate machine learning course - or have equivalent experience through \nopen-source material (e.g., lectures or similar). An example course would be: Part1B \"Artificial \nIntelligence\". Students should feel comfortable with SGD and optimization methods used to train \ncurrent popular neural networks and simple forms of neural network architectures. \nMoodle, timetable \n \nObjectives \nThis course aims to extend the machine learning knowledge available to students in Part I (or \npresent in typical undergraduate degrees in other universities), and allow them to understand \nhow these concepts can manifest in a decentralized setting. The course will consider both \ntheoretical (e.g., decentralized optimization) and practical (e.g., networking efficiency) aspects \nthat combine to define this growing area of machine learning.  \n \nAt the end of the course students should: \n \nUnderstand popular methods used in federated learning \nBe able to construct and scale a simple federated system \nHave gained an appreciation of the core limitations to existing methods, and the approaches \navailable to cope with these issues \nDeveloped an intuition for related technologies like differential privacy and secure aggregation, \nand are able to use them within typical federated settings  \nCan reason about the privacy and security issues with federated systems \nLectures \nCourse Overview. Introduction to Federated Learning. \nDecentralized Optimization. \nStatistical and Systems Heterogeneity. \nVariations of Federated Aggregation. \nSecure Aggregation. \nDifferential Privacy within Federated Systems. \nExtensions to Federated Analytics. \nApplications to Speech, Video, Images and Robotics. \nLab sessions \nFederating a Centralized ML Classifier. \nBehaviour under Heterogeneity. \nScaling a Federated Implementation. \nExploring Privacy with Federated Settings \nRecommended Reading \nReadings will be assigned for each lecture. Readings will be taken from either research papers, \ntutorials, source code or blogs that provide more comprehensive treatment of taught concepts. \n \nAssessment - Part II Students \n\nFour labs are performed during the course, and students receive 10% of their total grade for \nwork done as part of each lab. (For a total of 40% of the total grade from lab work alone). Labs \nwill primarily provide hands-on teaching opportunities, that are then utilized within the lab \nassignment which is completed outside of the lab contact time. MPhil and Part III students will \nbe given additional questions to answer within their version of the lab assignment which will \ndiffer from the assignment given to Part II CST students. \n \nThe remainder of the course grade (60%) will be given based on a hands-on project that applies \nthe concepts taught in lectures and labs. This hands-on project will be assessed based on upon \na combination of source code, related documentation and brief 8-minute pre-recorded talk that \nsummarizes key project elements (any slides used are also submitted as part of the project). \nPlease note, that in the case of Part II CST students, the talk itself is not examinable -- as such \nwill be made optional to those students. \n \nA range of possible practical projects will be described and offered to students to select from, or \nalternatively students may propose their own. MPhil and Part III students will select from a \nproject pool that is separate from those offered to Part II CST students. MPhil and Part III \nprojects will contain a greater emphasis on a research element, and the pre-recorded talks \nprovided by this student group will focus on this research contribution. The project will be \nassessed on the level of student understanding demonstrated, the degree of difficulty, \ncorrectness of implementation -- and for Part III/MPhil students the additional criteria of the \nquality and execution of the research methodology, and depth and quality of results analysis. \n \nThis project can be done individually or in groups -- although individual projects will be strongly \nencouraged. It will be required the project is performed using a code repository that also will \ncontain all documentation -- access to this repository will be shared with course staff (e.g., \nlecturer and TAs). Where needed, marks assigned to students within a group will be \ndifferentiated using this repository as an input. Furthermore if groups are formed, members must \nbe either entirely from Part III/MPhil students or Part II CST, i.e., these two student groups \nshould not mix to form a project group. \n \nProjects will be made available publicly. A maximum word count for written contributions for the \nproject will be enforced.  \n \n  \nAssessment - MPhil / Part III Students \n50% final mini-project. This hands-on project will be assessed based on upon a combination of \nsource code, related documentation and brief 8-minute pre-recorded talk that summarizes key \nproject elements (any slides used are also submitted as part of the project). \n \n30% midterm lab report (a written report of a bit more depth of thought than required in a lab, \nthe report provides the results of experiments -- with the skills to perform these experiments \ncoming from lab sessions),  \n \n\n20% for two individual hands-on labs. Labs will primarily provide hands-on teaching \nopportunities, that are then utilized within the lab assignment which is completed outside of the \nlab contact time. There will also be additional questions to answer within their version of the lab \nassignment. \n \nFor the mini-project, a range of possible practical projects will be described and offered to \nstudents to select from, or alternatively students may propose their own. The project will be \nassessed on the level of student understanding demonstrated, the degree of difficulty, \ncorrectness of implementation, the quality and execution of the research methodology, and \ndepth and quality of results analysis. \n \nThe project can be done individually or in groups -- although individual projects will be strongly \nencouraged. It will be required the project is performed using a code repository that also will \ncontain all documentation -- access to this repository will be shared with course staff (e.g., \nlecturer and TAs). Where needed, marks assigned to students within a group will be \ndifferentiated using this repository as an input. Furthermore if groups are formed, members must \nbe either entirely from Part III or entirely from MPhil students, i.e., these two student groups \nshould not mix to form a project group. \n \nProjects will be made available publicly. A maximum word count for written contributions for the \nproject will be enforced.   \n \nAdvanced Material sessions - MPhil / Part III Students \nThese sessions are mandatory for the MPhil and Part III students. Part II CST students may \nattend if they wish \n \nTopics and announced the first week of class. Selected to extend beyond topics covered in \nlectures and labs. Content is a mixture of sessions run in the lecture room the are either (1) \npresentations by guest lectures by an invited domain expert or (2) class-wide discussions \nregarding one or more related academic papers. Paper discussions will require students to read \nthe paper ahead of the lecture, and a brief discussion primer presentation will be given students \nbefore discussions begin.  \n \nThis module is shared with Part II of the Computer Science Tripos. Assessment will be adjusted \nfor the two groups of students to be at an appropriate level for whichever course the student is \nenrolled on. Further information about assessment and practicals will follow at the first lecture.\n\nGEOMETRIC DEEP LEARNING \n \nPrerequisites: Experience with machine learning and deep neural networks is recommended. \nKnowledge of concepts from graph theory and group theory is useful, although the relevant \nparts will be explicitly retaught. \nMoodle, timetable \n \nAims and Objectives \nMost of the patterns we see in nature can be elegantly reasoned about using spatial \nsymmetries\u2014transformations that leave underlying objects unchanged. This observation has \nhad direct implications for the development of modern deep learning architectures that are \nseemingly able to escape the \u201ccurse of dimensionality\u201d and fit complex high-dimensional tasks \nfrom noisy real-world data. Beyond this, one of the most generic symmetries\u2014the \npermutation\u2014will prove remarkably powerful in building models that reason over graph \nstructured-data, which is an excellent abstraction to reason about naturally-occurring, \nirregularly-structured data. Prominent examples include molecules (represented as graphs of \natoms and bonds, with three-dimensional coordinates provided), social networks and \ntransportation networks. Several already-impacted application areas include traffic forecasting, \ndrug discovery, social network analysis and recommender systems. The module will provide the \nstudents the capability to analyse irregularly- and nontrivially-structured data in an effective way, \nand position geometric deep learning in a proper context with related fields. The main aim of the \ncourse is to enable students to make direct contributions to the field, thoroughly assimilate the \nkey concepts in the area, and draw relevant connections to various other fields (such as NLP, \nFourier Analysis and Probabilistic Graphical Models). We assume only a basic background in \nmachine learning with deep neural networks. \n \nLearning outcomes \nThe framework of geometric deep learning, and its key building blocks: symmetries, \nrepresentations, invariance and equivariance \nFundamentals of processing data on graphs, as well as impactful application areas for graph \nrepresentation learning \nTheoretical principles of graph machine learning: permutation invariance and equivariance \nThe three \"flavours\" of spatial graph neural networks (GNNs) (convolutional, attentional, \nmessage passing) and their relative merits. The Transformer architecture as a special case. \nAttaching symmetries to graphs: CNNs on images, spheres and manifolds, Geometric Graphs \nand E(n)-equivariant GNNs \nRelevant connections of geometric deep learning to various other fields (such as NLP, Fourier \nAnalysis and Probabilistic Graphical Models) \nLectures \nThe lectures will cover the following topics: \n \nLearning with invariances and symmetries: geometric deep learning. Foundations of group \ntheory and representation theory. \n\nWhy study data on graphs? Success stories: drug screening, travel time estimation, \nrecommender systems. Fundamentals of graph data processing: network science, spectral \nclustering, node embeddings. \nPermutation invariance and equivariance on sets and graphs. The principal tasks of node, edge \nand graph classification. Neural networks for point clouds: Deep Sets, PointNet; universal \napproximation properties. \nThe three flavours of spatial GNNs: convolutional, attentional, message passing. Prominent \nexamples: GCN, SGC, ChebyNets, MoNet, GAT, GATv2, IN, MPNN, GraphNets. Tradeoffs of \nusing different GNN variants. \nGraph Rewiring: how to apply GNNs when there is no graph? Links to natural language \nprocessing---Transformers as a special case of attentional GNNs. Representative \nmethodologies for graph rewiring: GDC, SDRF, EGP, DGCNN. \nExpressive power of graph neural networks: the Weisfeiler-Lehman hierarchy. GINs as a \nmaximally expressive GNN. Links between GNNs and graph algorithms: neural algorithmic \nreasoning. \nCombining spatial symmetries with GNNs: E(n)-equivariant GNNs, TFNs, SE(3)-Transformer. A \ndeep dive into AlphaFold 2. \nWorked examples: Circulant matrices on grids, the discrete Fourier transform, and convolutional \nnetworks on spheres. Graph Fourier transform and the Laplacian eigenbasis. \nPracticals \nThe practical is designed to complement the knowledge learnt in lectures and teach students to \nderive additional important results and architectures not directly shown in lectures. The practical \nwill be given as a series of individual exercises (each either code implementation or \nproof/derivation). Each of these exercises can be individually assessed based on a specified \nmark budget. \n \nPossible practical topics include the study of higher-order GNNs and equivariant message \npassing. \n \nAssessment \n(60%) Group Mini-project (writeup) at the end of the course. The mini projects can either be \nself-proposed, or the students can express their preference for one of the provided topics, the \nlist of which will be announced at the start of term. The projects will consist of implementing \nand/or extending graph representation learning models in the literature, applying them to \npublicly available datasets. Students will undertake the project in pairs and submit a joint \nwriteup limited to 4,000 words (in line with other modules); appendix of work logs to be included \nbut ungraded; \n \n(10%) Short presentation and viva: students will give a short presentation to explain their \nindividual contribution to the mini-project and there will be a short viva following. \n \n(30%) Practical work completion. Completing the exercises specified in the practical to a \nsatisfactory standard. The practical assessor should be satisfied that the student derived their \nanswers using insight gained from the course; coupled with original thought, not by simple \n\ncopy-pasting of relevant related work. The students would submit code and a short report, which \nwould then be marked in line with the predetermined mark budget for each practical item. \nThe students will learn how to run advanced architectures on GPU but no specific need for \ndedicated GPU resources. Practicals will be made possible to do on CPU; if required, students \ncan use GPUs on publicly available free services (such as Colab) for their mini-project work. \n \nReferences \nThe course will be based on the following literature: \n \n\"Geometric Deep Learning: Grids, Graphs, Groups, Geodesics, and Gauges\", by Michael \nBronstein, Joan Bruna, Taco Cohen and Petar Veli\u010dkovi\u0107 \n\"Graph Representation Learning\", by Will Hamilton \n\"Deep Learning\", by Ian Goodfellow, Yoshua Bengio and Aaron Courville.\n \n\nMOBILE HEALTH \nAims \nThe course aims to explore how wearable and mobile systems sensors can be used to gather \ndata relevant to understand health, how the data can be analysed with advanced signal \nprocessing and machine learning and the performance of these systems in terms of diagnostics \nand disease progression detection. \n \nSyllabus \nCourse Overview. Introduction to Mobile Health. Evaluation metrics and methodology. Basics of \nSignal Processing. \nInertial Measurement Units, Human Activity Recognition (HAR) and Gait Analysis and Machine \nLearning for IMU data. \nRadios, Bluetooth, GPS and Cellular. Epidemiology and contact tracing, Social interaction \nsensing and applications. Location tracking in health monitoring and public health. \nAudio Signal Processing. Voice and Speech Analysis: concepts and data analysis. Body \nSounds analysis. \nPhotoplethysmogram and Light sensing for health (heart and sleep) \nContactless and wireless behaviour and physiological monitoring \nGenerative Models for Wearable Health Data \nTopical Guest Lectures \nObjectives \nThe course aims to explore how wearable and mobile systems sensors can be used to gather \ndata relevant to understand health, how the data can be analysed with advanced signal \nprocessing and machine learning and the performance of these systems in terms of diagnostics \nand disease progression detection. \n \nRoughly, each lecture contains a theory part about the working of \u201csensor signals\u201d or \u201cdata \nanalysis methods\u201d and an application part which contextualises the concepts. \n \nAt the end of the course students should: Understand how mobile/wearable sensors capture \ndata and their working. Understand different approaches to acquiring and analysing sensor data \nfrom different types of sensors. Understand the concept of signal processing applied to time \nseries data and their practical application in health. Be able to extract sensor data and analyse it \nwith basic signal processing and machine learning techniques. Be aware of the different health \napplications of the various sensor techniques. The course will also touch on privacy and ethics \nimplications of the approaches developed in an orthogonal fashion. \n \nRecommended Reading \nPlease see Course Materials for recommended reading for each session. \n \nAssessment - Part II students \nTwo assignments will be based on two datasets which will be provided to the students: \n \n\nAssignment 1 (shared for Part II and Part III/MPhil): this will be based on a dataset and will be \nworth 40% of the final mark. The task of the assessment will be to perform pre-processing and \nbasic data analysis in a \"colab\" and an answer sheet of no more than 1000 words. \n \nAssignment 2 (Part II): This assignment (worth 60% of the final mark) will be a fuller analysis of \na dataset focusing on machine learning algorithms and metrics. Discussion and interpretation of \nthe findings will be reported in a colab and a report of no more than 1200 words. \n \nAssessment  - Part III and MPhil students \nTwo assignments will be based on datasets which will be provided to the students: \n \nAssignment 1: this will be based on a dataset and will be worth 40% of the final mark. The task \nof the assessment will be to perform pre-processing and basic data analysis in a \"colab\" and an \nanswer sheet of no more than 1000 words. \n \nAssignment 2: The second assignment (worth 60% of the final mark) will be based on multiple \ndatasets. The students will be asked to compare and contract ML algorithms/solutions (trained \nand tested on the different data) and discuss the findings and interpretation in terms of health \ncontext. This will be in the form of a colab and a report of 1800 words. \n \nFurther Information \nCurrent Cambridge undergraduate students who are continuing onto Part III or the MPhil in \nAdvanced Computer Science may only take this module if they did NOT take it as a Unit of \nAssessment in Part II. \n \nThis module is shared with Part II of the Computer Science Tripos. Assessment will be adjusted \nfor the two groups of students to be at an appropriate level for whichever course the student is \nenrolled on. Further information about assessment and practicals will follow at the first lecture.\n\nMULTICORE SEMANTICS AND PROGRAMMING \nPrerequisites: Some familiarity with discrete mathematics (sets, partial orders, etc.) and with \nsequential Java programming will be assumed. Experience with operational semantics and with \nsome concurrent programming would be helpful. \nMoodle, timetable \n \nAims \nIn recent years multiprocessors have become ubiquitous, but building reliable concurrent \nsystems with good performance remains very challenging. The aim of this module is to \nintroduce some of the theory and the practice of concurrent programming, from hardware \nmemory models and the design of high-level programming languages to the correctness and \nperformance properties of concurrent algorithms. \n \nLectures \nPart 1: Introduction and relaxed-memory concurrency [Professor P. Sewell] \n \nIntroduction. Sequential consistency, atomicity, basic concurrent problems. [1 block] \nConcurrency on real multiprocessors: the relaxed memory model(s) for x86, ARM, and IBM \nPower, and theoretical tools for reasoning about x86-TSO programs. [2 blocks] \nHigh-level languages. An introduction to C/C++11 and Java shared-memory concurrency. [1 \nblock] \nPart 2: Concurrent algorithms [Dr T. Harris] \n \nConcurrent programming. Simple algorithms (readers/writers, stacks, queues) and correctness \ncriteria (linearisability and progress properties). Advanced synchronisation patterns (e.g. some \nof the following: optimistic and lazy list algorithms, hash tables, double-checked locking, RCU, \nhazard pointers), with discussion of performance and on the interaction between algorithm \ndesign and the underlying relaxed memory models. [3 blocks] \nResearch topics, likely to include one hour on transactional memory and one guest lecture. [1 \nblock] \nObjectives \nBy the end of the course students should: \n \nhave a good understanding of the semantics of concurrent programs, both at the multprocessor \nlevel and the C/Java programming language level; \nhave a good understanding of some key concurrent algorithms, with practical experience. \nAssessment - Part II Students \nTwo assignments each worth 50% \n \nRecommended reading \nHerlihy, M. and Shavit, N. (2008). The art of multiprocessor programming. Morgan Kaufmann. \n \nCoursework \nCoursework will consist of assessed exercises. \n\n \nPractical work \nPart 2 of the course will include a practical exercise sheet.  The practical exercises involve \nbuilding concurrent data structures and measuring their performance.  The work can be \ncompleted in C, Java, or similar languages. \n \nAssessment - Part III and MPhil Students \nAssignment 1, for 50% of the overall grade. Written coursework comprising a series of questions \non hardware and software relaxed-memory concurrency semantics. \nAssignment 2, for 50% of the overall grade. Written coursework comprising a series of questions \non the design of mutual exclusion locks and shared-memory data structures. \nFurther Information \nCurrent Cambridge undergraduate students who are continuing onto Part III or the MPhil in \nAdvanced Computer Science may only take this module if they did NOT take it as a Unit of \nAssessment in Part II. \n \nThis module is shared with Part II of the Computer Science Tripos. Assessment will be adjusted \nfor the two groups of students to be at an appropriate level for whichever course the student is \nenrolled on. Further information about assessment and practicals will follow at the first lecture.\n\nREINFORCEMENT LEARNING \n \nAims \nThe aim of this module is to introduce students to the foundations of the field of reinforcement \nlearning (RL), discuss state-of-the-art RL methods, incentivise students to produce critical \nanalysis of recent methods, and prompt students to propose novel solutions that address \nshortcomings of existing methods. If judged by the headlines, RL has seen an unprecedented \nsuccess in recent years. However, the vast majority of RL methods still have shortcomings that \nmight not be apparent at first glance. The aim of the course is to inspire students by \ncommunicating the promising aspects of RL, but ensure that students develop the ability to \nproduce a critical analysis of current RL limitations. \n \nObjectives \nStudents will learn the following technical concepts: \n \nfundamental RL terminology and mathematical formalism; a brief history of RL and its \nconnection to neuroscience and biological systems \nRL methods for discrete action spaces, e.g. deep Q-learning and large-scale Monte Carlo Tree \nSearch \nmethods for exploration, modelling uncertainty, and partial observability for RL \nmodern policy gradient and actor-critic methods \nconcepts needed to construct model-based RL and Model Predictive Control methods \napproaches to make RL data-efficient and ways to enable simulation-to-reality transfer \nexamples of fine-tuning foundation models and large language models (LLMs) with human \nfeedback; safe RL concepts; examples of using RL for safety validation \nexamples of using RL for scientific discovery \nStudents will also gain experience with analysing RL methods to uncover their strengths and \nshortcomings, as well as proposing extensions to improve performance. \n \nFinally, students will gain skills needed to create and deliver a successful presentation in a \nformat similar to that of conference presentations. \n \nWith all of the above, students who take part in this module will be well-prepared to start \nconducting research in the field of reinforcement learning. \n  \n \nSyllabus \nTopic 1: Introduction and Fundamentals \n \nOverview of RL: foundational ideas, history, and books; connection to neuroscience and \nbiological systems, recent industrial applications and research demonstrations \nMathematical fundamentals: Markov decision processes, Bellman equations, policy and value \niteration, temporal difference learning \nTopic 2: RL in Discrete Action Spaces \n\n \nQ-learning, function approximation and deep Q-learning; nonstationarity in RL and its \nimplications for deep learning; example applications (video games; initial example: Atari) \nMonte Carlo Tree Search; example applications (AlphaGo) \nTopic 3: Exploration, Uncertainty, Partial Observability \n \nMulti-armed bandits, Bayesian optimisation, regret analysis \nPartially observable Markov decision process; belief, memory, and sequence modelling \n(probabilistic methods, recurrent networks, transformers) \nTopic 4: Policy Gradient and Actor-critic Methods for Continuous Action Spaces \n \nImportance sampling, policy gradient theorem, actor-critic methods (SPG, DDPG) \nProximal policy optimisation; example applications \nTopic 5: Model-based RL and Model Predictive Control \n \nLearning dynamics models (graph networks, stochastic processes, diffusion models, \nphysics-based models, ensembles); planning with learned models \nModel predictive control; example applications (real-time control) \nTopic 6: Data-efficient RL and Simulation-to-reality Transfer \n \nData-efficient learning with probabilistic methods from real data (e.g. policy search in robotics), \nreal-to-sim inference and differentiable simulation, data-efficient simulation-to-reality transfer \nRL for physical systems (successful examples in locomotion, open problems in contact-rich \nmanipulation, applications to logistics, energy, and transport systems); examples of RL for \nhealthcare. \nTopic 7: RL with Human Feedback ; Safe RL and RL for Validation \n \nFine-tuning large language models (LLMs) and other foundation models with human feedback \n(TRLX,RL4LMs, a light-weight overview of RLHF) \nA review of SafeRL, example: optimising commercial HVAC systems using policy improvement \nwith constraints; improving safety using RL for validation: examples in autonomous driving and \nautonomous flying and aircraft collision avoidance \nTopic 8: RL for Scientific Discovery; Student Presentations \n \nExamples of RL for molecular design and drug discovery, active learning for synthesising new \nmaterials, RL for nuclear fusion experiments \nStudent presentations (based on essays and mini-projects) for other topics in RL, e.g. \nmulti-agent RL, hierarchical RL, RL for hyperparameter optimisation and NN architecture \nsearch, RL for multi-task transfer, lifelong RL, RL in biological systems, etc. \nAssessment \nThe assessment for this module consists of: \n \nEssay and mini-project (60%)  \n\nStudents will choose a concrete RL method from the literature, then complete a 2-part essay \ndescribed below: \nEssay Part 1 (1500 words, 20%): Introduce a formal description of the method and explain how \nthe method extended the state-of-the-art at the time of its publication \nEssay Part 2 or a mini-project (2500 words, 40%): Provide a critical analysis of the chosen RL \nmethod. \nPresentation of the essay and mini-project results (15%)  \nstudents will be expected to create and record a 15-minute video presentation of the analysis \ndescribed in their essay and mini-project. \nShort test of RL theory fundamentals (15%)  \nto test the understanding of RL fundamentals (~15 minutes, in-class, closed book) \nParticipation in seminar discussions (10%)  \ntake an active part in the seminars by asking clarifying questions and mentioning related works. \nRecommended Reading \nBooks \n \n[S&B] Reinforcement Learning: An Introduction (second print edition). Richard S. Sutton, \nAndrew G. Barto. [Available from the book\u2019s website as a free PDF updated in 2022] \n \n[CZ] Algorithms for Reinforcement Learning. Csaba Szepesvari. [Available from the book\u2019s \nwebsite as a free PDF updated in 2019] \n \n[MK] Algorithms for Decision Making. Mykel J. Kochenderfer Tim A. WheelerKyle H. Wray. \n[Available from the book\u2019s website as a free PDF updated in 2023] \n \n[DB] Reinforcement Learning and Optimal Control. Dimitri Bertsekas. [Available from the book\u2019s \nwebsite as a free PDF updated in 2023] \n \nPresentation guidelines \n \n[KN] Ten simple rules for effective presentation slides. Kristen M.Naegle. PLoS computational \nbiology 17, no. 12 (2021).\n \n\nTHEORIES OF SOCIO-DIGITAL DESIGN FOR HUMAN-CENTRED AI \n \nPrerequisites: This course is appropriate to any ACS student, and will assist in the development \nof critical thinking, argument and long-form writing skills. Prior experience of essay-based \nhumanities subjects at university or secondary school will be beneficial, but not required. \nMoodle, timetable \n \nAims \nThis module is a theoretically-oriented advanced introduction to the broad field of \nhuman-computer interaction, extending to consider topics such as intelligent user interfaces, \ncritical and speculative design, participatory design, cognitive models of users, human-centred, \nas well as more-than-human-centred design, and others. The course will not address purely \nengineering approaches to the development of user interfaces (unless there is a clear \ntheoretical question being addressed), but allow students to effectively link the political, social, \nand ethical considerations in HCI to specific technical and conceptual design challenges. The \nmodule builds on the Practical Research in Human-Centred AI course offered in Michaelmas \n(developed with researchers at Microsoft Research Cambridge) and a collaboration with the \nLeverhulme Centre for the Future of Intelligence at Cambridge. Participants may include visitors \nfrom these groups and/or interdisciplinary research students and academic guests from other \nUniversity departments, including Cambridge Digital Humanities. \n \nSyllabus \nThe syllabus will remain broadly within the area of human-computer interaction, including \ntheories of design practice and the social contexts of technology use. Individual seminar topics \nwill be selected in response to contemporary and recent research developments, in consultation \nwith members of the class and visiting contributors. \n \nRepresentative topics in the next year are likely to include: \n \nResponsible AI and HCI \nIntersectional design for social justice \nParticipatory design and co-design \nSustainable AI and more-than-human-centred design \nUsefulness and usability of ethical design toolkits \nThe EU AI Act in design practice \nAI and interface design for transparency \nDesigning otherwise: on anarchist HCI \n  \nObjectives \nOn completion of this module, students should have developed facility in discussing and \ncritiquing the aims of their research, especially for an audience drawn from other academic \ndisciplines, including the following skills: \n \nTheoretical motivation and defence of a research question. \n\nConsideration of a research proposal from one or more alternative theoretical perspectives. \nPotential critique of the theoretical basis for a programme of research. \nCoursework \nIn advance of each seminar, all participants must read the essential reading - generally one \nlong, plus one or two short papers. Further reading is suggested for some seminars. Some \nweeks, instead of shorter papers, students will be asked to explore specific design tools or \nresources. \n \nBefore seminars 2-8, each student will submit a written commentary on the paper, either \ngenerated using a large language model (LLM), or written in the style of an LLM, supplemented \nby a short original observation. \n \nEach member of the class must select one of the 8 seminar themes as the subject for a more \nextended critical review essay. The critical review should be written as an academic research \nessay, not in LLM style. \n \nEach seminar will include an informal design exercise to be carried out in small groups. The \npurpose of these exercises is to reflect on a variety of practical design resources, by applying \nthose resources to concrete case studies. \n \nThroughout the course, students will be expected to keep a \"reflective diary\", briefly reporting \nthemes that arise in discussion, reflections on the design exercises, and ways in which the \nreadings relate to their own research interests. \n  \n \nPractical work \nThere is no practical work element in this course. \n \nAssessment \nWritten critical review of a single discussion text (20%) \nReflective diary submitted at the end of the module (60%) \nIn advance of the session, each student will submit a written commentary on the paper, either \ngenerated using a large language model (LLM), or written in the style of an LLM. A short original \nobservation should be added (20%) \nRecommended reading \nTo be assigned during the module, as discussed above. Most set readings will be available \neither from the ACM Digital Library, or from institutional repositories of the authors. \n \nFurther Information \nStudents from the MPhil in the Ethics of AI, Data and Algorithms, MSt in AI, and MPhil in Digital \nHumanities courses also attend the lectures for this module.\n \n\nTHEORY OF DEEP LEARNING \nPrerequisites: A strong background in calculus, probability theory and linear algebra, familiarity \nwith differential equations, optimization and information theory is required. Students need to \nhave studied Deep Neural Networks, familiarity with deep learning terminology (e.g. \narchitectures, benchmark problems) as well as deep learning frameworks (pytorch, jax or \nsimilar) is assumed \nMoodle, timetable \n \nAims \nThe objectives of this course is to expose you to one of the most active contemporary research \ndirections within machine learning: the theory of deep learning (DL). While the first wave of \nmodern DL has focussed on empirical breakthroughs and ever more complex techniques, the \nattention is now shifting to building a solid mathematical understanding of why these techniques \nwork so well in the first place. The purpose of this course is to review this recent progress \nthrough a mixture of reading group sessions and invited talks by leading researchers in the \ntopic, and prepare you to embark on a PhD in modern deep learning research. Compared to \ntypical, non-mathematical courses on deep learning, this advanced module will appeal to those \nwho have strong foundations in mathematics and theoretical computer science. In a way, this \ncourse is our answer to the question \u201cWhat should the world\u2019s best computer science students \nknow about deep learning in 2023?\u201d \n \nLearning Outcomes \nThis course should prepare the best students to start a PhD in the theory and mathematics of \ndeep learning, and to start formulating their own hypotheses in this space. You will be \nintroduced to a range of empirical and mathematical tools developed in recent years for the \nstudy of deep learning behaviour, and you will build an awareness of the main open questions \nand current lines of attack. At the end of the course you will: \n \nbe able to explain why classical learning theory is insufficient to describe the phenomenon of \ngeneralization in DL \nbe able to design and interpret empirical studies aimed at understanding generalization \nbe able to explain the role of overparameterization: be able to use deep linear models as a \nmodel to study implicit regularisation of gradient-based learning \nbe able to state PAC-Bayes and Information-theoretic bounds, and apply them to DL \nbe able to explain the connection between Gaussian processes and neural networks, and will \nbe able to study learning dynamics in the neural tangent kernel (NTK) regime. \nbe able to formulate your own hypotheses about DL and choose tools to prove/test them \nleverage your deeper theoretical understanding to produce more robust, rigorous and \nreproducible solutions to practical machine learning problems. \nSyllabus \nEach week we'll have two to four student-lead presentations about a research paper chosen \nfrom a reading list. The reading list loosely follows the weekly breakdown below (but we adapt it \neach year based as this is an active research area): \n \n\nWeek 1: Introduction to the topic \nWeek 2: Empirical Studies of Deep Learning Phenomena \nWeek 3: Interpolation Regime and \u201cDouble Descent\u201d Phenomena \nWeek 4: Implicit Regularization in Deep Linear Models \nWeek 5: Approximation Theory \nWeek 6: Networks in the Infinite Width Limit \nWeek 7: PAC-Bayes and Information Theoretic Bounds for SGD \nWeek 8: Discussion and Coursework Spotlight Session \n \nAssessment \nStudents will be assessed on the following basis: \n \n20% for presentation/content contributed to the module: Each student will have an opportunity \nto present one of the recommended papers during Weeks 1-7 (30 minute slot including Q&A). \nFor the presentation, students should aim to communicate the core ideas behind the paper, and \nclearly present the results, conclusions, and future directions. Where possible, students are \nencouraged to comment on how the work itself fits into broader research goals. \n10% for active participation (regular attendance and contribution to discussions during the Q&A \nsessions). \n70% for a group project report, with a word limit of 4000. Either (1) an original research \nproposal/report with a hypothesis, review of related literature, and ideally preliminary findings, \nor, (2) reproduction and ideally extension of an existing relevant paper. \nCoursework reports are marked in line with general ACS guidelines, reports receiving top marks \nwill have have demonstrable research value (contain an original research idea, extension of \nexisting work, or a thorough reproduction effort which is valuable to the research community). \nAdditionally, some projects will be suggested during the first weeks of the course, although \nstudents are encouraged to come up with their own ideas. Students may be required to \nparticipate in group projects, with groups of size 2-3 (the class groups will be separated). For \nany given project, individual contributions would be noted for assessment though a viva \ncomponent. \nRelationship with related modules \nThis course can be considered as an advanced follow-up to the Part IIB course on Deep Neural \nNetworks. That course introduces some high level concepts that this course significantly \nexpands on. \n \nThis module complements L48: Machine Learning in the Physical World and L46: Principles of \nMachine Learning Systems, which focus on applications and hardware/systems aspects of ML \nrespectively. \n \nRecommended reading \nPNAS Colloquium on the Science of Deep Learning \nMathematics of Machine Learning book by Marc Deisenroth, Aldo Faisal and Cheng Soon Ong. \nProbabilistic Machine Learning: An Introduction book by Kevin Murphy \nMatus Telgarsky's lecture notes on deep learning theory  \n\nThese are in addition to the papers which will be discussed in the lectures.\n \n\nUNDERSTANDING NETWORKED-SYSTEMS PERFORMANCE \nPrerequisites: A student must possess knowledge comparable to the undergraduate subjects on \nUnix Tools, C/C++ programming and Computer Networks. Optionally; a student will be \nadvantaged by doing an introduction to Computer Systems Modelling; such as the Part 2 \nComputer Systems Modelling subject as well as having a background in measurement \nmethodologies such as that of L50: Introduction to networking and systems measurements \nprovided in the ACS/Part III curriculum. \nMoodle, timetable \n \nAims \nThis is a practical course, actively building and extending software tools to observe the detailed \nbehaviour of transaction-oriented datacenter-like software. Students will observe and \nunderstand sources of user-facing tail latency, including that stemming \nfrom resource contention, cross-program interference, bad software locking, and simple design \nerrors. \nA 2-hour weekly hybrid, lecture/lab format permits students continuous monitored progress with \ncomplexity of tasks building naturally upon the previous weeks learning. \n \nObjectives \nUpon successful completion of the course, students will be able to: \n \nMake order-of-magnitude estimates of software, hardware, and I/O speeds \nMake valid measurements of actual software, hardware, and I/O speeds \nCreate observation facilities, including logs and dashboards, as part of a software system \ndesign \nCreate tracing facilities to fully observe the execution of complex software \nTime-align traces across multiple computers \nDisplay dense tracing information in meaningful ways \nReason about the sources of real-time and transaction delays, including cross-program \nresource interference, remote procedure call delays, and software locking surprises \nFix programs based on the above reasoning, making their response times faster and more \nrobust \n \nSyllabus \nMeasurement \n   Week 1 Intro, Measuring CPU time, rdtsc, Measuring memory access times, Measuring disks, \n   Week 2 gettimeofday, logs Measuring networks, Remote Procedure Calls, Multi-threads, locks \nObservation \n   Week 3 RPC, logs, displaying traces, interference \n   Week 4 Antagonist programs, Logging, dashboards, profiling. \nKUtrace \n   Week 5 Kernel patches, hello world, post-processing \n   Week 6 KUtrace multi-CPU time display \n   Week 7 Client-server KUtrace, with antagonists and interference \n\n   Week 8 Other trace mysteries \n \n \nAssessment \nThis is a Lab-based module; assessment is based upon reports covering guided laboratory work \nperformed each week. Assessment will be via two submissions: \n \n20% assignment deadline week 4 based upon Lab work from Weeks 1-4; word target 1000, limit \n2000 \n80% assignment deadline week 8 based upon lab work from weeks 5-8; word target 2000, limit \n4000 \nThe intent of the first assessment point is to provide rich feedback to students based upon a \n20% assignment permitting focussed and improved work to be executedfor the final assignment. \n \nAll work in this module is expected to be the effort of the student. Enough equipment is \nprovisioned to allow each student an independent set of apparatus. While classmembers may \nfind sharing operational experience valuable all assessment is based upon a students sole \nsubmission based upon. their own experiments and findings. \n \nReading Material \nCore text \n \nRichard L. Sites, Understanding Software Dynamics, Addison-Wesley Professional Computing \nSeries, 2022 \nFurther reading: papers and presentations that you might find interesting. None are required \nreading \u2013 they are well-written and/or informative. \n \nJohn K. Ousterhout et al., A trace-driven analysis of the UNIX 4.2 BSD file system ACM \nSIGOPS Operating Systems Review, Vol. 19, No. 5, Dec, 1985 \nhttps://dl.acm.org/doi/pdf/10.1145/323627.323631 \nLuiz Andr\u00b4e Barroso, Jimmy Clidaras, Urs H\u00a8olzle, The Datacenter as a Computer: An \nIntroduction to the Design of Warehouse-Scale Machines, Second Edition \nJohn L. Hennessy, David A. Patterson, Computer Architecture, A Quantitative Approach 5th \nEdition \nGeorge Varghese, Network Algorithmics. Morgan Kaufmann \nActually keeping datacenter software up and running \u2013 how Google runs production systems \nSite Reliability Engineering Edited by Betsy Beyer, Chris Jones, Jennifer Petoff and Niall \nRichard Murphy free PDF: https://landing.google.com/sre/book.html \nHow NOT to run datacenters (27 minute video, funny/sad) dotScale 2014 - Robert Kennedy - \nLife in the Trenches of healthcare.gov https://www.youtube.com/watch?v=GLQyj-kBRdo \nAn extended (optional) reading list will also be provided. \n \n"
    ],
    "semesters_-6245011765149883041": [
        "1st Semester \n \nDATABASES \n \nAims \nThis course introduces basic concepts for database systems as seen from the perspective of \napplication designers. That is, the focus is on the abstractions supported by database \nmanagement systems and not on how those abstractions are implemented. \n \nThe database world is currently undergoing swift and dramatic transformations largely driven by \nInternet-oriented applications and services. Today many more options are available to database \napplication developers than in the past and so it is becoming increasingly difficult to sort fact \nfrom fiction. The course attempts to cut through the fog with a practical approach that \nemphasises engineering tradeoffs that underpin these recent developments and also guide our \nselection of \u201cthe right tool for the job.\u201d \n \nThis course covers three approaches. First, the traditional mainstay of the database industry -- \nthe relational approach -- is described with emphasis on eliminating logical redundancy in data. \nThen two representatives of recent trends are presented -- graph-oriented and \ndocument-oriented databases. The lectures are supported with two Help and Tick sessions, \nwhere students gain hands-on experience and guidance with the Assessed Exercises (ticks). \n \nLectures \nL1 Introduction. What is a database system? What is a data model? Typical DBMS \nconfigurations and variations (fast queries or reliable update). In-core, in secondary store or \ndistributed. \nL2 Conceptual modelling. Entities, relations, E/R diagrams and implementation-independent \nmodelling, weak entities, cardinality. \nL3+4 The relational database model. Implementing E/R models with relational tables. Relational \nalgebra and SQL. Basic query primitives. Update anomalies caused by redundancy. Minimising \nredundancy with normalised schemas. \nL5 Transactions. On-Line Transaction Processing. On-line Analytical Processing. Reliability, \nthroughput, normal forms, ACID, BASE, eventual consistency. \nL6 Documents and semi-structured data. The NoSQL, schema-free movement. XML/JSON. \nKey/value stores. Embracing data redundancy: representing data for fast, application-specific \naccess. Path queries (if time permits). \nL7 Further SQL. Multisets, NULL values, aggregates, transitive closure, expressibility (nested \nqueries and recursive SQL). \nL8 Graph databases. Optimised for processing enormous numbers of nodes and edges. \nImplementing E/R models in a graph-oriented database. Comparison of the presented models. \nObjectives \nAt the end of the course students should \n\n \nbe able to design entity-relationship diagrams to represent simple database application \nscenarios \nknow how to convert entity-relationship diagrams to relational- and graph-oriented \nimplementations \nunderstand the fundamental tradeoff between the ease of updating data and the response time \nof complex queries \nunderstand that no single data architecture can be used to meet all data management \nrequirements \nbe familiar with recent trends in the database area. \nRecommended reading \nLemahieu, W., Broucke, S. van den and Baesens, B. (2018) Principles of database \nmanagement. Cambridge University Press.\n \n\nDIGITAL ELECTRONICS \n \nAims \nThe aims of this course are to present the principles of combinational and sequential digital logic \ndesign and optimisation at a gate level. The use of n and p channel MOSFETs for building logic \ngates is also introduced. \n \nTopics \nIntroduction. Semiconductors to computers. Logic variables. Examples of simple logic. Logic \ngates. Boolean algebra. De Morgan\u2019s theorem. \nLogic minimisation. Truth tables and normal forms. Karnaugh maps. Quine-McCluskey method. \nBinary adders. Half adder, full adder, ripple carry adder, fast carry generation. \nCombinational logic design: further considerations. Multilevel logic. Gate propagation delay. An \nintroduction to timing diagrams. Hazards and hazard elimination. Other ways to implement \ncombinational logic. \nIntroduction to practical classes. Prototyping box. Breadboard and Dual in line (DIL) packages. \nWiring. \nSequential logic. Memory elements. RS latch. Transparent D latch. Master-slave D flip-flop. T \nand JK flip-flops. Setup and hold times. \nSequential logic. Counters: Ripple and synchronous. Shift registers. System timing - setup time \nconstraint, clock skew, metastability. \nSynchronous State Machines. Moore and Mealy finite state machines (FSMs). Reset and self \nstarting. State transition diagrams. Elimination of redundant states - row matching and state \nequivalence/implication table. \nFurther state machines. State assignment: sequential, sliding, shift register, one hot. \nImplementation of FSMs. \nIntroduction to microprocessors. Microarchitecture, fetch, register access, memory access, \nbranching, execution time. \nElectronics, Devices and Circuits. Current and voltage, conductors/insulators/semiconductors, \nresistance, basic circuit theory, the potential divider. Solving non-linear circuits. P-N junction \n(forward and reverse bias), N and p channel MOSFETs (operation and characteristics) and \nn-MOSFET logic, e.g., n-MOSFET inverter. Power consumption and switching time problems \nproblems in n-MOSFET logic. CMOS logic (NOT, NAND and NOR gates), logic families, noise \nmargin. \nObjectives \nAt the end of the course students should \n \nunderstand the relationships between combination logic and boolean algebra, and between \nsequential logic and finite state machines; \nbe able to design and minimise combinational logic; \nappreciate tradeoffs in complexity and speed of combinational designs; \nunderstand how state can be stored in a digital logic circuit; \nknow how to design a simple finite state machine from a specification and be able to implement \nthis in gates and edge triggered flip-flops; \n\nunderstand how to use MOSFETs to build digital logic circuits. \nRecommended reading \n* Harris, D.M. and Harris, S.L. (2013). Digital design and computer architecture. Morgan \nKaufmann (2nd ed.). The first edition is still relevant. \nKatz, R.H. (2004). Contemporary logic design. Benjamin/Cummings. The 1994 edition is more \nthan sufficient. \nHayes, J.P. (1993). Introduction to digital logic design. Addison-Wesley. \n \nBooks for reference: \n \nHorowitz, P. and Hill, W. (1989). The art of electronics. Cambridge University Press (2nd ed.) \n(more analog). \nWeste, N.H.E. and Harris, D. (2005). CMOS VLSI Design - a circuits and systems perspective. \nAddison-Wesley (3rd ed.). \nMead, C. and Conway, L. (1980). Introduction to VLSI systems. Addison-Wesley. \nCrowe, J. and Hayes-Gill, B. (1998). Introduction to digital electronics. Butterworth-Heinemann. \nGibson, J.R. (1992). Electronic logic circuits. Butterworth-Heinemann.\n \n\nDISCRETE MATHEMATICS \n \nAims \nThe course aims to introduce the mathematics of discrete structures, showing it as an essential \ntool for computer science that can be clever and beautiful. \n \nLectures \nProof [5 lectures]. \nProofs in practice and mathematical jargon. Mathematical statements: implication, bi-implication, \nuniversal quantification, conjunction, existential quantification, disjunction, negation. Logical \ndeduction: proof strategies and patterns, scratch work, logical equivalences. Proof by \ncontradiction. Divisibility and congruences. Fermat\u2019s Little Theorem. \n \nNumbers [5 lectures]. \nNumber systems: natural numbers, integers, rationals, modular integers. The Division Theorem \nand Algorithm. Modular arithmetic. Sets: membership and comprehension. The greatest \ncommon divisor, and Euclid\u2019s Algorithm and Theorem. The Extended Euclid\u2019s Algorithm and \nmultiplicative inverses in modular arithmetic. The Diffie-Hellman cryptographic method. \nMathematical induction: Binomial Theorem, Pascal\u2019s Triangle, Fundamental Theorem of \nArithmetic, Euclid\u2019s infinity of primes. \n \nSets [9 lectures]. \nExtensionality Axiom: subsets and supersets. Separation Principle: Russell\u2019s Paradox, the \nempty set. Powerset Axiom: the powerset Boolean algebra, Venn and Hasse diagrams. Pairing \nAxiom: singletons, ordered pairs, products. Union axiom: big unions, big intersections, disjoint \nunions. Relations: composition, matrices, directed graphs, preorders and partial orders. Partial \nand (total) functions. Bijections: sections and retractions. Equivalence relations and set \npartitions. Calculus of bijections: characteristic (or indicator) functions. Finite cardinality and \ncounting. Infinity axiom. Surjections. Enumerable and countable sets. Axiom of choice. \nInjections. Images: direct and inverse images. Replacement Axiom: set-indexed constructions. \nSet cardinality: Cantor-Schoeder-Bernstein Theorem, unbounded cardinality, diagonalisation, \nfixed-points. Foundation Axiom. \n \nFormal languages and automata [5 lectures]. \nIntroduction to inductive definitions using rules and proof by rule induction. Abstract syntax \ntrees. Regular expressions and their algebra. Finite automata and regular languages: Kleene\u2019s \ntheorem and the Pumping Lemma. \n \nObjectives \nOn completing the course, students should be able to \n \nprove and disprove mathematical statements using a variety of techniques; \napply the mathematical principle of induction; \nknow the basics of modular arithmetic and appreciate its role in cryptography; \n\nunderstand and use the language of set theory in applications to computer science; \ndefine sets inductively using rules and prove properties about them; \nconvert between regular expressions and finite automata; \nuse the Pumping Lemma to prove that a language is not regular. \nRecommended reading \nBiggs, N.L. (2002). Discrete mathematics. Oxford University Press (Second Edition). \nDavenport, H. (2008). The higher arithmetic: an introduction to the theory of numbers. \nCambridge University Press. \nHammack, R. (2013). Book of proof. Privately published (Second edition). Available at: \nhttp://www.people.vcu.edu/ rhammack/BookOfProof/index.html \nHouston, K. (2009). How to think like a mathematician: a companion to undergraduate \nmathematics. Cambridge University Press. \nKozen, D.C. (1997). Automata and computability. Springer. \nLehman, E.; Leighton, F.T.; Meyer, A.R. (2014). Mathematics for computer science. Available \non-line. \nVelleman, D.J. (2006). How to prove it: a structured approach. Cambridge University Press \n(Second Edition).\n \n\nFOUNDATIONS OF COMPUTER SCIENCE \nAims \nThe main aim of this course is to present the basic principles of programming. As the \nintroductory course of the Computer Science Tripos, it caters for students from all backgrounds. \nTo those who have had no programming experience, it will be comprehensible; to those \nexperienced in languages such as C, it will attempt to correct any bad habits that they have \nlearnt. \n \nA further aim is to introduce the principles of data structures and algorithms. The course will \nemphasise the algorithmic side of programming, focusing on problem-solving rather than on \nhardware-level bits and bytes. Accordingly it will present basic algorithms for sorting, searching, \netc., and discuss their efficiency using O-notation. Worked examples (such as polynomial \narithmetic) will demonstrate how algorithmic ideas can be used to build efficient applications. \n \nThe course will use a functional language (OCaml). OCaml is particularly appropriate for \ninexperienced programmers, since a faulty program cannot crash and OCaml\u2019s unobtrusive type \nsystem captures many program faults before execution. The course will present the elements of \nfunctional programming, such as curried and higher-order functions. But it will also introduce \ntraditional (procedural) programming, such as assignments, arrays and references. \n \nLectures \nIntroduction to Programming. The role of abstraction and representation. Introduction to integer \nand floating-point arithmetic. Declaring functions. Decisions and booleans. Example: integer \nexponentiation. \nRecursion and Efficiency. Examples: Exponentiation and summing integers. Iteration versus \nrecursion. Examples of growth rates. Dominance and O-Notation. The costs of some \nrepresentative functions. Cost estimation. \nLists. Basic list operations. Append. Na\u00efve versus efficient functions for length and reverse. \nStrings. \nMore on lists. The utilities take and drop. Pattern-matching: zip, unzip. A word on polymorphism. \nThe \u201cmaking change\u201d example. \nSorting. A random number generator. Insertion sort, mergesort, quicksort. Their efficiency. \nDatatypes and trees. Pattern-matching and case expressions. Exceptions. Binary tree traversal \n(conversion to lists): preorder, inorder, postorder. \nDictionaries and functional arrays. Functional arrays. Dictionaries: association lists (slow) versus \nbinary search trees. Problems with unbalanced trees. \nFunctions as values. Nameless functions. Currying. The \u201capply to all\u201d functional, map. \nExamples: The predicate functionals filter and exists. \nSequences, or lazy lists. Non-strict functions such as IF. Call-by-need versus call-by-name. Lazy \nlists. Their implementation in OCaml. Applications, for example Newton-Raphson square roots. \nQueues and search strategies. Depth-first search and its limitations. Breadth-first search (BFS). \nImplementing BFS using lists. An efficient representation of queues. Importance of efficient data \nrepresentation. \n\nElements of procedural programming. Address versus contents. Assignment versus binding. \nOwn variables. Arrays, mutable or not. Introduction to linked lists. \nObjectives \nAt the end of the course, students should \n \nbe able to write simple OCaml programs; \nunderstand the fundamentals of using a data structure to represent some mathematical \nabstraction; \nbe able to estimate the efficiency of simple algorithms, using the notions of average-case, \nworse-case and amortised costs; \nknow the comparative advantages of insertion sort, quick sort and merge sort; \nunderstand binary search and binary search trees; \nknow how to use currying and higher-order functions; \nunderstand how OCaml combines imperative and functional programming in a single language. \nRecommended reading \nJohn Whitington. OCaml from the Very Beginning. (http://ocaml-book.com). \n \nOkasaki, C. (1998). Purely functional data structures. Cambridge University Press.\n \n\nHARDWARE PRACTICAL CLASSES \nThe Hardware Practical Classes accompany the Digital Electronics series of lectures. The aim \nof the Practical Classes is to enable students to get hands-on experience of designing, building, \nand testing and debugging of digital electronic circuits. The labs will take place in the Intel Lab. \nlocated in the William Gates Building (WGB). \n \nThe Digital Electronics lecture series occupies 12 lectures in the first 4 weeks of the Michaelmas \nTerm, while the Practical Classes occupy the latter 6 weeks of the Michaelmas Term and the \nfirst 6 weeks of the Lent Term. If required, extra sessions will be available for any students \nneeding to 'catch-up' on missed sessions or to complete any remaining practical work. \n \nThe Practical Classes take the form of 4 workshops, specifically: \n \nWorkshop 1 \u2013 Electronic Die; \nWorkshop 2 \u2013 Shaft Position Encoder; \nWorkshop 3 \u2013 Debouncing a Switch; \nWorkshop 4 \u2013 Framestore for an LED Array. \nIn general, the workshops require some preparatory work to be done prior to the practical \nsession. These tasks are highlighted at the beginning of each Worksheet. Typically this involves \npreparing a design that you will then build, test and modify during the practical class. Note that \ninsufficient preparation prior to the practical classes may compromise effective use of your time \nin the Intel lab. \n \nIn the Practical Classes you will usually work on your own, and you are expected to complete \none Workshop in each of your scheduled sessions. Demonstrators are available during the \nsessions to assist you with any queries or problems you may have. \n \nImportant: Remember to get your work ticked.\n \n\nINTRODUCTION TO GRAPHICS \nAims \nTo introduce the necessary background, the basic algorithms, and the applications of computer \ngraphics and graphics hardware. \n \nLectures \nBackground. What is an image? Resolution and quantisation. Storage of images in memory. [1 \nlecture] \nRendering. Perspective. Reflection of light from surfaces and shading. Geometric models. Ray \ntracing. [2 lectures] \nGraphics pipeline. Polygonal mesh models. Transformations using matrices in 2D and 3D. \nHomogeneous coordinates. Projection: orthographic and perspective. [1 lecture] \nGraphics hardware and modern OpenGL. GPU rendering. GPU frameworks and APIs. Vertex \nprocessing. Rasterisation. Fragment processing. Working with meshes and textures. Z-buffer. \nDouble-buffering and frame synchronization. [2 lectures] \n  \nHuman vision, colour and tone mapping. Perception of colour. Tone mapping. Colour spaces. [2 \nlectures] \nObjectives \nBy the end of the course students should be able to: \n \nunderstand and apply in practice basic concepts of ray-tracing: ray-object intersection, \nreflections, refraction, shadow rays, distributed ray-tracing, direct and indirect illumination; \ndescribe and explain the following algorithms: z-buffer, texture mapping, double buffering, \nmip-map, and normal-mapping; \nuse matrices and homogeneous coordinates to represent and perform 2D and 3D \ntransformations; understand and use 3D to 2D projection, the viewing volume, and 3D clipping; \nimplement OpenGL code for rendering of polygonal objects, control camera and lighting, work \nwith vertex and fragment shaders; \ndescribe a number of colour spaces and their relative merits. \nexplain the need for tone mapping and colour processing in rendering pipeline. \nRecommended reading \n* Shirley, P. and Marschner, S. (2009). Fundamentals of Computer Graphics. CRC Press (3rd \ned.). \n \nFoley, J.D., van Dam, A., Feiner, S.K. and Hughes, J.F. (1990). Computer graphics: principles \nand practice. Addison-Wesley (2nd ed.). \n \nKessenich, J.M., Sellers, G. and Shreiner, D (2016). OpenGL Programming Guide: The Official \nGuide to Learning OpenGL, Version 4.5 with SPIR-V, [seventh edition and later]\n \n\nOBJECT-ORIENTED PROGRAMMING \n \nAims \nThe goal of this course is to provide students with an understanding of Object-Oriented \nProgramming. Concepts are demonstrated in multiple languages, but the primary language is \nJava. \n \nLecture syllabus \nTypes, Objects and Classes Moving from functional to imperative. Functions, methods. Control \nflow. values, variables and types. Primitive Types. Classes as custom types. Objects vs \nClasses. Class definition, constructors. Static data and methods. \nDesigning Classes Identifying classes. UML class diagrams. Modularity. Encapsulation/data \nhiding. Immutability. Access modifiers. Parameterised types (Generics). \nPointers, References and Memory Pointers and references. Reference types in Java. The call \nstack. The heap. Iteration and recursion. Pass-by-value and pass-by-reference. \nInheritance Inheritance. Casting. Shadowing. Overloading. Overriding. Abstract Methods and \nClasses. \nPolymorphism and Multiple Inheritance Polymorphism in ML and Java. Multiple inheritance. \nInterfaces in Java. \nLifecycle of an Object Constructors and chaining. Destructors. Finalizers. Garbage Collection: \nreference counting, tracing. \nJava Collections and Object Comparison Java Collection interface. Key classes. Collections \nclass. Iteration options and the use of Iterator. Comparing primitives and objects. Operator \noverloading. \nError Handling Types of errors. Limitations of return values. Deferred error handling. Exceptions. \nCustom exceptions. Checked vs unchecked. Inappropriate use of exceptions. Assertions. \nDesignLanguage evolution Need for languages to evolve. Generics in Java. Type erasure. \nIntroduction to Java 8: Lambda functions, functions as values, method references, streams. \nDesign Patterns Introduction to design patterns. Open-closed principle. Examples of Singleton, \nDecorator, State, Composite, Strategy, Observer. [2 lectures] \nObjectives \nAt the end of the course students should \n \nProvide an overview of key OOP concepts that are transferable in mainstream programming \nlanguages; \nGain an understanding of software development approaches adopted in the industry including \nmaintainability, testing, and software design patterns; \nIncrease programming familiarity with Java; \nRecommended reading \n1. Real-World Software Development: A Project-Driven Guide to Fundamentals in Java [Urma et \nal.] \n2. Modern Java in Action (2nd edition) [Urma et al.]. \n3. Java How to Program: early objects [Deitel et al.]\n \n\nOCAML PRACTICAL CLASSES \n \nThe Role of Tickers \nA ticking session is a short (5 minute) one-to-one session with a ticker conducted on Thursday \nafternoons. The role of the ticker is twofold: \n \nto check you understand your solution (and didn\u2019t just have some lucky guesses or copy code \nfrom elsewhere) \nto give you general feedback on ways to improve your code. \nTo those ends a Ticker will typically ask you questions related to the core material of the tick and \ndiscuss your code directly. \n \nDeadline Extensions \nDeadline extensions can be granted for illness or similar reasons. To obtain an extension please \nemail Jon Ludlam in the first instance, clearly stating your justification for an extension and \nCCing your Director of Studies, who will need to support your request.\n \n\nSCIENTIFIC COMPUTING \n \nAims \nThis course is a hands-on introduction to using computers to investigate scientific models and \ndata. \n \nSyllabus \nPython notebooks. Overview of the Python programming language. Use of notebooks for \nscientific computing. \nNumerical computation. Writing fast vectorized code in numpy. Optimization and fitting. \nSimulation. \nWorking with data. Data import. Common ways to summarize and plot data, for univariate and \nmultivariate analysis. \nObjectives \nAt the end of the course students should \n \nbe able to import data, plot it, and summarize it appropriately \nbe able to write fast vectorized code for scientific / data work\n \n\n",
        "2nd Semester \nALGORITHMS 1 \nAims \nThe aim of this course is to provide an introduction to computer algorithms and data structures, \nwith an emphasis on foundational material. \n \nLectures \nSorting. Review of complexity and O-notation. Trivial sorting algorithms of quadratic complexity. \nReview of merge sort and quicksort, understanding their memory behaviour on statically \nallocated arrays. Heapsort. Stability. Other sorting methods including sorting in linear time. \nMedian and order statistics. [Ref: CLRS3 chapters 1, 2, 3, 6, 7, 8, 9] [about 4 lectures] \nStrategies for algorithm design. Dynamic programming, divide and conquer, greedy algorithms \nand other useful paradigms. [Ref: CLRS3 chapters 4, 15, 16] [about 3 lectures] \nData structures. Elementary data structures: pointers, objects, stacks, queues, lists, trees. \nBinary search trees. Red-black trees. B-trees. Hash tables. Priority queues and heaps. [Ref: \nCLRS3 chapters 6, 10, 11, 12, 13, 18] [about 5 lectures]. \nObjectives \nAt the end of the course students should: \n \nhave a thorough understanding of several classical algorithms and data structures; \nbe able to analyse the space and time efficiency of most algorithms; \nhave a good understanding of how a smart choice of data structures may be used to increase \nthe efficiency of particular algorithms; \nbe able to design new algorithms or modify existing ones for new applications and reason about \nthe efficiency of the result. \nRecommended reading \n* Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest and Clifford Stein Introduction to \nAlgorithms, Fourth Edition ISBN 9780262046305 Published: April 5, 2022. \n \nSedgewick, R., Wayne, K. (2011). Algorithms. Addison-Wesley. ISBN 978-0-321-57351-3. \n \nKleinberg, J. and Tardos, \u00c9. (2006). Algorithm design. Addison-Wesley. ISBN \n978-0-321-29535-4. \n \nKnuth, D.A. (2011). The Art of Computer Programming. Addison-Wesley. ISBN \n978-0-321-75104-1.\n \n\nALGORITHMS 2 \n \nAims \nThe aim of this course is to provide an introduction to computer algorithms and data structures, \nwith an emphasis on foundational material. \n \nLectures \nGraphs and path-finding algorithms. Graph representations. Breadth-first and depth-first search. \nSingle-source shortest paths: Bellman-Ford and Dijkstra algorithms. All-pairs shortest paths: \ndynamic programming and Johnson\u2019s algorithms. [About 4 lectures] \nGraphs and subgraphs. Maximum flow: Ford-Fulkerson method, Max-flow min-cut theorem. \nMatchings in bipartite graphs. Minimum spanning tree: Kruskal and Prim algorithms. Topological \nsort. [About 4 lectures] \nAdvanced data structures. Binomial heap. Amortized analysis: aggregate analysis, potential \nmethod. Fibonacci heaps. Disjoint sets. [About 4 lectures] \nObjectives \nAt the end of the course students should: \n \nhave a thorough understanding of several classical algorithms and data structures; \nbe able to analyse the space and time efficiency of most algorithms; \nhave a good understanding of how a smart choice of data structures may be used to increase \nthe efficiency of particular algorithms; \nbe able to design new algorithms or modify existing ones for new applications and reason about \nthe efficiency of the result. \nRecommended reading \n* Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest and Clifford Stein Introduction to \nAlgorithms, Fourth Edition ISBN 9780262046305 Published: April 5, 2022. \n \nSedgewick, R., Wayne, K. (2011). Algorithms. Addison-Wesley. ISBN 978-0-321-57351-3. \n \nKleinberg, J. and Tardos, \u00c9. (2006). Algorithm design. Addison-Wesley. ISBN \n978-0-321-29535-4. \n \nKnuth, D.A. (2011). The Art of Computer Programming. Addison-Wesley. ISBN \n978-0-321-75104-1.\n \n\nMACHINE LEARNING AND REAL-WORLD DATA \n \nAims \nThis course introduces students to machine learning algorithms as used in real-world \napplications, and to the experimental methodology necessary to perform statistical analysis of \nlarge-scale data from unpredictable processes. Students will perform 3 extended practicals, as \nfollows: \n \nStatistical classification: Determining movie review sentiment using Naive Bayes; \nSequence Analysis: Hidden Markov Modelling and its application to a task from biology \n(predicting protein interactions with a cell membrane); \nAnalysis of social networks, including detection of cliques and central nodes. \nSyllabus \nTopic One: Statistical Classification \nIntroduction to sentiment classification. \nNaive Bayes parameter estimation. \nStatistical laws of language. \nStatistical tests for classification tasks. \nCross-validation and test sets. \nUncertainty and human agreement. \nTopic Two: Sequence Analysis  \nHidden Markov Models (HMM) and HMM training. \nThe Viterbi algorithm. \nUsing an HMM in a biological application. \nTopic Three: Social Networks  \nProperties of networks: Degree, Diameter. \nBetweenness Centrality. \nClustering using betweenness centrality. \nObjectives \nBy the end of the course students should be able to: \n \nunderstand and program two simple supervised machine learning algorithms; \nuse these algorithms in statistically valid experiments, including the design of baselines, \nevaluation metrics, statistical testing of results, and provision against overtraining; \nvisualise the connectivity and centrality in large networks; \nuse clustering (i.e., a type of unsupervised machine learning) for detection of cliques in \nunstructured networks. \nRecommended reading \nJurafsky, D. and Martin, J. (2008). Speech and language processing. Prentice Hall. \n \nEasley, D. and Kleinberg, J. (2010). Networks, crowds, and markets: reasoning about a highly \nconnected world. Cambridge University Press.\n \n\nOPERATING SYSTEMS \n \nAims \nThe overall aim of this course is to provide a general understanding of the structure and key \nfunctions of the operating system. Case studies will be used to illustrate and reinforce \nfundamental concepts. \n \nLectures \nIntroduction to operating systems. Elementary computer organisaton. Abstract view of an \noperating system. Key concepts: layering, multiplexing, performance, caching, buffering, policies \nversus mechanisms. OS evolution: multi-programming, time-sharing. Booting. [1 lecture] \nProtection. Dual-mode operation. Protecting CPU, memory, I/O, storage. Kernels, micro-kernels, \nand modules. System calls. Virtual machines and containers. Subjects and objects. \nAuthentication. Access matrix: ACLs and capabilities. Covert channels. [1 lecture] \nProcesses. Job/process concepts. Process lifecycle. Process management. Context switching. \nInter-process communication. [1 lecture] \nScheduling. CPU-I/O burst cycle. Scheduling basics: preemption and non-preemption. \nScheduling algorithms: FCFS, SJF, SRTF, Priority, and Round Robin. Multilevel scheduling. [2 \nlectures] \nMemory management. Processes in memory. Logical versus physical addresses. Partitions: \nstatic versus dynamic, free space management, external fragmentation. Segmented memory. \nPaged memory: concepts, internal fragmentation, page tables. Demand paging/segmentation. \nPage replacement strategies: FIFO, OPT, and LRU with approximations including NRU, LFU, \nMFU, MRU. Working set schemes. Thrashing. [3 lectures] \nI/O subsystem. General structure. Polled mode versus interrupt-driven I/O. Programmed I/O \n(PIO) versus Direct Memory Access (DMA). Application I/O interface: block and character \ndevices, buffering, blocking, non-blocking, asynchronous, and vectored I/O. Other issues: \ncaching, scheduling, spooling, performance. [1 lecture] \nFile management. File concept. Directory and storage services. File names and metadata. \nDirectory name-space: hierarchies, DAGs, hard and soft links. File operations. Access control. \nExistence and concurrency control. [1 lecture] \nUnix case study. History. General structure. Unix file system: file abstraction, directories, mount \npoints, implementation details. Processes: memory image, lifecycle, start of day. The shell: \nbasic operation, commands, standard I/O, redirection, pipes, signals. Character and block I/O. \nProcess scheduling. [2 lectures] \nObjectives \nAt the end of the course students should be able to: \n \ndescribe the general structure and purpose of an operating system; \nexplain the concepts of process, address space, and file; \ncompare and contrast various CPU scheduling algorithms; \ncompare and contrast mechanisms involved in memory and storage management; \ncompare and contrast polled, interrupt-driven, and DMA-based access to I/O devices. \nRecommended reading \n\n* Silberschatz, A., Galvin, P.C., and Gagne, G. (2018). Operating systems concepts. Wiley (10th \ned.). Prior editions, at least back to 8th ed., should also be acceptable. \nAnderson, T. and Dahlin, M. (2014). Operating Systems: Principles and Practice. Recursive \nBooks (2nd ed.). \nBacon, J. and Harris, T. (2003). Operating systems. Addison-Wesley (3rd ed.). Currently \nappears out-of-print. \nLeffler, S. (1989). The design and implementation of the 4.3BSD Unix operating system. \nAddison-Wesley. \nMcKusick, M.K., Neville-Neil, G.N. and Watson, R.N.M. (2014) The Design and Implementation \nof the FreeBSD Operating System. Pearson Education. (2nd ed.).\n \n\nINTERACTION DESIGN \nAims \nThe aim of this course is to provide an introduction to interaction design, with an emphasis on \nunderstanding and experiencing the user-centred design process, from conducting user \nresearch and requirements development to implementation and evaluation, while understanding \nthe background to human-computer interaction. \n \nLectures \nCourse overview and user research methods. Introduction to the course and practicals. \nUser-Centred Design. User research methods. \nKeeping usera in mind. User research data analysis. Identifying users and stakeholders. \nRepresenting user goals and activities. Identifying and establishing requirements. \nDesign and prototyping. Methods for exploring the design space. Prototyping and different kinds \nof prototypes. \nVisual and interaction design. Memory, perception, attention, and their implications for \ninteraction design. Modalities of interaction, interaction design patterns, information architecture, \nand their implications for interaction design. \nEvaluation. Practical methods for evaluating designs. Evaluation methods without users. \nEvaluation methods with users. \nCase studies from industry and research. Guest lectures (the topics of these lectures are \nsubject to change). \nObjectives \nBy the end of the course students should \n \nhave a thorough understanding of the user-centred design process and be able to apply it to \ninteraction design; \nbe able to design new user interfaces that are informed by principles of good design, and the \nprinciples of human visual perception, cognition and communication; \nbe able to prototype and implement interactive user interfaces with a strong emphasis on users, \nusability and appearance; \nbe able to evaluate existing or new user interfaces using multiple techniques; \nbe able to compare and contrast different design techniques and to critique their applicability to \nnew domains. \nRecommended reading \n* Preece, J., Rogers, Y. and Sharp, H. (2015). Interaction design. Wiley (4th ed.).\n \n\nINTRODUCTION TO PROBABILITY \n \nAims \nThis course provides an elementary introduction to probability and statistics with applications. \nProbability theory and the related field of statistical inference provide the foundations for \nanalysing and making sense of data. The focus of this course is to introduce the language and \ncore concepts of probability theory. The course will also cover some applications of statistical \ninference and algorithms in order to equip students with the ability to represent problems using \nthese concepts and analyse the data within these principles. \n \nLectures \nPart 1 - Introduction to Probability \n \nIntroduction. Counting/Combinatorics (revision), Probability Space, Axioms, Union Bound. \nConditional probability. Conditional Probabilities and Independence, Bayes\u2019Theorem, Partition \nTheorem \nPart 2 - Discrete Random Variables \n \nRandom variables. Definition of a Random Variable, Probability Mass Function, Cumulative \nDistribution, Expectation. \nProbability distributions. Definition and Properties of Expectation, Variance, different ways of \ncomputing them, Examples of important Distributions (Bernoulli, Binomial, Geometric, Poisson), \nPrimer on Continuous Distributions including Normal and Exponential Distributions. \nMultivariate distributions. Multiple Random Variables, Joint and Marginal Distributions, \nIndependence of Random Variables, Covariance. \nPart 3 - Moments and Limit Theorems \n \nIntroduction. Law of Average, Useful inequalities (Markov and Chebyshef), Weak Law of Large \nNumbers (including Proof using Chebyshef\u2019s inequality), Examples. \nMoments and Central Limit Theorem. Introduction to Moments of Random Variables, Central \nLimit Theorem (Proof using Moment Generating functions), Example. \nPart 4 - Applications/Statistics \n \nStatistics. Classical Parameter Estimation (Maximum-Likelihood-Estimation), bias, sample \nmean, sample variance), Examples (Collision-Sampling, Estimating Population Size). \nAlgorithms. Online Algorithms (Secretary Problem, Odd\u2019s Algorithm). \nObjectives \nAt the end of the course students should \n \nunderstand the basic principles of probability spaces and random variables \nbe able to formulate problems using concepts from probability theory and compute or estimate \nprobabilities \nbe familiar with more advanced concepts such as moments, limit theorems and applications \nsuch as parameter estimation \n\nRecommended reading \n* Ross, S.M. (2014). A First course in probability. Pearson (9th ed.). \n \nBertsekas, D.P. and Tsitsiklis, J.N. (2008). Introduction to probability. Athena Scientific. \n \nGrimmett, G. and Welsh, D. (2014). Probability: an Introduction. Oxford University Press (2nd \ned.). \n \nDekking, F.M., et. al. (2005) A modern introduction to probability and statistics. Springer.\n\nSOFTWARE AND SECURITY ENGINEERING \n \nAims \nThis course aims to introduce students to software and security engineering, and in particular to \nthe problems of building large systems, safety-critical systems and systems that must withstand \nattack by capable opponents. Case histories of failure are used to illustrate what can go wrong, \nwhile current software and security engineering practice is studied as a guide to how failures \ncan be avoided. \n \nLectures \n1. What is a security policy or a safety case? Definitions and examples; one-way flows for both \nconfidentiality and safety properties; separation of duties. Top-down and bottom-up analysis \nmethods. What architecture can do, versus benefits of decoupling policy from mechanism. \n \n2. Examples of safety and security policies. Safety and security usability; the pyramid of harms. \nPredicting and mitigating user errors. The prevention of fraud and error in accounting systems; \nthe safety usability of medical devices. \n \n3. Attitudes to risk: expected  utility, prospect theory, framing, status quo bias. Authority, \nconformity and gender; mental models, affordances and defaults. The characteristics of human \nmemory; forgetting passwords versus guessing them. \n \n4. Security protocols; how to enforce policy using  structured human interaction, cryptography or \nboth. Middleperson attacks.The role of verification and its limitations. \n \n5. Attacks on TLS, from rogue CAs through side channels to Heartbleed. Other types of \nsoftware bugs: syntactic, timing, concurrency, code injection, buffer overflows. Defensive \nprogramming: secure coding, contracts. Fuzzing. \n \n6. The software crisis. Examples of large-scale project failure, such as the London Ambulance \nService system and the NHS National Programme for IT. Intrinsic difficulties with complex \nsoftware. \n \n7. Software engineering as the management of complexity. The software life cycle; requirements \nanalysis methods; modular design; the role of prototyping; the waterfall, spiral and agile models. \n \n8. The economics of software as a Service (SaaS); the impact SaaS has on software \nengineering. Continuous integration, release engineering, behavioural analytics and experiment \nframeworks, rearchitecting systems while in operation. \n \n9. Critical systems: safety as an emergent system property. Examples of catastrophic failure: \nfrom Therac-25 to the Boeing 737Max. The problems of managing redundancy. The overall \nprocess of safety engineering. \n \n\n10. Managing the development of critical systems: tools and methods, individual versus group \nproductivity, economics of testing and agile development, measuring outcomes versus process, \nthe technical and human aspects of management, post-market surveillance and coordinated \ndisclosure. The sustainability of products with software components. \n \nAt the end of the course students should know how writing programs with tough assurance \ntargets, in large teams, or both, differs from the programming exercises they have engaged in \nso far. They should understand the different models of software development described in the \ncourse as well as the value of various development and management tools. They should \nunderstand the development life cycle and its basic economics. They should understand the \nvarious types of bugs, vulnerabilities and hazards, how to find them, and how to avoid \nintroducing them. Finally, they should be prepared for the organizational aspects of their Part IB \ngroup project. \n \nRecommended reading \nAnderson, R. (Third Edition 2020). Security engineering (Part 1 and Chapters 27-28). Wiley. \nAvailable at: http://www.cl.cam.ac.uk/users/rja14/book.html\n \n\n",
        "3rd Semester \nCONCURRENT AND DISTRIBUTED SYSTEMS \n \nAims \nThis course considers two closely related topics, Concurrent Systems and Distributed Systems, \nover 16 lectures. The aim of the first half of the course is to introduce concurrency control \nconcepts and their implications for system design and implementation. The aims of the latter \nhalf of the course are to study the fundamental characteristics of distributed systems, including \ntheir models and architectures; the implications for software design; some of the techniques that \nhave been used to build them; and the resulting details of good distributed algorithms and \napplications. \n \nLectures: Concurrency \nIntroduction to concurrency, threads, and mutual exclusion. Introduction to concurrent systems; \nthreads; interleaving; preemption; parallelism; execution orderings; processes and threads; \nkernel vs. user threads; M:N threads; atomicity; mutual exclusion; and mutual exclusion locks \n(mutexes). \nAutomata Composition. Synchronous and asynchronous parallelism; sequential consistency; \nrendezvous. Safety, liveness and deadlock; the Dining Philosophers; Hardware foundations for \natomicity: test-and-set, load-linked/store-conditional and fence instructions. Lamport bakery \nalgorithm. \nCommon design patterns: semaphores, producer-consumer, and MRSW. Locks and invariants; \nsemaphores; condition synchronisation; N-resource allocation; two-party and generalised \nproducer-consumer; Multi-Reader, Single-Write (MRSW) locks. \nCCR, monitors, and concurrency in practice. Conditional critical regions (CCR); monitors; \ncondition variables; signal-wait vs. signal-continue semantics; concurrency in practice (kernels, \npthreads, Java, Cilk, OpenMP). \nDeadlock and liveness guarantees Offline vs. online; model checking; resource allocation \ngraphs; lock order checking; deadlock prevention, avoidance, detection, and recovery; livelock; \npriority inversion; auto parallelisation. \nConcurrency without shared data; transactions. Active objects; message passing; tuple spaces; \nCSP; and actor models. Composite operations; transactions; ACID; isolation; and serialisability. \nFurther transactions History graphs; good and bad schedules; isolation vs. strict isolation; \n2-phase locking; rollback; timestamp ordering (TSO); and optimistic concurrency control (OCC). \nCrash recovery, lock-free programming, and transactional memory. Write-ahead logging, \ncheckpoints, and recovery. Lock-free programming. Hardware and software transactional \nmemories. \n  \n \nLectures: Distributed Systems \nIntroduction to distributed systems; RPC. Avantages and challenges of distributed systems; \nunbounded delay and partial failure; network protocols; transparency; client-server systems; \nremote procedure call (RPC); marshalling; interface definition languages (IDLs). \n\nSystem models and faults. Synchronous, partially synchronous, and asynchronous network \nmodels; crash-stop, crash-recovery, and Byzantine faults; failures, faults, and fault tolerance; \ntwo generals problem. \nTime, clocks, and ordering of events. Physical clocks; leap seconds; UTC; clock synchronisation \nand drift; Network Time Protocol (NTP). Causality; happens-before relation. \nLogical time; Lamport clocks; vector clocks. Broadcast (FIFO, causal, total order); gossip \nprotocols. \nReplication. Quorums; idempotence; replica consistency; read-after-write consistency. State \nmachine replication; leader-based replication. \nConsensus and total order broadcast. FLP result; leader election; the Raft consensus algorithm. \nReplica consistency. Two-phase commit; relationship between 2PC and consensus; \nlinearizability; ABD algorithm; eventual consistency; CAP theorem. \nCase studies. Conflict-free Replicated Data Types (CRDTs); collaborative text editing. Google's \nSpanner; TrueTime. \nObjectives \nAt the end of Concurrent Systems portion of the course, students should: \n \nunderstand the need for concurrency control in operating systems and applications, both mutual \nexclusion and condition synchronisation; \nunderstand how multi-threading can be supported and the implications of different approaches; \nbe familiar with the support offered by various programming languages for concurrency control \nand be able to judge the scope, performance implications and possible applications of the \nvarious approaches; \nbe aware that dynamic resource allocation can lead to deadlock; \nunderstand the concept of transaction; the properties of transactions, how they can be \nimplemented, and how their performance can be optimised based on optimistic assumptions; \nunderstand how the persistence properties of transactions are addressed through logging; and \nhave a high-level understanding of the evolution of software use of concurrency in the \noperating-system kernel case study. \nAt the end of the Distributed Systems portion of the course, students should: \n \nunderstand the difference between shared-memory concurrency and distributed systems; \nunderstand the fundamental properties of distributed systems and their implications for system \ndesign; \nunderstand notions of time, including logical clocks, vector clocks, and physical time \nsynchronisation; \nbe familiar with various approaches to data and service replication, as well as the concept of \nreplica consistency; \nunderstand the effects of large scale on the provision of fundamental services and the tradeoffs \narising from scale; \nappreciate the implications of individual node and network communications failures on \ndistributed computation; \nbe aware of a variety of programming models and abstractions for distributed systems, such as \nRPC, middleware, and total order broadcast; \n\nbe familiar with a range of distributed algorithms, such as consensus, causal broadcast, and \ntwo-phase commit. \nRecommended reading \nModern Operating Systems (free PDF available online) by Andrew S Tanenbaum, Herbert Bos \nJava Concurrency in Practice' (2006 but still highly relevant) by Brian Goetz \nKleppmann, M. (2017). Designing data-intensive applications. O\u2019Reilly. \nTanenbaum, A.S. and van Steen, M. (2017). Distributed systems, 3rd edition. available online. \nCachin, C., Guerraoui, R. and Rodrigues, L. (2011) Introduction to Reliable and Secure \nDistributed Programming. Springer (2nd edition).\n \n\nDATA SCIENCE \n \nAims \nThis course introduces fundamental tools for describing and reasoning about data. There are \ntwo themes: designing probability models to describe systems; and drawing conclusions based \non data generated by such systems. \n \nLectures \nSpecifying and fitting probability models. Random variables. Maximum likelihood estimation. \nGenerative and supervised models. Goodness of fit. \nFeature spaces. Vector spaces, bases, inner products, projection. Linear models. Model fitting \nas projection. Design of features. \nHandling probability models. Handling pdf and cdf. Bayes\u2019s rule. Monte Carlo estimation. \nEmpirical distribution. \nInference. Bayesianism. Frequentist confidence intervals, hypothesis testing. Bootstrap \nresampling. \nRandom processes. Markov chains. Stationarity, and drift analysis. Processes with memory. \nLearning a random process. \nObjectives \nAt the end of the course students should \n \nbe able to formulate basic probabilistic models, including discrete time Markov chains and linear \nmodels \nbe familiar with common random variables and their uses, and with the use of empirical \ndistributions rather than formulae \nunderstand different types of inference about noisy data, including model fitting, hypothesis \ntesting, and making predictions \nunderstand the fundamental properties of inner product spaces and orthonormal systems, and \ntheir application to modelling \nRecommended reading \n* F.M. Dekking, C. Kraaikamp, H.P. Lopuha\u00e4, L.E. Meester (2005). A modern introduction to \nprobability and statistics: understanding why and how. Springer. \n \nS.M. Ross (2002). Probability models for computer science. Harcourt / Academic Press. \n \nM. Mitzenmacher and E. Upfal (2005). Probability and computing: randomized algorithms and \nprobabilistic analysis. Cambridge University Press.\n \n\nECAD AND ARCHITECTURE PRACTICAL CLASSES \nAims \nThe aims of this course are to enable students to apply the concepts learned in the Computer \nDesign course. In particular a web based tutor is used to introduce the SystemVerilog hardware \ndescription language, while the remaining practical classes will then allow students to implement \nthe design of components in this language. \n \nPractical Classes \nWeb tutor The first class uses a web based tutor to rapidly teach the SystemVerilog language. \nFPGA design flow Test driven hardware development for FPGA including an embedded \nprocessor and peripherals [3 classes] \nEmbedded system implementation Embedded system implementation on FPGA [3-4 classes] \nObjectives \nGain experience in electronic computer aided design (ECAD) through learning a design-flow for \nfield programmable gate arrays (FPGAs). \nLearn how to interface to peripherals like a touch screen. \nLearn how to debug hardware and software systems in simulation. \nUnderstand how to construct and program a heterogeneous embedded system. \nRecommended reading \n* Harris, D.M. and Harris, S.L. (2007). Digital design and computer architecture: from gates to \nprocessors. Morgan Kaufmann. \n \nPointers to sources of more specialist information are included on the associated course web \npage. \n \nIntroduction \nThe ECAD and Architecture Laboratory sessions are a companion to the Introduction to \nComputer Architecture course. The objective is to provide experience of hardware design for \nFPGA including use of a small embedded processor. It covers hardware design in \nSystemVerilog, embedded software design in RISC-V assembly and C, and use of FPGA tools. \n \nPrerequisites \nThis course assumes familiarity with the material from Part IA Digital Electronics, although we \nwill use automated tools to perform many of the steps you might have done there by hand (for \nexample, logic minimisation). \n \nWe will be using a Linux command-line environment. We provide scripts and guidance on what \nyou need, however you may find it useful to review Unix Tools, in particular basic navigation \nsuch as ls, cd, mkdir, cp, mv. We'll be using the GCC compiler and Makefiles, described in parts \n31-32. We will also be using git for revision control and submission of work for ticking - you \nshould review at least parts 23-25 and 29. \n \nPracticalities \n\nThe course runs over the 8 weeks of Michaelmas term. The course has been designed so you \ncan do most of the work at home or at any time you wish. You should be able to run all the \nnecessary tools on your own machine, through the use of virtual machines and Docker \ncontainers. We have a number of routes to do this in case some of them aren't suitable for the \nmachine you have. \n \nWe're running the course in a hybrid mode this year. We will provide online help sessions via \nMicrosoft Teams as well as in-person lab sessions on Fridays and Tuesdays 2-5pm during term. \n \nThe core components of the course, being the RISC-V architecture and software and ticked \nexercise, can be done entirely online, without needing access to hardware. They can be \nundertaken with any of the platforms we support, including MCS Linux (on the Intel Lab \nworkstations and remotely) if your own machine isn't suitable. We expect everyone to complete \nthese parts. \n \nThe rest of the course is optional. The hardware simulation and FPGA compilation can be done \non your machine without access to hardware, if you are able to run the virtual machine. \n \nThe parts that require access to an FPGA board will need the ability to run the virtual machine \nand you be able to collect and return an FPGA board from the Department. That process will be \ndescribed when you come to those parts. There is an optional starred tick available for \ncompletion of this part, which will need assessment by a demonstrator in person. \n \nWe'll do our best to support all the different platforms and variations, but please bear with us - \nit's quite possible we'll come across a problem we haven't seen before! \n \nWe have provided four routes you can use different tools to complete the course. Some routes \nnecessarily exclude some material but this material is optional. \n \nAssessment \nIn a similar way to other practical courses, this course carries one 'tick'. Everyone should expect \nto receive this tick, and those who do not should expect to lose marks on their final exams. \n \nThis year we have adopted the 'chime' autoticker used by Further Java. You will check out the \ninitial files as a git repository from the chime server, commit your work, and push the repository \nback to submit it. You can then press a button to run tests against your code and the autoticker \nwill tell you whether you have passed. Additionally you may be selected for a (real/video) \ninterview with a ticker to discuss your code and understanding of the material. \n \nTicking procedures will only be available during weeks 1-8 of Michaelmas term. The deadline for \nsubmitting Tick 1 is 5pm on the Tuesday of week 6 (19 November 2024). After passing the \nautoticker you may be asked for a tick interview with a demonstrator (online or in person). You \ncan gain Tick 1* during the timetabled lab sessions, assuming demonstrators are available. \n \n\nIf you do not submit a successful Tick 1 to the autoticker by 5pm on the Tuesday of week 6 you \ncan self-certify an extension. If you need more time than this you will need to ask your Directory \nof Studies to organise a further extension. The hard deadline by which all ticks must be \ncompleted is noon on Friday 31 January 2025 (the Head of Department Notices is the definitive \ndocument). Extensions beyond this date due to exceptional circumstances require a formal \napplication from your college to the Examiners. \n \nScheduling \nWhile the full course contains 8 exercises, there is not a tight binding to doing one exercise per \nweek. Students should expect to spend about three hours per week on the course, however it is \nrecommended that you make a start on the next exercise if you have time in hand. \n \nDemonstrator help will be focused on the Tuesday and Friday 2-5pm slots during term the lab \nwould normally operate in, so you may find it helpful to work in these times as that will provide \nthe quickest response to questions. \n \nCollaboration \nWhen we have done these labs in person, we have often found they worked well when doing \nthem collaboratively. While your work must be your own, students can work together and help \neach other, and demonstrators contribute advice and experience with the tools. \n \nFor the timetabled sessions, which are Tuesdays and Fridays 2-5pm UK time during term, there \nwill be demonstrators available in the lab and at the same time we'll use the ECAD group on \nMicrosoft Teams. With this we can provide online audio/video help, screensharing and (for the \nWindows and possibly Mac Teams apps) optional remote control of your development \nenvironment. You will be added to the Team in week 1 - if you believe you have been missed \nplease post in the Moodle forum. In Teams there are a number of Helpdesk Channels (A-C) - if \nyou need help during lab times, post in Help Centre and a demonstrator can ask you to go to a \nspecific channel. There are also Student Breakout Channels (D-F) which are available for \nstudents to chat amongst themselves. \n \nFor help outside of timetabled sessions we've set up a Moodle forum where you can post \nquestions - we'll keep an eye on this at other times, but students are encouraged to use it to \nsupport each other. \n \nStudents are of course free to use other platforms to help each other. If you find anything useful \nthat we might use in future, please let us know! \n \nIf you are having difficulty accessing both Moodle and Teams, please email theo.markettos at \ncl.cam.ac.uk. \n \nFeedback \nWe revise the course each year, which may cause new bugs in the code or the notes. The \ncreators (Theo Markettos and Simon Moore) appreciate constructive feedback. \n\n \nCredits \nThis course was written by Theo Markettos and Simon Moore, with portions developed by \nRobert Eady, Jonas Fiala, Paul Fox, Vlad Gavrila, Brian Jones and Roy Spliet. We would like to \nthank Altera, Intel and Terasic for funding and other contributions.\n \n\nECONOMICS, LAW AND ETHICS \n \nAims \nThis course aims to give students an introduction to some basic concepts in economics, law and \nethics. \n \nLectures \nClassical economics and consumer theory. Prices and markets; Pareto efficiency; preferences; \nutility; supply and demand; the marginalist revolution; elasticity; the welfare theorems; \ntransaction costs. \nInformation economics. The discriminating monopolist; marginal costs; effects of technology on \nsupply and demand; competition and information; lock in; real and virtual networks; Metcalfe\u2019s \nlaw; the dominant firm model; price discrimination; bundling; income distribution. \nMarket failure and behavioural economics. Market failure: the business cycle; recession and \ntechnology; tragedy of the commons; externalities; monopoly rents; asymmetric information: the \nmarket for lemons; adverse selection; moral hazard; signalling. Behavioural economics: \nbounded rationality, heuristics and biases; nudge theory; the power of defaults; agency effects. \nAuction theory and game theory. Auction theory: types of auctions; strategic equivalence; the \nrevenue equivalence theorem; the winner\u2019s curse; problems with real auctions; mechanism \ndesign and the combinatorial auction; applicability of auction mechanisms in computer science; \nadvertising auctions. Game theory: the choice between cooperation and conflict; strategic \nforms; dominant strategy equilibrium; Nash equilibrium; the prisoners\u2019 dilemma; evolution of \nstrategies; stag hunt; volunteer\u2019s dilemma; chicken; iterated games; hawk-dove; application to \ncomputer science. \nPrinciples of law. Criminal and civil law; contract law; choice of law and jurisdiction; arbitration; \ntort; negligence; defamation; intellectual property rights. \nLaw and the Internet. Computer evidence; the General Data Protection Regulation; UK laws that \nspecifically affect the Internet; e-commerce regulations; privacy and electronic communications. \nPhilosophies of ethics. Authority, intuitionist, egoist and deontological theories; utilitarian and \nRawlsian models; morality; insights from evolutionary psychology, neurology, and experimental \nethics; professional codes of ethics; research ethics. \nContemporary ethical issues. The Internet and social policy; current debates on privacy, \nsurveillance, and censorship; responsible vulnerability disclosure; algorithmic bias; predictive \npolicing; gamification and engagement; targeted political advertising; environmental impacts. \nObjectives \nOn completion of this course, students should be able to: \n \nReflect on and discuss professional, economic, social, environmental, moral and ethical issues \nrelating to computer science \nDefine and explain economic and legal terminology and arguments \nApply the philosophies and theories covered to computer science problems and scenarios \nReflect on the main constraints that market, legislation and ethics place on firms dealing in \ninformation goods and services \nRecommended reading \n\n* Shapiro, C. & Varian, H. (1998). Information rules. Harvard Business School Press. \nHare, S. (2022). Technology is not neutral: A short guide to technology ethics. London \nPublishing Partnership. \n \nFurther reading: \nSmith, A. (1776). An inquiry into the nature and causes of the wealth of nations, available at    \nhttp://www.econlib.org/library/Smith/smWN.html \nThaler, R.H. (2016). Misbehaving. Penguin. \nGalbraith, J.K. (1991). A history of economics. Penguin. \nPoundstone, W. (1992). Prisoner\u2019s dilemma. Anchor Books. \nPinker, S (2011). The Better Angels of our Nature. Penguin. \nAnderson, R. (2008). Security engineering (Chapter 7). Wiley. \nVarian, H. (1999). Intermediate microeconomics - a modern approach. Norton. \nNuffield Council on Bioethics (2015) The collection, linking and use of data in biomedical \nresearch and health care.\n \n\nFURTHER GRAPHICS \n \nAims \nThe course introduces fundamental concepts of modern graphics pipelines. \n \nLectures \nThe order and content of lectures is provisional and subject to change. \n \nGeometry Representations. Parametric surfaces, implicit surfaces, meshes, point-set surfaces, \ngeometry processing pipeline. [1 lecture] \nDiscrete Differential Geometry. Surface normal and curvature, Laplace-Beltrami operator, heat \ndiffusion. [1 lecture] \nGeometry Processing.  Parametrization, filtering, 3D capture. [1 lecture] \nAnimation I. Animation types, animation pipeline, rigging/skinning, character animation. [1 \nlecture] \nAnimation II. Blending transformations, pose-space animation, controllers. [1 lecture] \nThe Rendering Equation. Radiosity, reflection models, BRDFs, local vs. global illumination. [1 \nlecture] \nDistributed Ray Tracing. Quadrature, importance sampling, recursive ray tracing. [1 lecture] \nInverse Rendering. Differentiable rendering, inverse problems. [1 lecture] \nObjectives \nOn completing the course, students should be able to \n \nunderstand and use fundamental 3D geometry/scene representations and operations; \nlearn animation techniques and controls; \nlearn how light transport is modeled and simulated in rendering; \nunderstand how graphics and rendering can be used to solve computer perception problems. \nRecommended reading \nStudents should expect to refer to one or more of these books, but should not find it necessary \nto purchase any of them. \n \nShirley, P. and Marschner, S. (2009). Fundamentals of Computer Graphics. CRC Press (3rd \ned.). \nWatt, A. (2000). 3D Computer Graphics. Addison-Wesley (3rd ed). \nHughes, van Dam, McGuire, Skalar, Foley, Feiner and Akeley (2013). Computer Graphics: \nPrinciples and Practice. Addison-Wesley (3rd edition) \nAkenine-M\u00f6ller, et. al. (2018). Real-time rendering. CRC Press (4th ed.).\n \n\nFURTHER JAVA \n \nAims \nThe goal of this course is to provide students with the ability to understand the advanced \nprogramming features available in the Java programming language, completing the coverage of \nthe language started in the Programming in Java course. The course is designed to \naccommodate students with diverse programming backgrounds; consequently Java is taught \nfrom first principles in a practical class setting where students can work at their own pace from a \ncourse handbook. \n \nObjectives \n \nAt the end of the course students should \n \nunderstand different mechanisms for communication between distributed applications and be \nable to evaluate their trade-offs; \nbe able to use Java generics and annotations to improve software usability, readability and \nsafety; \nunderstand and be able to exploit the Java class-loading mechansim; \nunderstand and be able to use concurrency control correctly; \nbe able to implement a vector clock algorithm and the happens-before relation. \nRecommended reading \n* Goetz, B. (2006). Java concurrency in practice. Addison-Wesley. \nGosling, J., Joy, B., Steele, G., Bracha, G. and Buckley, A. (2014). The Java language \nspecification, Java SE 8 Edition. Addison-Wesley. \nhttp://docs.oracle.com/javase/specs/jls/se8/html/\n \n\nGROUP PROJECTS \nExample Risk Report \nThe risk report should focus on the risks to your project succeeding on time (and not, say, on \ngeneral health and safety points). It is only 50 words. Example: \n \n\"Identified risks: \n * Training data requires access authorisation that has not been approved, and we don\u2019t know if \nclient can do this \n * Two group members have unreliable network connections and may miss meetings \n * One group member is in timezone UTC+8, and will have to attend meetings late at night \n * Bug reports on Github for a library we plan to use suggest maintenance updates will be \nnecessary for recent Android release\"\n \n\nINTRODUCTION TO COMPUTER ARCHITECTURE \n \nAims \nThe aims of this course are to introduce a hardware description language (SystemVerilog) and \ncomputer architecture concepts in order to design computer systems. The parallel ECAD+Arch \npractical classes will allow students to apply the concepts taught in lectures. \n \nLectures \nPart 1 - Gates to processors  \n \nTechnology trends and design challenges. Current technology, technology trends, ECAD trends, \nchallenges. [1 lecture] \nDigital system design. Practicalities of mapping SystemVerilog descriptions of hardware \n(including a processor) onto an FPGA board. Tips and pitfalls when generating larger modular \ndesigns. [1 lecture] \nEight great ideas in computer architecture. [1 lecture] \nReduced instruction set computers and RISC-V. Introduction to the RISC-V processor design. [1 \nlecture] \nExecutable and synthesisable models. [1 lecture] \nPipelining. [2 lectures] \nMemory hierarchy and caching. Caching, etc. [1 lecture] \nSupport for operating systems. Memory protection, exceptions, interrupts, etc. [1 lecture] \nOther instruction set architectures. CISC, stack, accumulator. [1 lecture] \nPart 2  \n \nOverview of Systems-on-Chip (SoCs) and DRAM. [1 lecture] High-level SoCs, DRAM storage \nand accessing. \nMulticore Processors. [2 lectures] Communication, cache coherence, barriers and \nsynchronisation primitives. \nGraphics processing units (GPUs) [2 lectures] Basic GPU architecture and programming. \nFuture Directions [1 lecture] Where is computer architecture heading? \nObjectives \nAt the end of the course students should \n \nbe able to read assembler given a guide to the instruction set and be able to write short pieces \nof assembler if given an instruction set or asked to invent an instruction set; \nunderstand the differences between RISC and CISC assembler; \nunderstand what facilities a processor provides to support operating systems, from memory \nmanagement to software interrupts; \nunderstand memory hierarchy including different cache structures and coherency needed for \nmulticore systems; \nunderstand how to implement a processor in SystemVerilog; \nappreciate the use of pipelining in processor design; \nhave an appreciation of control structures used in processor design; \n\nhave an appreciation of system-on-chips and their components; \nunderstand how DRAM stores data; \nunderstand how multicore processors communicate; \nunderstand how GPUs work and have an appreciation of how to program them. \nRecommended reading \n* Patterson, D.A. and Hennessy, J.L. (2017). Computer organization and design: The \nhardware/software interface RISC-V edition. Morgan Kaufmann. ISBN 978-0-12-812275-4. \n \nRecommended further reading: \n \nHarris, D.M. and Harris, S.L. (2012). Digital design and computer architecture. Morgan \nKaufmann. ISBN 978-0-12-394424-5. \nHennessy, J. and Patterson, D. (2006). Computer architecture: a quantitative approach. Elsevier \n(4th ed.). ISBN 978-0-12-370490-0. (Older versions of the book are also still generally relevant.) \n \nPointers to sources of more specialist information are included in the lecture notes and on the \nassociated course web page\n \n\nPROGRAMMING IN C AND C++ \n \nAims \nThis course aims \n \nto provide a solid introduction to programming in C and to provide an overview of the principles \nand constraints that affect the way in which the C programming language has been designed \nand is used; \nto introduce the key additional features of C++. \n  \n \nLectures \nIntroduction to the C language. Background and goals of C. Types, expressions, control flow \nand strings. \n(continued) Functions. Multiple compilation units. Scope. Segments. Incremental compilation. \nPreprocessor. \n(continued) Pointers and pointer arithmetic. Function pointers. Structures and Unions. \n(continued) Const and volatile qualifiers. Typedefs. Standard input/output. Heap allocation. \nMiscellaneous Features, Hints and Tips. \nC semantics and tools. Undefined vs implementation-defined behaviour. Buffer and integer \noverflows. ASan, MSan, UBsan, Valgrind checkers. \nMemory allocation, data structures and aliasing. Malloc/free, tree and DAG examples and their \ndeallocation using a memory arena. \nFurther memory management. Reference Counting and Garbage Collection \nMemory hierarchy and cache optimization. Data structure layouts. Intrusive lists. Array-of-structs \nvs struct-of-arrays representations. Loop blocking. \nDebugging. Using print statements, assertions and an interactive debugger. Unit testing and \nregression. \nIntroduction to C++. Goals of C++. Differences between C and C++. References versus \npointers. Overloaded functions. \nObjects in C++. Classes and structs. Destructors. Resource Acquisition is Initialisation. Operator \noverloading. Virtual functions. Casts. Multiple inheritance. Virtual base classes. \nOther C++ concepts. Templates and meta-programming. \nObjectives \nAt the end of the course students should \n \nbe able to read and write C programs; \nunderstand the interaction between C programs and the host operating system; \nbe familiar with the structure of C program execution in machine memory; \nunderstand the potential dangers of writing programs in C; \nunderstand the object model and main additional features of C++. \nRecommended reading \n* Kernighan, B.W. and Ritchie, D.M. (1988). The C programming language. Prentice Hall (2nd \ned.).\n \n\nSEMANTICS OF PROGRAMMING LANGUAGES \n \nAims \nThe aim of this course is to introduce the structural, operational approach to programming \nlanguage semantics. It will show how to specify the meaning of typical programming language \nconstructs, in the context of language design, and how to reason formally about semantic \nproperties of programs. \n \nLectures \nIntroduction. Transition systems. The idea of structural operational semantics. Transition \nsemantics of a simple imperative language. Language design options. [2 lectures] \nTypes. Introduction to formal type systems. Typing for the simple imperative language. \nStatements of desirable properties. [2 lectures] \nInduction. Review of mathematical induction. Abstract syntax trees and structural induction. \nRule-based inductive definitions and proofs. Proofs of type safety properties. [2 lectures] \nFunctions. Call-by-name and call-by-value function application, semantics and typing. Local \nrecursive definitions. [2 lectures] \nData. Semantics and typing for products, sums, records, references. [1 lecture] \nSubtyping. Record subtyping and simple object encoding. [1 lecture] \nSemantic equivalence. Semantic equivalence of phrases in a simple imperative language, \nincluding the congruence property. Examples of equivalence and non-equivalence. [1 lecture] \nConcurrency. Shared variable interleaving. Semantics for simple mutexes; a serializability \nproperty. [1 lecture] \nObjectives \nAt the end of the course students should \n \nbe familiar with rule-based presentations of the operational semantics and type systems for \nsome simple imperative, functional and interactive program constructs; \nbe able to prove properties of an operational semantics using various forms of induction \n(mathematical, structural, and rule-based); \nbe familiar with some operationally-based notions of semantic equivalence of program phrases \nand their basic properties. \nRecommended reading \n* Pierce, B.C. (2002). Types and programming languages. MIT Press. \nHennessy, M. (1990). The semantics of programming languages. Wiley. Out of print, but \navailable on the web at \nhttp://www.cs.tcd.ie/matthew.hennessy/splexternal2015/resources/sembookWiley.pdf \nWinskel, G. (1993). The formal semantics of programming languages. MIT Press.\n \n\nUNIX TOOLS \n \nAims \nThis video-lecture course gives students who have already basic Unix/Linux experience some \nadditional practical software-engineering knowledge: how to use the shell and related tools as \nan efficient working environment, how to automate routine tasks, and how version-control and \nautomated-build tools can help to avoid confusion and accidents, especially when working in \nteams. These are essential skills, both in industrial software development and student projects. \n \nLectures \nUnix concepts. Brief review of Unix history and design philosophy, documentation, terminals, \ninter-process communication mechanisms and conventions, shell, command-line arguments, \nenvironment variables, file descriptors. \nShell concepts. Program invocation, redirection, pipes, file-system navigation, argument \nexpansion, quoting, job control, signals, process groups, variables, locale, history and alias \nfunctions, security considerations. \nScripting. Plain-text formats, executables, #!, shell control structures and functions. Startup \nscripts. \nText, file and networking tools. sed, grep, chmod, find, ssh, rsync, tar, zip, etc. \nVersion control. diff, patch, RCS, Subversion, git. \nSoftware development tools. C compiler, linker, debugger, make. \nPerl. Introduction to a powerful scripting and text-manipulation language. [2 lectures] \nObjectives \nAt the end of the course students should \n \nbe confident in performing routine user tasks on a POSIX system, understand command-line \nuser-interface conventions and know how to find more detailed documentation; \nappreciate how simple tools can be combined to perform a large variety of tasks; \nbe familiar with the most common tools, file formats and configuration practices; \nbe able to understand, write, and maintain shell scripts and makefiles; \nappreciate how using a version-control system and fully automated build processes help to \nmaintain reproducibility and audit trails during software development; \nknow enough about basic development tools to be able to install, modify and debug C source \ncode; \nhave understood the main concepts of, and gained initial experience in, writing Perl scripts \n(excluding the facilities for object-oriented programming). \nRecommended reading \nRobbins, A. (2005). Unix in a nutshell. O\u2019Reilly (4th ed.). \nSchwartz, R.L., Foy, B.D. and Phoenix, T. (2011). Learning Perl. O\u2019Reilly (6th ed.).\n \n\n",
        "4th Semester \n \nCOMPILER CONSTRUCTION \n \nAims \nThis course aims to cover the main concepts associated with implementing compilers for \nprogramming languages. We use a running example called SLANG (a Small LANGuage) \ninspired by the languages described in 1B Semantics. A toy compiler (written in ML) is provided, \nand students are encouraged to extend it in various ways. \n \nLectures \nOverview of compiler structure The spectrum of interpreters and compilers; compile-time and \nrun-time. Compilation as a sequence of translations from higher-level to lower-level intermediate \nlanguages, where each translation preserves semantics. The structure of a simple compiler: \nlexical analysis and syntax analysis, type checking, intermediate representations, optimisations, \ncode generation. Overview of run-time data structures: stack and heap. Virtual machines. [1 \nlecture] \nLexical analysis and syntax analysis. Lexical analysis based on regular expressions and finite \nstate automata. Using LEX-tools. How does LEX work? Parsing based on context-free \ngrammars and push-down automata. Grammar ambiguity, left- and right-associativity and \noperator precedence. Using YACC-like tools. How does YACC work? LL(k) and LR(k) parsing \ntheory. [3 lectures] \nCompiler Correctness Recursive functions can be transformed into iterative functions using the \nContinuation-Passing Style (CPS) transformation. CPS applied to a (recursive) SLANG \ninterpreter to derive, in a step-by-step manner, a correct stack-based compiler. [3 lectures] \nData structures, procedures/functions Representing tuples, arrays, references. Procedures and \nfunctions: calling conventions, nested structure, non-local variables. Functions as first-class \nvalues represented as closures. Simple optimisations: inline expansion, constant folding, \nelimination of tail recursion, peephole optimisation. [5 lectures] \nAdvanced topics Run-time memory management (garbage collection). Static and dynamic \nlinking. Objects and inheritance; implementation of method dispatch. Try-catch exception \nmechanisms. Compiling a compiler via bootstrapping. [4 lectures] \nObjectives \nAt the end of the course students should understand the overall structure of a compiler, and will \nknow significant details of a number of important techniques commonly used. They will be \naware of the way in which language features raise challenges for compiler builders. \n \nRecommended reading \n* Aho, A.V., Sethi, R. and Ullman, J.D. (2007). Compilers: principles, techniques and tools. \nAddison-Wesley (2nd ed.). \nMogensen, T. \u00c6. (2011). Introduction to compiler design. Springer. http://www.diku.dk/ \ntorbenm/Basics.\n \n\nCOMPUTATION THEORY \n \nAims \nThe aim of this course is to introduce several apparently different formalisations of the informal \nnotion of algorithm; to show that they are equivalent; and to use them to demonstrate that there \nare uncomputable functions and algorithmically undecidable problems. \n \nLectures \nIntroduction: algorithmically undecidable problems. Decision problems. The informal notion of \nalgorithm, or effective procedure. Examples of algorithmically undecidable problems. [1 lecture] \nRegister machines. Definition and examples; graphical notation. Register machine computable \nfunctions. Doing arithmetic with register machines. [1 lecture] \nUniversal register machine. Natural number encoding of pairs and lists. Coding register machine \nprograms as numbers. Specification and implementation of a universal register machine. [2 \nlectures] \nUndecidability of the halting problem. Statement and proof. Example of an uncomputable partial \nfunction. Decidable sets of numbers; examples of undecidable sets of numbers. [1 lecture] \nTuring machines. Informal description. Definition and examples. Turing computable functions. \nEquivalence of register machine computability and Turing computability. The Church-Turing \nThesis. [2 lectures] \nPrimitive and partial recursive functions. Definition and examples. Existence of a recursive, but \nnot primitive recursive function. A partial function is partial recursive if and only if it is \ncomputable. [2 lectures] \nLambda-Calculus. Alpha and beta conversion. Normalization. Encoding data. Writing recursive \nfunctions in the lambda-calculus. The relationship between computable functions and \nlambda-definable functions. [3 lectures] \nObjectives \nAt the end of the course students should \n \nbe familiar with the register machine, Turing machine and lambda-calculus models of \ncomputability; \nunderstand the notion of coding programs as data, and of a universal machine; \nbe able to use diagonalisation to prove the undecidability of the Halting Problem; \nunderstand the mathematical notion of partial recursive function and its relationship to \ncomputability. \nRecommended reading \n* Hopcroft, J.E., Motwani, R. and Ullman, J.D. (2001). Introduction to automata theory, \nlanguages, and computation. Addison-Wesley (2nd ed.). \n* Hindley, J.R. and Seldin, J.P. (2008). Lambda-calculus and combinators, an introduction. \nCambridge University Press (2nd ed.). \nCutland, N.J. (1980). Computability: an introduction to recursive function theory. Cambridge \nUniversity Press. \nDavis, M.D., Sigal, R. and Weyuker, E.J. (1994). Computability, complexity and languages. \nAcademic Press (2nd ed.). \n\nSudkamp, T.A. (2005). Languages and machines. Addison-Wesley (3rd ed.).\n \n\nCOMPUTER NETWORKING \n \nAims \nThe aim of this course is to introduce key concepts and principles of computer networks. The \ncourse will use a top-down approach to study the Internet and its protocol stack. Instances of \narchitecture, protocol, application-examples will include email, web and media-streaming. We \nwill cover communications services (e.g., TCP/IP) required to support such network \napplications. The implementation and deployment of communications services in practical \nnetworks: including wired and wireless LAN environments, will be followed by a discussion of \nissues of network-management. Throughout the course, the Internet\u2019s architecture and \nprotocols will be used as the primary examples to illustrate the fundamental principles of \ncomputer networking. \n \nLectures \nIntroduction. Overview of networking using the Internet as an example. LANs and WANs. OSI \nreference model, Internet TCP/IP Protocol Stack. Circuit-switching, packet-switching, Internet \nstructure, networking delays and packet loss. [3 lectures] \nLink layer and local area networks. Link layer services, error detection and correction, Multiple \nAccess Protocols, link layer addressing, Ethernet, hubs and switches, Point-to-Point Protocol. [2 \nlectures] \nWireless and mobile networks. Wireless links and network characteristics, Wi-Fi: IEEE 802.11 \nwireless LANs. [1 lecture] \nNetwork layer addressing. Network layer services, IP, IP addressing, IPv4, DHCP, NAT, ICMP, \nIPv6. [3 lectures] \nNetwork layer routing. Routing and forwarding, routing algorithms, routing in the Internet, \nmulticast. [3 lectures] \nTransport layer. Service models, multiplexing/demultiplexing, connection-less transport (UDP), \nprinciples of reliable data transfer, connection-oriented transport (TCP), TCP congestion control, \nTCP variants. [6 lectures] \nApplication layer. Client/server paradigm, WWW, HTTP, Domain Name System, P2P. [1.5 \nlectures] \nMultimedia networking. Networked multimedia applications, multimedia delivery requirements, \nmultimedia protocols (SIP), content distribution networks. [0.5 lecture] \nObjectives \nAt the end of the course students should \n \nbe able to analyse a communication system by separating out the different functions provided \nby the network; \nunderstand that there are fundamental limits to any communications system; \nunderstand the general principles behind multiplexing, addressing, routing, reliable transmission \nand other stateful protocols as well as specific examples of each; \nunderstand what FEC is; \nbe able to compare communications systems in how they solve similar problems; \n\nhave an informed view of both the internal workings of the Internet and of a number of common \nInternet applications and protocols. \nRecommended reading \n* Peterson, L.L. and Davie, B.S. (2011). Computer networks: a systems approach. Morgan \nKaufmann (5th ed.). ISBN 9780123850591 \nKurose, J.F. and Ross, K.W. (2009). Computer networking: a top-down approach. \nAddison-Wesley (5th ed.). \nComer, D. and Stevens, D. (2005). Internetworking with TCP-IP, vol. 1 and 2. Prentice Hall (5th \ned.). \nStevens, W.R., Fenner, B. and Rudoff, A.M. (2003). UNIX network programming, Vol.I: The \nsockets networking API. Prentice Hall (3rd ed.).\n \n\nFURTHER HUMAN\u2013COMPUTER INTERACTION \n \nAims \nThis aim of this course is to provide an introduction to the theoretical foundations of Human \nComputer Interaction, and an understanding of how these can be applied to the design of \ncomplex technologies. \n \nLectures \nTheory driven approaches to HCI. What is a theory in HCI? Why take a theory driven approach \nto HCI? \nDesign of visual displays. Segmentation and variables of the display plane. Modes of \ncorrespondence. \nGoal-oriented interaction. Using cognitive theories of planning, learning and understanding to \nunderstand user behaviour, and what they find hard. \nDesigning smart systems. Using statistical methods to anticipate user needs and actions with \nBayesian strategies. \nDesigning efficient systems. Measuring and optimising human performance through quantitative \nexperimental methods. \nDesigning meaningful systems. Qualitative research methods to understand social context and \nrequirements of user experience. \nEvaluating interactive system designs. Approaches to evaluation in systems research and \nengineering, including Part II Projects. \nDesigning complex systems. Worked case studies of applying the theories to a hard HCI \nproblem. Research directions in HCI. \nObjectives \nAt the end of the course students should be able to apply theories of human performance and \ncognition to system design, including selection of appropriate techniques to analyse, observe \nand improve the usability of a wide range of technologies. \n \nRecommended reading \n* Preece, J., Sharp, H. and Rogers, Y. (2015). Interaction design: beyond human-computer \ninteraction. Wiley (Currently in 4th edition, but earlier editions will suffice). \n \nFurther reading: \n \nCarroll, J.M. (ed.) (2003). HCI models, theories and frameworks: toward a multi-disciplinary \nscience. Morgan Kaufmann.\n \n\nLOGIC AND PROOF \n \nAims \nThis course will teach logic, especially the predicate calculus. It will present the basic principles \nand definitions, then describe a variety of different formalisms and algorithms that can be used \nto solve problems in logic. Putting logic into the context of Computer Science, the course will \nshow how the programming language Prolog arises from the automatic proof method known as \nresolution. It will introduce topics that are important in mechanical verification, such as binary \ndecision diagrams (BDDs), SAT solvers and modal logic. \n \nLectures \nIntroduction to logic. Schematic statements. Interpretations and validity. Logical consequence. \nInference. \nPropositional logic. Basic syntax and semantics. Equivalences. Normal forms. Tautology \nchecking using CNF. \nThe sequent calculus. A simple (Hilbert-style) proof system. Natural deduction systems. \nSequent calculus rules. Sample proofs. \nFirst order logic. Basic syntax. Quantifiers. Semantics (truth definition). \nFormal reasoning in FOL. Free versus bound variables. Substitution. Equivalences for \nquantifiers. Sequent calculus rules. Examples. \nClausal proof methods. Clause form. A SAT-solving procedure. The resolution rule. Examples. \nRefinements. \nSkolem functions, Unification and Herbrand\u2019s theorem. Prenex normal form. Skolemisation. \nMost general unifiers. A unification algorithm. Herbrand models and their properties. \nResolution theorem-proving and Prolog. Binary resolution. Factorisation. Example of Prolog \nexecution. Proof by model elimination. \nSatisfiability Modulo Theories. Decision problems and procedures. How SMT solvers work. \nBinary decision diagrams. General concepts. Fast canonical form algorithm. Optimisations. \nApplications. \nModal logics. Possible worlds semantics. Truth and validity. A Hilbert-style proof system. \nSequent calculus rules. \nTableaux methods. Simplifying the sequent calculus. Examples. Adding unification. \nSkolemisation. The world\u2019s smallest theorem prover? \nObjectives \nAt the end of the course students should \n \nbe able to manipulate logical formulas accurately; \nbe able to perform proofs using the presented formal calculi; \nbe able to construct a small BDD; \nunderstand the relationships among the various calculi, e.g. SAT solving, resolution and Prolog; \nunderstand the concept of a decision procedure and the basic principles of \u201csatisfiability modulo \ntheories\u201d. \nbe able to apply the unification algorithm and to describe its uses. \nRecommended reading \n\n* Huth, M. and Ryan, M. (2004). Logic in computer science: modelling and reasoning about \nsystems. Cambridge University Press (2nd ed.). \nBen-Ari, M. (2001). Mathematical logic for computer science. Springer (2nd ed.).\n \n\nPROLOG \n \nAims \nThe aim of this course is to introduce programming in the Prolog language. Prolog encourages \na different programming style to Java or ML and particular focus is placed on programming to \nsolve real problems that are suited to this style. Practical experimentation with the language is \nstrongly encouraged. \n \nLectures \nIntroduction to Prolog. The structure of a Prolog program and how to use the Prolog interpreter. \nUnification. Some simple programs. \nArithmetic and lists. Prolog\u2019s support for evaluating arithmetic expressions and lists. The space \ncomplexity of program evaluation discussed with reference to last-call optimisation. \nBacktracking, cut, and negation. The cut operator for controlling backtracking. Negation as \nfailure and its uses. \nSearch and cut. Prolog\u2019s search method for solving problems. Graph searching exploiting \nProlog\u2019s built-in search mechanisms. \nDifference structures. Difference lists: introduction and application to example programs. \nBuilding on Prolog. How particular limitations of Prolog programs can be addressed by \ntechniques such as Constraint Logic Programming (CLP) and tabled resolution. \nObjectives \nAt the end of the course students should \n \nbe able to write programs in Prolog using techniques such as accumulators and difference \nstructures; \nknow how to model the backtracking behaviour of program execution; \nappreciate the unique perspective Prolog gives to problem solving and algorithm design; \nunderstand how larger programs can be created using the basic programming techniques used \nin this course. \nRecommended reading \n* Bratko, I. (2001). PROLOG programming for artificial intelligence. Addison-Wesley (3rd or 4th \ned.). \nSterling, L. and Shapiro, E. (1994). The art of Prolog. MIT Press (2nd ed.). \n \nFurther reading: \n \nO\u2019Keefe, R. (1990). The craft of Prolog. MIT Press. [This book is beyond the scope of this \ncourse, but it is very instructive. If you understand its contents, you\u2019re more than prepared for \nthe examination.]\n \n\nARTIFICIAL INTELLIGENCE \n \nPrerequisites: Algorithms 1, Algorithms 2. In addition the course requires some mathematics, in \nparticular some use of vectors and some calculus. Part IA Natural Sciences Mathematics or \nequivalent and Discrete Mathematics are likely to be helpful although not essential. Similarly, \nelements of Machine Learning and Real World Data, Foundations of Data Science, Logic and \nProof, Prolog and Complexity Theory are likely to be useful. This course is a prerequisite for the \nPart II courses Machine Learning and Bayesian Inference and Natural Language Processing. \nThis course is a prerequisite for: Natural Language Processing \nExam: Paper 7 Question 1, 2 \nPast exam questions, Moodle, timetable \n \nAims \nThe aim of this course is to provide an introduction to some fundamental issues and algorithms \nin artificial intelligence (AI). The course approaches AI from an algorithmic, computer \nscience-centric perspective; relatively little reference is made to the complementary \nperspectives developed within psychology, neuroscience or elsewhere. The course aims to \nprovide some fundamental tools and algorithms required to produce AI systems able to exhibit \nlimited human-like abilities, particularly in the form of problem solving by search, game-playing, \nrepresenting and reasoning with knowledge, planning, and learning. \n \nLectures \nIntroduction. Alternate ways of thinking about AI. Agents as a unifying view of AI systems. [1 \nlecture] \nSearch I. Search as a fundamental paradigm for intelligent problem-solving. Simple, uninformed \nsearch algorithms. Tree search and graph search. [1 lecture] \nSearch II. More sophisticated heuristic search algorithms. The A* algorithm and its properties. \nImproving memory efficiency: the IDA* and recursive best first search algorithms. Local search \nand gradient descent. [1 lecture] \nGame-playing. Search in an adversarial environment. The minimax algorithm and its \nshortcomings. Improving minimax using alpha-beta pruning. [1 lecture] \nConstraint satisfaction problems (CSPs). Standardising search problems to a common format. \nThe backtracking algorithm for CSPs. Heuristics for improving the search for a solution. Forward \nchecking, constraint propagation and arc consistency. [1 lecture] \nBackjumping in CSPs. Backtracking, backjumping using Gaschnig\u2019s algorithm, graph-based \nbackjumping. [1 lecture] \nKnowledge representation and reasoning I. How can we represent and deal with commonsense \nknowledge and other forms of knowledge? Semantic networks, frames and rules. How can we \nuse inference in conjunction with a knowledge representation scheme to perform reasoning \nabout the world and thereby to solve problems? Inheritance, forward and backward chaining. [1 \nlecture] \nKnowledge representation and reasoning II. Knowledge representation and reasoning using first \norder logic. The frame, qualification and ramification problems. The situation calculus. [1 lecture] \n\nPlanning I. Methods for planning in advance how to solve a problem. The STRIPS language. \nAchieving preconditions, backtracking and fixing threats by promotion or demotion: the \npartial-order planning algorithm. [1 lecture] \nPlanning II. Incorporating heuristics into partial-order planning. Planning graphs. The \nGRAPHPLAN algorithm. Planning using propositional logic. Planning as a constraint satisfaction \nproblem. [1 lecture] \nNeural Networks I. A brief introduction to supervised learning from examples. Learning as fitting \na curve to data. The perceptron. Learning by gradient descent. [1 lecture] \nNeural Networks II. Multilayer perceptrons and the backpropagation algorithm. [1 lecture] \nObjectives \nAt the end of the course students should: \n \nappreciate the distinction between the popular view of the field and the actual research results; \nappreciate the fact that the computational complexity of most AI problems requires us regularly \nto deal with approximate techniques; \nbe able to design basic problem solving methods based on AI-based search, knowledge \nrepresentation, reasoning, planning, and learning algorithms. \nRecommended reading \nThe recommended text is: \n \n* Russell, S. and Norvig, P. (2010). Artificial intelligence: a modern approach. Prentice Hall (3rd \ned.). \n \nThere are many good books available on artificial intelligence; one alternative is: \n \nPoole, D. L. and Mackworth, A. K. (2017). Artificial intelligence: foundations of computational \nagents. Cambridge University Press (2nd ed.). \n \nFor some of the material you might find it useful to consult more specialised texts, in particular: \n \nDechter, R. (2003). Constraint processing. Morgan Kaufmann. \n \nCawsey, A. (1998). The essence of artificial intelligence. Prentice Hall. \n \nGhallab, M., Nau, D. and Traverso, P. (2004). Automated planning: theory and practice. Morgan \nKaufmann. \n \nBishop, C.M. (2006). Pattern recognition and machine learning. Springer. \n \nBrachman, R.J and Levesque, H.J. (2004). Knowledge Representation and Reasoning. Morgan \nKaufmann.\n \n\nCOMPLEXITY THEORY \n \nAims \nThe aim of the course is to introduce the theory of computational complexity. The course will \nexplain measures of the complexity of problems and of algorithms, based on time and space \nused on abstract models. Important complexity classes will be defined, and the notion of \ncompleteness established through a thorough study of NP-completeness. Applications to \ncryptography will be considered. \n \nSyllabus \nIntroduction to Complexity Theory. Decidability and complexity; lower and upper bounds; the \nChurch-Turing hypothesis. \nTime complexity. Time complexity classes; polynomial time computation and the class P. \nNon-determinism. Non-deterministic machines; the class NP; the problem 3SAT. \nNP-completeness. Reductions and completeness; the Cook-Levin theorem; graph-theoretic \nproblems. \nMore NP-complete problems. 3-colourability; scheduling, matching, set covering, and knapsack.  \ncoNP. Complement classes. NP \u2229 coNP; primality and factorisation. \nCryptography. One-way functions; the class UP. \nSpace complexity. Space complexity classes; Savitch\u2019s theorem. \nHierarchy theorems. Time and space hierarchy theorems. \nRandomised algorithms. The class BPP; derandomisation; error-correcting codes, \ncommunication complexity. \nQuantum Complexity. The classes BQP and QMA. \nInteractive Proofs. IP=PSPACE; Zero Knowledge Proofs; Probabilistically Checkable Proofs \n(PCPs).  \n  \nObjectives \nAt the end of the course students should \n \nbe able to analyse practical problems and classify them according to their complexity; \nbe familiar with the phenomenon of NP-completeness, and be able to identify problems that are \nNP-complete; \nbe aware of a variety of complexity classes and their interrelationships; \nunderstand the role of complexity analysis in cryptography. \nRecommended reading \nA Survey of P vs. NP by Scott Aaronson. \nComputational Complexity: A Modern Approach by Arora and Barak \nComputational Complexity: A Conceptual Perspective by Oded Goldreich \nMathematics and Computation by Avi Wigderson\n \n\nCYBERSECURITY \n \nAims \nIn today\u2019s digital society, computer security is vital for commercial competitiveness, national \nsecurity and privacy of individuals. Protection against both criminal and state-sponsored attacks \nwill need a large cohort of skilled individuals with an understanding of the principles of security \nand with practical experience of the application of these principles. We want you to become one \nof them. In this adversarial game, to defeat the bad guys, the good guys have to be at least as \nskilled at them. Therefore this course has a strong foundation of becoming proficient at actual \nattacks. A theoretical understanding is of course necessary, but without some practice the bad \nguys will run circles around the good guys. In 12 hours I can\u2019t bring you from zero to the stage \nwhere you can invent new attacks and countermeasures, but at least by practicing and \ndissecting common attacks (akin to \u201cstudying the classics\u201d) I\u2019ll give you a feeling for the kind of \nadversarial thinking that is required in this field. Large portions of this course are hands-on: you \nwill need to acquire the relevant skills rather than just reading about it. The recommended \ncourse textbook will be especially helpful to those with no prior low-level hacking experience. \n \nLectures \nIntroduction. \nThe adversarial nature of security; thinking like the attacker; confidentiality, integrity and \navailability; systems-level thinking; Trusted Computing Base; role of cryptography. \n \nFundamentals of access control (chapter 1). \nDiscretionary vs mandatory access control; discretionary access control in POSIX; file \npermissions, file ownership, groups, permission bits. \n \nSoftware security (spanning 4 book chapters) \nSetuid (chapter 2): privileged programs, attack surfaces, exploitable vulnerabilities, privilege \nescalation from regular user to root. \nBuffer overflow (chapter 4): stack memory layout, exploiting a buffer overflow vulnerability, \nguessing unknown addresses, payload, countermeasures and counter-countermeasures. \nReturn to libc and return-oriented programming (chapter 5): exploiting a non-executable stack, \nchaining function calls, chaining ROP gadgets. \nSQL injection (chapter 14): vulnerability and its exploitation, countermeasures, input filtering, \nprepared statements. \n \nAuthentication. \nSomething you know, have, are; passwords, their dominance, their shortcomings and the many \nattempts at replacing them; password cracking; tokens; biometrics; taxonomy of Single Sign-On \nsystems; password managers. \n \nWeb and internet security (spanning 3 book chapters). \nCross-Site Request Forgery (chapter 12): why cross-site requests, CSRF attacks on HTTP GET \nand HTTP POST, countermeasures. \n\nCross-Site Scripting (chapter 13): non-persistent vs persistent XSS, javascript, self-propagating \nXSS worm, countermeasures; \n \nHuman factors. \nUsers are not the enemy; Compliance budget; Prospect theory; 7 principles for systems \nsecurity. \n \nAdditional topics. \nViruses, worms and quines; lockpicking; privilege escalation in physical locks; conclusions. \n \nTo complete the course successfully, students must acquire the practical ability to carry out (as \nopposed to just describing) the exploits mentioned in the syllabus, given a vulnerable system. \nThe low-level hands-on portions of the course are supported by the very helpful course \ntextbook. Those who choose not to study on the recommended textbook will be severely \ndisadvantaged. \n \nRecommended textbook: Wenliang Du. Computer Security: A Hands-on Approach. 3rd Edition. \nISBN: 978-17330039-5-7. Published: 1 May 2022. \n \nhttps://www.handsonsecurity.net. Some chapters freely available online. \n \n \nOptional additional books (not substitutes): \n \nRoss Anderson, Security Engineering 3rd Ed, Wiley, 2020. ISBN: 978-1-119-64278-7. \nhttps://www.cl.cam.ac.uk/~rja14/book.html. Some chapters freely available online. \n \nPaul van Oorschot, Computer Security and the Internet, Springer, 2020. ISBN: \n978-3-030-33648-6 (hardcopy), 978-3-030-33649-3 (eBook). \nhttps://people.scs.carleton.ca/~paulv/toolsjewels.html. All chapters freely available online.\n\nFORMAL MODELS OF LANGUAGE \n \nAims \nThis course studies formal models of language and considers how they might be relevant to the \nprocessing and acquisition of natural languages. The course will extend knowledge of formal \nlanguage theory; introduce several new grammars; and use concepts from information theory to \ndescribe natural language. \n \nLectures \nNatural language and the Chomsky hierarchy 1. Recap classes of language. Closure properties \nof language classes. Recap pumping lemma for regular languages. Discussion of relevance (or \nnot) to natural languages (example embedded clauses in English). \nNatural language and the Chomsky hierarchy 2. Pumping lemma for context free languages. \nDiscussion of relevance (or not) to natural languages (example Swiss-German cross serial \ndependancies). Properties of minimally context sensitive languages. Introduction to tree \nadjoining grammars. \nLanguage processing and context free grammar parsing 1. Recap of context free grammar \nparsing. Language processing predictions based on top down parsing models (example Yngve\u2019s \nlanguage processing predictions). Language processing predictions based on probabilistic \nparsing (example Halle\u2019s language processing predictions). \nLanguage processing and context free grammar parsing 2. Introduction to context free grammar \nequivalent dependancy grammars. Language processing predictions based on Shift-Reduce \nparsing (examples prosodic look-ahead parsers, Parsey McParseface). \nGrammar induction of language classes. Introduction to grammar induction. Discussion of \nrelevance (or not) to natural language acquisition. Gold\u2019s theorem. Introduction to context free \ngrammar equivalent categorial grammars and their learnable classes. \nNatural language and information theory 1. Entropy and natural language typology. Uniform \ninformation density as a predictor for language processing. \nNatural language and information theory 2. Noisy channel encoding as a model for spelling \nerror, translation and language processing. \nVector space models and word vectors. Introduction to word vectors (example Word2Vec). Word \nvectors as predictors for semantic language processing. \nObjectives \nAt the end of the course students should \n \nunderstand how known natural languages relate to formal languages in the Chomsky hierarchy; \nhave knowledge of several context free grammars equivalents; \nunderstand how we might make predictions about language processing and language \nacquisition from formal models; \nknow how to use information theoretic concepts to describe aspects of natural language. \nRecommended reading \n* Jurafsky, D. and Martin, J. (2008). Speech and language processing. Prentice Hall. \nManning, C.D. and Schutze, H. (1999) Foundations of statistical natural language processing. \nMIT Press. \n\nRuslan, M. (2003) The Oxford handbook of computational linguistics. Oxford University Press. \nClark, A., Fox, C. and Lappin, S. (2010) The handbook of computational linguistics and natural \nlanguage processing. Wiley-Blackwell. \nKozen, D. (1997) Automata and computibility. Springer.\n \n\n",
        "5th Semester \nBIOINFORMATICS \nAims \nThis course focuses on algorithms used in Bioinformatics and System Biology. Most of the \nalgorithms are general and can be applied in other fields on multidimensional and noisy data. All \nthe necessary biological terms and concepts useful for the course and the examination will be \ngiven in the lectures. The most important software implementing the described algorithms will be \ndemonstrated. \n \nLectures \nIntroduction to biological data: Bioinformatics as an interesting field in computer science. \nComputing and storing information with DNA (including Adleman\u2019s experiment). \nDynamic programming. Longest common subsequence, DNA global and local alignment, linear \nspace alignment, Nussinov algorithm for RNA, heuristics for multiple alignment. (Vol. 1, chapter \n5) \nSequence database search. Blast. (see notes and textbooks) \nGenome sequencing. De Bruijn graph. (Vol. 1, chapter 3) \nPhylogeny. Distance based algorithms (UPGMA, Neighbour-Joining). Parsimony-based \nalgorithms. Examples in Computer Science. (Vol. 2, chapter 7) \nClustering. Hard and soft K-means clustering, use of Expectation Maximization in clustering, \nHierarchical clustering, Markov clustering algorithm. (Vol. 2, chapter 8) \nGenomics Pattern Matching. Suffix Tree String Compression and the Burrows-Wheeler \nTransform. (Vol. 2, chapter 9) \nHidden Markov Models. The Viterbi algorithm, profile HMMs for sequence alignment, classifying \nproteins with profile HMMs, soft decoding problem, Baum-Welch learning. (Vol. 2, chapter 10) \nObjectives \nAt the end of this course students should \n \nunderstand Bioinformatics terminology; \nhave mastered the most important algorithms in the field; \nbe able to work with bioinformaticians and biologists; \nbe able to find data and literature in repositories. \nRecommended reading \n* Compeau, P. and Pevzner, P.A. (2015). Bioinformatics algorithms: an active learning approach. \nActive Learning Publishers. \nDurbin, R., Eddy, S., Krough, A. and Mitchison, G. (1998). Biological sequence analysis: \nprobabilistic models of proteins and nucleic acids. Cambridge University Press. \nJones, N.C. and Pevzner, P.A. (2004). An introduction to bioinformatics algorithms. MIT Press. \nFelsenstein, J. (2003). Inferring phylogenies. Sinauer Associates.\n \n\nBUSINESS STUDIES \n \nAims \nHow to start and run a computer company; the aims of this course are to introduce students to \nall the things that go to making a successful project or product other than just the programming. \nThe course will survey some of the issues that students are likely to encounter in the world of \ncommerce and that need to be considered when setting up a new computer company. \n \nSee also Business Seminars in the Easter Term. \n \nLectures \nSo you\u2019ve got an idea? Introduction. Why are you doing it and what is it? Types of company. \nMarket analysis. The business plan. \nMoney and tools for its management. Introduction to accounting: profit and loss, cash flow, \nbalance sheet, budgets. Sources of finance. Stocks and shares. Options and futures. \nSetting up: legal aspects. Company formation. Brief introduction to business law; duties of \ndirectors. Shares, stock options, profit share schemes and the like. Intellectual Property Rights, \npatents, trademarks and copyright. Company culture and management theory. \nPeople. Motivating factors. Groups and teams. Ego. Hiring and firing: employment law. \nInterviews. Meeting techniques. \nProject planning and management. Role of a manager. PERT and GANTT charts, and critical \npath analysis. Estimation techniques. Monitoring. \nQuality, maintenance and documentation. Development cycle. Productization. Plan for quality. \nPlan for maintenance. Plan for documentation. \nMarketing and selling. Sales and marketing are different. Marketing; channels; marketing \ncommunications. Stages in selling. Control and commissions. \nGrowth and exit routes. New markets: horizontal and vertical expansion. Problems of growth; \nsecond system effects. Management structures. Communication. Exit routes: acquisition, \nfloatation, MBO or liquidation. Futures: some emerging ideas for new computer businesses. \nSummary. Conclusion: now you do it! \nObjectives \nAt the end of the course students should \n \nbe able to write and analyse a business plan; \nknow how to construct PERT and GANTT diagrams and perform critical path analysis; \nappreciate the differences between profitability and cash flow, and have some notion of budget \nestimation; \nhave an outline view of company formation, share structure, capital raising, growth and exit \nroutes; \nhave been introduced to concepts of team formation and management; \nknow about quality documentation and productization processes; \nunderstand the rudiments of marketing and the sales process. \nRecommended reading \n\nLang, J. (2001). The high-tech entrepreneur\u2019s handbook: how to start and run a high-tech \ncompany. FT.COM/Prentice Hall. \n \nStudents will be expected to be able to use Microsoft Excel and Microsoft Project. \n \nFor additional reading on a lecture-by-lecture basis, please see the course website. \n \nStudents are strongly recommended to enter the CU Entrepreneurs Business Ideas Competition \nhttp://www.cue.org.uk/\n \n\nDENOTATIONAL SEMANTICS \n \nAims \nThe aims of this course are to introduce domain theory and denotational semantics, and to \nshow how they provide a mathematical basis for reasoning about the behaviour of programming \nlanguages. \n \nLectures \nIntroduction.The denotational approach to the semantics of programming \nlanguages.Recursively defined objects as limits of successive approximations. \nLeast fixed points.Complete partial orders (cpos) and least elements.Continuous functions and \nleast fixed points. \nConstructions on domains.Flat domains.Product domains. Function domains. \nScott induction.Chain-closed and admissible subsets of cpos and domains.Scott\u2019s fixed-point \ninduction principle. \nPCF.The Scott-Plotkin language PCF.Evaluation. Contextual equivalence. \nDenotational semantics of PCF.Denotation of types and terms. Compositionality. Soundness \nwith respect to evaluation. [2 lectures]. \nRelating denotational and operational semantics.Formal approximation relation and its \nfundamental property.Computational adequacy of the PCF denotational semantics with respect \nto evaluation. Extensionality properties of contextual equivalence. [2 lectures]. \nFull abstraction.Failure of full abstraction for the domain model. PCF with parallel or. \nObjectives \nAt the end of the course students should \n \nbe familiar with basic domain theory: cpos, continuous functions, admissible subsets, least fixed \npoints, basic constructions on domains; \nbe able to give denotational semantics to simple programming languages with simple types; \nbe able to apply denotational semantics; in particular, to understand the use of least fixed points \nto model recursive programs and be able to reason about least fixed points and simple \nrecursive programs using fixed point induction; \nunderstand the issues concerning the relation between denotational and operational semantics, \nadequacy and full abstraction, especially with respect to the language PCF. \nRecommended reading \nWinskel, G. (1993). The formal semantics of programming languages: an introduction. MIT \nPress. \nGunter, C. (1992). Semantics of programming languages: structures and techniques. MIT Press. \nTennent, R. (1991). Semantics of programming languages. Prentice Hall.\n \n\nINFORMATION THEORY \n \nAims \nThis course introduces the principles and applications of information theory: how information is \nmeasured in terms of probability and various entropies, how these are used to calculate the \ncapacity of communication channels, with or without noise, and to measure how much random \nvariables reveal about each other. Coding schemes including error correcting codes are studied \nalong with data compression, spectral analysis, transforms, and wavelet coding. Applications of \ninformation theory are reviewed, from astrophysics to pattern recognition. \n \nLectures \nInformation, probability, uncertainty, and surprise. How concepts of randomness and uncertainty \nare related to information. How the metrics of information are grounded in the rules of \nprobability. Shannon Information. Weighing problems and other examples. \nEntropy of discrete variables. Definition and link to Shannon information. Joint entropy, Mutual \ninformation. Visual depictions of the relationships between entropy types. Why entropy gives \nfundamental measures of information content. \nSource coding theorem and data compression; prefix, variable-, and fixed-length codes. \nInformation rates; Asymptotic equipartition principle; Symbol codes; Huffman codes and the \nprefix property. Binary symmetric channels. Capacity of a noiseless discrete channel. Stream \ncodes. \nNoisy discrete channel coding. Joint distributions; mutual information; Conditional Entropy; \nError-correcting codes; Capacity of a discrete channel. Noisy channel coding theorem. \nEntropy of continuous variables. Differential entropy; Mutual information; Channel Capacity; \nGaussian channels. \nEntropy for comparing probability distributions and for machine learning. Relative entropy/KL \ndivergence; cross-entropy; use as loss function. \nApplications of information theory in other sciences.  \nObjectives \nAt the end of the course students should be able to \n \ncalculate the information content of a random variable from its probability distribution; \nrelate the joint, conditional, and marginal entropies of variables in terms of their coupled \nprobabilities; \ndefine channel capacities and properties using Shannon\u2019s Theorems; \nconstruct efficient codes for data on imperfect communication channels; \ngeneralize the discrete concepts to continuous signals on continuous channels; \nunderstand encoding and communication schemes in terms of the spectral properties of signals \nand channels; \ndescribe compression schemes, and efficient coding using wavelets and other representations \nfor data. \nRecommended reading \n  Mackay's Information Theory, inference and Learning Algorithms \n \n\n Stone's Information Theory: A Tutorial Introduction.\n \n\nLATEX AND JULIA \n \nAims \nIntroduction to two widely-used languages for typesetting dissertations and scientific \npublications, for prototyping numerical algorithms and to visualize results. \n \nLectures \nLATEX. Workflow example, syntax, typesetting conventions, non-ASCII characters, document \nstructure, packages, mathematical typesetting, graphics and figures, cross references, build \ntools. \nJulia. Tools for technical computing and visualization. The Array type and its operators, 2D/3D \nplotting, functions and methods, notebooks, packages, vectorized audio demonstration. \nObjectives \nStudents should be able to avoid the most common LATEX mistakes, to prototype simple image \nand signal-processing algorithms in Julia, and to visualize the results. \n \nRecommended reading \n* Lamport, L. (1994). LATEX \u2013 a documentation preparation system user\u2019s guide and reference \nmanual. Addison-Wesley (2nd ed.). \nMittelbach, F., et al. (2023). The LATEX companion. Addison-Wesley (3rd ed.).\n \n\nPRINCIPLES OF COMMUNICATIONS \n \nAims \nThis course aims to provide a detailed understanding of the underlying principles for how \ncommunications systems operate. Practical examples (from wired and wireless \ncommunications, the Internet, and other communications systems) are used to illustrate the \nprinciples. \n \nLectures \nIntroduction. Course overview. Abstraction, layering. Review of structure of real networks, links, \nend systems and switching systems. [1 lecture] \nRouting. Central versus Distributed Routing Policy Routing. Multicast Routing Circuit Routing [6 \nlectures] \nFlow control and resource optimisation. 1[Control theory] is a branch of engineering familiar to \npeople building dynamic machines. It can be applied to network traffic. Stemming the flood, at \nsource, sink, or in between? Optimisation as a model of networkand user. TCP in the wild. [3 \nlectures] \nPacket Scheduling. Design choices for scheduling and queue management algorithms for \npacket forwarding, and fairness. [2 lectures] \nThe big picture for managing traffic. Economics and policy are relevant to networks in many \nways. Optimisation and game theory are both relevant topics discussed here. [2 lectures] \nSystem Structures and Summary. Abstraction, layering. The structure of real networks, links, \nend systems and switching. [2 lectures] \n1 Control theory was not taught and will not be the subject of any exam question. \n \nObjectives \nAt the end of the course students should be able to explain the underlying design and behaviour \nof protocols and networks, including capacity, topology, control and use. Several specific \nmathematical approaches are covered (control theory, optimisation). \n \nRecommended reading \n* Keshav, S. (2012). Mathematical Foundations of Computer Networking. Addison Wesley. ISBN \n9780321792105 \nBackground reading: \nKeshav, S. (1997). An engineering approach to computer networking. Addison-Wesley (1st ed.). \nISBN 0201634422 \nStevens, W.R. (1994). TCP/IP illustrated, vol. 1: the protocols. Addison-Wesley (1st ed.). ISBN \n0201633469\n \n\nTYPES \n \nAims \nThe aim of this course is to show by example how type systems for programming languages can \nbe defined and their properties developed, using techniques that were introduced in the Part IB \ncourse on Semantics of Programming Languages. The emphasis is on type systems for \nfunctional languages and their connection to constructive logic. \n \nLectures \nIntroduction. The role of type systems in programming languages. Review of rule-based \nformalisation of type systems. [1 lecture] \nPropositions as types. The Curry-Howard correspondence between intuitionistic propositional \ncalculus and simply-typed lambda calculus. Inductive types and iteration. Consistency and \ntermination. [2 lectures] \nPolymorphic lambda calculus (PLC). PLC syntax and reduction semantics. Examples of \ndatatypes definable in the polymorphic lambda calculus. Type inference. [3 lectures] \nMonads and effects. Explicit versus implicit effects. Using monadic types to control effects. \nReferences and polymorphism. Recursion and looping. [2 lectures] \nContinuations and classical logic. First-class continuations and control operators. Continuations \nas Curry-Howard for classical logic. Continuation-passing style. [2 lectures] \nDependent types. Dependent function types. Indexed datatypes. Equality types and combining \nproofs with programming. [2 lectures] \nObjectives \nAt the end of the course students should \n \nbe able to use a rule-based specification of a type system to carry out type checking and type \ninference; \nunderstand by example the Curry-Howard correspondence between type systems and logics; \nunderstand how types can be used to control side-effects in programming; \nappreciate the expressive power of parametric polymorphism and dependent types. \nRecommended reading \n* Pierce, B.C. (2002). Types and programming languages. MIT Press. \nPierce, B. C. (Ed.) (2005). Advanced Topics in Types and Programming Languages. MIT Press. \nGirard, J-Y. (tr. Taylor, P. and Lafont, Y.) (1989). Proofs and types. Cambridge University Press.\n\nADVANCED DATA SCIENCE \n \nPracticals \nThe module will have a practical session for each of the three stages of the pipeline. Each of the \nparts will be tied into an aspect of the final project. \n \nObjectives \nAt the end of the course students will be familiar with the purpose of data science, how it differs \nfrom the closely related fields of machine learning, statistics and artificial intelligence and what a \ntypical data analysis pipeline looks like in practice. As well as emphasising the importance of \nanalysis methods we will introduce a formalism for organising how data science is done in \npractice and what the different aspects the data scientist faces when giving data-driven answers \nto questions of interest. \n \nRecommended reading \nSimon Rogers et al. (2016). A First Course in Machine Learning, Second Edition. [] Chapman \nand Hall/CRC, nil \nShai Shalev-Shwartz et al. (2014). Understanding Machine Learning: From Theory to \nAlgorithms. New York, NY, USA: Cambridge  University Press \nChristopher M. Bishop (2006). Pattern Recognition and Machine Learning (Information Science \nand Statistics). Secaucus, NJ, USA: Springer-Verlag New York, Inc. \nAssessment \nPractical There will be four practicals contributing 20% to the final module mark. Data science is \na topic where there is rarely a single \ncorrect answer therefore the important thing is that an informed attempt has been made to \naddress the questions that can be supported and motivated. \nReport The course will be concluded by an individual report covering the material in the course \nthrough \npractical exercise working with real data. This report will make up 80% of the mark for the \ncourse. \n \n \n\nAFFECTIVE ARTIFICIAL INTELLIGENCE \n \nSynopsis \nAffective Artificial Intelligence (Affective AI) aims to imbue machines with social and emotional \nintelligence (EQ). More specifically, Affective AI aims to create artificially intelligent systems and \nmachines that can recognize, interpret, process, and simulate human social signals and \nbehaviours, expressions, and emotions, to enhance human-AI interaction and communication. \n\\r\\n\\r\\nTo achieve this goal, Affective AI draws upon various scientific disciplines, including \nmachine learning, computer vision, speech / natural language / signal processing, psychology \nand cognitive science, and ethics and social sciences. \n \n  \nBackground & Aims \nAffective Artificial Intelligence (Affective AI) aims to imbue machines with social and emotional \nintelligence (EQ). More specifically, Affective AI aims to create artificially intelligent systems and \nmachines that can recognize, interpret, process, and simulate human social signals and \nbehaviours, expressions, and emotions, to enhance human-AI interaction and communication.  \n \nTo achieve this goal, Affective AI draws upon various scientific disciplines, including machine \nlearning, computer vision, speech / natural language / signal processing, psychology and \ncognitive science, and ethics and social sciences. \n \nAffective AI has direct applications in and implications for the design of innovative interactive \ntechnology (e.g., interaction with chat bots, virtual agents, robots), single and multi-user smart \nenvironments ( e.g., in-car/ virtual / augmented / mixed reality, serious games), public speaking \nand cognitive training, and clinical and biomedical studies (e.g., autism, depression, pain). \n \nThe aim of this module is to impart knowledge and ability needed to make informed choices of \nmodels, data, and machine learning techniques for sensing, recognition, and generation of \naffective and social behaviour (e.g., smile, frown, head nodding/shaking, \nagreement/disagreement) in order to create Affectively intelligent AI systems, with a \nconsideration for various ethical issues (e.g., privacy, bias) arising from the real-world \ndeployment of these systems. \n \n  \n \nSyllabus \nThe following list provides a representative list of topics: \n \nIntroduction, definitions, and overview \nTheories from various disciplines \nSensing from multiple modalities (e.g., vision, audio, bio signals, text) \nData acquisition and annotation \nSignal processing / feature extraction \n\nLearning / prediction / recognition and evaluation \nBehaviour synthesis / generation (e.g., for embodied agents / robots) \nAdvanced topics and ethical considerations (e.g., bias and fairness) \nCross-disciplinary applications (via seminar presentations and discussions) \nGuest lectures (diverse topics \u2013 changes each year) \nHands-on research and programming work (i.e., mini project & report)  \n \n  \n \nObjectives \nOn completion of this module, students will: \n \nunderstand and demonstrate knowledge in key characteristics of affectively intelligent AI, which \ninclude: \nRecognition: How to equip Affective AI systems with capabilities of analysing facial expressions, \nvocal intonations, gestures, and other physiological signals to infer human affective states? \nGeneration: How to enable affectively intelligent AI simulate expressions of emotions in \nmachines, allowing them to respond in a more human-like manner, such as virtual agents and \nhumanoid robots, showing empathy or sympathy? \nAdaptation and Personalization: How to enable affectively intelligent AI adapt system responses \nbased on users' affective states and/or needs, or tailor interactions and experiences to individual \nusers based on their expressivity / emotional profiles, personalities, and past interactions? \nEmpathetic Communication: How to design Affective AI systems to communicate with users in a \nway that demonstrates empathy, understanding, and sensitivity to their affective states and \nneeds? \nEthical and societal considerations: What are the various human differences, ethical guidelines, \nand societal impacts that need to be considered when designing and deploying Affective AI to \nensure that these systems respect users' privacy, autonomy, and well-being? \ncomprehend and apply (appropriate) methods for collection, analysis, representation, and \nevaluation of human affective and communicative behavioural data. \nenhance programming skills for creating and implementing (components of) Affective AI \nsystems. \ndemonstrate critical thinking, analysis and synthesis while deciding on 'when' and 'how' to \nincorporate human affect and social signals in a specific AI system context and gain practical \nexperience in proposing and justifying computational solution(s) of suitable nature and scope. \n  \n \nAssessment - Part II Students \nSeminar presentation: 20% \nParticipating in Q&A and discussions: 10% \nMid-term mini project report and presentation (as a team of 2): 10%  \nFinal mini project report & code (as a team of 2): 60% \n \n\nCATEGORY THEORY \n \nAims \nCategory theory provides a unified treatment of mathematical properties and constructions that \ncan be expressed in terms of 'morphisms' between structures. It gives a precise framework for \ncomparing one branch of mathematics (organized as a category) with another and for the \ntransfer of problems in one area to another. Since its origins in the 1940s motivated by \nconnections between algebra and geometry, category theory has been applied to diverse fields, \nincluding computer science, logic and linguistics. This course introduces the basic notions of \ncategory theory: adjunction, natural transformation, functor and category. We will use category \ntheory to organize and develop the kinds of structure that arise in models and semantics for \nlogics and programming languages. \n \nSyllabus \nIntroduction; some history. Definition of category. The category of sets and functions. \nCommutative diagrams. Examples of categories: preorders and monotone functions; monoids \nand monoid homomorphisms; a preorder as a category; a monoid as a category. Definition of \nisomorphism. Informal notion of a 'category-theoretic' property. \nTerminal objects. The opposite of a category and the duality principle. Initial objects. Free \nmonoids as initial objects. \nBinary products and coproducts. Cartesian categories. \nExponential objects: in the category of sets and in general. Cartesian closed categories: \ndefinition and examples. \nIntuitionistic Propositional Logic (IPL) in Natural Deduction style. Semantics of IPL in a cartesian \nclosed preorder. \nSimply Typed Lambda Calculus (STLC). The typing relation. Semantics of STLC types and \nterms in a cartesian closed category (ccc). The internal language of a ccc. The \nCurry-Howard-Lawvere correspondence. \nFunctors. Contravariance. Identity and composition for functors. \nSize: small categories and locally small categories. The category of small categories. Finite \nproducts of categories. \nNatural transformations. Functor categories. The category of small categories is cartesian \nclosed. \nHom functors. Natural isomorphisms. Adjunctions. Examples of adjoint functors. Theorem \ncharacterising the existence of right (respectively left) adjoints in terms of a universal property. \nDependent types. Dependent product sets and dependent function sets as adjoint functors. \nEquivalence of categories. Example: the category of I-indexed sets and functions is equivalent \nto the slice category Set/I. \nPresheaves. The Yoneda Lemma. Categories of presheaves are cartesian closed. \nMonads. Modelling notions of computation as monads. Moggi's computational lambda calculus. \nObjectives \nOn completion of this module, students should: \n \n\nbe familiar with some of the basic notions of category theory and its connections with logic and \nprogramming language semantics \nAssessment \na graded exercise sheet (25% of the final mark), and \na take-home test (75%) \nRecommended reading \nAwodey, S. (2010). Category theory. Oxford University Press (2nd ed.). \n \nCrole, R. L. (1994). Categories for types. Cambridge University Press. \n \nLambek, J. and Scott, P. J. (1986). Introduction to higher order categorical logic. Cambridge \nUniversity Press. \n \nPitts, A. M. (2000). Categorical Logic. Chapter 2 of S. Abramsky, D. M. Gabbay and T. S. E. \nMaibaum (Eds) Handbook of Logic in Computer Science, Volume 5. Oxford University Press. \n(Draft copy available here.) \n \nClass Size \nThis module can accommodate upto 15 Part II students plus 15 MPhil / Part III students.\n\nDIGITAL SIGNAL PROCESSING \n \nAims \nThis course teaches basic signal-processing principles necessary to understand many modern \nhigh-tech systems, with examples from audio processing, image coding, radio communication, \nradar, and software-defined radio. Students will gain practical experience from numerical \nexperiments in programming assignments (in Julia, MATLAB or NumPy). \n \nLectures \nSignals and systems. Discrete sequences and systems: types and properties. Amplitude, \nphase, frequency, modulation, decibels, root-mean square. Linear time-invariant systems, \nconvolution. Some examples from electronics, optics and acoustics. \nPhasors. Eigen functions of linear time-invariant systems. Review of complex arithmetic. \nPhasors as orthogonal base functions. \nFourier transform. Forms and properties of the Fourier transform. Convolution theorem. Rect \nand sinc. \nDirac\u2019s delta function. Fourier representation of sine waves, impulse combs in the time and \nfrequency domain. Amplitude-modulation in the frequency domain. \nDiscrete sequences and spectra. Sampling of continuous signals, periodic signals, aliasing, \ninterpolation, sampling and reconstruction, sample-rate conversion, oversampling, spectral \ninversion. \nDiscrete Fourier transform. Continuous versus discrete Fourier transform, symmetry, linearity, \nFFT, real-valued FFT, FFT-based convolution, zero padding, FFT-based resampling, \ndeconvolution exercise. \nSpectral estimation. Short-time Fourier transform, leakage and scalloping phenomena, \nwindowing, zero padding. Audio and voice examples. DTFM exercise. \nFinite impulse-response filters. Properties of filters, implementation forms, window-based FIR \ndesign, use of frequency-inversion to obtain high-pass filters, use of modulation to obtain \nband-pass filters. \nInfinite impulse-response filters. Sequences as polynomials, z-transform, zeros and poles, some \nanalog IIR design techniques (Butterworth, Chebyshev I/II, elliptic filters, second-order cascade \nform). \nBand-pass signals. Band-pass sampling and reconstruction, IQ up and down conversion, \nsuperheterodyne receivers, software-defined radio front-ends, IQ representation of AM and FM \nsignals and their demodulation. \nDigital communication. Pulse-amplitude modulation. Matched-filter detector. Pulse shapes, \ninter-symbol interference, equalization. IQ representation of ASK, BSK, PSK, QAM and FSK \nsignals. [2 hours] \nRandom sequences and noise. Random variables, stationary and ergodic processes, \nautocorrelation, cross-correlation, deterministic cross-correlation sequences, filtered random \nsequences, white noise, periodic averaging. \nCorrelation coding. Entropy, delta coding, linear prediction, dependence versus correlation, \nrandom vectors, covariance, decorrelation, matrix diagonalization, eigen decomposition, \n\nKarhunen-Lo\u00e8ve transform, principal component analysis. Relation to orthogonal transform \ncoding using fixed basis vectors, such as DCT. \nLossy versus lossless compression. What information is discarded by human senses and can \nbe eliminated by encoders? Perceptual scales, audio masking, spatial resolution, colour \ncoordinates, some demonstration experiments. \nQuantization, image coding standards. Uniform and logarithmic quantization, A/\u00b5-law coding, \ndithering, JPEG. \nObjectives \napply basic properties of time-invariant linear systems; \nunderstand sampling, aliasing, convolution, filtering, the pitfalls of spectral estimation; \nexplain the above in time and frequency domain representations; \nuse filter-design software; \nvisualize and discuss digital filters in the z-domain; \nuse the FFT for convolution, deconvolution, filtering; \nimplement, apply and evaluate simple DSP applications; \nfamiliarity with a number of signal-processing concepts used in digital communication systems \nRecommended reading \nLyons, R.G. (2010). Understanding digital signal processing. Prentice Hall (3rd ed.). \nOppenheim, A.V. and Schafer, R.W. (2007). Discrete-time digital signal processing. Prentice \nHall (3rd ed.). \nStein, J. (2000). Digital signal processing \u2013 a computer science perspective. Wiley. \n \nClass size \nThis module can accommodate a maximum of 24 students (16 Part II students and 8 MPhil \nstudents) \n \nAssessment - Part II students \nThree homework programming assignments, each comprising 20% of the mark \nWritten test, comprising 40% of the total mark.\n \n\nMACHINE VISUAL PERCEPTION \n \nAims \nThis course aims at introducing the theoretical foundations and practical techniques for machine \nperception, the capability of computers to interpret data resulting from sensor measurements. \nThe course will teach the fundamentals and modern techniques of machine perception, i.e. \nreconstructing the real world starting from sensor measurements with a focus on machine \nperception for visual data. The topics covered will be image/geometry representations for \nmachine perception, semantic segmentation, object detection and recognition, geometry \ncapture, appearance modeling and acquisition, motion detection and estimation, \nhuman-in-the-loop machine perception, select topics in applied machine perception. \n \nMachine perception/computer vision is a rapidly expanding area of research with real-world \napplications. An understanding of machine perception is also important for robotics, interactive \ngraphics (especially AR/VR), applied machine learning, and several other fields and industries. \nThis course will provide a fundamental understanding of and practical experience with the \nrelevant techniques. \n \nLearning outcomes \nStudents will understand the theoretical underpinnings of the modern machine perception \ntechniques for reconstructing models of reality starting from an incomplete and imperfect view of \nreality. \nStudents will be able to apply machine perception theory to solve practical problems, e.g. \nclassification of images, geometry capture. \nStudents will gain an understanding of which machine perception techniques are appropriate for \ndifferent tasks and scenarios. \nStudents will have hands-on experience with some of these techniques via developing a \nfunctional machine perception system in their projects. \nStudents will have practical experience with the current prominent machine perception \nframeworks. \nSyllabus \nThe fundamentals of machine learning for machine perception \nDeep neural networks and frameworks for machine perception \nSemantic segmentation of objects and humans \nObject detection and recognition \nMotion estimation, tracking and recognition \n3D geometry capture \nAppearance modeling and acquisition \nSelect topics in applied machine perception \nAssessment \nA practical exercise, worth 20% of the mark. This will cover the basics and theory of machine \nperception and some of the practical techniques the students will likely use in their projects. This \nis individual work. No GPU hours will be needed for the practical work. \nA machine perception project worth 80% of the marks: \n\nCourse projects will be selected by the students following the possible project themes proposed \nby the lecturer, and will be checked by the lecturer for appropriateness.  \nThe students will form groups of 2-3 to design, implement, report, and present a project to tackle \na given task in machine perception. \nThe final mark will be composed of an implementation/report mark (60%) and a presentation \nmark (20%). Each team member will be evaluated based on her/his contribution. \nEach project will have extensions to be completed only by the ACS students. Each student will \nwrite a different part of the report, whose author will be clearly marked. Each student will further \nsummarise her/his contributions to the project in the same report. \nRecommended Reading List \nComputer Vision: Algorithms and Applications, Richard Szeliski, Springer, 2010. \nDeep Learning, Ian Goodfellow, Yoshua Bengio, and Aaron Courville, MIT Press, 2016. \nMachine Learning and Visual Perception, Baochang Zhang, De Gruyter, 2020.\n \n\nNATURAL LANGUAGE PROCESSING \n \nAims \nThis course introduces the fundamental techniques of natural language processing. It aims to \nexplain the potential and the main limitations of these techniques. Some current research issues \nare introduced and some current and potential applications discussed and evaluated. Students \nwill also be introduced to practical experimentation in natural language processing. \n \nLectures \nOverview. Brief history of NLP research, some current applications, components of NLP \nsystems. \nMorphology and Finite State Techniques. Morphology in different languages, importance of \nmorphological analysis in NLP, finite-state techniques in NLP. \nPart-of-Speech Tagging and Log-Linear Models. Lexical categories, word tagging, corpora and \nannotations, empirical evaluation. \nPhrase Structure and Structure Prediction. Phrase structures, structured prediction, context-free \ngrammars, weights and probabilities. Some limitations of context-free grammars. \nDependency Parsing. Dependency structure, grammar-free parsing, incremental processing.  \nGradient Descent and Neural Nets. Parameter optimisation by gradient descent. Non-linear \nfunctions with neural network layers. Log-linear model as softmax layer. Current findings of \nNeural NLP. \nWord representations. Representing words with vectors, count-based and prediction-based \napproaches, similarity metrics. \nRecurrent Neural Networks. Modelling sequences, parameter sharing in recurrent neural \nnetworks, neural language models, word prediction. \nCompositional Semantics. Logical representations, compositional semantics, lambda calculus, \ninference and robust entailment. \nLexical Semantics. Semantic relations, WordNet, word senses. \nDiscourse. Discourse relations, anaphora resolution, summarization. \nNatural Language Generation. Challenges of natural language generation (NLG), tasks in NLG, \nsurface realisation. \nPractical and assignments. Students will build a natural language processing system which will \nbe trained and evaluated on supplied data. The system will be built from existing components, \nbut students will be expected to compare approaches and some programming will be required \nfor this. Several assignments will be set during the practicals for assessment. \nObjectives \nBy the end of the course students should: \n \nbe able to discuss the current and likely future performance of several NLP applications; \nbe able to describe briefly a fundamental technique for processing language for several \nsubtasks, such as morphological processing, parsing, word sense disambiguation etc.; \nunderstand how these techniques draw on and relate to other areas of computer science. \nRecommended reading \n\nJurafsky, D. & Martin, J. (2023). Speech and language processing. Prentice Hall (3rd ed. draft, \nonline). \n \nAssessment - Part II Students \nAssignment 1 - 10% of marks \nAssignment 2 - 60% of marks \nAssignment 3 - 30% of marks\n \n\nPRACTICAL RESEARCH IN HUMAN-CENTRED AI \n \nAims \nThis is an advanced course in human-computer interaction, with a specialist focus on intelligent \nuser interfaces and interaction with machine-learning and artificial intelligence technologies. The \nformat will be largely Practical, with students carrying out a mini-project involving empirical \nresearch investigation. These studies will investigate human interaction with some kind of \nmodel-based system for planning, decision-making, automation etc. Possible study formats \nmight include: System evaluation, Field observation, Hypothesis testing experiment, Design \nintervention or Corpus analysis, following set examples from recent research publications. \nProject work will be formally evaluated through a report and presentation. \n \nLectures \n(note that Lectures 2-7 also include one hour class discussion of practical work) \n        \u2022 Current research themes in intelligent user interfaces \n        \u2022 Program synthesis \n        \u2022 Mixed initiative interaction \n        \u2022 Interpretability / explainable AI \n        \u2022 Labelling as a fundamental problem \n        \u2022 Machine learning risks and bias \n        \u2022 Visualisation and visual analytics \n        \u2022 Student research presentations \n \nObjectives \nBy the end of the course students should: \n        \u2022 be familiar with current state of the art in intelligent interactive systems \n        \u2022 understand the human factors that are most critical in the design of such systems \n        \u2022 be able to evaluate evidence for and against the utility of novel systems \n        \u2022 have experience of conducting user studies meeting the quality criteria of this field \n        \u2022 be able to write up and present user research in a professional manner \n \nClass Size \nThis module can accommodate upto 20 Part II, Part III and MPhil students. \n \nRecommended reading \nBrad A. Myers and Richard McDaniel (2000). Demonstrational Interfaces: Sometimes You Need \na Little Intelligence, Sometimes You Need a Lot. \n \nAlan Blackwell (2024). Moral Codes: Designing alternatives to AI \n \nAssessment - Part II Students \nThe format will be largely practical, with students carrying out an individual mini-project involving \nempirical research investigation. \n \n\nAssignment 1: six incremental submissions which together contribute 20% to the final module \nmark. \n \nAssignment 2: Final report - 80% of the final module mark \n \n\n",
        "6th Semester \nADVANCED COMPUTER ARCHITECTURE \nAims \nThis course examines the techniques and underlying principles that are used to design \nhigh-performance computers and processors. Particular emphasis is placed on understanding \nthe trade-offs involved when making design decisions at the architectural level. A range of \nprocessor architectures are explored and contrasted. In each case we examine their merits and \nlimitations and how ultimately the ability to scale performance is restricted. \n \nLectures \nIntroduction. The impact of technology scaling and market trends. \nFundamentals of Computer Design. Amdahl\u2019s law, energy/performance trade-offs, ISA design. \nAdvanced pipelining. Pipeline hazards; exceptions; optimal pipeline depth; branch prediction; \nthe branch target buffer [2 lectures] \nSuperscalar techniques. Instruction-Level Parallelism (ILP); superscalar processor architecture \n[2 lectures] \nSoftware approaches to exploiting ILP. VLIW architectures; local and global instruction \nscheduling techniques; predicated instructions and support for speculative compiler \noptimisations. \nMultithreaded processors. Coarse-grained, fine-grained, simultaneous multithreading \nThe memory hierarchy. Caches; programming for caches; prefetching [2 lectures] \nVector processors. Vector machines; short vector/SIMD instruction set extensions; stream \nprocessing \nChip multiprocessors. The communication model; memory consistency models; false sharing; \nmultiprocessor memory hierarchies; cache coherence protocols; synchronization [2 lectures] \nOn-chip interconnection networks. Bus-based interconnects; on-chip packet switched networks \nSpecial-purpose architectures. Converging approaches to computer design \nObjectives \nAt the end of the course students should \n \nunderstand what determines processor design goals; \nappreciate what constrains the design process and how architectural trade-offs are made within \nthese constraints; \nbe able to describe the architecture and operation of pipelined and superscalar processors, \nincluding techniques such as branch prediction, register renaming and out-of-order execution; \nhave an understanding of vector, multithreaded and multi-core processor architectures; \nfor the architectures discussed, understand what ultimately limits their performance and \napplication domain. \nRecommended reading \n* Hennessy, J. and Patterson, D. (2012). Computer architecture: a quantitative approach. \nElsevier (5th ed.) ISBN 9780123838728. (the 3rd and 4th editions are also good)\n \n\nCRYPTOGRAPHY \n \nAims \nThis course provides an overview of basic modern cryptographic techniques and covers \nessential concepts that users of cryptographic standards need to understand to achieve their \nintended security goals. \n \nLectures \nCryptography. Overview, private vs. public-key ciphers, MACs vs. signatures, certificates, \ncapabilities of adversary, Kerckhoffs\u2019 principle. \nClassic ciphers. Attacks on substitution and transposition ciphers, Vigen\u00e9re. Perfect secrecy: \none-time pads. \nPrivate-key encryption. Stream ciphers, pseudo-random generators, attacking \nlinear-congruential RNGs and LFSRs. Semantic security definitions, oracle queries, advantage, \ncomputational security, concrete-security proofs. \nBlock ciphers. Pseudo-random functions and permutations. Birthday problem, random \nmappings. Feistel/Luby-Rackoff structure, DES, TDES, AES. \nChosen-plaintext attack security. Security with multiple encryptions, randomized encryption. \nModes of operation: ECB, CBC, OFB, CNT. \nMessage authenticity. Malleability, MACs, existential unforgeability, CBC-MAC, ECBC-MAC, \nCMAC, birthday attacks, Carter-Wegman one-time MAC. \nAuthenticated encryption. Chosen-ciphertext attack security, ciphertext integrity, \nencrypt-and-authenticate, authenticate-then-encrypt, encrypt-then-authenticate, padding oracle \nexample, GCM. \nSecure hash functions. One-way functions, collision resistance, padding, Merkle-Damg\u00e5rd \nconstruction, sponge function, duplex construct, entropy pool, SHA standards. \nApplications of secure hash functions. HMAC, stream authentication, Merkle tree, commitment \nprotocols, block chains, Bitcoin. \nKey distribution problem. Needham-Schroeder protocol, Kerberos, hardware-security modules, \npublic-key encryption schemes, CPA and CCA security for asymmetric encryption. \nNumber theory, finite groups and fields. Modular arithmetic, Euclid\u2019s algorithm, inversion, \ngroups, rings, fields, GF(2n), subgroup order, cyclic groups, Euler\u2019s theorem, Chinese \nremainder theorem, modular roots, quadratic residues, modular exponentiation, easy and \ndifficult problems. [2 lectures] \nDiscrete logarithm problem. Baby-step-giant-step algorithm, computational and decision \nDiffie-Hellman problem, DH key exchange, ElGamal encryption, hybrid cryptography, Schnorr \ngroups, elliptic-curve systems, key sizes. [2 lectures] \nTrapdoor permutations. Security definition, turning one into a public-key encryption scheme, \nRSA, attacks on \u201ctextbook\u201d RSA, RSA as a trapdoor permutation, optimal asymmetric \nencryption padding, common factor attacks. \nDigital signatures. One-time signatures, RSA signatures, Schnorr identification scheme, \nElGamal signatures, DSA, PS3 hack, certificates, PKI. \nObjectives \nBy the end of the course students should \n\n \nbe familiar with commonly used standardized cryptographic building blocks; \nbe able to match application requirements with concrete security definitions and identify their \nabsence in naive schemes; \nunderstand various adversarial capabilities and basic attack algorithms and how they affect key \nsizes; \nunderstand and compare the finite groups most commonly used with discrete-logarithm \nschemes; \nunderstand the basic number theory underlying the most common public-key schemes, and \nsome efficient implementation techniques. \nRecommended reading \nKatz, J., Lindell, Y. (2015). Introduction to modern cryptography. Chapman and Hall/CRC (2nd \ned.).\n \n\nE-COMMERCE \n \nAims \nThis course aims to give students an outline of the issues involved in setting up an e-commerce \nsite. \n \nLectures \nThe history of electronic commerce. Mail order; EDI; web-based businesses, credit card \nprocessing, PKI, identity and other hot topics. \nNetwork economics. Real and virtual networks, supply-side versus demand-side scale \neconomies, Metcalfe\u2019s law, the dominant firm model, the differentiated pricing model Data \nProtection Act, Distance Selling regulations, business models. \nWeb site design. Stock and price control; domain names, common mistakes, dynamic pages, \ntransition diagrams, content management systems, multiple targets. \nWeb site implementation. Merchant systems, system design and sizing, enterprise integration, \npayment mechanisms, CRM and help desks. Personalisation and internationalisation. \nThe law and electronic commerce. Contract and tort; copyright; binding actions; liabilities and \nremedies. Legislation: RIP; Data Protection; EU Directives on Distance Selling and Electronic \nSignatures. \nPutting it into practice. Search engine interaction, driving and analysing traffic; dynamic pricing \nmodels. Integration with traditional media. Logs and audit, data mining modelling the user. \ncollaborative filtering and affinity marketing brand value, building communities, typical behaviour. \nFinance. How business plans are put together. Funding Internet ventures; the recent hysteria; \nmaximising shareholder value. Future trends. \nUK and International Internet Regulation. Data Protection Act and US Privacy laws; HIPAA, \nSarbanes-Oxley, Security Breach Disclosure, RIP Act 2000, Electronic Communications Act \n2000, Patriot Act, Privacy Directives, data retention; specific issues: deep linking, Inlining, brand \nmisuse, phishing. \nObjectives \nAt the end of the course students should know how to apply their computer science skills to the \nconduct of e-commerce with some understanding of the legal, security, commercial, economic, \nmarketing and infrastructure issues involved. \n \nRecommended reading \nShapiro, C. and Varian, H. (1998). Information rules. Harvard Business School Press. \n \nAdditional reading: \n \nStandage, T. (1999). The Victorian Internet. Phoenix Press. Klemperer, P. (2004). Auctions: \ntheory and practice. Princeton Paperback ISBN 0-691-11925-2.\n \n\nMACHINE LEARNING AND BAYESIAN INFERENCE \nAims \nThe Part 1B course Artificial Intelligence introduced simple neural networks for supervised \nlearning, and logic-based methods for knowledge representation and reasoning. This course \nhas two aims. First, to provide a rigorous introduction to machine learning, moving beyond the \nsupervised case and ultimately presenting state-of-the-art methods. Second, to provide an \nintroduction to the wider area of probabilistic methods for representing and reasoning with \nknowledge. \n \nLectures \nIntroduction to learning and inference. Supervised, unsupervised, semi-supervised and \nreinforcement learning. Bayesian inference in general. What the naive Bayes method actually \ndoes. Review of backpropagation. Other kinds of learning and inference. [1 lecture] \nHow to classify optimally. Treating learning probabilistically. Bayesian decision theory and Bayes \noptimal classification. Likelihood functions and priors. Bayes theorem as applied to supervised \nlearning. The maximum likelihood and maximum a posteriori hypotheses. What does this teach \nus about the backpropagation algorithm? [2 lectures] \nLinear classifiers I. Supervised learning via error minimization. Iterative reweighted least \nsquares. The maximum margin classifier. [2 lectures] \nGaussian processes. Learning and inference for regression using Gaussian process models. [2 \nlectures] \nSupport vector machines (SVMs). The kernel trick. Problem formulation. Constrained \noptimization and the dual problem. SVM algorithm. [2 lectures] \nPractical issues. Hyperparameters. Measuring performance. Cross-validation. Experimental \nmethods. [1 lecture] \nLinear classifiers II. The Bayesian approach to neural networks. [1 lecture] \nUnsupervised learning I. The k-means algorithm. Clustering as a maximum likelihood problem. \n[1 lecture] \nUnsupervised learning II. The EM algorithm and its application to clustering. [1 lecture] \nBayesian networks I. Representing uncertain knowledge using Bayesian networks. Conditional \nindependence. Exact inference in Bayesian networks. [2 lectures] \nBayesian networks II. Markov random fields. Approximate inference. Markov chain Monte Carlo \nmethods. [1 lecture] \nObjectives \nAt the end of this course students should: \n \nUnderstand how learning and inference can be captured within a probabilistic framework, and \nknow how probability theory can be applied in practice as a means of handling uncertainty in AI \nsystems. \nUnderstand several algorithms for machine learning and apply those methods in practice with \nproper regard for good experimental practice. \nRecommended reading \nIf you are going to buy a single book for this course I recommend: \n \n\n* Bishop, C.M. (2006). Pattern recognition and machine learning. Springer. \n \nThe course text for Artificial Intelligence I: \n \nRussell, S. and Norvig, P. (2010). Artificial intelligence: a modern approach. Prentice Hall (3rd \ned.). \n \ncovers some relevant material but often in insufficient detail. Similarly: \n \nMitchell, T.M. (1997). Machine Learning. McGraw-Hill. \n \ngives a gentle introduction to some of the course material, but only an introduction. \n \nRecently a few new books have appeared that cover a lot of relevant ground well. For example: \n \nBarber, D. (2012). Bayesian Reasoning and Machine Learning. Cambridge University Press. \nFlach, P. (2012). Machine Learning: The Art and Science of Algorithms that Make Sense of \nData. Cambridge University Press. \nMurphy, K.P. (2012). Machine Learning: A Probabilistic Perspective. MIT Press.\n \n\nOPTIMISING COMPILERS \n \nAims \nThe aims of this course are to introduce the principles of program optimisation and related \nissues in decompilation. The course will cover optimisations of programs at the abstract syntax, \nflowgraph and target-code level. It will also examine how related techniques can be used in the \nprocess of decompilation. \n \nLectures \nIntroduction and motivation. Outline of an optimising compiler. Optimisation partitioned: analysis \nshows a property holds which enables a transformation. The flow graph; representation of \nprogramming concepts including argument and result passing. The phase-order problem. \nKinds of optimisation. Local optimisation: peephole optimisation, instruction scheduling. Global \noptimisation: common sub-expressions, code motion. Interprocedural optimisation. The call \ngraph. \nClassical dataflow analysis. Graph algorithms, live and avail sets. Register allocation by register \ncolouring. Common sub-expression elimination. Spilling to memory; treatment of \nCSE-introduced temporaries. Data flow anomalies. Static Single Assignment (SSA) form. \nHigher-level optimisations. Abstract interpretation, Strictness analysis. Constraint-based \nanalysis, Control flow analysis for lambda-calculus. Rule-based inference of program properties, \nTypes and effect systems. Points-to and alias analysis. \nTarget-dependent optimisations. Instruction selection. Instruction scheduling and its phase-order \nproblem. \nDecompilation. Legal/ethical issues. Some basic ideas, control flow and type reconstruction. \nObjectives \nAt the end of the course students should \n \nbe able to explain program analyses as dataflow equations on a flowgraph; \nknow various techniques for high-level optimisation of programs at the abstract syntax level; \nunderstand how code may be re-scheduled to improve execution speed; \nknow the basic ideas of decompilation. \nRecommended reading \n* Nielson, F., Nielson, H.R. and Hankin, C.L. (1999). Principles of program analysis. Springer. \nGood on part A and part B. \nAppel, A. (1997). Modern compiler implementation in Java/C/ML (3 editions). \nMuchnick, S. (1997). Advanced compiler design and implementation. Morgan Kaufmann. \nWilhelm, R. (1995). Compiler design. Addison-Wesley. \nAho, A.V., Sethi, R. and Ullman, J.D. (2007). Compilers: principles, techniques and tools. \nAddison-Wesley (2nd ed.).\n \n\nQUANTUM COMPUTING \n \nAims \nThe principal aim of the course is to introduce students to the basics of the quantum model of \ncomputation. The model will be used to study algorithms for searching, factorisation and \nquantum chemistry as well as other important topics in quantum information such as \ncryptography and super-dense coding. Issues in the complexity of computation will also be \nexplored. A second aim of the course is to introduce student to near-term quantum computing. \nTo this end, error-correction and adiabatic quantum computing are studied. \n \nLectures \nBits and qubits. Introduction to quantum states and measurements with motivating examples. \nComparison with discrete classical states. \nLinear algebra. Review of linear algebra: vector spaces, linear operators, Dirac notation, the \ntensor product. \nThe postulates of quantum mechanics. Postulates of quantum mechanics, incl. evolution and \nmeasurement. \nImportant concepts in quantum mechanics. Entanglement, distinguishing orthogonal and \nnon-orthogonal quantum states, no-cloning and no signalling. \nThe quantum circuit model. The circuit model of quantum computation. Quantum gates and \ncircuits. Universality of the quantum circuit model, and efficient simulation of arbitrary two-qubit \ngates with a standard universal set of gates. \nSome applications of quantum information. Applications of quantum information (other than \nquantum computation): quantum key distribution, superdense coding and quantum teleportation. \nDeutsch-Jozsa algorithm. Introducing Deutsch\u2019s problem and Deutsch\u2019s algorithm leading onto \nits generalisation, the Deutsch-Jozsa algorithm. \nQuantum search. Grover\u2019s search algorithm: analysis and lower bounds. \nQuantum Fourier Transform and Quantum Phase Estimation. Definition of the Quantum Fourier \nTransform (QFT), and efficient representation thereof as a quantum circuit. Application of the \nQFT to enable Quantum Phase Estimation (QPE). \nApplication 1 of QFT / QPE: Factoring. Shor\u2019s algorithm: reduction of factoring to period finding \nand then using the QFT for period finding. \nApplication 2 of QFT / QPE: Quantum Chemistry. Efficient simulation of quantum systems, and \napplications to real-world problems in quantum chemistry. \nQuantum complexity. Quantum complexity classes and their relationship to classical complexity. \nComparison with probabilistic computation. \nQuantum error correction. Introducing the concept of quantum error correction required for the \nfollowing lecture on fault-tolerance. \nFault tolerant quantum computing. Elements of fault tolerant computing; the threshold theorem \nfor efficient suppression of errors. \nAdiabatic quantum computing and quantum optimisation. The quantum adiabatic theorem, and \nadiabatic optimisation. Quantum annealing and D-Wave. \nCase studies in near-term quantum computation. Examples of state-of-the-art quantum \nalgorithms and computers, including superconducting and networked quantum computers. \n\nObjectives \nAt the end of the course students should: \n \nunderstand the quantum model of computation and the basic principles of quantum mechanics; \nbe familiar with basic quantum algorithms and their analysis; \nbe familiar with basic quantum protocols such as teleportation and superdense coding; \nsee how the quantum model relates to classical models of deterministic and probabilistic \ncomputation. \nappreciate the importance of efficient error-suppression if quantum computation is to yield an \nadvantage over classical computation. \ngain a general understanding of the important topics in near-term quantum computing, including \nadiabatic quantum computing. \nRecommended reading \nBooks: \nNielsen M.A., Chuang I.L. (2010). Quantum Computation and Quantum Information. Cambridge \nUniversity Press. \nMermin N.D. (2007). Quantum Computer Science: An Introduction. Cambridge University Press. \nMcGeoch, C. (2014). Adiabatic Quantum Computation and Quantum Annealing Theory and \nPractice. Morgan and Claypool. https://ieeexplore.ieee.org/document/7055969 \n \nPapers: \n \nBraunstein S.L. (2003). Quantum computation tutorial. Available at: \nhttps://www-users.cs.york.ac.uk/~schmuel/comp/comp_best.pdf \nAharonov D., Quantum computation [arXiv:quant-ph/9812037] \nAlbash T., Adiabatic Quantum Computing https://arxiv.org/pdf/1611.04471.pdf \nMcCardle S. et al, Quantum computational chemistry https://arxiv.org/abs/1808.10402 \n \nOther lecture notes: \n \nAndrew Childs (University of Maryland): http://cs.umd.edu/~amchilds/qa/ \n \n\nRANDOMISED ALGORITHMS \n \nAims: \nThe aim of this course is to introduce advanced techniques in the design and analysis \nalgorithms, with a strong focus on randomised algorithms. It covers essential tools and ideas \nfrom probability, optimisation and graph theory, and develops this knowledge through a variety \nof examples of algorithms and processes. This course will demonstrate that randomness is not \nonly an important design technique which often leads to simpler and more elegant algorithms, \nbut may also be essential in settings where time and space are restricted.   \n \nLectures: \nIntroduction. Course Overview. Why Randomised Algorithms? A first Randomised Algorithm for \nthe MAX-CUT problem. [1 Lecture]  \n \nConcentration Inequalities. Moment-Generating Functions and Chernoff Bounds. Extension: \nMethod of Bounded Independent Differences. Applications: Balls-into-Bins, Quick-Sort and Load \nBalancing. [approx. 2 Lectures] \n \nMarkov Chains and Mixing Times. Random Walks on Graphs. Application: Randomised \nAlgorithm for the 2-SAT problem. Mixing Times of Markov Chains. Application: Load Balancing \non Networks. [approx. 2 Lectures] \n \nLinear Programming and Applications. Definitions and Applications. Formulating Linear \nPrograms. The Simplex Algorithm. Finding Initial Solutions. How to use Linear Programs and \nBranch & Bound to Solve a Classical TSP instance. [approx. 3 Lectures] \n \nRandomised Approximation Algorithms. Randomised Approximation Schemes. Linearity of \nExpectations, Derandomisation. Deterministic and Randomised Rounding of Linear Programs. \nApplications: MAX3-SAT problem, Weighted Vertex Cover, Weighted Set Cover, MAX-SAT. \n[approx. 2 Lectures] \n \nSpectral Graph Theory and Clustering. Eigenvalues of Graphs and Matrices: Relations between \nEigenvalues and Graph Properties, Spectral Graph Drawing. Spectral Clustering: Conductance, \nCheeger's Inequality. Spectral Partitioning Algorithm [approx. 2 Lectures] \n \nObjectives: \nBy the end of the course students should be able to: \n \nlearn how to use randomness in the design of algorithms, in particular, approximation \nalgorithms; \nlearn the basics of linear programming, integer programming and randomised rounding; \napply randomisation to various problems coming from optimisation, machine learning and data \nscience; \nuse results from probability theory to analyse the performance of randomised algorithms. \n\nRecommended reading \n* Michael Mitzenmacher and Eli Upfal. Probability and Computing: Randomized Algorithms and \nProbabilistic Analysis., Cambridge University Press, 2nd edition. \n \n* David P. Williamson and David B. Shmoys. The Design of Approximation Algorithms, \nCambridge University Press, 2011 \n \n* Cormen, T.H., Leiserson, C.D., Rivest, R.L. and Stein, C. (2009). Introduction to Algorithms. \nMIT Press (3rd ed.). ISBN 978-0-262-53305-8 \n \n \n\nCLOUD COMPUTING \nAims \nThis module aims to teach students the fundamentals of Cloud Computing covering topics such \nas virtualization, data centres, cloud resource management, cloud storage and popular cloud \napplications including batch and data stream processing. Emphasis is given on the different \nbackend technologies to build and run efficient clouds and the way clouds are used by \napplications to realise computing on demand. The course will include practical tutorials on cloud \ninfrastructure technologies. Students will be assessed via a Cloud-based coursework project. \n \nLectures \nIntroduction to Cloud Computing \nData centres \nVirtualization I \nVirtualization II \nMapReduce \nMapReduce advanced \nResource management for virtualized data centres \nCloud storage \nCloud-based data stream processing \nObjectives \nBy the end of the course students should: \n \nunderstand how modern clouds operate and provide computing on demand; \nunderstand about cloud availability, performance, scalability and cost; \nknow about cloud infrastructure technologies including virtualization, data centres, resource \nmanagement and storage; \nknow how popular applications such as batch and data stream processing run efficiently on \nclouds; \nAssessment \nAssignment 1, worth 55% of the final mark. Develop batch processing applications running on a \ncluster assessed through automatic testing, code inspection and a report. \nAssignment 2, worth 30% of the final mark: Design and develop a framework for running the \nbatch processing applications from Assignment 1 on a cluster according to certain resource \nallocation policies. This assigment will be assessed by an expert, testing, code inspection and a \nreport. \nAssignment 3, worth 15% of the final mark: Write two individual project reports describing your \nwork for assignments 1 and 2. Write two sets of scripts to manage and run your code. Reports \nand scripts will be submitted with each assignment. \nRecommended reading \nMarinescu, D.C. Cloud Computing, Theory and Practice. Morgan Kaufmann. \nBarham, P., et. al. (2003). \u201cXen and the Art of Virtualization\u201d. In Proceedings of SOSP 2003. \nCharkasova, L., Gupta, D. and Vahdat, A. (2007). \u201cComparison of the Three CPU Schedulers in \nXen\u201d. In SIGMETRICS 2007. \n\nDean, J. and Ghemawat, S. (2004). \u201cMapReduce: Simplified Data Processing on Large \nClusters\u201d. In Proceedings of OSDI 2004. \nZaharia, M, et al. (2008). \u201cImproving MapReduce Performance in Heterogeneous \nEnvironments\u201d. In Proceedings of OSDI 2008. \nHindman, A., et al. (2011). \u201cMesos: A Platform for Fine-Grained Resource Sharing in Data \nCenter\u201d. In Proceedings of NSDI 2011. \nSchwarzkopf, M., et al. (2013). \u201cOmega: Flexible, Scalable Schedulers for Large Compute \nClusters\u201d. In EuroSys 2013. \nGhemawat, S. (2003). \u201cThe Google File System\u201d. In Proceedings of SOSP 2003. \nChang, F. (2006). \u201cBigtable: A Distributed Storage System for Structured Data\u201d. In Proceedings \nof OSDI 2006. \nFernandez, R.C., et al. (2013). \u201cIntegrating Scale Out and Fault Tolerance in Stream Processing \nusing Operator State Management\u201d. In SIGMOD 2013.\n \n\nCOMPUTER SYSTEMS MODELLING \n \nAims \n \nThe aims of this course are to introduce the concepts and principles of mathematical modelling \n \nand simulation, with particular emphasis on using queuing theory and control theory for \nunderstanding the behaviour of computer and communications systems. \n \nSyllabus \n \n1.     Overview of computer systems modeling using both analytic techniques and simulation. \n \n2.     Introduction to discrete event simulation. \n \na.     Simulation techniques \n \nb.     Random number generation methods \n \nc.     Statistical aspects: confidence intervals, stopping criteria \n \nd.     Variance reduction techniques. \n \n3.     Stochastic processes (builds on starred material in Foundations of Data Science) \n \na.     Discrete and continuous stochastic processes. \n \nb.     Markov processes and Chapman-Kolmogorov equations. \n \nc.     Discrete time Markov chains. \n \nd.     Ergodicity and the stationary distribution. \n \ne.     Continuous time Markov chains. \n \nf.      Birth-death processes, flow balance equations. \n \ng.     The Poisson process. \n \n4.     Queueing theory \n \na.     The M/M/1 queue in detail. \n \n\nb.     The equilibrium distribution with conditions for existence and common performance \nmetrics. \n \nc.     Extensions of the M/M/1 queue: the M/M/k queue, the M/M/infinity queue. \n \nd.     Queueing networks. Jacksonian networks. \n \ne.     The M/G/1 queue. \n \n5.     Signals, systems, and transforms \n \na.     Discrete- and continuous-time convolution. \n \nb.     Signals. The complex exponential signal. \n \nc.     Linear Time-Invariant Systems. Modeling practical systems as an LTI system. \n \nd.     Fourier and Laplace transforms. \n \n6.     Control theory. \n \na.     Controlled systems. Modeling controlled systems. \n \nb.     State variables. The transfer function model. \n \nc.     First-order and second-order systems. \n \nd.     Feedback control. PID control. \n \ne.     Stability. BIBO stability. Lyapunov stability. \n \nf.      Introduction to Model Predictive Control. \n \nObjectives \n \nAt the end of the course students should \n \n\u00b7       Be aware of different approaches to modeling a computer system; their pros and cons \n \n\u00b7       Be aware of the issues in building a simulation of a computer system and analysing the \nresults obtained \n \n\u00b7       Understand the concept of a stochastic process and how they arise in practice \n \n\n\u00b7       Be able to build simple Markov models and understand the critical modelling assumptions \n \n\u00b7       Be able to solve simple birth-death processes \n \n\u00b7       Understand and use M/M/1 queues to model computer systems \n \n\u00b7       Be able to model a computer system as a linear time-invariant system \n \n\u00b7       Understand the dynamics of a second-order controlled system \n \n\u00b7       Design a PID control for an LTI system \n \n\u00b7       Understand what is meant by BIBO and Lyapunov stability \n \nAssessment \n \nThere will be one programming assignment to build a discrete event simulator and using it to \nsimulate an M/M/1 and an M/D/1 queue (worth 15% of the final marks). There will also be two \none-hour in-person assessments for the course on: Stochastic processes and Queueing theory \n(40%) and Signals, Systems, Transforms, and Control Theory (45%). \n \nReference books \n \nKeshav, S. (2012)*. Mathematical Foundations of Computer Networking. Addison-Wesley. A \ncustomized PDF will be made available to class participants. \n \nKleinrock, L. (1975). Queueing systems, vol. 1. Theory. Wiley. \n \nKraniauskas, Peter. Transforms in signals and systems. Addison-Wesley Longman, 1992. \n \nJain, R. (1991). The art of computer systems performance analysis. Wiley. \n \n \n\nCOMPUTING EDUCATION \n \nAims \nThis module considers various aspects of the teaching and learning of computer science in \nschool, including practical experience, and provides an introduction to the field of computing \neducation research. It is offered by the Raspberry Pi Computing Education Research Centre, \npart of the Department of Computer Science and Technology, in conjunction with local schools \nin the Cambridge area. Students will have the opportunity to observe, plan and deliver a lesson, \nand explore the teaching of computing in a secondary education setting. The lesson taught will \nbe on a topic within the computing/computer science curriculum in England but will be \nnegotiated with the teacher mentoring the school placement. In the sessions in the Department, \nstudents will be introduced to elements of pedagogy and assessment alongside current issues \nin computing education. The course can also serve as a useful introduction to research in \ncomputing education for those interested in this area. Assessment will include lesson planning, \npresentations, an in-person viva and a 3000-word report which will include evidence of \nengagement with relevant research underpinning the lessons and activities undertaken. An \ninformation session relating to the module will be available in the preceding Easter Term. The \narrangement of school placements and DBS checks will be carried out in Michaelmas Term, so \nstudents should be prepared to engage in these preparatory activities. \n \nLectures \nThis is not a traditional lecture course and the term will be structured as follows: \n \nWeek 1: Introduction to computing education & visit to school placement \nWeek 2: In school (observation, supporting teacher) \nWeek 3: In school (observation, supporting teacher) \nWeek 4: In school (co-teach / small group teaching) \nWeek 5: Computing pedagogy and assessment (school half-term) \nWeek 6: In school (co-teach / small group teaching) \nWeek 7: In school (teach part/whole lesson) \nWeek 8: Presentations \nObjectives \nBy the end of the course students should: \n \ngain experience of computing education in practice \ndemonstrate an ability to communicate an aspect of computer science clearly and effectively \nbe able to plan and design a teaching activity/lesson with consideration of the audience \nbe able to thoroughly evaluate the design and implementation of a teaching/activity lesson \nbe able to demonstrate an understanding of relevant theories of learning and pedagogy from the \nliterature and how they apply to the learning/teaching of the topic under consideration \ngain an understanding of the breadth and depth of the field of computing education research \nClass Size \nThis module can accommodate up to 10 Part II students. \n \n\nRecommended reading \nBrown, N. C., Sentance, S., Crick, T., & Humphreys, S. (2014). Restart: The resurgence of \ncomputer science in UK schools. ACM Transactions on Computing Education (TOCE), 14(2), \n1-22. https://doi.org/10.1145/2602484 \n \nSentance, S., Barendsen, E., Howard, N. R., & Schulte, C. (Eds.). (2023). Computer science \neducation: Perspectives on teaching and learning in school. Bloomsbury Publishing. \n \nGrover, S. (Ed). 2020. Computer Science in K-12: An A-to-Z Handbook on Teaching \nProgramming. Edfinity. https://www.shuchigrover.com/atozk12cs/ \n \nRaspberry Pi Foundation (2022). Hello World Big Book of Pedagogy. \nhttps://www.raspberrypi.org/hello-world/issues/the-big-book-of-computing-pedagogy \n \nAssessment \nWeek 2: Reflections on an observed lesson (10%, graded out of 20) \nWeek 4: Submission of a lesson plan outline (10% graded out of 20) \nWeek 8: Delivery of an oral presentation relating to the lesson delivery (10% graded out of 20) \nAfter course completion: 3000 word report (60%), graded out of 20 \nViva with assessor (10%) (pass/fail) \nFurther Information \nWe will find placements for students in secondary schools within cycling or bus distance from \nthe Department/centre of Cambridge. There will be a timetabled slot for the course that students \ncan use to go to their school placement, but students may need to be flexible and liaise with the \nschool to find the best time for working with a specific class. \n \n \n\nDEEP NEURAL NETWORKS \n \nObjectives \nYou will gain detailed knowledge of \n \nCurrent understanding of generalization in neural networks vs classical statistical models. \nOptimization procedures for neural network models such as stochastic gradient descent and \nADAM. \nAutomatic differentiation and at least one software framework (PyTorch, TensorFolow) as well as \nan overview of other software approaches. \nArchitectures that are deployed to deal with different data types such as images or sequences \nincluding a. convolutional networks b. recurrent networks and c. transformers. \nIn addition you will gain knowledge of more advanced topics reflecting recent research in \nmachine learning. These change from year to year, examples include: \n \nApproaches to unsupervised learning including autoencoders and self-supervised learning.. \nTechniques for deploying models in low data regimes such as transfer learning and \nmeta-learning. \nTechniques for propagating uncertainty such as Bayesian neural networks. \nReinforcement learning and its applications to robotics or games. \nGraph Neural Networks and Geometric Deep Learning. \nTeaching Style \nThe start of the course will focus on the latest undertanding of current theory of neural networks, \ncontrasting with previous classical understandings of generalization performance. Then we will \nmove to practical examples of network architectures and deployment. We will end with more \nadvanced topics reflecting current research, mostly delivered by guest lecturers from the \nDepartment and beyond. \n \nSchedule \nWeek 1 \nTwo lectures: Generalization and Approximation with Neural Networks \n \nWeek 2 \nTwo lectures: Optimization: Stochastic Gradient Descent and ADAM \n \nWeek 3 \nTwo lectures: Hardware Acceleration and Convolutional Neural Networks \n \nWeek 4 \nTwo lectures: Vanishing Gradients, Recurrent and Residual Networks, Sequence Modeling. \n \nWeek 5 \nTwo lectures: Attention, The Transformer Architecture, Large Language Models \n \n\nWeek 6-8 \nLectures and guest lectures from the special topics below. \n \nSpecial topics \nSelf-Supervised Learning, AutoEncoders, Generative Modeling of Images \nHardware Implementations \nReinforcement learning \nTransfer learning and meta-learning \nUncertainty and Bayesian Neural Networks \nGraph Neural Networks \nAssessment \nAssessment involves solving a number of exercises in a jupyter notebook environment. \nFamiliarity with the python programming language is assumed. The exercises rely on the \npytorch deep learning framework, the syntax of which we don\u2019t cover in detail in the lectures, \nstudents are referred to documentation and tutorials to learn this component of the assessment. \n \nAssignment 1 \n \n30% of the total marks, with guided exercises. \n \nAssignment 2 \n \n70% of the total marks, a combination of guided exercises and a more exploratory mini-project.\n\nEXTENDED REALITY \n \nSyllabus & Schedule \nWeek 1 \nIntroduction \nCourse schedule and concept \nThe history of AR/VR/XR \nApplications of AR/VR/XR \nOverview of the XR pipeline \u2013 3 main components of XR \nDisplay technologies and rendering \nMachine perception and input interfaces \nContent generation \u2013 geometry, appearance, motion \nOverview of the AR/VR/XR frameworks \nPractical 1 \nProject structure \nProject themes \nIntroduction to the XR programming platform \nWeek 2 \n3D scene representations and rendering \nObject representations with polygonal meshes \nEfficient rendering via rasterization \nPractical XR rendering of textured geometries \nTracking and pose estimation for XR \nRigid transforms \nCamera pose estimation \nBody/ hand tracking \nPractical 2 \nProject draft proposal due \nProject feedback \nRendering and displaying a first object \nWeek 3 \n3D scene capture and understanding \nDepth estimation \n3D geometry capture \nAppearance acquisition \nLighting estimation and relighting \nLighting models for 3D scenes \nEstimating lighting in a scene \nRelighting rendered models \nPractical 3 \nAcquiring and utilizing depth from sensors \nRelighting a displayed object \nWeek 4 \nPhysically-based simulation of rigid objects \n\nRigid-body dynamics \nCollision detection \nTime integration \nPhysically-based simulation of non-rigid volumetric phenomena \nParticle systems \nBasic fluid dynamics \nPractical 4 \nPractical due \nProject prototype due \nProject check-in \nSimulating and rendering a simple object \nWeek 5 \nAdvanced topics in XR I: AR/VR displays technologies \nMicro LED-based displays \nWaveguide-based displays \nNext generation displays \nWeek 6 \nAdvanced topics in XR II \u2013 Mobile AR \nComputer vision on limited hardware \nUX/UI challenges \nContent challenges \nProject due \n  \n \nAssessment \nA practical exercise, worth 20% of the mark. This will cover the basics of AR/VR development \nand some of the practical techniques the students will use in their projects. This is individual \nwork. No mobile device or GPU hours are needed. \nAn AR/VR project worth 80% of the marks: \nCourse projects will be designed by the students following the possible project themes proposed \nby the lecturer and will be checked by the lecturer for appropriateness. \nThe students will form groups of 2-3 to design, implement, report, and present the course \nproject. \nThe final mark will be an implementation mark (40%), a report mark (20%), and a \npresentation/demo mark (20%). Each team member will be evaluated based on their \ncontribution. \nEach project will have extensions to be completed only by the ACS students. Each student will \nwrite a different part of the report, whose author will be clearly marked. Each student will further \nsummarise her/his contributions to the project in the same report. \nNote: Submission is by one submission per group but work by each individual must be clearly \nlabelled.\n \n\nFEDERATED LEARNING: THEORY AND PRACTICE \n \nObjectives \nThis course aims to extend the machine learning knowledge available to students in Part I (or \npresent in typical undergraduate degrees in other universities), and allow them to understand \nhow these concepts can manifest in a decentralized setting. The course will consider both \ntheoretical (e.g., decentralized optimization) and practical (e.g., networking efficiency) aspects \nthat combine to define this growing area of machine learning.  \n \nAt the end of the course students should: \n \nUnderstand popular methods used in federated learning \nBe able to construct and scale a simple federated system \nHave gained an appreciation of the core limitations to existing methods, and the approaches \navailable to cope with these issues \nDeveloped an intuition for related technologies like differential privacy and secure aggregation, \nand are able to use them within typical federated settings  \nCan reason about the privacy and security issues with federated systems \nLectures \nCourse Overview. Introduction to Federated Learning. \nDecentralized Optimization. \nStatistical and Systems Heterogeneity. \nVariations of Federated Aggregation. \nSecure Aggregation. \nDifferential Privacy within Federated Systems. \nExtensions to Federated Analytics. \nApplications to Speech, Video, Images and Robotics. \nLab sessions \nFederating a Centralized ML Classifier. \nBehaviour under Heterogeneity. \nScaling a Federated Implementation. \nExploring Privacy with Federated Settings \nRecommended Reading \nReadings will be assigned for each lecture. Readings will be taken from either research papers, \ntutorials, source code or blogs that provide more comprehensive treatment of taught concepts. \n \nAssessment - Part II Students \nFour labs are performed during the course, and students receive 10% of their total grade for \nwork done as part of each lab. (For a total of 40% of the total grade from lab work alone). Labs \nwill primarily provide hands-on teaching opportunities, that are then utilized within the lab \nassignment which is completed outside of the lab contact time. MPhil and Part III students will \nbe given additional questions to answer within their version of the lab assignment which will \ndiffer from the assignment given to Part II CST students. \n \n\nThe remainder of the course grade (60%) will be given based on a hands-on project that applies \nthe concepts taught in lectures and labs. This hands-on project will be assessed based on upon \na combination of source code, related documentation and brief 8-minute pre-recorded talk that \nsummarizes key project elements (any slides used are also submitted as part of the project). \nPlease note, that in the case of Part II CST students, the talk itself is not examinable -- as such \nwill be made optional to those students. \n \nA range of possible practical projects will be described and offered to students to select from, or \nalternatively students may propose their own. MPhil and Part III students will select from a \nproject pool that is separate from those offered to Part II CST students. MPhil and Part III \nprojects will contain a greater emphasis on a research element, and the pre-recorded talks \nprovided by this student group will focus on this research contribution. The project will be \nassessed on the level of student understanding demonstrated, the degree of difficulty, \ncorrectness of implementation -- and for Part III/MPhil students the additional criteria of the \nquality and execution of the research methodology, and depth and quality of results analysis. \n \nThis project can be done individually or in groups -- although individual projects will be strongly \nencouraged. It will be required the project is performed using a code repository that also will \ncontain all documentation -- access to this repository will be shared with course staff (e.g., \nlecturer and TAs). Where needed, marks assigned to students within a group will be \ndifferentiated using this repository as an input. Furthermore if groups are formed, members must \nbe either entirely from Part III/MPhil students or Part II CST, i.e., these two student groups \nshould not mix to form a project group. \n \nProjects will be made available publicly. A maximum word count for written contributions for the \nproject will be enforced.\n \n\nMOBILE HEALTH \n \nAims \nThe course aims to explore how wearable and mobile systems sensors can be used to gather \ndata relevant to understand health, how the data can be analysed with advanced signal \nprocessing and machine learning and the performance of these systems in terms of diagnostics \nand disease progression detection. \n \nSyllabus \nCourse Overview. Introduction to Mobile Health. Evaluation metrics and methodology. Basics of \nSignal Processing. \nInertial Measurement Units, Human Activity Recognition (HAR) and Gait Analysis and Machine \nLearning for IMU data. \nRadios, Bluetooth, GPS and Cellular. Epidemiology and contact tracing, Social interaction \nsensing and applications. Location tracking in health monitoring and public health. \nAudio Signal Processing. Voice and Speech Analysis: concepts and data analysis. Body \nSounds analysis. \nPhotoplethysmogram and Light sensing for health (heart and sleep) \nContactless and wireless behaviour and physiological monitoring \nGenerative Models for Wearable Health Data \nTopical Guest Lectures \nObjectives \nThe course aims to explore how wearable and mobile systems sensors can be used to gather \ndata relevant to understand health, how the data can be analysed with advanced signal \nprocessing and machine learning and the performance of these systems in terms of diagnostics \nand disease progression detection. \n \nRoughly, each lecture contains a theory part about the working of \u201csensor signals\u201d or \u201cdata \nanalysis methods\u201d and an application part which contextualises the concepts. \n \nAt the end of the course students should: Understand how mobile/wearable sensors capture \ndata and their working. Understand different approaches to acquiring and analysing sensor data \nfrom different types of sensors. Understand the concept of signal processing applied to time \nseries data and their practical application in health. Be able to extract sensor data and analyse it \nwith basic signal processing and machine learning techniques. Be aware of the different health \napplications of the various sensor techniques. The course will also touch on privacy and ethics \nimplications of the approaches developed in an orthogonal fashion. \n \nRecommended Reading \nPlease see Course Materials for recommended reading for each session. \n \nAssessment - Part II students \nTwo assignments will be based on two datasets which will be provided to the students: \n \n\nAssignment 1 (shared for Part II and Part III/MPhil): this will be based on a dataset and will be \nworth 40% of the final mark. The task of the assessment will be to perform pre-processing and \nbasic data analysis in a \"colab\" and an answer sheet of no more than 1000 words. \n \nAssignment 2 (Part II): This assignment (worth 60% of the final mark) will be a fuller analysis of \na dataset focusing on machine learning algorithms and metrics. Discussion and interpretation of \nthe findings will be reported in a colab and a report of no more than 1200 words.\n \n\nMULTICORE SEMANTICS AND PROGRAMMING \n \nAims \nIn recent years multiprocessors have become ubiquitous, but building reliable concurrent \nsystems with good performance remains very challenging. The aim of this module is to \nintroduce some of the theory and the practice of concurrent programming, from hardware \nmemory models and the design of high-level programming languages to the correctness and \nperformance properties of concurrent algorithms. \n \nLectures \nPart 1: Introduction and relaxed-memory concurrency [Professor P. Sewell] \n \nIntroduction. Sequential consistency, atomicity, basic concurrent problems. [1 block] \nConcurrency on real multiprocessors: the relaxed memory model(s) for x86, ARM, and IBM \nPower, and theoretical tools for reasoning about x86-TSO programs. [2 blocks] \nHigh-level languages. An introduction to C/C++11 and Java shared-memory concurrency. [1 \nblock] \nPart 2: Concurrent algorithms [Dr T. Harris] \n \nConcurrent programming. Simple algorithms (readers/writers, stacks, queues) and correctness \ncriteria (linearisability and progress properties). Advanced synchronisation patterns (e.g. some \nof the following: optimistic and lazy list algorithms, hash tables, double-checked locking, RCU, \nhazard pointers), with discussion of performance and on the interaction between algorithm \ndesign and the underlying relaxed memory models. [3 blocks] \nResearch topics, likely to include one hour on transactional memory and one guest lecture. [1 \nblock] \nObjectives \nBy the end of the course students should: \n \nhave a good understanding of the semantics of concurrent programs, both at the multprocessor \nlevel and the C/Java programming language level; \nhave a good understanding of some key concurrent algorithms, with practical experience. \nAssessment - Part II Students \nTwo assignments each worth 50% \n \nRecommended reading \nHerlihy, M. and Shavit, N. (2008). The art of multiprocessor programming. Morgan Kaufmann.\n\nBUSINESS STUDIES SEMINARS \nAims \nThis course is a series of seminars by former members and friends of the Laboratory about their \nreal-world experiences of starting and running high technology companies. It is a follow on to \nthe Business Studies course in the Michaelmas Term. It provides practical examples and case \nstudies, and the opportunity to network with and learn from actual entrepreneurs. \n \nLectures \nEight lectures by eight different entrepreneurs. \n \nObjectives \nAt the end of the course students should have a better knowledge of the pleasures and pitfalls \nof starting a high tech company. \n \nRecommended reading \nLang, J. (2001). The high-tech entrepreneur\u2019s handbook: how to start and run a high-tech \ncompany. FT.COM/Prentice Hall. \nMaurya, A. (2012). Running Lean: Iterate from Plan A to a Plan That Works. O\u2019Reilly. \nOsterwalder, A. and Pigneur, Y. (2010). Business Model Generation: A Handbook for \nVisionaires, Game Changers, and Challengers. Wiley. \nKim, W. and Mauborgne, R. (2005). Blue Ocean Strategy. Harvard Business School Press. \n \nSee also the additional reading list on the Business Studies web page.\n \n\nHOARE LOGIC AND MODEL CHECKING \n \nAims \nThe course introduces two verification methods, Hoare Logic and Temporal Logic, and uses \nthem to formally specify and verify imperative programs and systems. \n \nThe first aim is to introduce Hoare logic for a simple imperative language and then to show how \nit can be used to formally specify programs (along with discussion of soundness and \ncompleteness), and also how to use it in a mechanised program verifier. \n \nThe second aim is to introduce model checking: to show how temporal models can be used to \nrepresent systems, how temporal logic can describe the behaviour of systems, and finally to \nintroduce model-checking algorithms to determine whether properties hold, and to find \ncounter-examples. \n \nCurrent research trends also will be outlined. \n \nLectures \nPart 1: Hoare logic. Formal versus informal methods. Specification using preconditions and \npostconditions. \nAxioms and rules of inference. Hoare logic for a simple language with assignments, sequences, \nconditionals and while-loops. Syntax-directedness. \nLoops and invariants. Various examples illustrating loop invariants and how they can be found. \nPartial and total correctness. Hoare logic for proving termination. Variants. \nSemantics and metatheory Mathematical interpretation of Hoare logic. Semantics and \nsoundness of Hoare logic. \nSeparation logic Separation logic as a resource-aware reinterpretation of Hoare logic to deal \nwith aliasing in programs with pointers. \nPart 2: Model checking. Models. Representation of state spaces. Reachable states. \nTemporal logic. Linear and branching time. Temporal operators. Path quantifiers. CTL, LTL, and \nCTL*. \nModel checking. Simple algorithms for verifying that temporal properties hold. \nApplications and more recent developments Simple software and hardware examples. CEGAR \n(counter-example guided abstraction refinement). \nObjectives \nAt the end of the course students should \n \nbe able to prove simple programs correct by hand and implement a simple program verifier; \nbe familiar with the theory and use of separation logic; \nbe able to write simple models and specify them using temporal logic; \nbe familiar with the core ideas of model checking, and be able to implement a simple model \nchecker. \nRecommended reading \n\nHuth, M. and Ryan M. (2004). Logic in Computer Science: Modelling and Reasoning about \nSystems. Cambridge University Press (2nd ed.).\n \n\n",
        "7th Semester \nADVANCED TOPICS IN COMPUTER ARCHITECTURE \nAims \nThis course aims to provide students with an introduction to a range of advanced topics in \ncomputer architecture. It will explore the current and future challenges facing the architects of \nmodern computers. These will also be used to illustrate the many different influences and \ntrade-offs involved in computer architecture. \n \nObjectives \nOn completion of this module students should: \n \nunderstand the challenges of designing and verifying modern microprocessors \nbe familiar with recent research themes and emerging challenges \nappreciate the complex trade-offs at the heart of computer architecture \nSyllabus \nEach seminar will focus on a different topic. The proposed topics are listed below but there may \nbe some minor changes to this: \n \nTrends in computer architecture \nState-of-the-art microprocessor design \nMemory system design \nHardware reliability \nSpecification, verification and test (may be be replaced with a different topic) \nHardware security (2) \nHW accelerators and accelerators for machine learning \nEach two hour seminar will include three student presentations (15mins) questions (5mins) and \na broader discussion of the topics (around 30mins). The last part of the seminar will include a \nshort scene setting lecture (around 20mins) to introduce the following week's topic. \n \nAssessment \nEach week students will compare and contrast two of the main papers and submit a written \nsummary and review in advance of each seminar (except when presenting). \n \nStudents will be expected to give a number of 15 minute presentations. \n \nEssays and presentations will be marked out of 10. After dropping the lowest mark, the \nremaining marks will be scaled to give a final score out of 100. \n \nStudents will give at least one presentation during the course. They will not be required to \nsubmit an essay during the weeks they are presenting. \n \nEach presentation will focus on a single paper from the reading list. Marks will be awarded for \nclarity and the communication of the paper's key ideas, an analysis of the work's strengths and \nweaknesses and the work\u2019s relationship to related work and broader trends and constraints. \n\n \nRecommended prerequisite reading \nPatterson, D. A., Hennessy, J. L. (2017). Computer organization and design: The \nHardware/software interface RISC-V edition Morgan Kaufmann. ISBN 978-0-12-812275-4. \n \nHennessy, J. and Patterson, D. (2012). Computer architecture: a quantitative approach. Elsevier \n(5th ed.) ISBN 9780123838728. (the 3rd and 4th editions are also relevant)\n \n\nADVANCED TOPICS IN PROGRAMMING LANGUAGES \nAims \nThis module explores various topics in programming languages beyond the scope of \nundergraduate courses. It aims to introduce students to ideas, results and techniques found in \nthe literature and prepare them for research in the field. \n \nSyllabus and structure \nThe module consists of eight two-hour seminars, each on a particular topic. Topics will vary from \nyear to year, but may include, for example, \n \nAbstract interpretation \nVerified software \nMetaprogramming \nBehavioural types \nProgram synthesis \nVerified compilation \nPartial evaluation \nGarbage collection \nDependent types \nAutomatic differentiation \nDelimited continuations \nModule systems \nThere will be three papers assigned for each topic, which students are expected to read before \nthe seminar. \nEach seminar will include three 20 minute student presentations (15 minutes + 5 minutes \nquestions), time for general discussion of the topic, and a brief overview lecture for the following \nweek\u2019s topic. \nBefore each seminar, except in weeks in which they give presentations, students will submit a \nshort essay about two of the papers. \n \n \nObjectives \nOn completion of this module, students should \n \nbe able to identify some major themes in programming language research \nbe familiar with some classic papers and recent advances \nhave an understanding of techniques used in the field \n \nAssessment \nAssessment consists of: \n \nPresentation of one of the papers from the reading list (typically once or twice in total for each \nstudent, depending on class numbers) \nOne essay per week (except on the first week and on presentation weeks) \n\nAll essays and presentations carry equal numbers of marks. \n \nEssay marks are awarded for understanding, for insight and analysis, and for writing quality. \nEssays should be around 1500 words (with a lower limit of 1450 and upper limit of 1650). \nPresentation marks are awarded for clarity, for effective communication, and for selection and \norganisation of topics. \n \nThere will be seven submissions (essays or presentations) in total and, as in other courses, the \nlowest mark for each student will be disregarded when computing the final mark. \n \nMarking, deadlines and extensions will be handled in accordance with the MPhil Assessment \nGuidelines. \n \nRecommended reading material and resources \nResearch and survey papers from programming language conferences and journals (e.g. \nPOPL, PLDI, TOPLAS, FTPL) will be assigned each week. General background material may \nbe found in: \n \n\u2022 Types and Programming Languages (Benjamin C. Pierce) \n   The MIT Press \n   ISBN 0-262-16209-1 \n \n\u2022 Advanced Topics in Types and Programming Languages (ed. Benjamin C. Pierce) \n   The MIT Press \n   ISBN 0-262-16228-8 \n \n\u2022 Practical Foundations for Programming Languages (Robert Harper) \n   Cambridge University Press \n   9781107150300\n \n\nAFFECTIVE ARTIFICIAL INTELLIGENCE \n \nSynopsis \nAffective Artificial Intelligence (Affective AI) aims to imbue machines with social and emotional \nintelligence (EQ). More specifically, Affective AI aims to create artificially intelligent systems and \nmachines that can recognize, interpret, process, and simulate human social signals and \nbehaviours, expressions, and emotions, to enhance human-AI interaction and communication. \n\\r\\n\\r\\nTo achieve this goal, Affective AI draws upon various scientific disciplines, including \nmachine learning, computer vision, speech / natural language / signal processing, psychology \nand cognitive science, and ethics and social sciences. \n \n  \nBackground & Aims \nAffective Artificial Intelligence (Affective AI) aims to imbue machines with social and emotional \nintelligence (EQ). More specifically, Affective AI aims to create artificially intelligent systems and \nmachines that can recognize, interpret, process, and simulate human social signals and \nbehaviours, expressions, and emotions, to enhance human-AI interaction and communication.  \n \nTo achieve this goal, Affective AI draws upon various scientific disciplines, including machine \nlearning, computer vision, speech / natural language / signal processing, psychology and \ncognitive science, and ethics and social sciences. \n \nAffective AI has direct applications in and implications for the design of innovative interactive \ntechnology (e.g., interaction with chat bots, virtual agents, robots), single and multi-user smart \nenvironments ( e.g., in-car/ virtual / augmented / mixed reality, serious games), public speaking \nand cognitive training, and clinical and biomedical studies (e.g., autism, depression, pain). \n \nThe aim of this module is to impart knowledge and ability needed to make informed choices of \nmodels, data, and machine learning techniques for sensing, recognition, and generation of \naffective and social behaviour (e.g., smile, frown, head nodding/shaking, \nagreement/disagreement) in order to create Affectively intelligent AI systems, with a \nconsideration for various ethical issues (e.g., privacy, bias) arising from the real-world \ndeployment of these systems. \n \n  \n \nSyllabus \nThe following list provides a representative list of topics: \n \nIntroduction, definitions, and overview \nTheories from various disciplines \nSensing from multiple modalities (e.g., vision, audio, bio signals, text) \nData acquisition and annotation \nSignal processing / feature extraction \n\nLearning / prediction / recognition and evaluation \nBehaviour synthesis / generation (e.g., for embodied agents / robots) \nAdvanced topics and ethical considerations (e.g., bias and fairness) \nCross-disciplinary applications (via seminar presentations and discussions) \nGuest lectures (diverse topics \u2013 changes each year) \nHands-on research and programming work (i.e., mini project & report)  \n \n  \n \nObjectives \nOn completion of this module, students will: \n \nunderstand and demonstrate knowledge in key characteristics of affectively intelligent AI, which \ninclude: \nRecognition: How to equip Affective AI systems with capabilities of analysing facial expressions, \nvocal intonations, gestures, and other physiological signals to infer human affective states? \nGeneration: How to enable affectively intelligent AI simulate expressions of emotions in \nmachines, allowing them to respond in a more human-like manner, such as virtual agents and \nhumanoid robots, showing empathy or sympathy? \nAdaptation and Personalization: How to enable affectively intelligent AI adapt system responses \nbased on users' affective states and/or needs, or tailor interactions and experiences to individual \nusers based on their expressivity / emotional profiles, personalities, and past interactions? \nEmpathetic Communication: How to design Affective AI systems to communicate with users in a \nway that demonstrates empathy, understanding, and sensitivity to their affective states and \nneeds? \nEthical and societal considerations: What are the various human differences, ethical guidelines, \nand societal impacts that need to be considered when designing and deploying Affective AI to \nensure that these systems respect users' privacy, autonomy, and well-being? \ncomprehend and apply (appropriate) methods for collection, analysis, representation, and \nevaluation of human affective and communicative behavioural data. \nenhance programming skills for creating and implementing (components of) Affective AI \nsystems. \ndemonstrate critical thinking, analysis and synthesis while deciding on 'when' and 'how' to \nincorporate human affect and social signals in a specific AI system context and gain practical \nexperience in proposing and justifying computational solution(s) of suitable nature and scope. \n  \n \nAssessment - Part II Students \nSeminar presentation: 20% \nParticipating in Q&A and discussions: 10% \nMid-term mini project report and presentation (as a team of 2): 10%  \nFinal mini project report & code (as a team of 2): 60% \n  \n \n\nAssessment - MPhil / Part III Students \nSeminar presentation: 20% \nParticipating in Q&A and discussions: 10% \nMid-term mini project report and presentation: 10%  \nFinal mini project report & code: 60% \nFurther Information \nCurrent Cambridge undergraduate students who are continuing onto Part III or the MPhil in \nAdvanced Computer Science may only take this module if they did NOT take it as a Unit of \nAssessment in Part II. \n \nThis module is shared with ACS. Assessment will be adjusted for the two groups of students to \nbe at an appropriate level for whichever course the student is enrolled on. More information will \nfollow at the first lecture. \n \nThis module is shared with Part II of the Computer Science Tripos. Assessment will be adjusted \nfor the two groups of students to be at an appropriate level for whichever course the student is \nenrolled on. Further information about assessment and practicals will follow at the first lecture.\n\nCATEGORY THEORY \n \nAims \nCategory theory provides a unified treatment of mathematical properties and constructions that \ncan be expressed in terms of 'morphisms' between structures. It gives a precise framework for \ncomparing one branch of mathematics (organized as a category) with another and for the \ntransfer of problems in one area to another. Since its origins in the 1940s motivated by \nconnections between algebra and geometry, category theory has been applied to diverse fields, \nincluding computer science, logic and linguistics. This course introduces the basic notions of \ncategory theory: adjunction, natural transformation, functor and category. We will use category \ntheory to organize and develop the kinds of structure that arise in models and semantics for \nlogics and programming languages. \n \nSyllabus \nIntroduction; some history. Definition of category. The category of sets and functions. \nCommutative diagrams. Examples of categories: preorders and monotone functions; monoids \nand monoid homomorphisms; a preorder as a category; a monoid as a category. Definition of \nisomorphism. Informal notion of a 'category-theoretic' property. \nTerminal objects. The opposite of a category and the duality principle. Initial objects. Free \nmonoids as initial objects. \nBinary products and coproducts. Cartesian categories. \nExponential objects: in the category of sets and in general. Cartesian closed categories: \ndefinition and examples. \nIntuitionistic Propositional Logic (IPL) in Natural Deduction style. Semantics of IPL in a cartesian \nclosed preorder. \nSimply Typed Lambda Calculus (STLC). The typing relation. Semantics of STLC types and \nterms in a cartesian closed category (ccc). The internal language of a ccc. The \nCurry-Howard-Lawvere correspondence. \nFunctors. Contravariance. Identity and composition for functors. \nSize: small categories and locally small categories. The category of small categories. Finite \nproducts of categories. \nNatural transformations. Functor categories. The category of small categories is cartesian \nclosed. \nHom functors. Natural isomorphisms. Adjunctions. Examples of adjoint functors. Theorem \ncharacterising the existence of right (respectively left) adjoints in terms of a universal property. \nDependent types. Dependent product sets and dependent function sets as adjoint functors. \nEquivalence of categories. Example: the category of I-indexed sets and functions is equivalent \nto the slice category Set/I. \nPresheaves. The Yoneda Lemma. Categories of presheaves are cartesian closed. \nMonads. Modelling notions of computation as monads. Moggi's computational lambda calculus. \nObjectives \nOn completion of this module, students should: \n \n\nbe familiar with some of the basic notions of category theory and its connections with logic and \nprogramming language semantics \nAssessment \na graded exercise sheet (25% of the final mark), and \na take-home test (75%) \nRecommended reading \nAwodey, S. (2010). Category theory. Oxford University Press (2nd ed.). \n \nCrole, R. L. (1994). Categories for types. Cambridge University Press. \n \nLambek, J. and Scott, P. J. (1986). Introduction to higher order categorical logic. Cambridge \nUniversity Press. \n \nPitts, A. M. (2000). Categorical Logic. Chapter 2 of S. Abramsky, D. M. Gabbay and T. S. E. \nMaibaum (Eds) Handbook of Logic in Computer Science, Volume 5. Oxford University Press. \n(Draft copy available here.) \n \nClass Size \nThis module can accommodate upto 15 Part II students plus 15 MPhil / Part III students. \n \nPre-registration evaluation form  \nStudents who are interested in taking this module will be asked to complete a pre-registration \nevaluation form to determine whether they have enough mathematical background to take the \nmodule.  \n \nThis will take place during the week Monday 12 August - Friday 16 August.  \n \nCoursework \nThe coursework in this module will consist of a graded exercise sheet. \n \nFurther Information \nCurrent Cambridge undergraduate students who are continuing onto Part III or the MPhil in \nAdvanced Computer Science may only take this module if they did NOT take 'Catgeory Theory' \nas a Unit of Assessment in Part II. \n \nThis module is shared with Part II of the Computer Science Tripos. Assessment will be adjusted \nfor the two groups of students to be at an appropriate level for whichever course the student is \nenrolled on. Further information about assessment and practicals will follow at the first lecture.\n\nDIGITAL MONEY AND DECENTRALISED FINANCE \n \nAims \nOur society is evolving towards digital payments and the elimination of physical cash. Digital \nalternatives to cash require trade-offs between various properties including unforgeability, \ntraceability, divisibility, transferability without intermediaries, privacy, redeemability, control over \nthe money supply and many more, often in conflict with each other. Since the 1980s, \ncryptographers have proposed a variety of clever technical solutions addressing some of these \nissues but it is only with the appearance of Bitcoin, blockchain and a plethora of copycat \ncryptocurrencies that a new asset class has now emerged, worth in excess of two trillion dollars \n(although plagued by extreme volatility). Meanwhile, most major countries have been planning \nthe introduction of Central Bank Digital Currencies in an attempt to retain control. This \nseminar-style interactive module encourages the students to understand what's happening, \nwhat the underlying problems and opportunities are (technical, social and economic) and where \nwe should go from here. We are at a historical turning point where an enterprising candidate \nmight disrupt the status quo and truly make a difference. \n \nSyllabus \nUnderstanding money: from gold standard to digital cash \nIntro to the Decentralised Finance (DeFi) ecosystem: Bitcoin, blockchain, wallets, smart \ncontracts \nStablecoins and Central Bank Digital Currencies \nDecentralised Autonomous Organisations and Automated Market Makers \nYield Farming, Perpetual Futures and Maximal Extractable Value \nWeb3, Non Fungible Tokens \nCryptocurrency crashes; crimes facilitated by cryptocurrencies \nNew areas of development. Where from here? \nThe module is structured as a reading club with a high degree of interactivity. Aside from the first \nand last session, which are treated differently, the regular two-hour sessions have the following \nstructure (not necessarily in this order). \n \nTwo half-hour debates on each of two papers \nTwo rapid-fire presentations on open questions previously assigned by the lecturers. \nHalf an hour of presentation by the lecturers. \nAbout ten minutes of buffer that can account for switchover time or be absorbed into the \nlecturers' presentation. \nThe first session is scene-setting by the lecturers, with no student presentations. The last \nsession has no paper debates, only one rapid-fire presentation on an open question from each \nof the students. \n \nObjectives \nOn completion of this module, students will gain an in-depth understanding of digital money \nalternatives as well as many modern applications and components of Decentralised Finance. By \ncomparing DeFi with current processes in traditional banking, students will be able to discuss \n\nthe merits and limitations of these new technologies as well as to identify potential new areas of \ndevelopment. \n \nThrough having to write, present and defend very brief extended abstracts, the students will \nimprove their communication skills and in particular the ability to distil and convey the most \nimportant points forcefully and concisely. \n \n \nAssessment \nThe module is an interactive reading club. Each student produces four output triplets throughout \nthe course. Each output triplet consists of three parts: a 1200-word essay, a 200-word extended \nabstract and a 7-minute presentation. \n \nThe essay and abstract must be submitted in advance, using the LaTeX templates provided. \nThe abstract must consist of full sentences, not bullet points, and must fit on a single slide. \nDuring the presentation, the slide with the abstract will be projected on screen as the only visual \naid; the essay will not be accessible to either presenter or audience. Reading out the slide \nverbatim will obviously not count as a great presentation. Presenters must be able to expand \nand defend their ideas live, and answer questions from lecturers and audience. \n \nThe essays, the abstracts and the presentations are graded separately, each out of 10, and \nassigned weights of 60%, 20%, 20%, respectively, within the triplet. \n \nIn a regular 2-hour session (all but the first and last) there will be two paper slots and two \ninvestigation slots, plus a lecturer slot, described next. \n \nEach paper slot (30 min) involves a \u201csupporter\u201d and a \u201cdissenter\u201d for the paper, each \ncontributing one output triplet, and then a panel Q&A where all the other participants quiz the \nsupporter and dissenter. \n \nEach investigation slot (10 min) involves a single student contributing an output triplet on a \ngiven theme. \n \nRoles, papers and themes are assigned by the lecturers the previous week. \n \nIn the first session there will be no student presentations, only lecturer-led exploration. In the \nlast session there will be 12 investigation slots and no paper slots. \n \nOverall, each student will be supporter once, dissenter once and investigator twice, for a total of \n4 output triplets. The output triplets of the last session will be weighed twice as much as the \nothers, i.e. 40% each, while the others 20% each. \n \nAttendance is required at all sessions. Missing a session in which the candidate was due to \npresent will result in the loss of the corresponding presentation marks, while the written work will \n\nstill have to be submitted and will be marked as usual. Missing a session in which the candidate \nwas not due to present will result in the loss of 3%. \n \nRecommended reading \nThere isn't a textbook covering all the material in this course. The following textbook, also \nadopted by R47 Distributed Ledger Technologies (also freely available online), is a thorough \nintroduction to Bitcoin and Blockchain, but we will complement it with research papers and \nindustry white papers for each lecture. \n \nNarayanan, A. , Bonneau, J., Felten, E., Miller, A. and Goldfeder, S. (2016). Bitcoin and \nCryptocurrency Technologies: A Comprehensive Introduction. Princeton University Press. \n(2016 Draft available here: \nhttps://d28rh4a8wq0iu5.cloudfront.net/bitcointech/readings/princeton_bitcoin_book.pdf)\n \n\nDIGITAL SIGNAL PROCESSING \n \nAims \nThis course teaches basic signal-processing principles necessary to understand many modern \nhigh-tech systems, with examples from audio processing, image coding, radio communication, \nradar, and software-defined radio. Students will gain practical experience from numerical \nexperiments in programming assignments (in Julia, MATLAB or NumPy). \n \nLectures \nSignals and systems. Discrete sequences and systems: types and properties. Amplitude, \nphase, frequency, modulation, decibels, root-mean square. Linear time-invariant systems, \nconvolution. Some examples from electronics, optics and acoustics. \nPhasors. Eigen functions of linear time-invariant systems. Review of complex arithmetic. \nPhasors as orthogonal base functions. \nFourier transform. Forms and properties of the Fourier transform. Convolution theorem. Rect \nand sinc. \nDirac\u2019s delta function. Fourier representation of sine waves, impulse combs in the time and \nfrequency domain. Amplitude-modulation in the frequency domain. \nDiscrete sequences and spectra. Sampling of continuous signals, periodic signals, aliasing, \ninterpolation, sampling and reconstruction, sample-rate conversion, oversampling, spectral \ninversion. \nDiscrete Fourier transform. Continuous versus discrete Fourier transform, symmetry, linearity, \nFFT, real-valued FFT, FFT-based convolution, zero padding, FFT-based resampling, \ndeconvolution exercise. \nSpectral estimation. Short-time Fourier transform, leakage and scalloping phenomena, \nwindowing, zero padding. Audio and voice examples. DTFM exercise. \nFinite impulse-response filters. Properties of filters, implementation forms, window-based FIR \ndesign, use of frequency-inversion to obtain high-pass filters, use of modulation to obtain \nband-pass filters. \nInfinite impulse-response filters. Sequences as polynomials, z-transform, zeros and poles, some \nanalog IIR design techniques (Butterworth, Chebyshev I/II, elliptic filters, second-order cascade \nform). \nBand-pass signals. Band-pass sampling and reconstruction, IQ up and down conversion, \nsuperheterodyne receivers, software-defined radio front-ends, IQ representation of AM and FM \nsignals and their demodulation. \nDigital communication. Pulse-amplitude modulation. Matched-filter detector. Pulse shapes, \ninter-symbol interference, equalization. IQ representation of ASK, BSK, PSK, QAM and FSK \nsignals. [2 hours] \nRandom sequences and noise. Random variables, stationary and ergodic processes, \nautocorrelation, cross-correlation, deterministic cross-correlation sequences, filtered random \nsequences, white noise, periodic averaging. \nCorrelation coding. Entropy, delta coding, linear prediction, dependence versus correlation, \nrandom vectors, covariance, decorrelation, matrix diagonalization, eigen decomposition, \n\nKarhunen-Lo\u00e8ve transform, principal component analysis. Relation to orthogonal transform \ncoding using fixed basis vectors, such as DCT. \nLossy versus lossless compression. What information is discarded by human senses and can \nbe eliminated by encoders? Perceptual scales, audio masking, spatial resolution, colour \ncoordinates, some demonstration experiments. \nQuantization, image coding standards. Uniform and logarithmic quantization, A/\u00b5-law coding, \ndithering, JPEG. \nObjectives \napply basic properties of time-invariant linear systems; \nunderstand sampling, aliasing, convolution, filtering, the pitfalls of spectral estimation; \nexplain the above in time and frequency domain representations; \nuse filter-design software; \nvisualize and discuss digital filters in the z-domain; \nuse the FFT for convolution, deconvolution, filtering; \nimplement, apply and evaluate simple DSP applications; \nfamiliarity with a number of signal-processing concepts used in digital communication systems \nRecommended reading \nLyons, R.G. (2010). Understanding digital signal processing. Prentice Hall (3rd ed.). \nOppenheim, A.V. and Schafer, R.W. (2007). Discrete-time digital signal processing. Prentice \nHall (3rd ed.). \nStein, J. (2000). Digital signal processing \u2013 a computer science perspective. Wiley. \n \nClass size \nThis module can accommodate a maximum of 24 students (16 Part II students and 8 MPhil \nstudents) \n \nAssessment - Part II students \nThree homework programming assignments, each comprising 20% of the mark \nWritten test, comprising 40% of the total mark. \nAssessment - Part III and MPhil students \nThree homework programming exercises, each comprising 10% of the mark. \nWritten test, comprising 20% of the total mark. \nIndividual implementation project with 8-page written report, topic agreed with lecturer, \ncomprising 50% of the mark. \nFurther Information \nCurrent Cambridge undergraduate students who are continuing onto Part III or the MPhil in \nAdvanced Computer Science may only take this module if they did NOT take it as a Unit of \nAssessment in Part II. \n \nThis module is shared with Part II of the Computer Science Tripos. Assessment will be adjusted \nfor the two groups of students to be at an appropriate level for whichever course the student is \nenrolled on. Further information about assessment and practicals will follow at the first lecture.\n\nINTRODUCTION TO COMPUTATIONAL SEMANTICS \n \nAims \nThis is a lecture-style course that introduces students to various aspects of the semantics of \nNatural Languages (mainly English): \n \nLexical Semantics, with an emphasis on theory and phenomenology  \nCompositional Semantics \nDiscourse and pragmatics-related aspects of semantics \nObjectives \nGive an operational definition of what is meant by \u201cmeaning\u201d (for instance, above and beyond \nsyntax); \nName the types of phenomena in language that require semantic consideration, in terms of \nlexical, compositional and discourse/pragmatic aspects, in other words, argue why semantics is \nimportant; \nDemonstrate an understanding of the basics of various semantic representations, including \nlogic-based and graph-based semantic representations, their properties, how they are used and \nwhy they are important, and how they are different from syntactic representations; \nKnow how such semantic representations are derived during or after parsing, and how they can \nbe analysed and mapped to surface strings; \nUnderstand applications of semantic representations e.g. reasoning, validation, and methods \nhow these are approached. \nWhen designing NL tasks that clearly require semantic processing (e.g. knowledge-based QA), \nto be aware of and reuse semantic representations and algorithms when designing the task, \nrather than reinventing the wheel. \nPractical advantages of this course for NLP students \nKnowledge of underlying semantic effects helps improve NLP evaluation, for instance by \nproviding more meaningful error analysis. You will be able to link particular errors to design \ndecisions inside your system. \nYou will learn methods for better benchmarking of your system, whatever the task may be. \nSupervised ML systems (in particular black-box systems such as Deep Learning) are only as \nclever as the datasets they are based on. In this course, you will learn to design datasets so that \nthey are harder to trick without real understanding, or critique existing datasets. \nYou will be able to design tests for ML systems that better pinpoint which aspects of language \nan end-to-end system has \u201cunderstood\u201d. \nYou will learn to detect ambiguity and ill-formed semantics in human-human communication. \nThis can serve to write more clearly and logically. \nYou will learn about decomposing complex semantics-reliant tasks sensibly so that you can \nreuse the techniques underlying semantic analyzers in a modular way. In this way, rather than \nbeing forced to treat complex tasks in an end-to-end manner, you will be able to profit from \npartial explanations and a better error analysis already built into the system. \nSyllabus \nOverview of the course \nEvents and semantic role labelling \n\nReferentiality and coreference resolution \nTruth-conditional semantics \nGraph-based meaning representation \nCompositional semantics \nContext-free Graph Rewriting \nSurface realisation \nNegation and psychological approach to semantics \nDynamic semantics \nGricean pragmatics \nVector space models \nCross-modality \nAcquisition of semantics \nDiachronic change of semantics \nSummary of the course \n  \n \nAssessment \n5 take-home exercises worth 20% each: \n \nTake-home assignment 1 is given in Lecture 4 and due is Lecture 6 \n \nStudents are given 10 English sentences and asked to provide their semantic analysis \naccording to truth-conditions. Students will be expected to return 10 logical expressions as their \nanswers. No word limit. Assessment criteria: correctness of the 10 logical expressions; 2 points \nfor each logical expression. \n \nTake-home assignment 2 is given in Lecture 7 and due is Lecture 9  \n \nStudents are given 10 English sentences and asked to provide their syntactico-semantic \nderivations according to the compositionality principle. Students will be expected to return \nderivation graphs as their answers. No word limit. Assessment criteria: correctness of the 10 \nderivation graphs; 2 points for each derivation. \n \nTake-home assignment 3 is given in Lecture 11 and due is Lecture 13  \n \nAll students are assigned with a paper on modelling common ground in dialogue system. \nStudents will receive related but different papers. Each student will write a review of their \nassigned paper, including a comprehensive summary and their own thoughts. Word limit: 1000 \nwords. Assessment criteria: 15 points on whether a student understands the paper correctly; 5 \npoints on whether a student is able to think critically. \n \nTake-home assignment 4 is given in Lecture 13 and due is Lecture 15 \n \n\nAll students are assigned with a paper on language-vision interaction. Students will receive \nrelated but different papers. Each student will write a review of their assigned paper, including a \ncomprehensive summary and their own thoughts. Word limit: 1000 words. Assessment criteria: \n15 points on whether a student understands the paper correctly; 5 points on whether a student \nis able to think critically. \n \nTake-home assignment 5 is given in Lecture 14 and due is two weeks later. \n \nContent: All students are assigned with a paper on bootstrapping language acquisition. All \nstudents will receive the same paper. Each student will write a review of the paper, including a \ncomprehensive summary and their own thoughts. Word limit: 1000 words. Assessment criteria: \n15 points on whether a student understands the paper correctly; 5 points on whether a student \nis able to think critically.\n \n\nINTRODUCTION TO NATURAL LANGUAGE SYNTAX AND PARSING \n \nAims \nThis module aims to provide a brief introduction to linguistics for computer scientists and then \ngoes on to cover some of the core tasks in natural language processing (NLP), focussing on \nstatistical parsing of sentences to yield syntactic representations. We will look at how to \nevaluate parsers and see how well state-of-the-art tools perform given current techniques. \n \nSyllabus \nLinguistics for NLP focusing on morphology and syntax \nGrammars and representations \nStatistical and neural parsing \nTreebanks and evaluation \nObjectives \nOn completion of this module, students should: \n \nunderstand the basic properties of human languages and be familiar with descriptive and \ntheoretical frameworks for handling these properties; \nunderstand the design of tools for NLP tasks such as parsing and be able to apply them to text \nand evaluate their performance; \nPractical work \nWeek 1-7: There will be non-assessed practical exercises between sessions and during \nsessions. \nWeek 8: Download and apply two parsers to a designated text. Evaluate the performance of the \ntools quantitatively and qualitatively. \nAssessment \nThere will be one presentation worth 10% of the final mark; presentation topics will be allocated \nat the start of term. \nAn assessed practical report of not more 8 pages in ACL conference format. It will contribute \n90% of the final mark. \nRecommended reading \nJurafsky, D. and Martin, J. (2008). Speech and Language Processing. Prentice-Hall (2nd ed.). \n(See also 3rd ed. available online.)\n \n\nINTRODUCTION TO NETWORKING AND SYSTEMS MEASUREMENTS \n \nAims \nSystems research refers to the study of a broad range of behaviours arising from complex \nsystem design, including: resource sharing and scheduling; interactions between hardware and \nsoftware; network topology, protocol and device design and implementation; low-level operating \nsystems; Interconnect, storage and more. This module will: \n \nTeach performance characteristics and performance measurement methodology and practice \nthrough profiling experiments; \nExpose students to real-world systems artefacts evident through different measurement tools; \nDevelop scientific writing skills through a series of laboratory reports; \nProvide research skills for characterization and modelling of systems and networks using \nmeasurements. \nPrerequisites \nIt is strongly recommended that students have previously (and successfully) completed an \nundergraduate networking course -- or have equivalent experience through project or \nopen-source work. \n \nSyllabus \nIntroduction to performance measurements, performance characteristics [1 lecture] \nPerformance measurements tools and techniques [2 lectures + 2 lab sessions] \nReproducible experiments [1 lecture + 1 lab session] \nCommon pitfalls in measurements [1 lecture] \nDevice and system characterisation [1 lecture + 2 lab sessions] \nObjectives \nOn completion of this module, students should: \n \nDescribe the objectives of measurements, and what they can achieve; \nCharacterise and model a system using measurements; \nPerform reproducible measurements experiments; \nEvaluate the performance of a system using measurements; \nOperate measurements tools and be aware of their limitations; \nDetect anomalies in the network and avoid common measurements pitfalls; \nWrite system-style performance evaluations. \nPractical work \nFive 2-hour in-classroom labs will ask students to develop and use skills in performance \nmeasurements as applied to real-world systems and networking artefacts. Results from these \nlabs (and follow-up work by students outside of the classroom) will by the primary input to lab \nreports. \n \nThe first three labs will provide an introduction and hands on experience with measurement \ntools and measurements methodologies, while the last two labs will focus on practical \nmeasurements and evaluation of specific platforms. Students may find it useful to work in pairs \n\nwithin the lab, but must prepare lab reports independently. The module lecturer will give a short \nintroductory at the start of each lab, and instructors will be on-hand throughout labs to provide \nassistance. \n \nLab participation is not directly included in the final mark, but lab work is a key input to lab \nreports that are assessed. Guided lab experiments resulting in practical write ups. \n \nAssessment \nEach student will write two lab reports. The first lab report will summarise the experiments done \nin the first three labs (20%). The second will be a lab report (5000 words) summarising the \nevaluation of a device or a system (80%). \n \nRecommended reading \nThe following list provides some background to the course materials, but is not mandatory. A \nreading list, including research papers, will be provided in the course materials. \n \nGeorge Varghese. Network algorithmics. Chapman and Hall/CRC, 2010. \nMark Crovella and Balachander Krishnamurthy. Internet measurement: infrastructure, traffic and \napplications. John Wiley and Sons, Inc., 2006. \nBrendan Gregg. Systems Performance: Enterprise and the Cloud, Prentice Hall Press, Upper \nSaddle River, NJ, USA, October 2013. \nRaj Jain, The Art of Computer Systems Performance Analysis: Techniques for Experimental \nDesign, Measurement, Simulation, and Modeling, Wiley - Interscience, New York, NY, USA, \nApril 1991.\n \n\nLARGE-SCALE DATA PROCESSING AND OPTIMISATION \n \nAims \nThis module provides an introduction to large-scale data processing, optimisation, and the \nimpact on computer system's architecture. Large-scale distributed applications with high volume \ndata processing such as training of machine learning will grow ever more in importance. \nSupporting the design and implementation of robust, secure, and heterogeneous large-scale \ndistributed systems is essential. To deal with distributed systems with a large and complex \nparameter space, tuning and optimising computer systems is becoming an important and \ncomplex task, which also deals with the characteristics of input data and algorithms used in the \napplications. Algorithm designers are often unaware of the constraints imposed by systems and \nthe best way to consider these when designing algorithms with massive volume of data. On the \nother hand, computer systems often miss advances in algorithm design that can be used to cut \ndown processing time and scale up systems in terms of the size of the problem they can \naddress. Integrating machine learning approaches (e.g. Bayesian Optimisation, Reinforcement \nLearning) for system optimisation will be explored in this course. \n \nSyllabus \nThis course provides perspectives on large-scale data processing, including data-flow \nprogramming, graph data processing, probabilistic programming and computer system \noptimisation, especially using machine learning approaches, thus providing a solid basis to work \non the next generation of distributed systems. \n \nThe module consists of 8 sessions, with 5 sessions on specific aspects of large-scale data \nprocessing research. Each session discusses 3-4 papers, led by the assigned students. One \nsession is a hands-on tutorial on on dataflow programming fundamentals. The first session \nadvises on how to read/review a paper together with a brief introduction on different \nperspectives in large-scale data \nprocessing and optimisation. The last session is dedicated to the student presentation of \nopensource project studies. \n \nIntroduction to large-scale data processing and optimisation \nData flow programming: Map/Reduce to TensorFlow \nLarge-scale graph data processing: storage, processing model and parallel processing \nDataflow programming hands-on tutorial \nProbabilistic Programming \nOptimisations in ML Compiler \nOptimisations of Computer systems using ML \nPresentation of Open Source Project Study \nObjectives \nOn completion of this module, students should: \n \nUnderstand key concepts of scalable data processing approaches in future computer systems. \n\nObtain a clear understanding of building distributed systems using data centric programming \nand large-scale data processing. \nUnderstand a large and complex parameter space in computer system's optimisation and \napplicability of Machine Learning approach. \nCoursework \nReading Club: \nThe preparation for the reading club will involve 1-3 papers every week. At each session, \naround 3-4 papers are selected under the given topic, and the students present their review \nwork. \nHands-on tutorial session of data flow programming including writing an application of \nprocessing streaming in Twitter data and/or Deep Neural Networks using Google TensorFlow \nusing cluster computing. \nReports \nThe following three reports are required, which could be extended from the assignment of the \nreading club, within the scope of data centric systems. \n \nReview report on a full length paper (max 1800 words) \nDescribe the contribution of the paper in depth with criticisms \nCrystallise the significant novelty in contrast to other related work \nSuggestions for future work \nSurvey report on sub-topic in large-scale data processing and optimisation (max 2000 words) \nPick up to 5 papers as core papers in the survey scope \nRead the above and expand reading through related work \nComprehend the view and finish an own survey paper \nProject study and exploration of a prototype (max 2500 words) \nWhat is the significance of the project in the research domain? \nCompare with similar and succeeding projects \nDemonstrate the project by exploring its prototype \nReports 1 should be handed in by the end of 5th week; Report 2 in the 8th week and Report 3 \non the first day of Lent Term. \n \nAssessment \nThe final grade for the course will be provided as a percentage, and the assessment will consist \nof two parts: \n \n25%: for reading club (participation, presentation) \n75%: for the three reports: \n15%: Intensive review report \n25%: Survey report \n35%: Project study \nRecommended reading \nM. Abadi et al. TensorFlow: A System for Large-Scale Machine Learning, OSDI, 2016. \nD. Aken et al.: Automatic Database Management System Tuning Through Large-scale Machine \nLearning, SIGMOD, 2017. \n\nJ. Ansel et al. Opentuner: an extensible framework for program autotuning. PACT, 2014. \nV. Dalibard, M. Schaarschmidt, E. Yoneki. BOAT: Building Auto-Tuners with Structured Bayesian \nOptimization, WWW, 2017. \nJ. Dean et al. Large scale distributed deep networks. NIPS, 2012. \nG. Malewicz, M. Austern, A. Bik, J. Dehnert, I. Horn, N. Leiser, G. Czajkowski. Pregel: A System \nfor Large-Scale Graph Processing, SIGMOD, 2010. \nA. Mirhoseini et al. Device Placement Optimization with Reinforcement Learning, ICML, 2017. \nD. Murray, F. McSherry, R. Isaacs, M. Isard, P. Barham, M. Abadi. Naiad: A Timely Dataflow \nSystem, SOSP, 2013. \nM. Schaarschmidt, S. Mika, K. Fricke and E. Yoneki: RLgraph: Modular Computation Graphs for \nDeep Reinforcement Learning, SysML, 2019. \nZ. Jia, O. Padon, J. Thomas, T. Warszawski, M. Zaharia,  A. Aiken: TASO: Optimizing Deep \nLearning Computation with Automated Generation of Graph Substitutions: SOSP, 2019. \nH. Mao et al.: Park: An Open Platform for Learning-Augmented Computer Systems, \nOpenReview, 2019. \nJ. Shao, et al.: Tensor Program Optimization with Probabilistic Programs, NeurIPS, 2022.  \nA complete list can be found on the course material web page. See also 2023-2024 course \nmaterial on the previous course Large-Scale Data Processing and Optimisation.\n \n\nMACHINE LEARNING AND THE PHYSICAL WORLD \nAims \nThe module \u201cMachine Learning and the Physical World\u201d is focused on machine learning \nsystems that interact directly with the real world. Building artificial systems that interact with the \nphysical world have significantly different challenges compared to the purely digital domain. In \nthe real world data is scares, often uncertain and decisions can have costly and irreversible \nconsequences. However, we also have the benefit of centuries of scientific knowledge that we \ncan draw from. This module will provide the methodological background to machine learning \napplied in this scenario. We will study how we can build models with a principled treatment of \nuncertainty, allowing us to leverage prior knowledge and provide decisions that can be \ninterrogated. \n \nThere are three principle points about machine learning in the real world that will concern us. \n \nWe often have a mechanistic understanding of the real world which we should be able to \nbootstrap to make decisions. For example, equations from physics or an understanding of \neconomics. \nReal world decisions have consequences which may have costs, and often these cost functions \nneed to be assimilated into our machine learning system. \nThe real world is surprising, it does things that you do not expect and accounting for these \nchallenges requires us to build more robust and or interpretable systems. \nDecision making in the real world hasn\u2019t begun only with the advent of machine learning \ntechnologies. There are other domains which take these areas seriously, physics, environmental \nscientists, econometricians, statisticians, operational researchers. This course identifies how \nmachine learning can contribute and become a tool within these fields. It will equip you with an \nunderstanding of methodologies based on uncertainty and decision making functions for \ndelivering on these challenges. \n \nObjectives \nYou will gain detailed knowledge of \n \nsurrogate models and uncertainty \nsurrogate-based optimization \nsensitivity analysis \nexperimental design \nYou will gain knowledge of \n \ncounterfactual analysis \nsurrogate-based quadrature \nSchedule \nWeek 1: Introduction to the unit and foundation of probabilistic modelling \n \nWeek 2: Gaussian processes and probablistic inference \n \n\nWeek 3: Simulation and Sequential decision making under uncertainty \n \nWeek 4: Emulation and Experimental Design \n \nWeek 5: Sensitivity Analysis and Multifidelity Modelling \n \nWeek 6-8: Case studies of applications and Projects \n \nPractical work \nDuring the first five weeks of the unit we will provide a weekly worksheet that will focus on \nimplementation and practical exploration of the material covered in the lectures. The worksheets \nwill allow you to build up a these methods without relying on extensive external libraries. You are \nfree to use any programming language of choice however we highly recommended the use of \n=Python=. \n \nAssessment \nTwo individual tasks (15% each) \nGroup Project (70%). Each group will work on an application of uncertainty that covers the \nmaterial of the first 5 weeks of lectures in the unit. Each group will submit a report which will \nform the basis of the assessment. In addition to the report each group will also attend a short \noral examination based on the material covered both in the report and the taught material. \nRecommended Reading \nRasmussen, C. E. and Williams, C. K. I. (2006). Gaussian Processes for Machine Learning. MIT \nPress \n \nBishop, C. (2006). Pattern recognition and machine learning. Springer. \nhttps://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-an\nd-Machine-Learning-2006.pdf \n \nLaplace, P. S. (1902). A Philosophical Essay on Probabilities. John Wiley & Sons. \nhttps://archive.org/details/philosophicaless00lapliala\n \n\nMACHINE VISUAL PERCEPTION \nAims \nThis course aims at introducing the theoretical foundations and practical techniques for machine \nperception, the capability of computers to interpret data resulting from sensor measurements. \nThe course will teach the fundamentals and modern techniques of machine perception, i.e. \nreconstructing the real world starting from sensor measurements with a focus on machine \nperception for visual data. The topics covered will be image/geometry representations for \nmachine perception, semantic segmentation, object detection and recognition, geometry \ncapture, appearance modeling and acquisition, motion detection and estimation, \nhuman-in-the-loop machine perception, select topics in applied machine perception. \n \nMachine perception/computer vision is a rapidly expanding area of research with real-world \napplications. An understanding of machine perception is also important for robotics, interactive \ngraphics (especially AR/VR), applied machine learning, and several other fields and industries. \nThis course will provide a fundamental understanding of and practical experience with the \nrelevant techniques. \n \nLearning outcomes \nStudents will understand the theoretical underpinnings of the modern machine perception \ntechniques for reconstructing models of reality starting from an incomplete and imperfect view of \nreality. \nStudents will be able to apply machine perception theory to solve practical problems, e.g. \nclassification of images, geometry capture. \nStudents will gain an understanding of which machine perception techniques are appropriate for \ndifferent tasks and scenarios. \nStudents will have hands-on experience with some of these techniques via developing a \nfunctional machine perception system in their projects. \nStudents will have practical experience with the current prominent machine perception \nframeworks. \nSyllabus \nThe fundamentals of machine learning for machine perception \nDeep neural networks and frameworks for machine perception \nSemantic segmentation of objects and humans \nObject detection and recognition \nMotion estimation, tracking and recognition \n3D geometry capture \nAppearance modeling and acquisition \nSelect topics in applied machine perception \nAssessment \nA practical exercise, worth 20% of the mark. This will cover the basics and theory of machine \nperception and some of the practical techniques the students will likely use in their projects. This \nis individual work. No GPU hours will be needed for the practical work. \nA machine perception project worth 80% of the marks: \n\nCourse projects will be selected by the students following the possible project themes proposed \nby the lecturer, and will be checked by the lecturer for appropriateness.  \nThe students will form groups of 2-3 to design, implement, report, and present a project to tackle \na given task in machine perception. \nThe final mark will be composed of an implementation/report mark (60%) and a presentation \nmark (20%). Each team member will be evaluated based on her/his contribution. \nEach project will have extensions to be completed only by the ACS students. Each student will \nwrite a different part of the report, whose author will be clearly marked. Each student will further \nsummarise her/his contributions to the project in the same report. \nRecommended Reading List \nComputer Vision: Algorithms and Applications, Richard Szeliski, Springer, 2010. \nDeep Learning, Ian Goodfellow, Yoshua Bengio, and Aaron Courville, MIT Press, 2016. \nMachine Learning and Visual Perception, Baochang Zhang, De Gruyter, 2020. \nFurther Information \nCurrent Cambridge undergraduate students who are continuing onto Part III or the MPhil in \nAdvanced Computer Science may only take this module if they did NOT take it as a Unit of \nAssessment in Part II. \n \nThis module is shared with Part II of the Computer Science Tripos. Assessment will be adjusted \nfor the two groups of students to be at an appropriate level for whichever course the student is \nenrolled on. Further information about assessment and practicals will follow at the first lecture.\n\nMOBILE, WEARABLE SYSTEMS AND MACHINE LEARNING \n \nAims \nThis module aims to introduce the latest research advancements in mobile systems and mobile \ndata machine learning, spanning a range of domains including systems, data gathering, \nanalytics and machine learning, on device machine learning and applications such as health, \ntransportation, behavior monitoring, cyber-physical systems, autonomous vehicles, drones. The \ncourse will cover current and seminal research papers in the area of research. \n \nSyllabus \nThe course will consist of one introductory lecture, seven two-hour, and one three-hour, \nsessions covering a variety of topics roughly including the following material (some variation in \nthe topics might happen from year to year): \n \nSystem, Energy and Security \nBackscatter Communication, Battery Free and Energy Harvesting Devices \nNew Sensing Modalities \nMachine Learning on Wearable Data \nOn Device Machine Learning \nMobile and Wearable Health \nMobile and Wearable Systems of Sustainability \nEach week, three class participants will be assigned to introduce assigned three papers via \n20-minute presentations, conference-style and highlighting critically its features. Each \npresentation will be followed by 10 minutes of questions. This will be followed by 10 minutes of \ngeneral discussion. Slides will be used for presentation. \n \nStudents will give one or more presentations each term. Each student will submit a paper review \neach week for one of the three papers presented except for the week they will be presenting \nslides. Each review will follow a template and be up to 1,000 words. Each review will receive a \nmaximum of 10 points. As a result, each student will produce 6-7 reviews and at least one \npresentation, probably two. All participants are expected to attend and participate in every class; \nthe instructor must be notified of any absences in advance. \n \nObjectives \nOn completion of this module students should have an understanding of the recent key research \nin mobile and sensor systems and mobile analytics as well as an improved critical thinking over \nresearch papers. \n \nAssessment \nAggregate mark for 7 assignments from 6-7 reports and 1-2 presentations. Each report or \npresentation will contribute one seventh of the course mark. \nA tick for presence and participation to each class will also be awarded. \nRecommended reading \n\nReadings from the most recent conferences such as AAAI, ACM KDD, ACM MobiCom, ACM \nMobiSys, ACM SenSys, ACM UbiComp, ICLR, ICML Neurips, and WWW pertinent to mobile \nsystems and data.\n \n\nNETWORK ARCHITECTURES \nAims \nThis module aims to provide the world with more network architects. The 2011-2012 version \nwas oriented around the evolution of IP to support new services like multicast, mobility, \nmultihoming, pub/sub and, in general, data oriented networking. The course is a paper reading \nwhich puts the onus on the student to do the work. \n \nSyllabus \nIPng [2 lectures, Jon Crowcroft] \nNew Architectures [2 lectures, Jon Crowcroft] \nMulticast [2 lectures, Jon Crowcroft] \nContent Distribution and Content Centric Networks [2 lectures, Jon Crowcroft] \nResource Pooling [2 lectures, Jon Crowcroft] \nGreen Networking [2 lectures, Jon Crowcroft] \nAlternative Router Implementions [2 lectures, Jon Crowcroft] \nData Center Networks [2 Lectures, Jon Crowcroft] \nObjectives \nOn completion of this module, students should be able to: \n \ncontribute to new network system designs; \nengineer evolutionary changes in network systems; \nidentify and repair architectural design flaws in networked systems; \nsee that there are no perfect solutions (aside from academic ones) for routing, addressing, \nnaming; \nunderstand tradeoffs in modularisation and other pressures on clean software systems \nimplementation, and see how the world is changing the proper choices in protocol layering, or \nnon layered or cross-layered. \nCoursework \nAssessment is through three graded essays (each chosen individually from a number of \nsuggested or student-chosen topics), as follows: \n \nAnalysis of two different architectures for a particular scenario in terms of cost/performance \ntradeoffs for some functionality and design dimension, for example: \nATM \u2013 e.g. for hardware versus software tradeoff \nIP \u2013 e.g. for mobility, multi-homing, multicast, multipath \n3GPP \u2013 e.g. for plain complexity versus complicatedness \nA discursive essay on a specific communications systems component, in a particular context, \nsuch as ad hoc routing, or wireless sensor networks. \nA bespoke network design for a narrow, well specified specialised target scenario, for example: \nA customer baggage tracking network for an airport. \nin-flight entertainment system. \nin-car network for monitoring and control. \ninter-car sensor/control network for automatic highways. \nPractical work \n\nThis course does not feature any implementation work due to time constraints. \n \nAssessment \nThree 1,200-word essays (worth 25% each), and \nan annotated bibliography (25%). \nRecommended reading \nPre-course reading: \n \nKeshav, S. (1997). An engineering approach to computer networking. Addison-Wesley (1st ed.). \nISBN 0201634422 \nPeterson, L.L. and Davie, B.S. (2007). Computer networks: a systems approach. Morgan \nKaufmann (4th ed.). \n \nDesign patterns: \n \nDay, John (2007). Patterns in network architecture: a return to fundamentals. Prentice Hall. \n \nExample systems: \n \nKrishnamurthy, B. and Rexford, J. (2001). Web protocols and practice: HTTP/1.1, Networking \nprotocols, caching, and traffic measurement. Addison-Wesley. \n \nEconomics and networks: \n \nFrank, Robert H. (2008). The economic naturalist: why economics explains almost everything. \n \nPapers: \n \nCertainly, a collection of papers (see ACM CCR which publishes notable network researchers' \nfavourite ten papers every 6 months or so).\n \n\nOVERVIEW OF NATURAL LANGUAGE PROCESSING \n \nAims \nThis course introduces the fundamental techniques of natural language processing. It aims to \nexplain the potential and the main limitations of these techniques. Some current research issues \nare introduced and some current and potential applications discussed and evaluated. Students \nwill also be introduced to practical experimentation in natural language processing. \n \nLectures \nOverview. Brief history of NLP research, some current applications, components of NLP \nsystems. \nMorphology and Finite State Techniques. Morphology in different languages, importance of \nmorphological analysis in NLP, finite-state techniques in NLP. \nPart-of-Speech Tagging and Log-Linear Models. Lexical categories, word tagging, corpora and \nannotations, empirical evaluation. \nPhrase Structure and Structure Prediction. Phrase structures, structured prediction, context-free \ngrammars, weights and probabilities. Some limitations of context-free grammars. \nDependency Parsing. Dependency structure, grammar-free parsing, incremental processing.  \nGradient Descent and Neural Nets. Parameter optimisation by gradient descent. Non-linear \nfunctions with neural network layers. Log-linear model as softmax layer. Current findings of \nNeural NLP. \nWord representations. Representing words with vectors, count-based and prediction-based \napproaches, similarity metrics. \nRecurrent Neural Networks. Modelling sequences, parameter sharing in recurrent neural \nnetworks, neural language models, word prediction. \nCompositional Semantics. Logical representations, compositional semantics, lambda calculus, \ninference and robust entailment. \nLexical Semantics. Semantic relations, WordNet, word senses. \nDiscourse. Discourse relations, anaphora resolution, summarization. \nNatural Language Generation. Challenges of natural language generation (NLG), tasks in NLG, \nsurface realisation. \nPractical and assignments. Students will build a natural language processing system which will \nbe trained and evaluated on supplied data. The system will be built from existing components, \nbut students will be expected to compare approaches and some programming will be required \nfor this. Several assignments will be set during the practicals for assessment. \nObjectives \nBy the end of the course students should: \n \nbe able to discuss the current and likely future performance of several NLP applications; \nbe able to describe briefly a fundamental technique for processing language for several \nsubtasks, such as morphological processing, parsing, word sense disambiguation etc.; \nunderstand how these techniques draw on and relate to other areas of computer science. \nRecommended reading \n\nJurafsky, D. & Martin, J. (2023). Speech and language processing. Prentice Hall (3rd ed. draft, \nonline). \n \nAssessment - Part II Students \nAssignment 1 - 10% of marks \nAssignment 2 - 60% of marks \nAssignment 3 - 30% of marks \nCoursework \nSubmit work for 3 assignments as part of practical sessions on information extraction. \n \nThese include an annotation exercise, a feature-based classifier along with code repository and \ndocumentation, and a 4,000-word report on results and analysis from an extended information \nextraction experiment. \n \nPractical work \nStudents will build a natural language processing system which will be trained and evaluated on \nsupplied data. The system will be built from existing components, but students will be expected \nto compare approaches and some programming will be required for this. \n \nAssessment - Part III and MPhil Students \nAssessment will be based on the practicals: \n \nFirst practical exercise (10%, ticked) \nSecond practical exercise (25%, code repository or notebook with documentation) \nFinal report (65%, 4,000 words, excluding references) \nFurther Information \nAlthough the lectures don't assume any exposure to linguistics, the course will be easier to \nfollow if students have some understanding of basic linguistic concepts. The following may be \nuseful for this: The Internet Grammar of English \n \nCurrent Cambridge undergraduate students who are continuing onto Part III or the MPhil in \nAdvanced Computer Science may only take this module if they did NOT take it as a Unit of \nAssessment in Part II. \n \nStudents from other departments may attend the lectures for this module if space allows. \nHowever students wanting to take it for credit will need to make arrangements for assessment \nwithin their own department. \n \nThis module is shared with Part II of the Computer Science Tripos. Assessment will be adjusted \nfor the two groups of students to be at an appropriate level for whichever course the student is \nenrolled on. Further information about assessment and practicals will follow at the first lecture.\n\nPRACTICAL RESEARCH IN HUMAN-CENTRED AI \n \nAims \nThis is an advanced course in human-computer interaction, with a specialist focus on intelligent \nuser interfaces and interaction with machine-learning and artificial intelligence technologies. The \nformat will be largely Practical, with students carrying out a mini-project involving empirical \nresearch investigation. These studies will investigate human interaction with some kind of \nmodel-based system for planning, decision-making, automation etc. Possible study formats \nmight include: System evaluation, Field observation, Hypothesis testing experiment, Design \nintervention or Corpus analysis, following set examples from recent research publications. \nProject work will be formally evaluated through a report and presentation. \n \nLectures \n(note that Lectures 2-7 also include one hour class discussion of practical work) \n        \u2022 Current research themes in intelligent user interfaces \n        \u2022 Program synthesis \n        \u2022 Mixed initiative interaction \n        \u2022 Interpretability / explainable AI \n        \u2022 Labelling as a fundamental problem \n        \u2022 Machine learning risks and bias \n        \u2022 Visualisation and visual analytics \n        \u2022 Student research presentations \n \nObjectives \nBy the end of the course students should: \n        \u2022 be familiar with current state of the art in intelligent interactive systems \n        \u2022 understand the human factors that are most critical in the design of such systems \n        \u2022 be able to evaluate evidence for and against the utility of novel systems \n        \u2022 have experience of conducting user studies meeting the quality criteria of this field \n        \u2022 be able to write up and present user research in a professional manner \n \nClass Size \nThis module can accommodate upto 20 Part II, Part III and MPhil students. \n \nRecommended reading \nBrad A. Myers and Richard McDaniel (2000). Demonstrational Interfaces: Sometimes You Need \na Little Intelligence, Sometimes You Need a Lot. \n \nAlan Blackwell (2024). Moral Codes: Designing alternatives to AI \n \nAssessment - Part II Students \nThe format will be largely practical, with students carrying out an individual mini-project involving \nempirical research investigation. \n \n\nAssignment 1: six incremental submissions which together contribute 20% to the final module \nmark. \n \nAssignment 2: Final report - 80% of the final module mark \n  \n \nAssessment - MPhil / Part III Students \nThere will be a minor assessment component (20%) in which students compile a reflective diary \nthroughout the term, reporting on the weekly sessions. Diary entries should include citations to \nany key references, notes of possible further reading, summary of key points, questions relevant \nto the personal project, and points of interest noted in relation to the work of other students. \n \nThe major assessment component (80%) will involve a report on research findings, in the style \nof a submission to the ACM CHI or IUI conferences. This work will be submitted incrementally \nthrough the term, in order that feedback can be provided before final assessment of the full \nreport. Phased submissions will cover the following aspects of the empirical study: \n \nResearch question \nMethod \nLiterature Review \nIntroduction \nResults \nDiscussion / Conclusion \nFeedback on each phased submission will include an indicative mark for guidance, but with the \nunderstanding that the final grade will be based on the final delivered report, and that this may \ngo up (or possibly down), depending on how well the student responds to earlier feedback. \n \nReflective diary entries will also be assessed, but graded at a relatively coarse granularity \ncorresponding to the ACS grading bands, and with minimal written feedback. Informal and \ngeneric feedback will be offered verbally in class, and also potentially supplemented with peer \nassessment. \n \nFurther Information \nCurrent Cambridge undergraduate students who are continuing onto Part III or the MPhil in \nAdvanced Computer Science may only take this module if they did NOT take it as a Unit of \nAssessment in Part II. \n \nThis module is shared with Part II of the Computer Science Tripos. Assessment will be adjusted \nfor the two groups of students to be at an appropriate level for whichever course the student is \nenrolled on. Further information about assessment and practicals will follow at the first lecture.\n\nPRINCIPLES OF MACHINE LEARNING SYSTEMS \n \nPrerequisites: It is recommended that students have successfully completed introductory \ncourses, at an undergraduate level, in: 1) operating systems, 2) computer architecture, and 3) \nmachine learning. In addition, this course will heavily focus on deep neural network \nmethodologies and assume familiarity with common neural architectures and related algorithms. \nIf these topics were not covered within the machine learning course taken, students should \nsupplement by reviewing material in a book like: \"Dive into Deep Learning\". Finally, students are \nassumed to be comfortable with programming in Python. \nMoodle, timetable \n \nAims \nThis course will examine the emerging principles and methodologies that underpin scalable and \nefficient machine learning systems. Primarily, the course will focus on an exciting cross-section \nof algorithms and system techniques that are used to support the training and inference of \nmachine learning models under a spectrum of computing systems that range from constrained \nembedded systems up to large-scale distributed systems. It will also touch upon the new \nengineering practices that are developing in support of such systems at scale. When needed to \nappreciate issues of scalability and efficiency, the course will drill down to certain aspects of \ncomputer architecture, systems software and distributed systems and explore how these \ninteract with the usage and deployment of state-of-the-art machine learning. \n \nSyllabus \nTopics covered may include the following, with confirmation a month before the course begins: \n \nSystem Performance Trade-offs \nDistributed Learning Algorithms  \nModel Compression  \nDeep Learning Compilers  \nFrameworks and Run-times  \nScalable Inference Serving  \nDevelopment Practices  \nAutomated Machine Learning  \nFederated Learning  \nPrimarily, topics are covered with conventional lectures. However, where appropriate, material \nwill be delivered through hands-on lab tutorials. Lab tutorials will make use of hardware \nincluding ARM microcontrollers and multi-GPU machines to explore forms of efficient machine \nlearning (any necessary equipment will be provided to students) \n \nAssessment \nEach student will be assessed on 3 labs which will be worth 30% of their grade. They will also \nundertake a written project report which will be worth 70% of the grade. This report will detail an \ninvestigation into a particular aspect of machine learning systems, this report will be made \navailable publicly.\n \n\nPROOF ASSISTANTS \nAims \nThis module introduces students to interactive theorem proving using Isabelle and Coq. It \nincludes techniques for specifying formal models of software and hardware systems and for \nderiving properties of these models. \n \n  \n \nSyllabus \nIntroduction to proof assistants and logic. \nReasoning in predicate logic and typed set theory. \nReasoning in dependent type theory. \nInductive definitions and recursive functions: modelling them in logic, reasoning about them. \nModelling operational semantics definitions and proving properties. \n  \n \nObjectives \nOn completion of this module, students should: \n \npossess good skills in the use of Isabelle and Coq; \nbe able to specify inductive definitions and perform proofs by induction; \nbe able to formalise and reason about a variety of specifications in a proof assistant. \nCoursework \nPractical sessions will allow students to develop skills. Some of the exercises will serve as the \nbasis for assessment. \n \nAssessment \nAssessment will be based on two marked assignments (due in weeks 5 and 8) with students \nperforming small formalisation and verification projects in a proof assistant together with a \nwrite-up explaining their work (word limit 2,500 per project for the write-up). Each of the \nassignments will be worth 100 marks, distributed as follows: \n \n50 marks for completing basic formalisation and verification tasks assessing grasp of the \nmaterial taught in the lecture. Students will submit their work as theory files for either the \nIsabelle or Coq proof assistant, and they will be assessed for correctness and completeness of \nthe specifications and proofs. \n20 marks for completing designated more challenging tasks, requiring the use of advanced \ntechniques or creative proof strategies. \n30 marks for a clear write-up explaining the design decisions made during the formalisation and \nthe strategy used for the proofs, where 10 of these marks will be reserved for write-ups of \nexceptional quality, e.g. demonstrating particular insight. \nThe main tasks in the assignments will be designed to assess the student's proficiency with the \nbasic material and techniques taught in the lectures and the practical sessions, while the more \nchallenging tasks will give exceptional students the opportunity to earn distinction marks \n\n \nRecommended reading \nNipkow, T., Klein, G. (2014). Concrete Semantics with Isabelle/HOL. (The first part of this book, \n\u201cProgramming and Proving in Isabelle/HOL\u201d, comes with the Isabelle distribution.)  \n \nNipkow, T., Paulson, L.C. and Wenzel, M. (2002). A proof assistant for higher-order logic. \nSpringer LNCS 2283.  \n \nThe Software Foundations series of books, in particular the first two: \n \nPierce, B., Azevedo de Amorim, A., Casinghino, C., Gaboardi, M., Greenberg, M., Hri\u0163cu, C., \nSj\u00f6berg, V., Yorgey, B. (2023). Logical Foundations  \nPierce, B., Azevedo de Amorim, A., Casinghino, C., Gaboardi, M., Greenberg, M., Hri\u0163cu, C., \nSj\u00f6berg, V., Tolmach, A., Yorgey, B. (2023). Programming Language Foundations  \nChlipala, A. (2022). Certified programming with dependent types: a pragmatic introduction to the \nCoq proof assistant. MIT Press.  \n \nSergey, I. (2014). Programs and Proofs: Mechanizing Mathematics with Dependent Types.  \n \nAll of these are freely available online.\n \n\nQUANTUM ALGORITHMS AND COMPLEXITY \n \nAims \nThis module is a research-focused introduction to the theory of quantum computing. The aim is \nto prepare the students to conduct research in quantum algorithms and quantum complexity \ntheory. \n \nSyllabus \nTopics will vary from year to year, based on developments in cutting edge research. \nRepresentative topics include: \n \nQuantum algorithms: \n \nQuantum learning theory \nQuantum property testing \nShadow tomography \nQuantum walks \nQuantum state/unitary synthesis. \nStructure vs randomness in quantum algorithms \nQuantum complexity theory: \n \nThe quantum PCP conjecture \nEntanglement and MIP \nState complexity, AdS/CFT, and quantum gravity \nQuantum locally testable codes \nPseudorandom states and unitaries \nQuantum zero-knowledge proofs \nObjectives \nOn completion of this module, students should: \n \nbe familiar with contemporary quantum algorithms \ndevelop an understanding of the power and limitations of quantum computation \nconduct research in quantum algorithms and complexity theory \nAssessment \nMini research project (70%) \nPresentation (20%) \nAttendance and participation (10%) \nTimelines for the assignment submissions:  \n \nsubmit questions/observations on a weekly basis (i.e., Weeks 2-8) \ndeliver talks in Weeks 5-8 \nsubmit their mini-project at the end of term \nRecommended reading material and resources \n\u201cQuantum Computing Since Democritus\u201d by Scott Aaronson \n\n \n\u201cIntroduction to Quantum Information Science\u201d by Scott Aaronson \n \n\u201cQuantum Computing: Lecture Notes\u201d by Ronald de Wolf \n \n\u201cQuantum Computation and Quantum Information\u201d by Nielsen and Chuang\n \n\nUNDERSTANDING QUANTUM ARCHITECTURE \nPrerequisites: Requires introductory linear algebra - concepts such as eigenvalues, Hermitian \nmatrices, unitary matrices. Taking the Part II Quantum Computing course (or similar) is helpful \nbut not required. Familiarity with computer architecture and compilers is helpful. \nMoodle, timetable \n \nAims \nThis course covers the architecture of a practical-scale quantum computer. We will examine the \nresource requirements of practical quantum applications, understand the different layers of the \nquantum stack, the techniques used in these layers and examine how these layers come \ntogether to enable practical quantum advantage over classical computing. \n \nSyllabus \nThe course has two parts: a series of lectures to cover important aspects of the quantum stack \nand a set of student presentations. The following are a list of representative topics: \n \n- Basics of quantum computing \n- The fault-tolerant quantum stack \n- Compilation \n- Instruction sets \n- Implementations of quantum error correction \n- Implementation of magic state distillation \n- Resource estimation \n \nStudent presentations will be based on a reading list of important papers in quantum \narchitecture.  \n  \n \nObjectives \nAt the end of the course, students will have a broad understanding of the quantum computing \nstack. They will understand how major qubit technologies work, design challenges in real \nquantum hardware, how quantum applications are mapped to a system and the importance of \nquantum error correction for scalability.  \n \nAssessment \nSeminar presentation: 20% \nRead one paper from a provided reading list \nPrepare a 20 minute presentation on it + 10 minutes of answering questions. \nThe student presentation should be similar to a conference presentation - convey the problem, \nwhat are prior solutions, what did the paper do, what are the results, what are future directions \nFor the 20% of the score, the score will be split as 15% and 5%. 15% based on how well the \nstudent understands and explains the paper to the rest of the audience. 5% based on the Q&A \nsession. \n \n\n  \n \nCourse project: 80% (split across a proposal, mid-term report, final report) \nA. Research proposal - at least 500 words (10% of total) \nB. Mid-term report - at least 1000 words (including aspects like the research problem, literature \nreview, what questions will be evaluated, any early ideas or methods (20% of total)  \nC. Final report - up to 4 pages double column in a conference paper style with 3000-4000 \nwords. In addition to the mid term report, should include aspects like results and future \ndirections. (50% of the total) \nD. Report on contributions (in case of group work by 2 students) - 1 paragraph on individual \ncontribution of the student. 1 paragraph on teammate's contribution. \nStudents may work alone in the project, in which case they need only components A-C. \nStudents may work in pairs of two, but they should in addition individually submit D. The \ninstructor may conduct a short viva in case contributions from both team members are not clear \nor imbalanced. \nRecommended reading \nNielsen M.A., Chuang I.L. (2010). Quantum Computation and Quantum Information. Cambridge \nUniversity Press. \n \nMermin N.D. (2007). Quantum Computer Science: An Introduction. Cambridge University Press. \n \nAssessing requirements to scale to practical quantum advantage (2022) Beverland et al.\n\n",
        "8th Semester \nADVANCED TOPICS IN CATEGORY THEORY \n \n \nTeaching \nThe teaching style will be lecture-based, but supported by a practical component where \nstudents will learn to use a proof assistant for higher category theory, and build a small portfolio \nof proofs. Towards the end of the course we will explore some of the exciting computer science \nresearch literature on monoidal and higher categories, and students will choose a paper and \npresent it to the class. \n \nAims \nThe module will introduce advanced topics in category theory. The aim is to train students to \nengage and start modern research on the mathematical foundations of higher categories, the \ngraphical calculus, monoids and representations, type theories, and their applications in \ntheoretical computer science, both classical and quantum. \n \nObjectives \nOn completion of this module, students should: \n \nBe familiar with the techniques of compositional category theory. \nHave a strong understanding of basic categorical semantic models. \nBegun exploring current research in monoidal categories and higher structures. \nSyllabus \nPart 1, lecture course: \nThe first part of the course introduces concepts from monoidal categories and higher categories, \nand explores their application in computer science. \n\u2010 Monoidal categories and the graphical calculus \n\u2010 The proof assistant homotopy.io \n\u2010 Coherence theorems and higher category theory \n\u2010 Linearity, superposition, duality, quantum entanglement \n\u2010 Monoids, Frobenius algebras and bialgebras \n\u2010 Type theory for higher category theory \n \nPart 2, exploring the research frontier: \nIn the second part of the course, students choose a research paper to study, and give a \npresentation to the class. \nThere is a nice varied literature related to the topics of the course, and the lecturer will supply a \nlist of suggested papers.  \n \nClasses \nThere will be three exercise sheets for homework, with accompanying classes by a teaching \nassistant to go over them. \n \n\nAssessment \nProblem sheets (50%) \nClass presentation (20%) \nPractical portfolio (30%) \nReading List \nChris Heunen and Jamie Vicary, \u201cCategory for Quantum Theory: An Introduction\u201d, Oxford \nUniversity Press\n \n\nADVANCED TOPICS IN COMPUTER SYSTEMS \n \nAims \nThis module will attempt to provide an overview of \u201csystems\u201d research. This is a very broad field \nwhich has existed for over 50 years and which has historically included areas such as operating \nsystems, database systems, file systems, distributed systems and networking, to name but a \nfew. The course will thus necessarily cover only a tiny subset of the field. \n \nMany good ideas in systems research are the result of discussing and debating previous work. \nA primary aim of this course therefore will be to educate students in the art of critical thinking: \nthe ability to argue for and/or against a particular approach or idea. This will be done by having \nstudents read and critique a set of papers each week. In addition, each week will include \npresentations from a number of participants which aim to advocate or criticise each piece of \nwork. \n \nSyllabus \nThe syllabus for this course will vary from year to year so as to cover a mixture of older and \nmore contemporary systems papers. Contemporary papers will be generally selected from the \npast 5 years, primarily drawn from high quality conferences such as SOSP, OSDI, ASPLOS, \nFAST, NSDI and EuroSys. Example topics might include: \n \nSystems Research and System Design \nOS Structure and Virtual Memory \nVirtualisation \nConsensus \nScheduling \nPrivacy \nData Intensive Computing \nBugs \nThe reading each week will involve a load equivalent to 3 full length papers. Students will be \nexpected to read these in detail and prepare a written summary and review. In addition, each \nweek will contain one or more short presentations by students for each paper. The types of \npresentation will include: \n \nOverview: a balanced presentation of the paper, covering both positive and negative aspects. \nAdvocacy: a positive spin on the paper, aiming to convince others of its value. \nCriticism: a negative take on the paper, focusing on its weak spots and omissions. \nThese presentation roles will be assigned in advance, regardless of the soi disant absolute merit \nof the paper or the preference of the student. Furthermore, all students \u2013 regardless of any \nassigned presentation role in a given week \u2013 will be expected to participate in the class by \nasking questions and generally entering into the debate. \n \nObjectives \n\nOn completion of this module students should have a broad understanding of some key papers \nand concepts in computer systems research, as well as an appreciation of how to argue for or \nagainst any particular idea. \n \nCoursework and practical work \nCoursework will be the production of the weekly paper reviews. Practical work will be presenting \npapers as appropriate, as well as ongoing participation in the class. \n \nAssessment \nAssessment consists of: \n \nOne essay per week for 7 weeks (10% each) \nPresentation (20%) \nParticipation in class over the term (10%) \nRecommended reading \nMost of the reading for this course will be in the form of the selected papers each week. \nHowever, the following may be useful background reading to refresh your knowledge from \nundergraduate courses: \n \nSilberschatz, A., Peterson, J.L. and Galvin, P.C. (2005). Operating systems concepts. \nAddison-Wesley (7th ed.). \n \nTanenbaum, A.S. (2008). Modern Operating Systems. Prentice-Hall (3rd ed.). \n \nBacon, J. and Harris, T. (2003). Operating systems. Addison-Wesley (3rd ed.). \n \nAnderson, T. and Dahlin, M. (2014). Operating Systems: Principles and Practice. Recursive \nBooks (2nd ed.). \n \nHennessy, J. and Patterson, D. (2006). Computer architecture: a quantitative approach. Elsevier \n(4th ed.). ISBN 978-0-12-370490-0. \n \nKleppmann M (2016) Designing Data-Intensive Applications, O'Reilly (1st ed.)\n \n\nADVANCED TOPICS IN MACHINE LEARNING \nAims \nThis course explores current research topics in machine learning in sufficient depth that, at the \nend of the course, participants will be in a position to contribute to research on their chosen \ntopics. Each topic will be introduced with a lecture which, building on the material covered in the \nprerequisite courses, will make the current research literature accessible. Each lecture will be \nfollowed by up to three sessions which will typically be run as a reading group with student \npresentations on recent papers from the literature followed by a discussion, or a practical, or \nsimilar. \n \nStructure \nEach student will attend 3 topics and each topic's sessions will be spread over 5 contact hours. \nStudents will be expected to undertake readings for their selected topics. There will be some \ngroup work. \n \nThere will be a briefing session in Michaelmas term. \n \nSyllabus \nStudents choose five topics in preferential order from a list to be published in Michaelmas term. \nThey will be assigned to three topics out of their list. Students are assessed on one of these \ntopics which may not necessarily be their first choice topic. \n \nThe topics to be offered in 2024-25 are yet to be decided but to give an indicative idea of the \ntypes of topics, the ones offered in 2023-24 were: \n \nImitation learning  Prof A. Vlachos \nThe Future of Large Language Models: data architectures ethics Dr M. Tomalin \nPhysics and Geometry in Machine Learning Dr C. Mishra \nDiffusion Models and SDEs Francisco Vargas, Dr C. H. Ek \nExplainable Artificial Intelligence M. Espinosa, Z. Shams, Prof M. Jamnik \nUnconventional approaches to AI Dr S. Banerjee \nNarratives in Artificial Intelligence and Machine Learning Prof N. Lawrence \nMultimodal Machine Learning K. Hemker, N. Simidjievski, Prof M. Jamnik \nDeep Reinforcement Learning S. Morad, Dr C. H. Ek \nAutomation in Proof Assistants A. Jiang, W. Li, Prof M. Jamnik \nOn completion of this module, students should: \n \nbe in a strong position to contribute to the research topics covered; \nunderstand the fundamental methods (algorithms, data analysis, specific tasks) underlying each \ntopic; \nand be familiar with recent research papers and advances in the field. \nCoursework \nStudents will typically work in groups to give a presentation on assigned papers. Alternatively, a \ntopic may include practical sessions. Each topic will typically consist of one preliminary lecture \n\nfollowed by 3 reading and discussion sessions, or several lectures followed by a practical \nsession. A typical topic can accommodate up to 9 students presenting papers. There will be at \nleast 10 minutes general discussion per session. \n \nFull coursework details will be published by October. \n \nAssessment \nCoursework will be marked by the topic leaders and second marked by the module conveners. \n \nParticipation in all assigned topics, 10% \nPresentation or practical work or similar (for one of the chosen topics), 20% \nTopic coursework (for one of the chosen topics), 70% \nIndividual topic coursework will be published late Michaelmas term. \n \nAssessment criteria for topic coursework will follow project assessment criteria here: \nhttps://www.cl.cam.ac.uk/teaching/exams/acs_project_marking.pdf \n \nPlease note that students will be assessed on one of their three chosen topics but this may not \nbe their first choice. \n \nRecommended reading \nTo be confirmed by each topic convenor.\n \n\nCOMPUTING FOR COLLECTIVE INTELLIGENCE \nPrerequisites: Familiarity with core machine learning paradigms - Basic calculus (analytical \nskills) - Basic discretre optimization (combinatorics) \nMoodle, timetable \n \nObjectives \nThere is a substantial body of academic work demonstrating that complex life is built by \ncooperation across scales: collections of genes cooperate to produce organisms, cells \ncooperate to produce multi-cellular organisms and multicellular animals cooperate to form \ncomplex social groups. Arguably, all intelligence is collective intelligence. Yet, to-date, many \nartificially intelligent agents (both embodied and virtual), are generally not conceived from the \nground up to interact with other intelligent agents (be it machines or humans). The canonical AI \nproblem is that of a monolithic and solitary machine confronting a non-social environment. This \ncourse aims to balance this trend by (i) equipping students with conceptual and practical \nknowledge on collective intelligence from a computational standpoint, and (ii) by conveying \nvarious computational paradigms by which collective intelligence can be modelled as well as \nsynthesized. \n \nAssessment \nThe assessment will be based on a reading-group paper presentation (individual or in pairs), \naccompanied by a 2-page summary, and a technical position paper (individual work) handed in \nafter the course. The position paper will be assessed based on its technical correctness, \nstrength or arguments, \nand clarity of research vision, and will be accompanied by a brief 5-minute pre-recorded talk that \nsummarizes key arguments (any slides used are also submitted as part of the project). \n \nPaper presentation and 1-page summary: 25% \nTechnical position paper: 75% (4000 word limit) \nRecommended reading \nSwarm Intelligence : From Natural to Artificial Systems, Bonabeau et al (1999) \nJoined-Up Thinking, Hannah Critchlow, (2024) \nSupercooperators, Martin Nowak (2011) \nA Course on Cooperative Game Theory, Chakravarty et al., (2015) \nGoverning the Commons: The Evolution of Institutions for Collective Action. Ostrom (1990).  \n \n\nCRYPTOGRAPHY AND PROTOCOL ENGINEERING \nAims \nWe all use cryptographic protocols every day: whenever we access an https:// website or send a \nmessage via WhatsApp or Signal, for example. In this module, students will get hands-on \nexperience of how those protocols are implemented. Students start by writing their own secure \nmessaging protocol from scratch, and then move on to more advanced examples of \ncryptographic protocols for security and privacy, such as private information retrieval (searching \nfor data without revealing what you\u2019re searching for). \n \nSyllabus \nThis is a practical course in which the first half of each of the 2-hour weekly sessions is a lecture \nthat introduces a topic, while the second half is lab time during which the students can work on \ntheir implementations, discuss the topics in small groups, and get help from the lecturers. The \nmodule is structured into three blocks as follows:  \n \nCryptographic primitives (2 weeks). Using common building blocks such as symmetric ciphers \nand hash functions. Implementing selected aspects of elliptic curve Diffie-Hellman and digital \nsignatures. Protocol security, Dolev-Yao model, side-channel attacks.  \nSecure communication (3 weeks). Authenticated key exchange (e.g. SIGMA, TLS), \npassword-authenticated key exchange (e.g. SPAKE2), forward secrecy, post-compromise \nsecurity, double ratchet, the Signal protocol.  \nPrivate database lookups (3 weeks). Lattice-based post-quantum cryptography, learning with \nerrors, homomorphic encryption, private information retrieval.  \nIn each block, students will be given a description of the algorithms and protocols covered (e.g. \na research paper or an RFC standards document), and a code template that the students \nshould use for their implementation. The implementation language will probably be Python (to \nbe confirmed after the practical materials have been developed ). Students will also be given a \nset of test cases to check their code, and will have the opportunity to test their implementation \ncommunicating with other students\u2019 implementations. Finally, for each block, students will write \nand submit a lab report explaining their approach and the findings from their implementation. \n \nObjectives \nOn completion of this module, students should: \n \nUnderstand that cryptography is not magic! The mathematics can look intimidating, but this \nmodule will show students that they needn\u2019t be afraid of cryptography. \nGain appreciation for the complexity and careful implementation of real-world cryptography \nlibraries ,and understand why one should prefer vetted implementations. \nBe familiar with the mathematical notation and concepts commonly used in descriptions of \ncryptographic protocols, and be able to understand and implement research papers and RFCs \nin this field. \nBe comfortable with cryptographic primitives commonly used in protocols, and able to correctly \nuse software libraries that implement them. \n\nHave gained hands-on experience of attacks on cryptographic protocols, and an appreciation \nfor the difficulty of making these protocols correct. \nHave been exposed to ideas and techniques from recent research, which may be useful for their \nown future research. \nHave practised technical writing through lab reports. \nAssessment \nStudents will be provided with code skeletons in one or two different programming languages \n(probably Python, maybe Rust) to avoid them struggling with setup issues. For each \nassignment, students are required to produce a working implementation of the protocols \ndiscussed in the course, using the programming language and skeleton provided. Here, \n\u201cworking\u201d means that the algorithms are functionally correct, but they are not production-quality \n(in particular, no side-channel countermeasures such as constant-time algorithms are required). \nFor some primitives (e.g. symmetric ciphers and hash functions) existing libraries should be \nused, whereas other cryptographic algorithms will be implemented from scratch. Students \nshould submit their code as a Docker container that can be run against specified test cases.  \n \nAfter each of the three blocks, students will be asked to submit a lab report of approximately \n2,000 words along with their code for that block. In the lab report, the students should explain \nhow their implementation works and why it is correct, as well as any findings from the work (e.g. \nany limitations or trade-offs they found with the protocols). The report structure and marking \nscheme will be specified in advance. The lab reports provide an opportunity for students to \nprovide critical insights and creativity. For instance, they might compare their implementation \nwith existing real-world implementations which provide additional features and guarantees.  \n \nEach of the three lab reports (along with the associated code) will be marked, forming the basis \nof assessment, and feedback on the reports will be given to the students before the next \nsubmission is due, so that students can take the feedback on board for their next submission. \nOf the three reports, the worst mark will be discarded, and the other two reports each contribute \n50% of the final mark. As such, students might decide to submit only two reports.  \n \nStudents may discuss their work with others, but the implementations and lab reports must be \nindividual work. \n \nRecommended reading \nTextbook: Jonathan Katz and Yehuda Lindell. Introduction to Modern Cryptography (3rd edition). \nCRC Press, 2020. \n \nThe textbook covers prerequisite knowledge on cryptographic primitives, such as ciphers, \nMACs, hash functions, and signatures. This book is also used in the Part II Cryptography \ncourse. For the protocols we cover in this course, we are not aware of a suitable textbook; \ninstead we will use research papers, lecture slides, and RFCs as resources, which will be \nprovided at the start of the module.\n \n\nDISTRIBUTED LEDGER TECHNOLOGIES: FOUNDATIONS AND APPLICATIONS \n \nAims \nThis reading group course examines foundations and current research into distributed ledger \n(blockchain) technologies and their applications. Students will read, review, and present seminal \nresearch papers in this area. Once completed, students should be able to integrate blockchain \ntechnologies into their own research and gain familiarity with a range of research skills. \n \nLectures \nIntroduction \nConsensus protocols \nBitcoin and its variants \nEthereum, smart contracts, and other permissionless DLTs \nHybrid and permissioned DLTs \nApplications \nLearning objectives \nThere are two broad objectives: to acquire familiarity with a body of work in the area of \ndistributed ledgers and to learn some specific research skills: \n \nHow to read a paper \nHow to review a paper \nHow to analyze a paper\u2019s strengths and weaknesses \nWritten and oral presentation skills \nAssessment \nYou are expected to read all assigned papers and submit paper reviews each week. Each \nreview must either follow the provided review form [PDF] [Latex source]. Each \u201creview\u201d is worth \n5% of your total mark, and is marked out of 100 with 60 a passing grade. Marks will be awarded \nand penalties for late submission applied according to ACS Assessment Guidelines.  \n \nOne paper review for the first week, then two paper reviews each week for 6 weeks (13 reviews, \n5% each) 65% (approx 600 words per review) \nSummative essay 25% (max 3000 words) \nPresentation 5% \n100% attendance in class 5% (2 marks deducted per missed class) \nRecommended Reading \nNarayanan, A. , Bonneau, J., Felten, E., Miller, A. and Goldfeder, S. (2016). Bitcoin and \nCryptocurrency Technologies: A Comprehensive Introduction. Princeton University Press. \n(2016 Draft available here: \nhttps://d28rh4a8wq0iu5.cloudfront.net/bitcointech/readings/princeton_bitcoin_book.pdf)\n \n\nEXPLAINABLE ARTIFICIAL INTELLIGENCE \nPrerequisites: A solid background in statistics, calculus and linear algebra. We strongly \nrecommend some experience with machine learning and deep neural networks (to the level of \nthe first chapters of Goodfellow et al.\u2019s \u201cDeep Learning\u201d). Students are expected to be \ncomfortable reading and writing Python code for the module\u2019s practical sessions. \nMoodle, timetable \n \nAims \nThe recent palpable introduction of Artificial Intelligence (AI) models to everyday \nconsumer-facing products, services, and tools brings forth several new technical challenges and \nethical considerations. Amongst these is the fact that most of these models are driven by Deep \nNeural Networks (DNNs), models that, although extremely expressive and useful, are \nnotoriously complex and opaque. This \u201cblack-box\u201d nature of DNNs limits their ability to be \nsuccessfully deployed in critical scenarios such as those in healthcare and law. Explainable \nArtificial Intelligence (XAI) is a fast-moving subfield of AI that aims to circumvent this crucial \nlimitation of DNNs by either  \n \n(i) constructing human-understandable explanations for their predictions,  \n \nor (ii) designing novel neural architectures that are interpretable by construction.  \n \nIn this module, we will introduce the key ideas behind XAI methods and discuss some of the \nimportant application areas of these methods (e.g., healthcare, scientific discovery, debugging, \nmodel auditing, etc.). We will approach this by focusing on the nature of what constitutes an \nexplanation, and discussing different ways in which explanations can be constructed or learnt to \nbe generated as a by-product of a model. The main aim of this module is to introduce students \nto several commonly used approaches in XAI, both theoretically in lectures and through \nhands-on exercises in practicals, while also bringing recent promising directions within this field \nto their attention. We hope that, by the end of this module, students will be able to directly \ncontribute to XAI research and will understand how methods discussed in this module may be \npowerful tools for their own work and research. \n \nSyllabus \nOverview and taxonomy of XAI (why is explainability needed, definition of terms, taxonomy of \nthe XAI space, etc.)  \nPerturbation-based feature attribution (e.g., LIME, Anchors, SHAP, etc.) \nPropagation-based feature attribution methods (e.g., Relevance Propagation, Saliency \nmethods, etc.) \nConcept-based explainability (Net2Vec, T-CAV, ACE, etc.) \nInterpretable architectures (CBMs and variants, Concept Whitening, SENNs, etc.) \nNeurosymbolic methods (DeepProbLog, Neural Reasoners, etc.) \nSample-based Explanations (Influence functions, ProtoPNets, etc.) \nCounterfactual explanations \nProposed Schedule \n\nThe 16 hours of lectures across 8 weeks will be divided as follows:  \nWeek 1: 1h lecture + 1h lecture  \nWeek 2: 1h lecture + 1h reading group & presentations  \nWeek 3: 1h lecture + 1h reading group & presentations  \nWeek 4: 2h practical  \nWeek 5: 1h lecture + 1h reading group & presentations  \nWeek 6: 2h practical  \nWeek 7: 1h lecture + 1h reading group & presentations  \nWeek 8: 1h reading group & presentation + 1h reading group & presentations \n \nIn weeks where lectures are planned, one-hour lecture slots will intercalate with one hour of \nstudent paper presentations. During each paper presentation session, three students will \npresent a paper related to the topic covered in the earlier lecture that week for about 10 minutes \neach + 5 minutes of questions. At the end of all paper presentations, there will be a discussion \non all the papers. The order of student presentations will be allocated randomly during the first \nweek so that students know in advance when they are expected to present. For the sake of \nfairness, we will release the paper to be presented by each student a week before their \npresentation slot. \n \nObjectives \nBy the end of this module, students should be able to: \n \nRecognise and identify key concepts in XAI together with their connection to related subfields in \nAI such as fairness, accountability, and trustworthy AI. \nUnderstand how to use, design, and deploy model-agnostic perturbation methods such as \nLIME, Anchors, and RISE. In particular, students should understand the connection between \nfeature importance and cooperative game theory, and its uses in methods such as SHAP. \nIdentify the uses and limitations of propagation-based feature importance methods such as \nSaliency, SmoothGrad, GradCAM, and Integrated Gradients. Students should be able to \nimplement each of these methods on their own and connect the theoretical ideas behind them \nto practical code, exploiting modern frameworks\u2019 auto-differentiation. \nUnderstand what concept learning is and what limitations it overcomes compared to traditional \nfeature-based methods. Specifically, students should understand how probing a DNN\u2019s latent \nspace may be exploited for learning useful concepts for explainability. \nReason about the key components of inherently interpretable architectures and neuro-symbolic \nmethods and understand how interpretable neural networks can be designed from first \nprinciples. \nElaborate on what sample-based explanations are and how influence functions and prototypical \narchitectures such as ProtoPNet can be used to construct such explanations. \nExplain what counterfactual explanations are, how they are related to causality, and under which \nconditions they may be useful. \nUpon completion of this module, students will have the technical background and tools to use \nXAI as part of their own research or partake in XAI research itself. Moreover, we hope that by \ndetailing a clear timeline of how this relatively young subfield has developed, students may be \n\nable to better understand what are some fundamental open questions in this area and what are \nsome promising directions that are currently actively being explored. \n  \n \nAssessment \n(10%) student presentation: each student will be randomly assigned a presentation slot at the \nbeginning of the course. We will then distribute a paper for them to present in their slot a week \nbefore the presentation\u2019s scheduled time. Students are expected to prepare a 10-minute \npresentation of their assigned paper where they will present the motivation of their assigned \nwork and discuss the main methodology and findings reported in that paper. We will encourage \nstudents to focus on fully communicating the intuition of the work in their assigned papers and \ntry and connect it with ideas that we have previously discussed in previous lectures. All students \nnot presenting each week will be asked to submit one question pertaining to each paper \npresented that week before the paper presentations. \n \n(20%) practical exercises: We will run two practical sessions where students will be asked to \nperform a series of exercises that require them to use concepts we have introduced in lecture \nup to that point. For each practical session, we will prepare a colab notebook to guide the \nstudent through exercises and we will ask the students to submit their solutions through this \ncolab notebook. We expect students to complete about \u2154 of the exercises in the practical class \nand complete the rest at home as homework. Each practical will be worth 10%. \n \n(70%) mini-project: At the end of week 3, we will hand out a list of papers for students to select \ntheir mini-projects from. Each mini-project will consist of a student selecting a paper from our list \nand reimplementing and expanding the key idea in the paper. We encourage students to be as \ncreative as they want with how they drive their mini-project once the paper has been selected. \nFor example, they can reimplement the technique in the paper and combine it with \nmethodologies from other works we discussed in lecture, or they can apply their paper\u2019s \nmethodology to a new domain, datasets, or setup, where the technique may offer interesting \nand potentially novel insights. We will ask all students to submit a report in a workshop format of \nup to 4,000 words. This report, due roughly a week after Lent term ends, should describe their \nmethodology, experiments, and results. A crucial aspect of this report involves explaining the \nrationale behind different choices in methodology and experiments, as well as elaborating on \nthe choices made and hypotheses tested throughout their mini-project (potentially showing a \ndeep understanding of the work they are basing their mini-project on). To aid students with \nselecting their projects and making progress on them, we will hold regular office hours when \nstudents can come to discuss their progress and questions with us. \n \nRecommended reading \nTextbooks \n \n* Christoph Molnar, \u201cInterpretable Machine Learning\u201d. (2022): \nhttps://christophm.github.io/interpretable-ml-book/ \n \n\nOnline Courses and Tutorial \n \n* Su-in Lee and Ian Cover, \u201cCSEP 590B Explainable AI\u201d University of Washington* Explaining \nMachine Learning Predictions: State-of-the-art, Challenges, and Opportunities \n \n* On Explainable AI: From Theory to Motivation, Industrial Applications, XAI Coding & \nEngineering Practices - AAAI 2022 Turorial \n \nSurvey papers \n \n* Arrieta, Alejandro Barredo, et al. \"Explainable Artificial Intelligence (XAI): Concepts, \ntaxonomies, opportunities and challenges toward responsible AI.\" Information fusion 58 (2020): \n82-115. \n \n* Rudin, Cynthia, et al. \"Interpretable machine learning: Fundamental principles and 10 grand \nchallenges.\" Statistics Surveys 16 (2022): 1-85.\n \n\nFEDERATED LEARNING: THEORY AND PRACTICE \nPrerequisites: It is strongly recommended that students have previously (and successfully) \ncompleted an undergraduate machine learning course - or have equivalent experience through \nopen-source material (e.g., lectures or similar). An example course would be: Part1B \"Artificial \nIntelligence\". Students should feel comfortable with SGD and optimization methods used to train \ncurrent popular neural networks and simple forms of neural network architectures. \nMoodle, timetable \n \nObjectives \nThis course aims to extend the machine learning knowledge available to students in Part I (or \npresent in typical undergraduate degrees in other universities), and allow them to understand \nhow these concepts can manifest in a decentralized setting. The course will consider both \ntheoretical (e.g., decentralized optimization) and practical (e.g., networking efficiency) aspects \nthat combine to define this growing area of machine learning.  \n \nAt the end of the course students should: \n \nUnderstand popular methods used in federated learning \nBe able to construct and scale a simple federated system \nHave gained an appreciation of the core limitations to existing methods, and the approaches \navailable to cope with these issues \nDeveloped an intuition for related technologies like differential privacy and secure aggregation, \nand are able to use them within typical federated settings  \nCan reason about the privacy and security issues with federated systems \nLectures \nCourse Overview. Introduction to Federated Learning. \nDecentralized Optimization. \nStatistical and Systems Heterogeneity. \nVariations of Federated Aggregation. \nSecure Aggregation. \nDifferential Privacy within Federated Systems. \nExtensions to Federated Analytics. \nApplications to Speech, Video, Images and Robotics. \nLab sessions \nFederating a Centralized ML Classifier. \nBehaviour under Heterogeneity. \nScaling a Federated Implementation. \nExploring Privacy with Federated Settings \nRecommended Reading \nReadings will be assigned for each lecture. Readings will be taken from either research papers, \ntutorials, source code or blogs that provide more comprehensive treatment of taught concepts. \n \nAssessment - Part II Students \n\nFour labs are performed during the course, and students receive 10% of their total grade for \nwork done as part of each lab. (For a total of 40% of the total grade from lab work alone). Labs \nwill primarily provide hands-on teaching opportunities, that are then utilized within the lab \nassignment which is completed outside of the lab contact time. MPhil and Part III students will \nbe given additional questions to answer within their version of the lab assignment which will \ndiffer from the assignment given to Part II CST students. \n \nThe remainder of the course grade (60%) will be given based on a hands-on project that applies \nthe concepts taught in lectures and labs. This hands-on project will be assessed based on upon \na combination of source code, related documentation and brief 8-minute pre-recorded talk that \nsummarizes key project elements (any slides used are also submitted as part of the project). \nPlease note, that in the case of Part II CST students, the talk itself is not examinable -- as such \nwill be made optional to those students. \n \nA range of possible practical projects will be described and offered to students to select from, or \nalternatively students may propose their own. MPhil and Part III students will select from a \nproject pool that is separate from those offered to Part II CST students. MPhil and Part III \nprojects will contain a greater emphasis on a research element, and the pre-recorded talks \nprovided by this student group will focus on this research contribution. The project will be \nassessed on the level of student understanding demonstrated, the degree of difficulty, \ncorrectness of implementation -- and for Part III/MPhil students the additional criteria of the \nquality and execution of the research methodology, and depth and quality of results analysis. \n \nThis project can be done individually or in groups -- although individual projects will be strongly \nencouraged. It will be required the project is performed using a code repository that also will \ncontain all documentation -- access to this repository will be shared with course staff (e.g., \nlecturer and TAs). Where needed, marks assigned to students within a group will be \ndifferentiated using this repository as an input. Furthermore if groups are formed, members must \nbe either entirely from Part III/MPhil students or Part II CST, i.e., these two student groups \nshould not mix to form a project group. \n \nProjects will be made available publicly. A maximum word count for written contributions for the \nproject will be enforced.  \n \n  \nAssessment - MPhil / Part III Students \n50% final mini-project. This hands-on project will be assessed based on upon a combination of \nsource code, related documentation and brief 8-minute pre-recorded talk that summarizes key \nproject elements (any slides used are also submitted as part of the project). \n \n30% midterm lab report (a written report of a bit more depth of thought than required in a lab, \nthe report provides the results of experiments -- with the skills to perform these experiments \ncoming from lab sessions),  \n \n\n20% for two individual hands-on labs. Labs will primarily provide hands-on teaching \nopportunities, that are then utilized within the lab assignment which is completed outside of the \nlab contact time. There will also be additional questions to answer within their version of the lab \nassignment. \n \nFor the mini-project, a range of possible practical projects will be described and offered to \nstudents to select from, or alternatively students may propose their own. The project will be \nassessed on the level of student understanding demonstrated, the degree of difficulty, \ncorrectness of implementation, the quality and execution of the research methodology, and \ndepth and quality of results analysis. \n \nThe project can be done individually or in groups -- although individual projects will be strongly \nencouraged. It will be required the project is performed using a code repository that also will \ncontain all documentation -- access to this repository will be shared with course staff (e.g., \nlecturer and TAs). Where needed, marks assigned to students within a group will be \ndifferentiated using this repository as an input. Furthermore if groups are formed, members must \nbe either entirely from Part III or entirely from MPhil students, i.e., these two student groups \nshould not mix to form a project group. \n \nProjects will be made available publicly. A maximum word count for written contributions for the \nproject will be enforced.   \n \nAdvanced Material sessions - MPhil / Part III Students \nThese sessions are mandatory for the MPhil and Part III students. Part II CST students may \nattend if they wish \n \nTopics and announced the first week of class. Selected to extend beyond topics covered in \nlectures and labs. Content is a mixture of sessions run in the lecture room the are either (1) \npresentations by guest lectures by an invited domain expert or (2) class-wide discussions \nregarding one or more related academic papers. Paper discussions will require students to read \nthe paper ahead of the lecture, and a brief discussion primer presentation will be given students \nbefore discussions begin.  \n \nThis module is shared with Part II of the Computer Science Tripos. Assessment will be adjusted \nfor the two groups of students to be at an appropriate level for whichever course the student is \nenrolled on. Further information about assessment and practicals will follow at the first lecture.\n\nGEOMETRIC DEEP LEARNING \n \nPrerequisites: Experience with machine learning and deep neural networks is recommended. \nKnowledge of concepts from graph theory and group theory is useful, although the relevant \nparts will be explicitly retaught. \nMoodle, timetable \n \nAims and Objectives \nMost of the patterns we see in nature can be elegantly reasoned about using spatial \nsymmetries\u2014transformations that leave underlying objects unchanged. This observation has \nhad direct implications for the development of modern deep learning architectures that are \nseemingly able to escape the \u201ccurse of dimensionality\u201d and fit complex high-dimensional tasks \nfrom noisy real-world data. Beyond this, one of the most generic symmetries\u2014the \npermutation\u2014will prove remarkably powerful in building models that reason over graph \nstructured-data, which is an excellent abstraction to reason about naturally-occurring, \nirregularly-structured data. Prominent examples include molecules (represented as graphs of \natoms and bonds, with three-dimensional coordinates provided), social networks and \ntransportation networks. Several already-impacted application areas include traffic forecasting, \ndrug discovery, social network analysis and recommender systems. The module will provide the \nstudents the capability to analyse irregularly- and nontrivially-structured data in an effective way, \nand position geometric deep learning in a proper context with related fields. The main aim of the \ncourse is to enable students to make direct contributions to the field, thoroughly assimilate the \nkey concepts in the area, and draw relevant connections to various other fields (such as NLP, \nFourier Analysis and Probabilistic Graphical Models). We assume only a basic background in \nmachine learning with deep neural networks. \n \nLearning outcomes \nThe framework of geometric deep learning, and its key building blocks: symmetries, \nrepresentations, invariance and equivariance \nFundamentals of processing data on graphs, as well as impactful application areas for graph \nrepresentation learning \nTheoretical principles of graph machine learning: permutation invariance and equivariance \nThe three \"flavours\" of spatial graph neural networks (GNNs) (convolutional, attentional, \nmessage passing) and their relative merits. The Transformer architecture as a special case. \nAttaching symmetries to graphs: CNNs on images, spheres and manifolds, Geometric Graphs \nand E(n)-equivariant GNNs \nRelevant connections of geometric deep learning to various other fields (such as NLP, Fourier \nAnalysis and Probabilistic Graphical Models) \nLectures \nThe lectures will cover the following topics: \n \nLearning with invariances and symmetries: geometric deep learning. Foundations of group \ntheory and representation theory. \n\nWhy study data on graphs? Success stories: drug screening, travel time estimation, \nrecommender systems. Fundamentals of graph data processing: network science, spectral \nclustering, node embeddings. \nPermutation invariance and equivariance on sets and graphs. The principal tasks of node, edge \nand graph classification. Neural networks for point clouds: Deep Sets, PointNet; universal \napproximation properties. \nThe three flavours of spatial GNNs: convolutional, attentional, message passing. Prominent \nexamples: GCN, SGC, ChebyNets, MoNet, GAT, GATv2, IN, MPNN, GraphNets. Tradeoffs of \nusing different GNN variants. \nGraph Rewiring: how to apply GNNs when there is no graph? Links to natural language \nprocessing---Transformers as a special case of attentional GNNs. Representative \nmethodologies for graph rewiring: GDC, SDRF, EGP, DGCNN. \nExpressive power of graph neural networks: the Weisfeiler-Lehman hierarchy. GINs as a \nmaximally expressive GNN. Links between GNNs and graph algorithms: neural algorithmic \nreasoning. \nCombining spatial symmetries with GNNs: E(n)-equivariant GNNs, TFNs, SE(3)-Transformer. A \ndeep dive into AlphaFold 2. \nWorked examples: Circulant matrices on grids, the discrete Fourier transform, and convolutional \nnetworks on spheres. Graph Fourier transform and the Laplacian eigenbasis. \nPracticals \nThe practical is designed to complement the knowledge learnt in lectures and teach students to \nderive additional important results and architectures not directly shown in lectures. The practical \nwill be given as a series of individual exercises (each either code implementation or \nproof/derivation). Each of these exercises can be individually assessed based on a specified \nmark budget. \n \nPossible practical topics include the study of higher-order GNNs and equivariant message \npassing. \n \nAssessment \n(60%) Group Mini-project (writeup) at the end of the course. The mini projects can either be \nself-proposed, or the students can express their preference for one of the provided topics, the \nlist of which will be announced at the start of term. The projects will consist of implementing \nand/or extending graph representation learning models in the literature, applying them to \npublicly available datasets. Students will undertake the project in pairs and submit a joint \nwriteup limited to 4,000 words (in line with other modules); appendix of work logs to be included \nbut ungraded; \n \n(10%) Short presentation and viva: students will give a short presentation to explain their \nindividual contribution to the mini-project and there will be a short viva following. \n \n(30%) Practical work completion. Completing the exercises specified in the practical to a \nsatisfactory standard. The practical assessor should be satisfied that the student derived their \nanswers using insight gained from the course; coupled with original thought, not by simple \n\ncopy-pasting of relevant related work. The students would submit code and a short report, which \nwould then be marked in line with the predetermined mark budget for each practical item. \nThe students will learn how to run advanced architectures on GPU but no specific need for \ndedicated GPU resources. Practicals will be made possible to do on CPU; if required, students \ncan use GPUs on publicly available free services (such as Colab) for their mini-project work. \n \nReferences \nThe course will be based on the following literature: \n \n\"Geometric Deep Learning: Grids, Graphs, Groups, Geodesics, and Gauges\", by Michael \nBronstein, Joan Bruna, Taco Cohen and Petar Veli\u010dkovi\u0107 \n\"Graph Representation Learning\", by Will Hamilton \n\"Deep Learning\", by Ian Goodfellow, Yoshua Bengio and Aaron Courville.\n \n\nMOBILE HEALTH \nAims \nThe course aims to explore how wearable and mobile systems sensors can be used to gather \ndata relevant to understand health, how the data can be analysed with advanced signal \nprocessing and machine learning and the performance of these systems in terms of diagnostics \nand disease progression detection. \n \nSyllabus \nCourse Overview. Introduction to Mobile Health. Evaluation metrics and methodology. Basics of \nSignal Processing. \nInertial Measurement Units, Human Activity Recognition (HAR) and Gait Analysis and Machine \nLearning for IMU data. \nRadios, Bluetooth, GPS and Cellular. Epidemiology and contact tracing, Social interaction \nsensing and applications. Location tracking in health monitoring and public health. \nAudio Signal Processing. Voice and Speech Analysis: concepts and data analysis. Body \nSounds analysis. \nPhotoplethysmogram and Light sensing for health (heart and sleep) \nContactless and wireless behaviour and physiological monitoring \nGenerative Models for Wearable Health Data \nTopical Guest Lectures \nObjectives \nThe course aims to explore how wearable and mobile systems sensors can be used to gather \ndata relevant to understand health, how the data can be analysed with advanced signal \nprocessing and machine learning and the performance of these systems in terms of diagnostics \nand disease progression detection. \n \nRoughly, each lecture contains a theory part about the working of \u201csensor signals\u201d or \u201cdata \nanalysis methods\u201d and an application part which contextualises the concepts. \n \nAt the end of the course students should: Understand how mobile/wearable sensors capture \ndata and their working. Understand different approaches to acquiring and analysing sensor data \nfrom different types of sensors. Understand the concept of signal processing applied to time \nseries data and their practical application in health. Be able to extract sensor data and analyse it \nwith basic signal processing and machine learning techniques. Be aware of the different health \napplications of the various sensor techniques. The course will also touch on privacy and ethics \nimplications of the approaches developed in an orthogonal fashion. \n \nRecommended Reading \nPlease see Course Materials for recommended reading for each session. \n \nAssessment - Part II students \nTwo assignments will be based on two datasets which will be provided to the students: \n \n\nAssignment 1 (shared for Part II and Part III/MPhil): this will be based on a dataset and will be \nworth 40% of the final mark. The task of the assessment will be to perform pre-processing and \nbasic data analysis in a \"colab\" and an answer sheet of no more than 1000 words. \n \nAssignment 2 (Part II): This assignment (worth 60% of the final mark) will be a fuller analysis of \na dataset focusing on machine learning algorithms and metrics. Discussion and interpretation of \nthe findings will be reported in a colab and a report of no more than 1200 words. \n \nAssessment  - Part III and MPhil students \nTwo assignments will be based on datasets which will be provided to the students: \n \nAssignment 1: this will be based on a dataset and will be worth 40% of the final mark. The task \nof the assessment will be to perform pre-processing and basic data analysis in a \"colab\" and an \nanswer sheet of no more than 1000 words. \n \nAssignment 2: The second assignment (worth 60% of the final mark) will be based on multiple \ndatasets. The students will be asked to compare and contract ML algorithms/solutions (trained \nand tested on the different data) and discuss the findings and interpretation in terms of health \ncontext. This will be in the form of a colab and a report of 1800 words. \n \nFurther Information \nCurrent Cambridge undergraduate students who are continuing onto Part III or the MPhil in \nAdvanced Computer Science may only take this module if they did NOT take it as a Unit of \nAssessment in Part II. \n \nThis module is shared with Part II of the Computer Science Tripos. Assessment will be adjusted \nfor the two groups of students to be at an appropriate level for whichever course the student is \nenrolled on. Further information about assessment and practicals will follow at the first lecture.\n\nMULTICORE SEMANTICS AND PROGRAMMING \nPrerequisites: Some familiarity with discrete mathematics (sets, partial orders, etc.) and with \nsequential Java programming will be assumed. Experience with operational semantics and with \nsome concurrent programming would be helpful. \nMoodle, timetable \n \nAims \nIn recent years multiprocessors have become ubiquitous, but building reliable concurrent \nsystems with good performance remains very challenging. The aim of this module is to \nintroduce some of the theory and the practice of concurrent programming, from hardware \nmemory models and the design of high-level programming languages to the correctness and \nperformance properties of concurrent algorithms. \n \nLectures \nPart 1: Introduction and relaxed-memory concurrency [Professor P. Sewell] \n \nIntroduction. Sequential consistency, atomicity, basic concurrent problems. [1 block] \nConcurrency on real multiprocessors: the relaxed memory model(s) for x86, ARM, and IBM \nPower, and theoretical tools for reasoning about x86-TSO programs. [2 blocks] \nHigh-level languages. An introduction to C/C++11 and Java shared-memory concurrency. [1 \nblock] \nPart 2: Concurrent algorithms [Dr T. Harris] \n \nConcurrent programming. Simple algorithms (readers/writers, stacks, queues) and correctness \ncriteria (linearisability and progress properties). Advanced synchronisation patterns (e.g. some \nof the following: optimistic and lazy list algorithms, hash tables, double-checked locking, RCU, \nhazard pointers), with discussion of performance and on the interaction between algorithm \ndesign and the underlying relaxed memory models. [3 blocks] \nResearch topics, likely to include one hour on transactional memory and one guest lecture. [1 \nblock] \nObjectives \nBy the end of the course students should: \n \nhave a good understanding of the semantics of concurrent programs, both at the multprocessor \nlevel and the C/Java programming language level; \nhave a good understanding of some key concurrent algorithms, with practical experience. \nAssessment - Part II Students \nTwo assignments each worth 50% \n \nRecommended reading \nHerlihy, M. and Shavit, N. (2008). The art of multiprocessor programming. Morgan Kaufmann. \n \nCoursework \nCoursework will consist of assessed exercises. \n\n \nPractical work \nPart 2 of the course will include a practical exercise sheet.  The practical exercises involve \nbuilding concurrent data structures and measuring their performance.  The work can be \ncompleted in C, Java, or similar languages. \n \nAssessment - Part III and MPhil Students \nAssignment 1, for 50% of the overall grade. Written coursework comprising a series of questions \non hardware and software relaxed-memory concurrency semantics. \nAssignment 2, for 50% of the overall grade. Written coursework comprising a series of questions \non the design of mutual exclusion locks and shared-memory data structures. \nFurther Information \nCurrent Cambridge undergraduate students who are continuing onto Part III or the MPhil in \nAdvanced Computer Science may only take this module if they did NOT take it as a Unit of \nAssessment in Part II. \n \nThis module is shared with Part II of the Computer Science Tripos. Assessment will be adjusted \nfor the two groups of students to be at an appropriate level for whichever course the student is \nenrolled on. Further information about assessment and practicals will follow at the first lecture.\n\nREINFORCEMENT LEARNING \n \nAims \nThe aim of this module is to introduce students to the foundations of the field of reinforcement \nlearning (RL), discuss state-of-the-art RL methods, incentivise students to produce critical \nanalysis of recent methods, and prompt students to propose novel solutions that address \nshortcomings of existing methods. If judged by the headlines, RL has seen an unprecedented \nsuccess in recent years. However, the vast majority of RL methods still have shortcomings that \nmight not be apparent at first glance. The aim of the course is to inspire students by \ncommunicating the promising aspects of RL, but ensure that students develop the ability to \nproduce a critical analysis of current RL limitations. \n \nObjectives \nStudents will learn the following technical concepts: \n \nfundamental RL terminology and mathematical formalism; a brief history of RL and its \nconnection to neuroscience and biological systems \nRL methods for discrete action spaces, e.g. deep Q-learning and large-scale Monte Carlo Tree \nSearch \nmethods for exploration, modelling uncertainty, and partial observability for RL \nmodern policy gradient and actor-critic methods \nconcepts needed to construct model-based RL and Model Predictive Control methods \napproaches to make RL data-efficient and ways to enable simulation-to-reality transfer \nexamples of fine-tuning foundation models and large language models (LLMs) with human \nfeedback; safe RL concepts; examples of using RL for safety validation \nexamples of using RL for scientific discovery \nStudents will also gain experience with analysing RL methods to uncover their strengths and \nshortcomings, as well as proposing extensions to improve performance. \n \nFinally, students will gain skills needed to create and deliver a successful presentation in a \nformat similar to that of conference presentations. \n \nWith all of the above, students who take part in this module will be well-prepared to start \nconducting research in the field of reinforcement learning. \n  \n \nSyllabus \nTopic 1: Introduction and Fundamentals \n \nOverview of RL: foundational ideas, history, and books; connection to neuroscience and \nbiological systems, recent industrial applications and research demonstrations \nMathematical fundamentals: Markov decision processes, Bellman equations, policy and value \niteration, temporal difference learning \nTopic 2: RL in Discrete Action Spaces \n\n \nQ-learning, function approximation and deep Q-learning; nonstationarity in RL and its \nimplications for deep learning; example applications (video games; initial example: Atari) \nMonte Carlo Tree Search; example applications (AlphaGo) \nTopic 3: Exploration, Uncertainty, Partial Observability \n \nMulti-armed bandits, Bayesian optimisation, regret analysis \nPartially observable Markov decision process; belief, memory, and sequence modelling \n(probabilistic methods, recurrent networks, transformers) \nTopic 4: Policy Gradient and Actor-critic Methods for Continuous Action Spaces \n \nImportance sampling, policy gradient theorem, actor-critic methods (SPG, DDPG) \nProximal policy optimisation; example applications \nTopic 5: Model-based RL and Model Predictive Control \n \nLearning dynamics models (graph networks, stochastic processes, diffusion models, \nphysics-based models, ensembles); planning with learned models \nModel predictive control; example applications (real-time control) \nTopic 6: Data-efficient RL and Simulation-to-reality Transfer \n \nData-efficient learning with probabilistic methods from real data (e.g. policy search in robotics), \nreal-to-sim inference and differentiable simulation, data-efficient simulation-to-reality transfer \nRL for physical systems (successful examples in locomotion, open problems in contact-rich \nmanipulation, applications to logistics, energy, and transport systems); examples of RL for \nhealthcare. \nTopic 7: RL with Human Feedback ; Safe RL and RL for Validation \n \nFine-tuning large language models (LLMs) and other foundation models with human feedback \n(TRLX,RL4LMs, a light-weight overview of RLHF) \nA review of SafeRL, example: optimising commercial HVAC systems using policy improvement \nwith constraints; improving safety using RL for validation: examples in autonomous driving and \nautonomous flying and aircraft collision avoidance \nTopic 8: RL for Scientific Discovery; Student Presentations \n \nExamples of RL for molecular design and drug discovery, active learning for synthesising new \nmaterials, RL for nuclear fusion experiments \nStudent presentations (based on essays and mini-projects) for other topics in RL, e.g. \nmulti-agent RL, hierarchical RL, RL for hyperparameter optimisation and NN architecture \nsearch, RL for multi-task transfer, lifelong RL, RL in biological systems, etc. \nAssessment \nThe assessment for this module consists of: \n \nEssay and mini-project (60%)  \n\nStudents will choose a concrete RL method from the literature, then complete a 2-part essay \ndescribed below: \nEssay Part 1 (1500 words, 20%): Introduce a formal description of the method and explain how \nthe method extended the state-of-the-art at the time of its publication \nEssay Part 2 or a mini-project (2500 words, 40%): Provide a critical analysis of the chosen RL \nmethod. \nPresentation of the essay and mini-project results (15%)  \nstudents will be expected to create and record a 15-minute video presentation of the analysis \ndescribed in their essay and mini-project. \nShort test of RL theory fundamentals (15%)  \nto test the understanding of RL fundamentals (~15 minutes, in-class, closed book) \nParticipation in seminar discussions (10%)  \ntake an active part in the seminars by asking clarifying questions and mentioning related works. \nRecommended Reading \nBooks \n \n[S&B] Reinforcement Learning: An Introduction (second print edition). Richard S. Sutton, \nAndrew G. Barto. [Available from the book\u2019s website as a free PDF updated in 2022] \n \n[CZ] Algorithms for Reinforcement Learning. Csaba Szepesvari. [Available from the book\u2019s \nwebsite as a free PDF updated in 2019] \n \n[MK] Algorithms for Decision Making. Mykel J. Kochenderfer Tim A. WheelerKyle H. Wray. \n[Available from the book\u2019s website as a free PDF updated in 2023] \n \n[DB] Reinforcement Learning and Optimal Control. Dimitri Bertsekas. [Available from the book\u2019s \nwebsite as a free PDF updated in 2023] \n \nPresentation guidelines \n \n[KN] Ten simple rules for effective presentation slides. Kristen M.Naegle. PLoS computational \nbiology 17, no. 12 (2021).\n \n\nTHEORIES OF SOCIO-DIGITAL DESIGN FOR HUMAN-CENTRED AI \n \nPrerequisites: This course is appropriate to any ACS student, and will assist in the development \nof critical thinking, argument and long-form writing skills. Prior experience of essay-based \nhumanities subjects at university or secondary school will be beneficial, but not required. \nMoodle, timetable \n \nAims \nThis module is a theoretically-oriented advanced introduction to the broad field of \nhuman-computer interaction, extending to consider topics such as intelligent user interfaces, \ncritical and speculative design, participatory design, cognitive models of users, human-centred, \nas well as more-than-human-centred design, and others. The course will not address purely \nengineering approaches to the development of user interfaces (unless there is a clear \ntheoretical question being addressed), but allow students to effectively link the political, social, \nand ethical considerations in HCI to specific technical and conceptual design challenges. The \nmodule builds on the Practical Research in Human-Centred AI course offered in Michaelmas \n(developed with researchers at Microsoft Research Cambridge) and a collaboration with the \nLeverhulme Centre for the Future of Intelligence at Cambridge. Participants may include visitors \nfrom these groups and/or interdisciplinary research students and academic guests from other \nUniversity departments, including Cambridge Digital Humanities. \n \nSyllabus \nThe syllabus will remain broadly within the area of human-computer interaction, including \ntheories of design practice and the social contexts of technology use. Individual seminar topics \nwill be selected in response to contemporary and recent research developments, in consultation \nwith members of the class and visiting contributors. \n \nRepresentative topics in the next year are likely to include: \n \nResponsible AI and HCI \nIntersectional design for social justice \nParticipatory design and co-design \nSustainable AI and more-than-human-centred design \nUsefulness and usability of ethical design toolkits \nThe EU AI Act in design practice \nAI and interface design for transparency \nDesigning otherwise: on anarchist HCI \n  \nObjectives \nOn completion of this module, students should have developed facility in discussing and \ncritiquing the aims of their research, especially for an audience drawn from other academic \ndisciplines, including the following skills: \n \nTheoretical motivation and defence of a research question. \n\nConsideration of a research proposal from one or more alternative theoretical perspectives. \nPotential critique of the theoretical basis for a programme of research. \nCoursework \nIn advance of each seminar, all participants must read the essential reading - generally one \nlong, plus one or two short papers. Further reading is suggested for some seminars. Some \nweeks, instead of shorter papers, students will be asked to explore specific design tools or \nresources. \n \nBefore seminars 2-8, each student will submit a written commentary on the paper, either \ngenerated using a large language model (LLM), or written in the style of an LLM, supplemented \nby a short original observation. \n \nEach member of the class must select one of the 8 seminar themes as the subject for a more \nextended critical review essay. The critical review should be written as an academic research \nessay, not in LLM style. \n \nEach seminar will include an informal design exercise to be carried out in small groups. The \npurpose of these exercises is to reflect on a variety of practical design resources, by applying \nthose resources to concrete case studies. \n \nThroughout the course, students will be expected to keep a \"reflective diary\", briefly reporting \nthemes that arise in discussion, reflections on the design exercises, and ways in which the \nreadings relate to their own research interests. \n  \n \nPractical work \nThere is no practical work element in this course. \n \nAssessment \nWritten critical review of a single discussion text (20%) \nReflective diary submitted at the end of the module (60%) \nIn advance of the session, each student will submit a written commentary on the paper, either \ngenerated using a large language model (LLM), or written in the style of an LLM. A short original \nobservation should be added (20%) \nRecommended reading \nTo be assigned during the module, as discussed above. Most set readings will be available \neither from the ACM Digital Library, or from institutional repositories of the authors. \n \nFurther Information \nStudents from the MPhil in the Ethics of AI, Data and Algorithms, MSt in AI, and MPhil in Digital \nHumanities courses also attend the lectures for this module.\n \n\nTHEORY OF DEEP LEARNING \nPrerequisites: A strong background in calculus, probability theory and linear algebra, familiarity \nwith differential equations, optimization and information theory is required. Students need to \nhave studied Deep Neural Networks, familiarity with deep learning terminology (e.g. \narchitectures, benchmark problems) as well as deep learning frameworks (pytorch, jax or \nsimilar) is assumed \nMoodle, timetable \n \nAims \nThe objectives of this course is to expose you to one of the most active contemporary research \ndirections within machine learning: the theory of deep learning (DL). While the first wave of \nmodern DL has focussed on empirical breakthroughs and ever more complex techniques, the \nattention is now shifting to building a solid mathematical understanding of why these techniques \nwork so well in the first place. The purpose of this course is to review this recent progress \nthrough a mixture of reading group sessions and invited talks by leading researchers in the \ntopic, and prepare you to embark on a PhD in modern deep learning research. Compared to \ntypical, non-mathematical courses on deep learning, this advanced module will appeal to those \nwho have strong foundations in mathematics and theoretical computer science. In a way, this \ncourse is our answer to the question \u201cWhat should the world\u2019s best computer science students \nknow about deep learning in 2023?\u201d \n \nLearning Outcomes \nThis course should prepare the best students to start a PhD in the theory and mathematics of \ndeep learning, and to start formulating their own hypotheses in this space. You will be \nintroduced to a range of empirical and mathematical tools developed in recent years for the \nstudy of deep learning behaviour, and you will build an awareness of the main open questions \nand current lines of attack. At the end of the course you will: \n \nbe able to explain why classical learning theory is insufficient to describe the phenomenon of \ngeneralization in DL \nbe able to design and interpret empirical studies aimed at understanding generalization \nbe able to explain the role of overparameterization: be able to use deep linear models as a \nmodel to study implicit regularisation of gradient-based learning \nbe able to state PAC-Bayes and Information-theoretic bounds, and apply them to DL \nbe able to explain the connection between Gaussian processes and neural networks, and will \nbe able to study learning dynamics in the neural tangent kernel (NTK) regime. \nbe able to formulate your own hypotheses about DL and choose tools to prove/test them \nleverage your deeper theoretical understanding to produce more robust, rigorous and \nreproducible solutions to practical machine learning problems. \nSyllabus \nEach week we'll have two to four student-lead presentations about a research paper chosen \nfrom a reading list. The reading list loosely follows the weekly breakdown below (but we adapt it \neach year based as this is an active research area): \n \n\nWeek 1: Introduction to the topic \nWeek 2: Empirical Studies of Deep Learning Phenomena \nWeek 3: Interpolation Regime and \u201cDouble Descent\u201d Phenomena \nWeek 4: Implicit Regularization in Deep Linear Models \nWeek 5: Approximation Theory \nWeek 6: Networks in the Infinite Width Limit \nWeek 7: PAC-Bayes and Information Theoretic Bounds for SGD \nWeek 8: Discussion and Coursework Spotlight Session \n \nAssessment \nStudents will be assessed on the following basis: \n \n20% for presentation/content contributed to the module: Each student will have an opportunity \nto present one of the recommended papers during Weeks 1-7 (30 minute slot including Q&A). \nFor the presentation, students should aim to communicate the core ideas behind the paper, and \nclearly present the results, conclusions, and future directions. Where possible, students are \nencouraged to comment on how the work itself fits into broader research goals. \n10% for active participation (regular attendance and contribution to discussions during the Q&A \nsessions). \n70% for a group project report, with a word limit of 4000. Either (1) an original research \nproposal/report with a hypothesis, review of related literature, and ideally preliminary findings, \nor, (2) reproduction and ideally extension of an existing relevant paper. \nCoursework reports are marked in line with general ACS guidelines, reports receiving top marks \nwill have have demonstrable research value (contain an original research idea, extension of \nexisting work, or a thorough reproduction effort which is valuable to the research community). \nAdditionally, some projects will be suggested during the first weeks of the course, although \nstudents are encouraged to come up with their own ideas. Students may be required to \nparticipate in group projects, with groups of size 2-3 (the class groups will be separated). For \nany given project, individual contributions would be noted for assessment though a viva \ncomponent. \nRelationship with related modules \nThis course can be considered as an advanced follow-up to the Part IIB course on Deep Neural \nNetworks. That course introduces some high level concepts that this course significantly \nexpands on. \n \nThis module complements L48: Machine Learning in the Physical World and L46: Principles of \nMachine Learning Systems, which focus on applications and hardware/systems aspects of ML \nrespectively. \n \nRecommended reading \nPNAS Colloquium on the Science of Deep Learning \nMathematics of Machine Learning book by Marc Deisenroth, Aldo Faisal and Cheng Soon Ong. \nProbabilistic Machine Learning: An Introduction book by Kevin Murphy \nMatus Telgarsky's lecture notes on deep learning theory  \n\nThese are in addition to the papers which will be discussed in the lectures.\n \n\nUNDERSTANDING NETWORKED-SYSTEMS PERFORMANCE \nPrerequisites: A student must possess knowledge comparable to the undergraduate subjects on \nUnix Tools, C/C++ programming and Computer Networks. Optionally; a student will be \nadvantaged by doing an introduction to Computer Systems Modelling; such as the Part 2 \nComputer Systems Modelling subject as well as having a background in measurement \nmethodologies such as that of L50: Introduction to networking and systems measurements \nprovided in the ACS/Part III curriculum. \nMoodle, timetable \n \nAims \nThis is a practical course, actively building and extending software tools to observe the detailed \nbehaviour of transaction-oriented datacenter-like software. Students will observe and \nunderstand sources of user-facing tail latency, including that stemming \nfrom resource contention, cross-program interference, bad software locking, and simple design \nerrors. \nA 2-hour weekly hybrid, lecture/lab format permits students continuous monitored progress with \ncomplexity of tasks building naturally upon the previous weeks learning. \n \nObjectives \nUpon successful completion of the course, students will be able to: \n \nMake order-of-magnitude estimates of software, hardware, and I/O speeds \nMake valid measurements of actual software, hardware, and I/O speeds \nCreate observation facilities, including logs and dashboards, as part of a software system \ndesign \nCreate tracing facilities to fully observe the execution of complex software \nTime-align traces across multiple computers \nDisplay dense tracing information in meaningful ways \nReason about the sources of real-time and transaction delays, including cross-program \nresource interference, remote procedure call delays, and software locking surprises \nFix programs based on the above reasoning, making their response times faster and more \nrobust \n \nSyllabus \nMeasurement \n   Week 1 Intro, Measuring CPU time, rdtsc, Measuring memory access times, Measuring disks, \n   Week 2 gettimeofday, logs Measuring networks, Remote Procedure Calls, Multi-threads, locks \nObservation \n   Week 3 RPC, logs, displaying traces, interference \n   Week 4 Antagonist programs, Logging, dashboards, profiling. \nKUtrace \n   Week 5 Kernel patches, hello world, post-processing \n   Week 6 KUtrace multi-CPU time display \n   Week 7 Client-server KUtrace, with antagonists and interference \n\n   Week 8 Other trace mysteries \n \n \nAssessment \nThis is a Lab-based module; assessment is based upon reports covering guided laboratory work \nperformed each week. Assessment will be via two submissions: \n \n20% assignment deadline week 4 based upon Lab work from Weeks 1-4; word target 1000, limit \n2000 \n80% assignment deadline week 8 based upon lab work from weeks 5-8; word target 2000, limit \n4000 \nThe intent of the first assessment point is to provide rich feedback to students based upon a \n20% assignment permitting focussed and improved work to be executedfor the final assignment. \n \nAll work in this module is expected to be the effort of the student. Enough equipment is \nprovisioned to allow each student an independent set of apparatus. While classmembers may \nfind sharing operational experience valuable all assessment is based upon a students sole \nsubmission based upon. their own experiments and findings. \n \nReading Material \nCore text \n \nRichard L. Sites, Understanding Software Dynamics, Addison-Wesley Professional Computing \nSeries, 2022 \nFurther reading: papers and presentations that you might find interesting. None are required \nreading \u2013 they are well-written and/or informative. \n \nJohn K. Ousterhout et al., A trace-driven analysis of the UNIX 4.2 BSD file system ACM \nSIGOPS Operating Systems Review, Vol. 19, No. 5, Dec, 1985 \nhttps://dl.acm.org/doi/pdf/10.1145/323627.323631 \nLuiz Andr\u00b4e Barroso, Jimmy Clidaras, Urs H\u00a8olzle, The Datacenter as a Computer: An \nIntroduction to the Design of Warehouse-Scale Machines, Second Edition \nJohn L. Hennessy, David A. Patterson, Computer Architecture, A Quantitative Approach 5th \nEdition \nGeorge Varghese, Network Algorithmics. Morgan Kaufmann \nActually keeping datacenter software up and running \u2013 how Google runs production systems \nSite Reliability Engineering Edited by Betsy Beyer, Chris Jones, Jennifer Petoff and Niall \nRichard Murphy free PDF: https://landing.google.com/sre/book.html \nHow NOT to run datacenters (27 minute video, funny/sad) dotScale 2014 - Robert Kennedy - \nLife in the Trenches of healthcare.gov https://www.youtube.com/watch?v=GLQyj-kBRdo \nAn extended (optional) reading list will also be provided. \n \n"
    ],
    "semesters_8027745290339884176": [
        "1st Semester \n \nDATABASES \n \nAims \nThis course introduces basic concepts for database systems as seen from the perspective of \napplication designers. That is, the focus is on the abstractions supported by database \nmanagement systems and not on how those abstractions are implemented. \n \nThe database world is currently undergoing swift and dramatic transformations largely driven by \nInternet-oriented applications and services. Today many more options are available to database \napplication developers than in the past and so it is becoming increasingly difficult to sort fact \nfrom fiction. The course attempts to cut through the fog with a practical approach that \nemphasises engineering tradeoffs that underpin these recent developments and also guide our \nselection of \u201cthe right tool for the job.\u201d \n \nThis course covers three approaches. First, the traditional mainstay of the database industry -- \nthe relational approach -- is described with emphasis on eliminating logical redundancy in data. \nThen two representatives of recent trends are presented -- graph-oriented and \ndocument-oriented databases. The lectures are supported with two Help and Tick sessions, \nwhere students gain hands-on experience and guidance with the Assessed Exercises (ticks). \n \nLectures \nL1 Introduction. What is a database system? What is a data model? Typical DBMS \nconfigurations and variations (fast queries or reliable update). In-core, in secondary store or \ndistributed. \nL2 Conceptual modelling. Entities, relations, E/R diagrams and implementation-independent \nmodelling, weak entities, cardinality. \nL3+4 The relational database model. Implementing E/R models with relational tables. Relational \nalgebra and SQL. Basic query primitives. Update anomalies caused by redundancy. Minimising \nredundancy with normalised schemas. \nL5 Transactions. On-Line Transaction Processing. On-line Analytical Processing. Reliability, \nthroughput, normal forms, ACID, BASE, eventual consistency. \nL6 Documents and semi-structured data. The NoSQL, schema-free movement. XML/JSON. \nKey/value stores. Embracing data redundancy: representing data for fast, application-specific \naccess. Path queries (if time permits). \nL7 Further SQL. Multisets, NULL values, aggregates, transitive closure, expressibility (nested \nqueries and recursive SQL). \nL8 Graph databases. Optimised for processing enormous numbers of nodes and edges. \nImplementing E/R models in a graph-oriented database. Comparison of the presented models. \nObjectives \nAt the end of the course students should \n\n \nbe able to design entity-relationship diagrams to represent simple database application \nscenarios \nknow how to convert entity-relationship diagrams to relational- and graph-oriented \nimplementations \nunderstand the fundamental tradeoff between the ease of updating data and the response time \nof complex queries \nunderstand that no single data architecture can be used to meet all data management \nrequirements \nbe familiar with recent trends in the database area. \nRecommended reading \nLemahieu, W., Broucke, S. van den and Baesens, B. (2018) Principles of database \nmanagement. Cambridge University Press.\n \n\nDIGITAL ELECTRONICS \n \nAims \nThe aims of this course are to present the principles of combinational and sequential digital logic \ndesign and optimisation at a gate level. The use of n and p channel MOSFETs for building logic \ngates is also introduced. \n \nTopics \nIntroduction. Semiconductors to computers. Logic variables. Examples of simple logic. Logic \ngates. Boolean algebra. De Morgan\u2019s theorem. \nLogic minimisation. Truth tables and normal forms. Karnaugh maps. Quine-McCluskey method. \nBinary adders. Half adder, full adder, ripple carry adder, fast carry generation. \nCombinational logic design: further considerations. Multilevel logic. Gate propagation delay. An \nintroduction to timing diagrams. Hazards and hazard elimination. Other ways to implement \ncombinational logic. \nIntroduction to practical classes. Prototyping box. Breadboard and Dual in line (DIL) packages. \nWiring. \nSequential logic. Memory elements. RS latch. Transparent D latch. Master-slave D flip-flop. T \nand JK flip-flops. Setup and hold times. \nSequential logic. Counters: Ripple and synchronous. Shift registers. System timing - setup time \nconstraint, clock skew, metastability. \nSynchronous State Machines. Moore and Mealy finite state machines (FSMs). Reset and self \nstarting. State transition diagrams. Elimination of redundant states - row matching and state \nequivalence/implication table. \nFurther state machines. State assignment: sequential, sliding, shift register, one hot. \nImplementation of FSMs. \nIntroduction to microprocessors. Microarchitecture, fetch, register access, memory access, \nbranching, execution time. \nElectronics, Devices and Circuits. Current and voltage, conductors/insulators/semiconductors, \nresistance, basic circuit theory, the potential divider. Solving non-linear circuits. P-N junction \n(forward and reverse bias), N and p channel MOSFETs (operation and characteristics) and \nn-MOSFET logic, e.g., n-MOSFET inverter. Power consumption and switching time problems \nproblems in n-MOSFET logic. CMOS logic (NOT, NAND and NOR gates), logic families, noise \nmargin. \nObjectives \nAt the end of the course students should \n \nunderstand the relationships between combination logic and boolean algebra, and between \nsequential logic and finite state machines; \nbe able to design and minimise combinational logic; \nappreciate tradeoffs in complexity and speed of combinational designs; \nunderstand how state can be stored in a digital logic circuit; \nknow how to design a simple finite state machine from a specification and be able to implement \nthis in gates and edge triggered flip-flops; \n\nunderstand how to use MOSFETs to build digital logic circuits. \nRecommended reading \n* Harris, D.M. and Harris, S.L. (2013). Digital design and computer architecture. Morgan \nKaufmann (2nd ed.). The first edition is still relevant. \nKatz, R.H. (2004). Contemporary logic design. Benjamin/Cummings. The 1994 edition is more \nthan sufficient. \nHayes, J.P. (1993). Introduction to digital logic design. Addison-Wesley. \n \nBooks for reference: \n \nHorowitz, P. and Hill, W. (1989). The art of electronics. Cambridge University Press (2nd ed.) \n(more analog). \nWeste, N.H.E. and Harris, D. (2005). CMOS VLSI Design - a circuits and systems perspective. \nAddison-Wesley (3rd ed.). \nMead, C. and Conway, L. (1980). Introduction to VLSI systems. Addison-Wesley. \nCrowe, J. and Hayes-Gill, B. (1998). Introduction to digital electronics. Butterworth-Heinemann. \nGibson, J.R. (1992). Electronic logic circuits. Butterworth-Heinemann.\n \n\nDISCRETE MATHEMATICS \n \nAims \nThe course aims to introduce the mathematics of discrete structures, showing it as an essential \ntool for computer science that can be clever and beautiful. \n \nLectures \nProof [5 lectures]. \nProofs in practice and mathematical jargon. Mathematical statements: implication, bi-implication, \nuniversal quantification, conjunction, existential quantification, disjunction, negation. Logical \ndeduction: proof strategies and patterns, scratch work, logical equivalences. Proof by \ncontradiction. Divisibility and congruences. Fermat\u2019s Little Theorem. \n \nNumbers [5 lectures]. \nNumber systems: natural numbers, integers, rationals, modular integers. The Division Theorem \nand Algorithm. Modular arithmetic. Sets: membership and comprehension. The greatest \ncommon divisor, and Euclid\u2019s Algorithm and Theorem. The Extended Euclid\u2019s Algorithm and \nmultiplicative inverses in modular arithmetic. The Diffie-Hellman cryptographic method. \nMathematical induction: Binomial Theorem, Pascal\u2019s Triangle, Fundamental Theorem of \nArithmetic, Euclid\u2019s infinity of primes. \n \nSets [9 lectures]. \nExtensionality Axiom: subsets and supersets. Separation Principle: Russell\u2019s Paradox, the \nempty set. Powerset Axiom: the powerset Boolean algebra, Venn and Hasse diagrams. Pairing \nAxiom: singletons, ordered pairs, products. Union axiom: big unions, big intersections, disjoint \nunions. Relations: composition, matrices, directed graphs, preorders and partial orders. Partial \nand (total) functions. Bijections: sections and retractions. Equivalence relations and set \npartitions. Calculus of bijections: characteristic (or indicator) functions. Finite cardinality and \ncounting. Infinity axiom. Surjections. Enumerable and countable sets. Axiom of choice. \nInjections. Images: direct and inverse images. Replacement Axiom: set-indexed constructions. \nSet cardinality: Cantor-Schoeder-Bernstein Theorem, unbounded cardinality, diagonalisation, \nfixed-points. Foundation Axiom. \n \nFormal languages and automata [5 lectures]. \nIntroduction to inductive definitions using rules and proof by rule induction. Abstract syntax \ntrees. Regular expressions and their algebra. Finite automata and regular languages: Kleene\u2019s \ntheorem and the Pumping Lemma. \n \nObjectives \nOn completing the course, students should be able to \n \nprove and disprove mathematical statements using a variety of techniques; \napply the mathematical principle of induction; \nknow the basics of modular arithmetic and appreciate its role in cryptography; \n\nunderstand and use the language of set theory in applications to computer science; \ndefine sets inductively using rules and prove properties about them; \nconvert between regular expressions and finite automata; \nuse the Pumping Lemma to prove that a language is not regular. \nRecommended reading \nBiggs, N.L. (2002). Discrete mathematics. Oxford University Press (Second Edition). \nDavenport, H. (2008). The higher arithmetic: an introduction to the theory of numbers. \nCambridge University Press. \nHammack, R. (2013). Book of proof. Privately published (Second edition). Available at: \nhttp://www.people.vcu.edu/ rhammack/BookOfProof/index.html \nHouston, K. (2009). How to think like a mathematician: a companion to undergraduate \nmathematics. Cambridge University Press. \nKozen, D.C. (1997). Automata and computability. Springer. \nLehman, E.; Leighton, F.T.; Meyer, A.R. (2014). Mathematics for computer science. Available \non-line. \nVelleman, D.J. (2006). How to prove it: a structured approach. Cambridge University Press \n(Second Edition).\n \n\nFOUNDATIONS OF COMPUTER SCIENCE \nAims \nThe main aim of this course is to present the basic principles of programming. As the \nintroductory course of the Computer Science Tripos, it caters for students from all backgrounds. \nTo those who have had no programming experience, it will be comprehensible; to those \nexperienced in languages such as C, it will attempt to correct any bad habits that they have \nlearnt. \n \nA further aim is to introduce the principles of data structures and algorithms. The course will \nemphasise the algorithmic side of programming, focusing on problem-solving rather than on \nhardware-level bits and bytes. Accordingly it will present basic algorithms for sorting, searching, \netc., and discuss their efficiency using O-notation. Worked examples (such as polynomial \narithmetic) will demonstrate how algorithmic ideas can be used to build efficient applications. \n \nThe course will use a functional language (OCaml). OCaml is particularly appropriate for \ninexperienced programmers, since a faulty program cannot crash and OCaml\u2019s unobtrusive type \nsystem captures many program faults before execution. The course will present the elements of \nfunctional programming, such as curried and higher-order functions. But it will also introduce \ntraditional (procedural) programming, such as assignments, arrays and references. \n \nLectures \nIntroduction to Programming. The role of abstraction and representation. Introduction to integer \nand floating-point arithmetic. Declaring functions. Decisions and booleans. Example: integer \nexponentiation. \nRecursion and Efficiency. Examples: Exponentiation and summing integers. Iteration versus \nrecursion. Examples of growth rates. Dominance and O-Notation. The costs of some \nrepresentative functions. Cost estimation. \nLists. Basic list operations. Append. Na\u00efve versus efficient functions for length and reverse. \nStrings. \nMore on lists. The utilities take and drop. Pattern-matching: zip, unzip. A word on polymorphism. \nThe \u201cmaking change\u201d example. \nSorting. A random number generator. Insertion sort, mergesort, quicksort. Their efficiency. \nDatatypes and trees. Pattern-matching and case expressions. Exceptions. Binary tree traversal \n(conversion to lists): preorder, inorder, postorder. \nDictionaries and functional arrays. Functional arrays. Dictionaries: association lists (slow) versus \nbinary search trees. Problems with unbalanced trees. \nFunctions as values. Nameless functions. Currying. The \u201capply to all\u201d functional, map. \nExamples: The predicate functionals filter and exists. \nSequences, or lazy lists. Non-strict functions such as IF. Call-by-need versus call-by-name. Lazy \nlists. Their implementation in OCaml. Applications, for example Newton-Raphson square roots. \nQueues and search strategies. Depth-first search and its limitations. Breadth-first search (BFS). \nImplementing BFS using lists. An efficient representation of queues. Importance of efficient data \nrepresentation. \n\nElements of procedural programming. Address versus contents. Assignment versus binding. \nOwn variables. Arrays, mutable or not. Introduction to linked lists. \nObjectives \nAt the end of the course, students should \n \nbe able to write simple OCaml programs; \nunderstand the fundamentals of using a data structure to represent some mathematical \nabstraction; \nbe able to estimate the efficiency of simple algorithms, using the notions of average-case, \nworse-case and amortised costs; \nknow the comparative advantages of insertion sort, quick sort and merge sort; \nunderstand binary search and binary search trees; \nknow how to use currying and higher-order functions; \nunderstand how OCaml combines imperative and functional programming in a single language. \nRecommended reading \nJohn Whitington. OCaml from the Very Beginning. (http://ocaml-book.com). \n \nOkasaki, C. (1998). Purely functional data structures. Cambridge University Press.\n \n\nHARDWARE PRACTICAL CLASSES \nThe Hardware Practical Classes accompany the Digital Electronics series of lectures. The aim \nof the Practical Classes is to enable students to get hands-on experience of designing, building, \nand testing and debugging of digital electronic circuits. The labs will take place in the Intel Lab. \nlocated in the William Gates Building (WGB). \n \nThe Digital Electronics lecture series occupies 12 lectures in the first 4 weeks of the Michaelmas \nTerm, while the Practical Classes occupy the latter 6 weeks of the Michaelmas Term and the \nfirst 6 weeks of the Lent Term. If required, extra sessions will be available for any students \nneeding to 'catch-up' on missed sessions or to complete any remaining practical work. \n \nThe Practical Classes take the form of 4 workshops, specifically: \n \nWorkshop 1 \u2013 Electronic Die; \nWorkshop 2 \u2013 Shaft Position Encoder; \nWorkshop 3 \u2013 Debouncing a Switch; \nWorkshop 4 \u2013 Framestore for an LED Array. \nIn general, the workshops require some preparatory work to be done prior to the practical \nsession. These tasks are highlighted at the beginning of each Worksheet. Typically this involves \npreparing a design that you will then build, test and modify during the practical class. Note that \ninsufficient preparation prior to the practical classes may compromise effective use of your time \nin the Intel lab. \n \nIn the Practical Classes you will usually work on your own, and you are expected to complete \none Workshop in each of your scheduled sessions. Demonstrators are available during the \nsessions to assist you with any queries or problems you may have. \n \nImportant: Remember to get your work ticked.\n \n\nINTRODUCTION TO GRAPHICS \nAims \nTo introduce the necessary background, the basic algorithms, and the applications of computer \ngraphics and graphics hardware. \n \nLectures \nBackground. What is an image? Resolution and quantisation. Storage of images in memory. [1 \nlecture] \nRendering. Perspective. Reflection of light from surfaces and shading. Geometric models. Ray \ntracing. [2 lectures] \nGraphics pipeline. Polygonal mesh models. Transformations using matrices in 2D and 3D. \nHomogeneous coordinates. Projection: orthographic and perspective. [1 lecture] \nGraphics hardware and modern OpenGL. GPU rendering. GPU frameworks and APIs. Vertex \nprocessing. Rasterisation. Fragment processing. Working with meshes and textures. Z-buffer. \nDouble-buffering and frame synchronization. [2 lectures] \n  \nHuman vision, colour and tone mapping. Perception of colour. Tone mapping. Colour spaces. [2 \nlectures] \nObjectives \nBy the end of the course students should be able to: \n \nunderstand and apply in practice basic concepts of ray-tracing: ray-object intersection, \nreflections, refraction, shadow rays, distributed ray-tracing, direct and indirect illumination; \ndescribe and explain the following algorithms: z-buffer, texture mapping, double buffering, \nmip-map, and normal-mapping; \nuse matrices and homogeneous coordinates to represent and perform 2D and 3D \ntransformations; understand and use 3D to 2D projection, the viewing volume, and 3D clipping; \nimplement OpenGL code for rendering of polygonal objects, control camera and lighting, work \nwith vertex and fragment shaders; \ndescribe a number of colour spaces and their relative merits. \nexplain the need for tone mapping and colour processing in rendering pipeline. \nRecommended reading \n* Shirley, P. and Marschner, S. (2009). Fundamentals of Computer Graphics. CRC Press (3rd \ned.). \n \nFoley, J.D., van Dam, A., Feiner, S.K. and Hughes, J.F. (1990). Computer graphics: principles \nand practice. Addison-Wesley (2nd ed.). \n \nKessenich, J.M., Sellers, G. and Shreiner, D (2016). OpenGL Programming Guide: The Official \nGuide to Learning OpenGL, Version 4.5 with SPIR-V, [seventh edition and later]\n \n\nOBJECT-ORIENTED PROGRAMMING \n \nAims \nThe goal of this course is to provide students with an understanding of Object-Oriented \nProgramming. Concepts are demonstrated in multiple languages, but the primary language is \nJava. \n \nLecture syllabus \nTypes, Objects and Classes Moving from functional to imperative. Functions, methods. Control \nflow. values, variables and types. Primitive Types. Classes as custom types. Objects vs \nClasses. Class definition, constructors. Static data and methods. \nDesigning Classes Identifying classes. UML class diagrams. Modularity. Encapsulation/data \nhiding. Immutability. Access modifiers. Parameterised types (Generics). \nPointers, References and Memory Pointers and references. Reference types in Java. The call \nstack. The heap. Iteration and recursion. Pass-by-value and pass-by-reference. \nInheritance Inheritance. Casting. Shadowing. Overloading. Overriding. Abstract Methods and \nClasses. \nPolymorphism and Multiple Inheritance Polymorphism in ML and Java. Multiple inheritance. \nInterfaces in Java. \nLifecycle of an Object Constructors and chaining. Destructors. Finalizers. Garbage Collection: \nreference counting, tracing. \nJava Collections and Object Comparison Java Collection interface. Key classes. Collections \nclass. Iteration options and the use of Iterator. Comparing primitives and objects. Operator \noverloading. \nError Handling Types of errors. Limitations of return values. Deferred error handling. Exceptions. \nCustom exceptions. Checked vs unchecked. Inappropriate use of exceptions. Assertions. \nDesignLanguage evolution Need for languages to evolve. Generics in Java. Type erasure. \nIntroduction to Java 8: Lambda functions, functions as values, method references, streams. \nDesign Patterns Introduction to design patterns. Open-closed principle. Examples of Singleton, \nDecorator, State, Composite, Strategy, Observer. [2 lectures] \nObjectives \nAt the end of the course students should \n \nProvide an overview of key OOP concepts that are transferable in mainstream programming \nlanguages; \nGain an understanding of software development approaches adopted in the industry including \nmaintainability, testing, and software design patterns; \nIncrease programming familiarity with Java; \nRecommended reading \n1. Real-World Software Development: A Project-Driven Guide to Fundamentals in Java [Urma et \nal.] \n2. Modern Java in Action (2nd edition) [Urma et al.]. \n3. Java How to Program: early objects [Deitel et al.]\n \n\nOCAML PRACTICAL CLASSES \n \nThe Role of Tickers \nA ticking session is a short (5 minute) one-to-one session with a ticker conducted on Thursday \nafternoons. The role of the ticker is twofold: \n \nto check you understand your solution (and didn\u2019t just have some lucky guesses or copy code \nfrom elsewhere) \nto give you general feedback on ways to improve your code. \nTo those ends a Ticker will typically ask you questions related to the core material of the tick and \ndiscuss your code directly. \n \nDeadline Extensions \nDeadline extensions can be granted for illness or similar reasons. To obtain an extension please \nemail Jon Ludlam in the first instance, clearly stating your justification for an extension and \nCCing your Director of Studies, who will need to support your request.\n \n\nSCIENTIFIC COMPUTING \n \nAims \nThis course is a hands-on introduction to using computers to investigate scientific models and \ndata. \n \nSyllabus \nPython notebooks. Overview of the Python programming language. Use of notebooks for \nscientific computing. \nNumerical computation. Writing fast vectorized code in numpy. Optimization and fitting. \nSimulation. \nWorking with data. Data import. Common ways to summarize and plot data, for univariate and \nmultivariate analysis. \nObjectives \nAt the end of the course students should \n \nbe able to import data, plot it, and summarize it appropriately \nbe able to write fast vectorized code for scientific / data work\n \n\n",
        "2nd Semester \nALGORITHMS 1 \nAims \nThe aim of this course is to provide an introduction to computer algorithms and data structures, \nwith an emphasis on foundational material. \n \nLectures \nSorting. Review of complexity and O-notation. Trivial sorting algorithms of quadratic complexity. \nReview of merge sort and quicksort, understanding their memory behaviour on statically \nallocated arrays. Heapsort. Stability. Other sorting methods including sorting in linear time. \nMedian and order statistics. [Ref: CLRS3 chapters 1, 2, 3, 6, 7, 8, 9] [about 4 lectures] \nStrategies for algorithm design. Dynamic programming, divide and conquer, greedy algorithms \nand other useful paradigms. [Ref: CLRS3 chapters 4, 15, 16] [about 3 lectures] \nData structures. Elementary data structures: pointers, objects, stacks, queues, lists, trees. \nBinary search trees. Red-black trees. B-trees. Hash tables. Priority queues and heaps. [Ref: \nCLRS3 chapters 6, 10, 11, 12, 13, 18] [about 5 lectures]. \nObjectives \nAt the end of the course students should: \n \nhave a thorough understanding of several classical algorithms and data structures; \nbe able to analyse the space and time efficiency of most algorithms; \nhave a good understanding of how a smart choice of data structures may be used to increase \nthe efficiency of particular algorithms; \nbe able to design new algorithms or modify existing ones for new applications and reason about \nthe efficiency of the result. \nRecommended reading \n* Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest and Clifford Stein Introduction to \nAlgorithms, Fourth Edition ISBN 9780262046305 Published: April 5, 2022. \n \nSedgewick, R., Wayne, K. (2011). Algorithms. Addison-Wesley. ISBN 978-0-321-57351-3. \n \nKleinberg, J. and Tardos, \u00c9. (2006). Algorithm design. Addison-Wesley. ISBN \n978-0-321-29535-4. \n \nKnuth, D.A. (2011). The Art of Computer Programming. Addison-Wesley. ISBN \n978-0-321-75104-1.\n \n\nALGORITHMS 2 \n \nAims \nThe aim of this course is to provide an introduction to computer algorithms and data structures, \nwith an emphasis on foundational material. \n \nLectures \nGraphs and path-finding algorithms. Graph representations. Breadth-first and depth-first search. \nSingle-source shortest paths: Bellman-Ford and Dijkstra algorithms. All-pairs shortest paths: \ndynamic programming and Johnson\u2019s algorithms. [About 4 lectures] \nGraphs and subgraphs. Maximum flow: Ford-Fulkerson method, Max-flow min-cut theorem. \nMatchings in bipartite graphs. Minimum spanning tree: Kruskal and Prim algorithms. Topological \nsort. [About 4 lectures] \nAdvanced data structures. Binomial heap. Amortized analysis: aggregate analysis, potential \nmethod. Fibonacci heaps. Disjoint sets. [About 4 lectures] \nObjectives \nAt the end of the course students should: \n \nhave a thorough understanding of several classical algorithms and data structures; \nbe able to analyse the space and time efficiency of most algorithms; \nhave a good understanding of how a smart choice of data structures may be used to increase \nthe efficiency of particular algorithms; \nbe able to design new algorithms or modify existing ones for new applications and reason about \nthe efficiency of the result. \nRecommended reading \n* Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest and Clifford Stein Introduction to \nAlgorithms, Fourth Edition ISBN 9780262046305 Published: April 5, 2022. \n \nSedgewick, R., Wayne, K. (2011). Algorithms. Addison-Wesley. ISBN 978-0-321-57351-3. \n \nKleinberg, J. and Tardos, \u00c9. (2006). Algorithm design. Addison-Wesley. ISBN \n978-0-321-29535-4. \n \nKnuth, D.A. (2011). The Art of Computer Programming. Addison-Wesley. ISBN \n978-0-321-75104-1.\n \n\nMACHINE LEARNING AND REAL-WORLD DATA \n \nAims \nThis course introduces students to machine learning algorithms as used in real-world \napplications, and to the experimental methodology necessary to perform statistical analysis of \nlarge-scale data from unpredictable processes. Students will perform 3 extended practicals, as \nfollows: \n \nStatistical classification: Determining movie review sentiment using Naive Bayes; \nSequence Analysis: Hidden Markov Modelling and its application to a task from biology \n(predicting protein interactions with a cell membrane); \nAnalysis of social networks, including detection of cliques and central nodes. \nSyllabus \nTopic One: Statistical Classification \nIntroduction to sentiment classification. \nNaive Bayes parameter estimation. \nStatistical laws of language. \nStatistical tests for classification tasks. \nCross-validation and test sets. \nUncertainty and human agreement. \nTopic Two: Sequence Analysis  \nHidden Markov Models (HMM) and HMM training. \nThe Viterbi algorithm. \nUsing an HMM in a biological application. \nTopic Three: Social Networks  \nProperties of networks: Degree, Diameter. \nBetweenness Centrality. \nClustering using betweenness centrality. \nObjectives \nBy the end of the course students should be able to: \n \nunderstand and program two simple supervised machine learning algorithms; \nuse these algorithms in statistically valid experiments, including the design of baselines, \nevaluation metrics, statistical testing of results, and provision against overtraining; \nvisualise the connectivity and centrality in large networks; \nuse clustering (i.e., a type of unsupervised machine learning) for detection of cliques in \nunstructured networks. \nRecommended reading \nJurafsky, D. and Martin, J. (2008). Speech and language processing. Prentice Hall. \n \nEasley, D. and Kleinberg, J. (2010). Networks, crowds, and markets: reasoning about a highly \nconnected world. Cambridge University Press.\n \n\nOPERATING SYSTEMS \n \nAims \nThe overall aim of this course is to provide a general understanding of the structure and key \nfunctions of the operating system. Case studies will be used to illustrate and reinforce \nfundamental concepts. \n \nLectures \nIntroduction to operating systems. Elementary computer organisaton. Abstract view of an \noperating system. Key concepts: layering, multiplexing, performance, caching, buffering, policies \nversus mechanisms. OS evolution: multi-programming, time-sharing. Booting. [1 lecture] \nProtection. Dual-mode operation. Protecting CPU, memory, I/O, storage. Kernels, micro-kernels, \nand modules. System calls. Virtual machines and containers. Subjects and objects. \nAuthentication. Access matrix: ACLs and capabilities. Covert channels. [1 lecture] \nProcesses. Job/process concepts. Process lifecycle. Process management. Context switching. \nInter-process communication. [1 lecture] \nScheduling. CPU-I/O burst cycle. Scheduling basics: preemption and non-preemption. \nScheduling algorithms: FCFS, SJF, SRTF, Priority, and Round Robin. Multilevel scheduling. [2 \nlectures] \nMemory management. Processes in memory. Logical versus physical addresses. Partitions: \nstatic versus dynamic, free space management, external fragmentation. Segmented memory. \nPaged memory: concepts, internal fragmentation, page tables. Demand paging/segmentation. \nPage replacement strategies: FIFO, OPT, and LRU with approximations including NRU, LFU, \nMFU, MRU. Working set schemes. Thrashing. [3 lectures] \nI/O subsystem. General structure. Polled mode versus interrupt-driven I/O. Programmed I/O \n(PIO) versus Direct Memory Access (DMA). Application I/O interface: block and character \ndevices, buffering, blocking, non-blocking, asynchronous, and vectored I/O. Other issues: \ncaching, scheduling, spooling, performance. [1 lecture] \nFile management. File concept. Directory and storage services. File names and metadata. \nDirectory name-space: hierarchies, DAGs, hard and soft links. File operations. Access control. \nExistence and concurrency control. [1 lecture] \nUnix case study. History. General structure. Unix file system: file abstraction, directories, mount \npoints, implementation details. Processes: memory image, lifecycle, start of day. The shell: \nbasic operation, commands, standard I/O, redirection, pipes, signals. Character and block I/O. \nProcess scheduling. [2 lectures] \nObjectives \nAt the end of the course students should be able to: \n \ndescribe the general structure and purpose of an operating system; \nexplain the concepts of process, address space, and file; \ncompare and contrast various CPU scheduling algorithms; \ncompare and contrast mechanisms involved in memory and storage management; \ncompare and contrast polled, interrupt-driven, and DMA-based access to I/O devices. \nRecommended reading \n\n* Silberschatz, A., Galvin, P.C., and Gagne, G. (2018). Operating systems concepts. Wiley (10th \ned.). Prior editions, at least back to 8th ed., should also be acceptable. \nAnderson, T. and Dahlin, M. (2014). Operating Systems: Principles and Practice. Recursive \nBooks (2nd ed.). \nBacon, J. and Harris, T. (2003). Operating systems. Addison-Wesley (3rd ed.). Currently \nappears out-of-print. \nLeffler, S. (1989). The design and implementation of the 4.3BSD Unix operating system. \nAddison-Wesley. \nMcKusick, M.K., Neville-Neil, G.N. and Watson, R.N.M. (2014) The Design and Implementation \nof the FreeBSD Operating System. Pearson Education. (2nd ed.).\n \n\nINTERACTION DESIGN \nAims \nThe aim of this course is to provide an introduction to interaction design, with an emphasis on \nunderstanding and experiencing the user-centred design process, from conducting user \nresearch and requirements development to implementation and evaluation, while understanding \nthe background to human-computer interaction. \n \nLectures \nCourse overview and user research methods. Introduction to the course and practicals. \nUser-Centred Design. User research methods. \nKeeping usera in mind. User research data analysis. Identifying users and stakeholders. \nRepresenting user goals and activities. Identifying and establishing requirements. \nDesign and prototyping. Methods for exploring the design space. Prototyping and different kinds \nof prototypes. \nVisual and interaction design. Memory, perception, attention, and their implications for \ninteraction design. Modalities of interaction, interaction design patterns, information architecture, \nand their implications for interaction design. \nEvaluation. Practical methods for evaluating designs. Evaluation methods without users. \nEvaluation methods with users. \nCase studies from industry and research. Guest lectures (the topics of these lectures are \nsubject to change). \nObjectives \nBy the end of the course students should \n \nhave a thorough understanding of the user-centred design process and be able to apply it to \ninteraction design; \nbe able to design new user interfaces that are informed by principles of good design, and the \nprinciples of human visual perception, cognition and communication; \nbe able to prototype and implement interactive user interfaces with a strong emphasis on users, \nusability and appearance; \nbe able to evaluate existing or new user interfaces using multiple techniques; \nbe able to compare and contrast different design techniques and to critique their applicability to \nnew domains. \nRecommended reading \n* Preece, J., Rogers, Y. and Sharp, H. (2015). Interaction design. Wiley (4th ed.).\n \n\nINTRODUCTION TO PROBABILITY \n \nAims \nThis course provides an elementary introduction to probability and statistics with applications. \nProbability theory and the related field of statistical inference provide the foundations for \nanalysing and making sense of data. The focus of this course is to introduce the language and \ncore concepts of probability theory. The course will also cover some applications of statistical \ninference and algorithms in order to equip students with the ability to represent problems using \nthese concepts and analyse the data within these principles. \n \nLectures \nPart 1 - Introduction to Probability \n \nIntroduction. Counting/Combinatorics (revision), Probability Space, Axioms, Union Bound. \nConditional probability. Conditional Probabilities and Independence, Bayes\u2019Theorem, Partition \nTheorem \nPart 2 - Discrete Random Variables \n \nRandom variables. Definition of a Random Variable, Probability Mass Function, Cumulative \nDistribution, Expectation. \nProbability distributions. Definition and Properties of Expectation, Variance, different ways of \ncomputing them, Examples of important Distributions (Bernoulli, Binomial, Geometric, Poisson), \nPrimer on Continuous Distributions including Normal and Exponential Distributions. \nMultivariate distributions. Multiple Random Variables, Joint and Marginal Distributions, \nIndependence of Random Variables, Covariance. \nPart 3 - Moments and Limit Theorems \n \nIntroduction. Law of Average, Useful inequalities (Markov and Chebyshef), Weak Law of Large \nNumbers (including Proof using Chebyshef\u2019s inequality), Examples. \nMoments and Central Limit Theorem. Introduction to Moments of Random Variables, Central \nLimit Theorem (Proof using Moment Generating functions), Example. \nPart 4 - Applications/Statistics \n \nStatistics. Classical Parameter Estimation (Maximum-Likelihood-Estimation), bias, sample \nmean, sample variance), Examples (Collision-Sampling, Estimating Population Size). \nAlgorithms. Online Algorithms (Secretary Problem, Odd\u2019s Algorithm). \nObjectives \nAt the end of the course students should \n \nunderstand the basic principles of probability spaces and random variables \nbe able to formulate problems using concepts from probability theory and compute or estimate \nprobabilities \nbe familiar with more advanced concepts such as moments, limit theorems and applications \nsuch as parameter estimation \n\nRecommended reading \n* Ross, S.M. (2014). A First course in probability. Pearson (9th ed.). \n \nBertsekas, D.P. and Tsitsiklis, J.N. (2008). Introduction to probability. Athena Scientific. \n \nGrimmett, G. and Welsh, D. (2014). Probability: an Introduction. Oxford University Press (2nd \ned.). \n \nDekking, F.M., et. al. (2005) A modern introduction to probability and statistics. Springer.\n\nSOFTWARE AND SECURITY ENGINEERING \n \nAims \nThis course aims to introduce students to software and security engineering, and in particular to \nthe problems of building large systems, safety-critical systems and systems that must withstand \nattack by capable opponents. Case histories of failure are used to illustrate what can go wrong, \nwhile current software and security engineering practice is studied as a guide to how failures \ncan be avoided. \n \nLectures \n1. What is a security policy or a safety case? Definitions and examples; one-way flows for both \nconfidentiality and safety properties; separation of duties. Top-down and bottom-up analysis \nmethods. What architecture can do, versus benefits of decoupling policy from mechanism. \n \n2. Examples of safety and security policies. Safety and security usability; the pyramid of harms. \nPredicting and mitigating user errors. The prevention of fraud and error in accounting systems; \nthe safety usability of medical devices. \n \n3. Attitudes to risk: expected  utility, prospect theory, framing, status quo bias. Authority, \nconformity and gender; mental models, affordances and defaults. The characteristics of human \nmemory; forgetting passwords versus guessing them. \n \n4. Security protocols; how to enforce policy using  structured human interaction, cryptography or \nboth. Middleperson attacks.The role of verification and its limitations. \n \n5. Attacks on TLS, from rogue CAs through side channels to Heartbleed. Other types of \nsoftware bugs: syntactic, timing, concurrency, code injection, buffer overflows. Defensive \nprogramming: secure coding, contracts. Fuzzing. \n \n6. The software crisis. Examples of large-scale project failure, such as the London Ambulance \nService system and the NHS National Programme for IT. Intrinsic difficulties with complex \nsoftware. \n \n7. Software engineering as the management of complexity. The software life cycle; requirements \nanalysis methods; modular design; the role of prototyping; the waterfall, spiral and agile models. \n \n8. The economics of software as a Service (SaaS); the impact SaaS has on software \nengineering. Continuous integration, release engineering, behavioural analytics and experiment \nframeworks, rearchitecting systems while in operation. \n \n9. Critical systems: safety as an emergent system property. Examples of catastrophic failure: \nfrom Therac-25 to the Boeing 737Max. The problems of managing redundancy. The overall \nprocess of safety engineering. \n \n\n10. Managing the development of critical systems: tools and methods, individual versus group \nproductivity, economics of testing and agile development, measuring outcomes versus process, \nthe technical and human aspects of management, post-market surveillance and coordinated \ndisclosure. The sustainability of products with software components. \n \nAt the end of the course students should know how writing programs with tough assurance \ntargets, in large teams, or both, differs from the programming exercises they have engaged in \nso far. They should understand the different models of software development described in the \ncourse as well as the value of various development and management tools. They should \nunderstand the development life cycle and its basic economics. They should understand the \nvarious types of bugs, vulnerabilities and hazards, how to find them, and how to avoid \nintroducing them. Finally, they should be prepared for the organizational aspects of their Part IB \ngroup project. \n \nRecommended reading \nAnderson, R. (Third Edition 2020). Security engineering (Part 1 and Chapters 27-28). Wiley. \nAvailable at: http://www.cl.cam.ac.uk/users/rja14/book.html\n \n\n",
        "3rd Semester \nCONCURRENT AND DISTRIBUTED SYSTEMS \n \nAims \nThis course considers two closely related topics, Concurrent Systems and Distributed Systems, \nover 16 lectures. The aim of the first half of the course is to introduce concurrency control \nconcepts and their implications for system design and implementation. The aims of the latter \nhalf of the course are to study the fundamental characteristics of distributed systems, including \ntheir models and architectures; the implications for software design; some of the techniques that \nhave been used to build them; and the resulting details of good distributed algorithms and \napplications. \n \nLectures: Concurrency \nIntroduction to concurrency, threads, and mutual exclusion. Introduction to concurrent systems; \nthreads; interleaving; preemption; parallelism; execution orderings; processes and threads; \nkernel vs. user threads; M:N threads; atomicity; mutual exclusion; and mutual exclusion locks \n(mutexes). \nAutomata Composition. Synchronous and asynchronous parallelism; sequential consistency; \nrendezvous. Safety, liveness and deadlock; the Dining Philosophers; Hardware foundations for \natomicity: test-and-set, load-linked/store-conditional and fence instructions. Lamport bakery \nalgorithm. \nCommon design patterns: semaphores, producer-consumer, and MRSW. Locks and invariants; \nsemaphores; condition synchronisation; N-resource allocation; two-party and generalised \nproducer-consumer; Multi-Reader, Single-Write (MRSW) locks. \nCCR, monitors, and concurrency in practice. Conditional critical regions (CCR); monitors; \ncondition variables; signal-wait vs. signal-continue semantics; concurrency in practice (kernels, \npthreads, Java, Cilk, OpenMP). \nDeadlock and liveness guarantees Offline vs. online; model checking; resource allocation \ngraphs; lock order checking; deadlock prevention, avoidance, detection, and recovery; livelock; \npriority inversion; auto parallelisation. \nConcurrency without shared data; transactions. Active objects; message passing; tuple spaces; \nCSP; and actor models. Composite operations; transactions; ACID; isolation; and serialisability. \nFurther transactions History graphs; good and bad schedules; isolation vs. strict isolation; \n2-phase locking; rollback; timestamp ordering (TSO); and optimistic concurrency control (OCC). \nCrash recovery, lock-free programming, and transactional memory. Write-ahead logging, \ncheckpoints, and recovery. Lock-free programming. Hardware and software transactional \nmemories. \n  \n \nLectures: Distributed Systems \nIntroduction to distributed systems; RPC. Avantages and challenges of distributed systems; \nunbounded delay and partial failure; network protocols; transparency; client-server systems; \nremote procedure call (RPC); marshalling; interface definition languages (IDLs). \n\nSystem models and faults. Synchronous, partially synchronous, and asynchronous network \nmodels; crash-stop, crash-recovery, and Byzantine faults; failures, faults, and fault tolerance; \ntwo generals problem. \nTime, clocks, and ordering of events. Physical clocks; leap seconds; UTC; clock synchronisation \nand drift; Network Time Protocol (NTP). Causality; happens-before relation. \nLogical time; Lamport clocks; vector clocks. Broadcast (FIFO, causal, total order); gossip \nprotocols. \nReplication. Quorums; idempotence; replica consistency; read-after-write consistency. State \nmachine replication; leader-based replication. \nConsensus and total order broadcast. FLP result; leader election; the Raft consensus algorithm. \nReplica consistency. Two-phase commit; relationship between 2PC and consensus; \nlinearizability; ABD algorithm; eventual consistency; CAP theorem. \nCase studies. Conflict-free Replicated Data Types (CRDTs); collaborative text editing. Google's \nSpanner; TrueTime. \nObjectives \nAt the end of Concurrent Systems portion of the course, students should: \n \nunderstand the need for concurrency control in operating systems and applications, both mutual \nexclusion and condition synchronisation; \nunderstand how multi-threading can be supported and the implications of different approaches; \nbe familiar with the support offered by various programming languages for concurrency control \nand be able to judge the scope, performance implications and possible applications of the \nvarious approaches; \nbe aware that dynamic resource allocation can lead to deadlock; \nunderstand the concept of transaction; the properties of transactions, how they can be \nimplemented, and how their performance can be optimised based on optimistic assumptions; \nunderstand how the persistence properties of transactions are addressed through logging; and \nhave a high-level understanding of the evolution of software use of concurrency in the \noperating-system kernel case study. \nAt the end of the Distributed Systems portion of the course, students should: \n \nunderstand the difference between shared-memory concurrency and distributed systems; \nunderstand the fundamental properties of distributed systems and their implications for system \ndesign; \nunderstand notions of time, including logical clocks, vector clocks, and physical time \nsynchronisation; \nbe familiar with various approaches to data and service replication, as well as the concept of \nreplica consistency; \nunderstand the effects of large scale on the provision of fundamental services and the tradeoffs \narising from scale; \nappreciate the implications of individual node and network communications failures on \ndistributed computation; \nbe aware of a variety of programming models and abstractions for distributed systems, such as \nRPC, middleware, and total order broadcast; \n\nbe familiar with a range of distributed algorithms, such as consensus, causal broadcast, and \ntwo-phase commit. \nRecommended reading \nModern Operating Systems (free PDF available online) by Andrew S Tanenbaum, Herbert Bos \nJava Concurrency in Practice' (2006 but still highly relevant) by Brian Goetz \nKleppmann, M. (2017). Designing data-intensive applications. O\u2019Reilly. \nTanenbaum, A.S. and van Steen, M. (2017). Distributed systems, 3rd edition. available online. \nCachin, C., Guerraoui, R. and Rodrigues, L. (2011) Introduction to Reliable and Secure \nDistributed Programming. Springer (2nd edition).\n \n\nDATA SCIENCE \n \nAims \nThis course introduces fundamental tools for describing and reasoning about data. There are \ntwo themes: designing probability models to describe systems; and drawing conclusions based \non data generated by such systems. \n \nLectures \nSpecifying and fitting probability models. Random variables. Maximum likelihood estimation. \nGenerative and supervised models. Goodness of fit. \nFeature spaces. Vector spaces, bases, inner products, projection. Linear models. Model fitting \nas projection. Design of features. \nHandling probability models. Handling pdf and cdf. Bayes\u2019s rule. Monte Carlo estimation. \nEmpirical distribution. \nInference. Bayesianism. Frequentist confidence intervals, hypothesis testing. Bootstrap \nresampling. \nRandom processes. Markov chains. Stationarity, and drift analysis. Processes with memory. \nLearning a random process. \nObjectives \nAt the end of the course students should \n \nbe able to formulate basic probabilistic models, including discrete time Markov chains and linear \nmodels \nbe familiar with common random variables and their uses, and with the use of empirical \ndistributions rather than formulae \nunderstand different types of inference about noisy data, including model fitting, hypothesis \ntesting, and making predictions \nunderstand the fundamental properties of inner product spaces and orthonormal systems, and \ntheir application to modelling \nRecommended reading \n* F.M. Dekking, C. Kraaikamp, H.P. Lopuha\u00e4, L.E. Meester (2005). A modern introduction to \nprobability and statistics: understanding why and how. Springer. \n \nS.M. Ross (2002). Probability models for computer science. Harcourt / Academic Press. \n \nM. Mitzenmacher and E. Upfal (2005). Probability and computing: randomized algorithms and \nprobabilistic analysis. Cambridge University Press.\n \n\nECAD AND ARCHITECTURE PRACTICAL CLASSES \nAims \nThe aims of this course are to enable students to apply the concepts learned in the Computer \nDesign course. In particular a web based tutor is used to introduce the SystemVerilog hardware \ndescription language, while the remaining practical classes will then allow students to implement \nthe design of components in this language. \n \nPractical Classes \nWeb tutor The first class uses a web based tutor to rapidly teach the SystemVerilog language. \nFPGA design flow Test driven hardware development for FPGA including an embedded \nprocessor and peripherals [3 classes] \nEmbedded system implementation Embedded system implementation on FPGA [3-4 classes] \nObjectives \nGain experience in electronic computer aided design (ECAD) through learning a design-flow for \nfield programmable gate arrays (FPGAs). \nLearn how to interface to peripherals like a touch screen. \nLearn how to debug hardware and software systems in simulation. \nUnderstand how to construct and program a heterogeneous embedded system. \nRecommended reading \n* Harris, D.M. and Harris, S.L. (2007). Digital design and computer architecture: from gates to \nprocessors. Morgan Kaufmann. \n \nPointers to sources of more specialist information are included on the associated course web \npage. \n \nIntroduction \nThe ECAD and Architecture Laboratory sessions are a companion to the Introduction to \nComputer Architecture course. The objective is to provide experience of hardware design for \nFPGA including use of a small embedded processor. It covers hardware design in \nSystemVerilog, embedded software design in RISC-V assembly and C, and use of FPGA tools. \n \nPrerequisites \nThis course assumes familiarity with the material from Part IA Digital Electronics, although we \nwill use automated tools to perform many of the steps you might have done there by hand (for \nexample, logic minimisation). \n \nWe will be using a Linux command-line environment. We provide scripts and guidance on what \nyou need, however you may find it useful to review Unix Tools, in particular basic navigation \nsuch as ls, cd, mkdir, cp, mv. We'll be using the GCC compiler and Makefiles, described in parts \n31-32. We will also be using git for revision control and submission of work for ticking - you \nshould review at least parts 23-25 and 29. \n \nPracticalities \n\nThe course runs over the 8 weeks of Michaelmas term. The course has been designed so you \ncan do most of the work at home or at any time you wish. You should be able to run all the \nnecessary tools on your own machine, through the use of virtual machines and Docker \ncontainers. We have a number of routes to do this in case some of them aren't suitable for the \nmachine you have. \n \nWe're running the course in a hybrid mode this year. We will provide online help sessions via \nMicrosoft Teams as well as in-person lab sessions on Fridays and Tuesdays 2-5pm during term. \n \nThe core components of the course, being the RISC-V architecture and software and ticked \nexercise, can be done entirely online, without needing access to hardware. They can be \nundertaken with any of the platforms we support, including MCS Linux (on the Intel Lab \nworkstations and remotely) if your own machine isn't suitable. We expect everyone to complete \nthese parts. \n \nThe rest of the course is optional. The hardware simulation and FPGA compilation can be done \non your machine without access to hardware, if you are able to run the virtual machine. \n \nThe parts that require access to an FPGA board will need the ability to run the virtual machine \nand you be able to collect and return an FPGA board from the Department. That process will be \ndescribed when you come to those parts. There is an optional starred tick available for \ncompletion of this part, which will need assessment by a demonstrator in person. \n \nWe'll do our best to support all the different platforms and variations, but please bear with us - \nit's quite possible we'll come across a problem we haven't seen before! \n \nWe have provided four routes you can use different tools to complete the course. Some routes \nnecessarily exclude some material but this material is optional. \n \nAssessment \nIn a similar way to other practical courses, this course carries one 'tick'. Everyone should expect \nto receive this tick, and those who do not should expect to lose marks on their final exams. \n \nThis year we have adopted the 'chime' autoticker used by Further Java. You will check out the \ninitial files as a git repository from the chime server, commit your work, and push the repository \nback to submit it. You can then press a button to run tests against your code and the autoticker \nwill tell you whether you have passed. Additionally you may be selected for a (real/video) \ninterview with a ticker to discuss your code and understanding of the material. \n \nTicking procedures will only be available during weeks 1-8 of Michaelmas term. The deadline for \nsubmitting Tick 1 is 5pm on the Tuesday of week 6 (19 November 2024). After passing the \nautoticker you may be asked for a tick interview with a demonstrator (online or in person). You \ncan gain Tick 1* during the timetabled lab sessions, assuming demonstrators are available. \n \n\nIf you do not submit a successful Tick 1 to the autoticker by 5pm on the Tuesday of week 6 you \ncan self-certify an extension. If you need more time than this you will need to ask your Directory \nof Studies to organise a further extension. The hard deadline by which all ticks must be \ncompleted is noon on Friday 31 January 2025 (the Head of Department Notices is the definitive \ndocument). Extensions beyond this date due to exceptional circumstances require a formal \napplication from your college to the Examiners. \n \nScheduling \nWhile the full course contains 8 exercises, there is not a tight binding to doing one exercise per \nweek. Students should expect to spend about three hours per week on the course, however it is \nrecommended that you make a start on the next exercise if you have time in hand. \n \nDemonstrator help will be focused on the Tuesday and Friday 2-5pm slots during term the lab \nwould normally operate in, so you may find it helpful to work in these times as that will provide \nthe quickest response to questions. \n \nCollaboration \nWhen we have done these labs in person, we have often found they worked well when doing \nthem collaboratively. While your work must be your own, students can work together and help \neach other, and demonstrators contribute advice and experience with the tools. \n \nFor the timetabled sessions, which are Tuesdays and Fridays 2-5pm UK time during term, there \nwill be demonstrators available in the lab and at the same time we'll use the ECAD group on \nMicrosoft Teams. With this we can provide online audio/video help, screensharing and (for the \nWindows and possibly Mac Teams apps) optional remote control of your development \nenvironment. You will be added to the Team in week 1 - if you believe you have been missed \nplease post in the Moodle forum. In Teams there are a number of Helpdesk Channels (A-C) - if \nyou need help during lab times, post in Help Centre and a demonstrator can ask you to go to a \nspecific channel. There are also Student Breakout Channels (D-F) which are available for \nstudents to chat amongst themselves. \n \nFor help outside of timetabled sessions we've set up a Moodle forum where you can post \nquestions - we'll keep an eye on this at other times, but students are encouraged to use it to \nsupport each other. \n \nStudents are of course free to use other platforms to help each other. If you find anything useful \nthat we might use in future, please let us know! \n \nIf you are having difficulty accessing both Moodle and Teams, please email theo.markettos at \ncl.cam.ac.uk. \n \nFeedback \nWe revise the course each year, which may cause new bugs in the code or the notes. The \ncreators (Theo Markettos and Simon Moore) appreciate constructive feedback. \n\n \nCredits \nThis course was written by Theo Markettos and Simon Moore, with portions developed by \nRobert Eady, Jonas Fiala, Paul Fox, Vlad Gavrila, Brian Jones and Roy Spliet. We would like to \nthank Altera, Intel and Terasic for funding and other contributions.\n \n\nECONOMICS, LAW AND ETHICS \n \nAims \nThis course aims to give students an introduction to some basic concepts in economics, law and \nethics. \n \nLectures \nClassical economics and consumer theory. Prices and markets; Pareto efficiency; preferences; \nutility; supply and demand; the marginalist revolution; elasticity; the welfare theorems; \ntransaction costs. \nInformation economics. The discriminating monopolist; marginal costs; effects of technology on \nsupply and demand; competition and information; lock in; real and virtual networks; Metcalfe\u2019s \nlaw; the dominant firm model; price discrimination; bundling; income distribution. \nMarket failure and behavioural economics. Market failure: the business cycle; recession and \ntechnology; tragedy of the commons; externalities; monopoly rents; asymmetric information: the \nmarket for lemons; adverse selection; moral hazard; signalling. Behavioural economics: \nbounded rationality, heuristics and biases; nudge theory; the power of defaults; agency effects. \nAuction theory and game theory. Auction theory: types of auctions; strategic equivalence; the \nrevenue equivalence theorem; the winner\u2019s curse; problems with real auctions; mechanism \ndesign and the combinatorial auction; applicability of auction mechanisms in computer science; \nadvertising auctions. Game theory: the choice between cooperation and conflict; strategic \nforms; dominant strategy equilibrium; Nash equilibrium; the prisoners\u2019 dilemma; evolution of \nstrategies; stag hunt; volunteer\u2019s dilemma; chicken; iterated games; hawk-dove; application to \ncomputer science. \nPrinciples of law. Criminal and civil law; contract law; choice of law and jurisdiction; arbitration; \ntort; negligence; defamation; intellectual property rights. \nLaw and the Internet. Computer evidence; the General Data Protection Regulation; UK laws that \nspecifically affect the Internet; e-commerce regulations; privacy and electronic communications. \nPhilosophies of ethics. Authority, intuitionist, egoist and deontological theories; utilitarian and \nRawlsian models; morality; insights from evolutionary psychology, neurology, and experimental \nethics; professional codes of ethics; research ethics. \nContemporary ethical issues. The Internet and social policy; current debates on privacy, \nsurveillance, and censorship; responsible vulnerability disclosure; algorithmic bias; predictive \npolicing; gamification and engagement; targeted political advertising; environmental impacts. \nObjectives \nOn completion of this course, students should be able to: \n \nReflect on and discuss professional, economic, social, environmental, moral and ethical issues \nrelating to computer science \nDefine and explain economic and legal terminology and arguments \nApply the philosophies and theories covered to computer science problems and scenarios \nReflect on the main constraints that market, legislation and ethics place on firms dealing in \ninformation goods and services \nRecommended reading \n\n* Shapiro, C. & Varian, H. (1998). Information rules. Harvard Business School Press. \nHare, S. (2022). Technology is not neutral: A short guide to technology ethics. London \nPublishing Partnership. \n \nFurther reading: \nSmith, A. (1776). An inquiry into the nature and causes of the wealth of nations, available at    \nhttp://www.econlib.org/library/Smith/smWN.html \nThaler, R.H. (2016). Misbehaving. Penguin. \nGalbraith, J.K. (1991). A history of economics. Penguin. \nPoundstone, W. (1992). Prisoner\u2019s dilemma. Anchor Books. \nPinker, S (2011). The Better Angels of our Nature. Penguin. \nAnderson, R. (2008). Security engineering (Chapter 7). Wiley. \nVarian, H. (1999). Intermediate microeconomics - a modern approach. Norton. \nNuffield Council on Bioethics (2015) The collection, linking and use of data in biomedical \nresearch and health care.\n \n\nFURTHER GRAPHICS \n \nAims \nThe course introduces fundamental concepts of modern graphics pipelines. \n \nLectures \nThe order and content of lectures is provisional and subject to change. \n \nGeometry Representations. Parametric surfaces, implicit surfaces, meshes, point-set surfaces, \ngeometry processing pipeline. [1 lecture] \nDiscrete Differential Geometry. Surface normal and curvature, Laplace-Beltrami operator, heat \ndiffusion. [1 lecture] \nGeometry Processing.  Parametrization, filtering, 3D capture. [1 lecture] \nAnimation I. Animation types, animation pipeline, rigging/skinning, character animation. [1 \nlecture] \nAnimation II. Blending transformations, pose-space animation, controllers. [1 lecture] \nThe Rendering Equation. Radiosity, reflection models, BRDFs, local vs. global illumination. [1 \nlecture] \nDistributed Ray Tracing. Quadrature, importance sampling, recursive ray tracing. [1 lecture] \nInverse Rendering. Differentiable rendering, inverse problems. [1 lecture] \nObjectives \nOn completing the course, students should be able to \n \nunderstand and use fundamental 3D geometry/scene representations and operations; \nlearn animation techniques and controls; \nlearn how light transport is modeled and simulated in rendering; \nunderstand how graphics and rendering can be used to solve computer perception problems. \nRecommended reading \nStudents should expect to refer to one or more of these books, but should not find it necessary \nto purchase any of them. \n \nShirley, P. and Marschner, S. (2009). Fundamentals of Computer Graphics. CRC Press (3rd \ned.). \nWatt, A. (2000). 3D Computer Graphics. Addison-Wesley (3rd ed). \nHughes, van Dam, McGuire, Skalar, Foley, Feiner and Akeley (2013). Computer Graphics: \nPrinciples and Practice. Addison-Wesley (3rd edition) \nAkenine-M\u00f6ller, et. al. (2018). Real-time rendering. CRC Press (4th ed.).\n \n\nFURTHER JAVA \n \nAims \nThe goal of this course is to provide students with the ability to understand the advanced \nprogramming features available in the Java programming language, completing the coverage of \nthe language started in the Programming in Java course. The course is designed to \naccommodate students with diverse programming backgrounds; consequently Java is taught \nfrom first principles in a practical class setting where students can work at their own pace from a \ncourse handbook. \n \nObjectives \n \nAt the end of the course students should \n \nunderstand different mechanisms for communication between distributed applications and be \nable to evaluate their trade-offs; \nbe able to use Java generics and annotations to improve software usability, readability and \nsafety; \nunderstand and be able to exploit the Java class-loading mechansim; \nunderstand and be able to use concurrency control correctly; \nbe able to implement a vector clock algorithm and the happens-before relation. \nRecommended reading \n* Goetz, B. (2006). Java concurrency in practice. Addison-Wesley. \nGosling, J., Joy, B., Steele, G., Bracha, G. and Buckley, A. (2014). The Java language \nspecification, Java SE 8 Edition. Addison-Wesley. \nhttp://docs.oracle.com/javase/specs/jls/se8/html/\n \n\nGROUP PROJECTS \nExample Risk Report \nThe risk report should focus on the risks to your project succeeding on time (and not, say, on \ngeneral health and safety points). It is only 50 words. Example: \n \n\"Identified risks: \n * Training data requires access authorisation that has not been approved, and we don\u2019t know if \nclient can do this \n * Two group members have unreliable network connections and may miss meetings \n * One group member is in timezone UTC+8, and will have to attend meetings late at night \n * Bug reports on Github for a library we plan to use suggest maintenance updates will be \nnecessary for recent Android release\"\n \n\nINTRODUCTION TO COMPUTER ARCHITECTURE \n \nAims \nThe aims of this course are to introduce a hardware description language (SystemVerilog) and \ncomputer architecture concepts in order to design computer systems. The parallel ECAD+Arch \npractical classes will allow students to apply the concepts taught in lectures. \n \nLectures \nPart 1 - Gates to processors  \n \nTechnology trends and design challenges. Current technology, technology trends, ECAD trends, \nchallenges. [1 lecture] \nDigital system design. Practicalities of mapping SystemVerilog descriptions of hardware \n(including a processor) onto an FPGA board. Tips and pitfalls when generating larger modular \ndesigns. [1 lecture] \nEight great ideas in computer architecture. [1 lecture] \nReduced instruction set computers and RISC-V. Introduction to the RISC-V processor design. [1 \nlecture] \nExecutable and synthesisable models. [1 lecture] \nPipelining. [2 lectures] \nMemory hierarchy and caching. Caching, etc. [1 lecture] \nSupport for operating systems. Memory protection, exceptions, interrupts, etc. [1 lecture] \nOther instruction set architectures. CISC, stack, accumulator. [1 lecture] \nPart 2  \n \nOverview of Systems-on-Chip (SoCs) and DRAM. [1 lecture] High-level SoCs, DRAM storage \nand accessing. \nMulticore Processors. [2 lectures] Communication, cache coherence, barriers and \nsynchronisation primitives. \nGraphics processing units (GPUs) [2 lectures] Basic GPU architecture and programming. \nFuture Directions [1 lecture] Where is computer architecture heading? \nObjectives \nAt the end of the course students should \n \nbe able to read assembler given a guide to the instruction set and be able to write short pieces \nof assembler if given an instruction set or asked to invent an instruction set; \nunderstand the differences between RISC and CISC assembler; \nunderstand what facilities a processor provides to support operating systems, from memory \nmanagement to software interrupts; \nunderstand memory hierarchy including different cache structures and coherency needed for \nmulticore systems; \nunderstand how to implement a processor in SystemVerilog; \nappreciate the use of pipelining in processor design; \nhave an appreciation of control structures used in processor design; \n\nhave an appreciation of system-on-chips and their components; \nunderstand how DRAM stores data; \nunderstand how multicore processors communicate; \nunderstand how GPUs work and have an appreciation of how to program them. \nRecommended reading \n* Patterson, D.A. and Hennessy, J.L. (2017). Computer organization and design: The \nhardware/software interface RISC-V edition. Morgan Kaufmann. ISBN 978-0-12-812275-4. \n \nRecommended further reading: \n \nHarris, D.M. and Harris, S.L. (2012). Digital design and computer architecture. Morgan \nKaufmann. ISBN 978-0-12-394424-5. \nHennessy, J. and Patterson, D. (2006). Computer architecture: a quantitative approach. Elsevier \n(4th ed.). ISBN 978-0-12-370490-0. (Older versions of the book are also still generally relevant.) \n \nPointers to sources of more specialist information are included in the lecture notes and on the \nassociated course web page\n \n\nPROGRAMMING IN C AND C++ \n \nAims \nThis course aims \n \nto provide a solid introduction to programming in C and to provide an overview of the principles \nand constraints that affect the way in which the C programming language has been designed \nand is used; \nto introduce the key additional features of C++. \n  \n \nLectures \nIntroduction to the C language. Background and goals of C. Types, expressions, control flow \nand strings. \n(continued) Functions. Multiple compilation units. Scope. Segments. Incremental compilation. \nPreprocessor. \n(continued) Pointers and pointer arithmetic. Function pointers. Structures and Unions. \n(continued) Const and volatile qualifiers. Typedefs. Standard input/output. Heap allocation. \nMiscellaneous Features, Hints and Tips. \nC semantics and tools. Undefined vs implementation-defined behaviour. Buffer and integer \noverflows. ASan, MSan, UBsan, Valgrind checkers. \nMemory allocation, data structures and aliasing. Malloc/free, tree and DAG examples and their \ndeallocation using a memory arena. \nFurther memory management. Reference Counting and Garbage Collection \nMemory hierarchy and cache optimization. Data structure layouts. Intrusive lists. Array-of-structs \nvs struct-of-arrays representations. Loop blocking. \nDebugging. Using print statements, assertions and an interactive debugger. Unit testing and \nregression. \nIntroduction to C++. Goals of C++. Differences between C and C++. References versus \npointers. Overloaded functions. \nObjects in C++. Classes and structs. Destructors. Resource Acquisition is Initialisation. Operator \noverloading. Virtual functions. Casts. Multiple inheritance. Virtual base classes. \nOther C++ concepts. Templates and meta-programming. \nObjectives \nAt the end of the course students should \n \nbe able to read and write C programs; \nunderstand the interaction between C programs and the host operating system; \nbe familiar with the structure of C program execution in machine memory; \nunderstand the potential dangers of writing programs in C; \nunderstand the object model and main additional features of C++. \nRecommended reading \n* Kernighan, B.W. and Ritchie, D.M. (1988). The C programming language. Prentice Hall (2nd \ned.).\n \n\nSEMANTICS OF PROGRAMMING LANGUAGES \n \nAims \nThe aim of this course is to introduce the structural, operational approach to programming \nlanguage semantics. It will show how to specify the meaning of typical programming language \nconstructs, in the context of language design, and how to reason formally about semantic \nproperties of programs. \n \nLectures \nIntroduction. Transition systems. The idea of structural operational semantics. Transition \nsemantics of a simple imperative language. Language design options. [2 lectures] \nTypes. Introduction to formal type systems. Typing for the simple imperative language. \nStatements of desirable properties. [2 lectures] \nInduction. Review of mathematical induction. Abstract syntax trees and structural induction. \nRule-based inductive definitions and proofs. Proofs of type safety properties. [2 lectures] \nFunctions. Call-by-name and call-by-value function application, semantics and typing. Local \nrecursive definitions. [2 lectures] \nData. Semantics and typing for products, sums, records, references. [1 lecture] \nSubtyping. Record subtyping and simple object encoding. [1 lecture] \nSemantic equivalence. Semantic equivalence of phrases in a simple imperative language, \nincluding the congruence property. Examples of equivalence and non-equivalence. [1 lecture] \nConcurrency. Shared variable interleaving. Semantics for simple mutexes; a serializability \nproperty. [1 lecture] \nObjectives \nAt the end of the course students should \n \nbe familiar with rule-based presentations of the operational semantics and type systems for \nsome simple imperative, functional and interactive program constructs; \nbe able to prove properties of an operational semantics using various forms of induction \n(mathematical, structural, and rule-based); \nbe familiar with some operationally-based notions of semantic equivalence of program phrases \nand their basic properties. \nRecommended reading \n* Pierce, B.C. (2002). Types and programming languages. MIT Press. \nHennessy, M. (1990). The semantics of programming languages. Wiley. Out of print, but \navailable on the web at \nhttp://www.cs.tcd.ie/matthew.hennessy/splexternal2015/resources/sembookWiley.pdf \nWinskel, G. (1993). The formal semantics of programming languages. MIT Press.\n \n\nUNIX TOOLS \n \nAims \nThis video-lecture course gives students who have already basic Unix/Linux experience some \nadditional practical software-engineering knowledge: how to use the shell and related tools as \nan efficient working environment, how to automate routine tasks, and how version-control and \nautomated-build tools can help to avoid confusion and accidents, especially when working in \nteams. These are essential skills, both in industrial software development and student projects. \n \nLectures \nUnix concepts. Brief review of Unix history and design philosophy, documentation, terminals, \ninter-process communication mechanisms and conventions, shell, command-line arguments, \nenvironment variables, file descriptors. \nShell concepts. Program invocation, redirection, pipes, file-system navigation, argument \nexpansion, quoting, job control, signals, process groups, variables, locale, history and alias \nfunctions, security considerations. \nScripting. Plain-text formats, executables, #!, shell control structures and functions. Startup \nscripts. \nText, file and networking tools. sed, grep, chmod, find, ssh, rsync, tar, zip, etc. \nVersion control. diff, patch, RCS, Subversion, git. \nSoftware development tools. C compiler, linker, debugger, make. \nPerl. Introduction to a powerful scripting and text-manipulation language. [2 lectures] \nObjectives \nAt the end of the course students should \n \nbe confident in performing routine user tasks on a POSIX system, understand command-line \nuser-interface conventions and know how to find more detailed documentation; \nappreciate how simple tools can be combined to perform a large variety of tasks; \nbe familiar with the most common tools, file formats and configuration practices; \nbe able to understand, write, and maintain shell scripts and makefiles; \nappreciate how using a version-control system and fully automated build processes help to \nmaintain reproducibility and audit trails during software development; \nknow enough about basic development tools to be able to install, modify and debug C source \ncode; \nhave understood the main concepts of, and gained initial experience in, writing Perl scripts \n(excluding the facilities for object-oriented programming). \nRecommended reading \nRobbins, A. (2005). Unix in a nutshell. O\u2019Reilly (4th ed.). \nSchwartz, R.L., Foy, B.D. and Phoenix, T. (2011). Learning Perl. O\u2019Reilly (6th ed.).\n \n\n",
        "4th Semester \n \nCOMPILER CONSTRUCTION \n \nAims \nThis course aims to cover the main concepts associated with implementing compilers for \nprogramming languages. We use a running example called SLANG (a Small LANGuage) \ninspired by the languages described in 1B Semantics. A toy compiler (written in ML) is provided, \nand students are encouraged to extend it in various ways. \n \nLectures \nOverview of compiler structure The spectrum of interpreters and compilers; compile-time and \nrun-time. Compilation as a sequence of translations from higher-level to lower-level intermediate \nlanguages, where each translation preserves semantics. The structure of a simple compiler: \nlexical analysis and syntax analysis, type checking, intermediate representations, optimisations, \ncode generation. Overview of run-time data structures: stack and heap. Virtual machines. [1 \nlecture] \nLexical analysis and syntax analysis. Lexical analysis based on regular expressions and finite \nstate automata. Using LEX-tools. How does LEX work? Parsing based on context-free \ngrammars and push-down automata. Grammar ambiguity, left- and right-associativity and \noperator precedence. Using YACC-like tools. How does YACC work? LL(k) and LR(k) parsing \ntheory. [3 lectures] \nCompiler Correctness Recursive functions can be transformed into iterative functions using the \nContinuation-Passing Style (CPS) transformation. CPS applied to a (recursive) SLANG \ninterpreter to derive, in a step-by-step manner, a correct stack-based compiler. [3 lectures] \nData structures, procedures/functions Representing tuples, arrays, references. Procedures and \nfunctions: calling conventions, nested structure, non-local variables. Functions as first-class \nvalues represented as closures. Simple optimisations: inline expansion, constant folding, \nelimination of tail recursion, peephole optimisation. [5 lectures] \nAdvanced topics Run-time memory management (garbage collection). Static and dynamic \nlinking. Objects and inheritance; implementation of method dispatch. Try-catch exception \nmechanisms. Compiling a compiler via bootstrapping. [4 lectures] \nObjectives \nAt the end of the course students should understand the overall structure of a compiler, and will \nknow significant details of a number of important techniques commonly used. They will be \naware of the way in which language features raise challenges for compiler builders. \n \nRecommended reading \n* Aho, A.V., Sethi, R. and Ullman, J.D. (2007). Compilers: principles, techniques and tools. \nAddison-Wesley (2nd ed.). \nMogensen, T. \u00c6. (2011). Introduction to compiler design. Springer. http://www.diku.dk/ \ntorbenm/Basics.\n \n\nCOMPUTATION THEORY \n \nAims \nThe aim of this course is to introduce several apparently different formalisations of the informal \nnotion of algorithm; to show that they are equivalent; and to use them to demonstrate that there \nare uncomputable functions and algorithmically undecidable problems. \n \nLectures \nIntroduction: algorithmically undecidable problems. Decision problems. The informal notion of \nalgorithm, or effective procedure. Examples of algorithmically undecidable problems. [1 lecture] \nRegister machines. Definition and examples; graphical notation. Register machine computable \nfunctions. Doing arithmetic with register machines. [1 lecture] \nUniversal register machine. Natural number encoding of pairs and lists. Coding register machine \nprograms as numbers. Specification and implementation of a universal register machine. [2 \nlectures] \nUndecidability of the halting problem. Statement and proof. Example of an uncomputable partial \nfunction. Decidable sets of numbers; examples of undecidable sets of numbers. [1 lecture] \nTuring machines. Informal description. Definition and examples. Turing computable functions. \nEquivalence of register machine computability and Turing computability. The Church-Turing \nThesis. [2 lectures] \nPrimitive and partial recursive functions. Definition and examples. Existence of a recursive, but \nnot primitive recursive function. A partial function is partial recursive if and only if it is \ncomputable. [2 lectures] \nLambda-Calculus. Alpha and beta conversion. Normalization. Encoding data. Writing recursive \nfunctions in the lambda-calculus. The relationship between computable functions and \nlambda-definable functions. [3 lectures] \nObjectives \nAt the end of the course students should \n \nbe familiar with the register machine, Turing machine and lambda-calculus models of \ncomputability; \nunderstand the notion of coding programs as data, and of a universal machine; \nbe able to use diagonalisation to prove the undecidability of the Halting Problem; \nunderstand the mathematical notion of partial recursive function and its relationship to \ncomputability. \nRecommended reading \n* Hopcroft, J.E., Motwani, R. and Ullman, J.D. (2001). Introduction to automata theory, \nlanguages, and computation. Addison-Wesley (2nd ed.). \n* Hindley, J.R. and Seldin, J.P. (2008). Lambda-calculus and combinators, an introduction. \nCambridge University Press (2nd ed.). \nCutland, N.J. (1980). Computability: an introduction to recursive function theory. Cambridge \nUniversity Press. \nDavis, M.D., Sigal, R. and Weyuker, E.J. (1994). Computability, complexity and languages. \nAcademic Press (2nd ed.). \n\nSudkamp, T.A. (2005). Languages and machines. Addison-Wesley (3rd ed.).\n \n\nCOMPUTER NETWORKING \n \nAims \nThe aim of this course is to introduce key concepts and principles of computer networks. The \ncourse will use a top-down approach to study the Internet and its protocol stack. Instances of \narchitecture, protocol, application-examples will include email, web and media-streaming. We \nwill cover communications services (e.g., TCP/IP) required to support such network \napplications. The implementation and deployment of communications services in practical \nnetworks: including wired and wireless LAN environments, will be followed by a discussion of \nissues of network-management. Throughout the course, the Internet\u2019s architecture and \nprotocols will be used as the primary examples to illustrate the fundamental principles of \ncomputer networking. \n \nLectures \nIntroduction. Overview of networking using the Internet as an example. LANs and WANs. OSI \nreference model, Internet TCP/IP Protocol Stack. Circuit-switching, packet-switching, Internet \nstructure, networking delays and packet loss. [3 lectures] \nLink layer and local area networks. Link layer services, error detection and correction, Multiple \nAccess Protocols, link layer addressing, Ethernet, hubs and switches, Point-to-Point Protocol. [2 \nlectures] \nWireless and mobile networks. Wireless links and network characteristics, Wi-Fi: IEEE 802.11 \nwireless LANs. [1 lecture] \nNetwork layer addressing. Network layer services, IP, IP addressing, IPv4, DHCP, NAT, ICMP, \nIPv6. [3 lectures] \nNetwork layer routing. Routing and forwarding, routing algorithms, routing in the Internet, \nmulticast. [3 lectures] \nTransport layer. Service models, multiplexing/demultiplexing, connection-less transport (UDP), \nprinciples of reliable data transfer, connection-oriented transport (TCP), TCP congestion control, \nTCP variants. [6 lectures] \nApplication layer. Client/server paradigm, WWW, HTTP, Domain Name System, P2P. [1.5 \nlectures] \nMultimedia networking. Networked multimedia applications, multimedia delivery requirements, \nmultimedia protocols (SIP), content distribution networks. [0.5 lecture] \nObjectives \nAt the end of the course students should \n \nbe able to analyse a communication system by separating out the different functions provided \nby the network; \nunderstand that there are fundamental limits to any communications system; \nunderstand the general principles behind multiplexing, addressing, routing, reliable transmission \nand other stateful protocols as well as specific examples of each; \nunderstand what FEC is; \nbe able to compare communications systems in how they solve similar problems; \n\nhave an informed view of both the internal workings of the Internet and of a number of common \nInternet applications and protocols. \nRecommended reading \n* Peterson, L.L. and Davie, B.S. (2011). Computer networks: a systems approach. Morgan \nKaufmann (5th ed.). ISBN 9780123850591 \nKurose, J.F. and Ross, K.W. (2009). Computer networking: a top-down approach. \nAddison-Wesley (5th ed.). \nComer, D. and Stevens, D. (2005). Internetworking with TCP-IP, vol. 1 and 2. Prentice Hall (5th \ned.). \nStevens, W.R., Fenner, B. and Rudoff, A.M. (2003). UNIX network programming, Vol.I: The \nsockets networking API. Prentice Hall (3rd ed.).\n \n\nFURTHER HUMAN\u2013COMPUTER INTERACTION \n \nAims \nThis aim of this course is to provide an introduction to the theoretical foundations of Human \nComputer Interaction, and an understanding of how these can be applied to the design of \ncomplex technologies. \n \nLectures \nTheory driven approaches to HCI. What is a theory in HCI? Why take a theory driven approach \nto HCI? \nDesign of visual displays. Segmentation and variables of the display plane. Modes of \ncorrespondence. \nGoal-oriented interaction. Using cognitive theories of planning, learning and understanding to \nunderstand user behaviour, and what they find hard. \nDesigning smart systems. Using statistical methods to anticipate user needs and actions with \nBayesian strategies. \nDesigning efficient systems. Measuring and optimising human performance through quantitative \nexperimental methods. \nDesigning meaningful systems. Qualitative research methods to understand social context and \nrequirements of user experience. \nEvaluating interactive system designs. Approaches to evaluation in systems research and \nengineering, including Part II Projects. \nDesigning complex systems. Worked case studies of applying the theories to a hard HCI \nproblem. Research directions in HCI. \nObjectives \nAt the end of the course students should be able to apply theories of human performance and \ncognition to system design, including selection of appropriate techniques to analyse, observe \nand improve the usability of a wide range of technologies. \n \nRecommended reading \n* Preece, J., Sharp, H. and Rogers, Y. (2015). Interaction design: beyond human-computer \ninteraction. Wiley (Currently in 4th edition, but earlier editions will suffice). \n \nFurther reading: \n \nCarroll, J.M. (ed.) (2003). HCI models, theories and frameworks: toward a multi-disciplinary \nscience. Morgan Kaufmann.\n \n\nLOGIC AND PROOF \n \nAims \nThis course will teach logic, especially the predicate calculus. It will present the basic principles \nand definitions, then describe a variety of different formalisms and algorithms that can be used \nto solve problems in logic. Putting logic into the context of Computer Science, the course will \nshow how the programming language Prolog arises from the automatic proof method known as \nresolution. It will introduce topics that are important in mechanical verification, such as binary \ndecision diagrams (BDDs), SAT solvers and modal logic. \n \nLectures \nIntroduction to logic. Schematic statements. Interpretations and validity. Logical consequence. \nInference. \nPropositional logic. Basic syntax and semantics. Equivalences. Normal forms. Tautology \nchecking using CNF. \nThe sequent calculus. A simple (Hilbert-style) proof system. Natural deduction systems. \nSequent calculus rules. Sample proofs. \nFirst order logic. Basic syntax. Quantifiers. Semantics (truth definition). \nFormal reasoning in FOL. Free versus bound variables. Substitution. Equivalences for \nquantifiers. Sequent calculus rules. Examples. \nClausal proof methods. Clause form. A SAT-solving procedure. The resolution rule. Examples. \nRefinements. \nSkolem functions, Unification and Herbrand\u2019s theorem. Prenex normal form. Skolemisation. \nMost general unifiers. A unification algorithm. Herbrand models and their properties. \nResolution theorem-proving and Prolog. Binary resolution. Factorisation. Example of Prolog \nexecution. Proof by model elimination. \nSatisfiability Modulo Theories. Decision problems and procedures. How SMT solvers work. \nBinary decision diagrams. General concepts. Fast canonical form algorithm. Optimisations. \nApplications. \nModal logics. Possible worlds semantics. Truth and validity. A Hilbert-style proof system. \nSequent calculus rules. \nTableaux methods. Simplifying the sequent calculus. Examples. Adding unification. \nSkolemisation. The world\u2019s smallest theorem prover? \nObjectives \nAt the end of the course students should \n \nbe able to manipulate logical formulas accurately; \nbe able to perform proofs using the presented formal calculi; \nbe able to construct a small BDD; \nunderstand the relationships among the various calculi, e.g. SAT solving, resolution and Prolog; \nunderstand the concept of a decision procedure and the basic principles of \u201csatisfiability modulo \ntheories\u201d. \nbe able to apply the unification algorithm and to describe its uses. \nRecommended reading \n\n* Huth, M. and Ryan, M. (2004). Logic in computer science: modelling and reasoning about \nsystems. Cambridge University Press (2nd ed.). \nBen-Ari, M. (2001). Mathematical logic for computer science. Springer (2nd ed.).\n \n\nPROLOG \n \nAims \nThe aim of this course is to introduce programming in the Prolog language. Prolog encourages \na different programming style to Java or ML and particular focus is placed on programming to \nsolve real problems that are suited to this style. Practical experimentation with the language is \nstrongly encouraged. \n \nLectures \nIntroduction to Prolog. The structure of a Prolog program and how to use the Prolog interpreter. \nUnification. Some simple programs. \nArithmetic and lists. Prolog\u2019s support for evaluating arithmetic expressions and lists. The space \ncomplexity of program evaluation discussed with reference to last-call optimisation. \nBacktracking, cut, and negation. The cut operator for controlling backtracking. Negation as \nfailure and its uses. \nSearch and cut. Prolog\u2019s search method for solving problems. Graph searching exploiting \nProlog\u2019s built-in search mechanisms. \nDifference structures. Difference lists: introduction and application to example programs. \nBuilding on Prolog. How particular limitations of Prolog programs can be addressed by \ntechniques such as Constraint Logic Programming (CLP) and tabled resolution. \nObjectives \nAt the end of the course students should \n \nbe able to write programs in Prolog using techniques such as accumulators and difference \nstructures; \nknow how to model the backtracking behaviour of program execution; \nappreciate the unique perspective Prolog gives to problem solving and algorithm design; \nunderstand how larger programs can be created using the basic programming techniques used \nin this course. \nRecommended reading \n* Bratko, I. (2001). PROLOG programming for artificial intelligence. Addison-Wesley (3rd or 4th \ned.). \nSterling, L. and Shapiro, E. (1994). The art of Prolog. MIT Press (2nd ed.). \n \nFurther reading: \n \nO\u2019Keefe, R. (1990). The craft of Prolog. MIT Press. [This book is beyond the scope of this \ncourse, but it is very instructive. If you understand its contents, you\u2019re more than prepared for \nthe examination.]\n \n\nARTIFICIAL INTELLIGENCE \n \nPrerequisites: Algorithms 1, Algorithms 2. In addition the course requires some mathematics, in \nparticular some use of vectors and some calculus. Part IA Natural Sciences Mathematics or \nequivalent and Discrete Mathematics are likely to be helpful although not essential. Similarly, \nelements of Machine Learning and Real World Data, Foundations of Data Science, Logic and \nProof, Prolog and Complexity Theory are likely to be useful. This course is a prerequisite for the \nPart II courses Machine Learning and Bayesian Inference and Natural Language Processing. \nThis course is a prerequisite for: Natural Language Processing \nExam: Paper 7 Question 1, 2 \nPast exam questions, Moodle, timetable \n \nAims \nThe aim of this course is to provide an introduction to some fundamental issues and algorithms \nin artificial intelligence (AI). The course approaches AI from an algorithmic, computer \nscience-centric perspective; relatively little reference is made to the complementary \nperspectives developed within psychology, neuroscience or elsewhere. The course aims to \nprovide some fundamental tools and algorithms required to produce AI systems able to exhibit \nlimited human-like abilities, particularly in the form of problem solving by search, game-playing, \nrepresenting and reasoning with knowledge, planning, and learning. \n \nLectures \nIntroduction. Alternate ways of thinking about AI. Agents as a unifying view of AI systems. [1 \nlecture] \nSearch I. Search as a fundamental paradigm for intelligent problem-solving. Simple, uninformed \nsearch algorithms. Tree search and graph search. [1 lecture] \nSearch II. More sophisticated heuristic search algorithms. The A* algorithm and its properties. \nImproving memory efficiency: the IDA* and recursive best first search algorithms. Local search \nand gradient descent. [1 lecture] \nGame-playing. Search in an adversarial environment. The minimax algorithm and its \nshortcomings. Improving minimax using alpha-beta pruning. [1 lecture] \nConstraint satisfaction problems (CSPs). Standardising search problems to a common format. \nThe backtracking algorithm for CSPs. Heuristics for improving the search for a solution. Forward \nchecking, constraint propagation and arc consistency. [1 lecture] \nBackjumping in CSPs. Backtracking, backjumping using Gaschnig\u2019s algorithm, graph-based \nbackjumping. [1 lecture] \nKnowledge representation and reasoning I. How can we represent and deal with commonsense \nknowledge and other forms of knowledge? Semantic networks, frames and rules. How can we \nuse inference in conjunction with a knowledge representation scheme to perform reasoning \nabout the world and thereby to solve problems? Inheritance, forward and backward chaining. [1 \nlecture] \nKnowledge representation and reasoning II. Knowledge representation and reasoning using first \norder logic. The frame, qualification and ramification problems. The situation calculus. [1 lecture] \n\nPlanning I. Methods for planning in advance how to solve a problem. The STRIPS language. \nAchieving preconditions, backtracking and fixing threats by promotion or demotion: the \npartial-order planning algorithm. [1 lecture] \nPlanning II. Incorporating heuristics into partial-order planning. Planning graphs. The \nGRAPHPLAN algorithm. Planning using propositional logic. Planning as a constraint satisfaction \nproblem. [1 lecture] \nNeural Networks I. A brief introduction to supervised learning from examples. Learning as fitting \na curve to data. The perceptron. Learning by gradient descent. [1 lecture] \nNeural Networks II. Multilayer perceptrons and the backpropagation algorithm. [1 lecture] \nObjectives \nAt the end of the course students should: \n \nappreciate the distinction between the popular view of the field and the actual research results; \nappreciate the fact that the computational complexity of most AI problems requires us regularly \nto deal with approximate techniques; \nbe able to design basic problem solving methods based on AI-based search, knowledge \nrepresentation, reasoning, planning, and learning algorithms. \nRecommended reading \nThe recommended text is: \n \n* Russell, S. and Norvig, P. (2010). Artificial intelligence: a modern approach. Prentice Hall (3rd \ned.). \n \nThere are many good books available on artificial intelligence; one alternative is: \n \nPoole, D. L. and Mackworth, A. K. (2017). Artificial intelligence: foundations of computational \nagents. Cambridge University Press (2nd ed.). \n \nFor some of the material you might find it useful to consult more specialised texts, in particular: \n \nDechter, R. (2003). Constraint processing. Morgan Kaufmann. \n \nCawsey, A. (1998). The essence of artificial intelligence. Prentice Hall. \n \nGhallab, M., Nau, D. and Traverso, P. (2004). Automated planning: theory and practice. Morgan \nKaufmann. \n \nBishop, C.M. (2006). Pattern recognition and machine learning. Springer. \n \nBrachman, R.J and Levesque, H.J. (2004). Knowledge Representation and Reasoning. Morgan \nKaufmann.\n \n\nCOMPLEXITY THEORY \n \nAims \nThe aim of the course is to introduce the theory of computational complexity. The course will \nexplain measures of the complexity of problems and of algorithms, based on time and space \nused on abstract models. Important complexity classes will be defined, and the notion of \ncompleteness established through a thorough study of NP-completeness. Applications to \ncryptography will be considered. \n \nSyllabus \nIntroduction to Complexity Theory. Decidability and complexity; lower and upper bounds; the \nChurch-Turing hypothesis. \nTime complexity. Time complexity classes; polynomial time computation and the class P. \nNon-determinism. Non-deterministic machines; the class NP; the problem 3SAT. \nNP-completeness. Reductions and completeness; the Cook-Levin theorem; graph-theoretic \nproblems. \nMore NP-complete problems. 3-colourability; scheduling, matching, set covering, and knapsack.  \ncoNP. Complement classes. NP \u2229 coNP; primality and factorisation. \nCryptography. One-way functions; the class UP. \nSpace complexity. Space complexity classes; Savitch\u2019s theorem. \nHierarchy theorems. Time and space hierarchy theorems. \nRandomised algorithms. The class BPP; derandomisation; error-correcting codes, \ncommunication complexity. \nQuantum Complexity. The classes BQP and QMA. \nInteractive Proofs. IP=PSPACE; Zero Knowledge Proofs; Probabilistically Checkable Proofs \n(PCPs).  \n  \nObjectives \nAt the end of the course students should \n \nbe able to analyse practical problems and classify them according to their complexity; \nbe familiar with the phenomenon of NP-completeness, and be able to identify problems that are \nNP-complete; \nbe aware of a variety of complexity classes and their interrelationships; \nunderstand the role of complexity analysis in cryptography. \nRecommended reading \nA Survey of P vs. NP by Scott Aaronson. \nComputational Complexity: A Modern Approach by Arora and Barak \nComputational Complexity: A Conceptual Perspective by Oded Goldreich \nMathematics and Computation by Avi Wigderson\n \n\nCYBERSECURITY \n \nAims \nIn today\u2019s digital society, computer security is vital for commercial competitiveness, national \nsecurity and privacy of individuals. Protection against both criminal and state-sponsored attacks \nwill need a large cohort of skilled individuals with an understanding of the principles of security \nand with practical experience of the application of these principles. We want you to become one \nof them. In this adversarial game, to defeat the bad guys, the good guys have to be at least as \nskilled at them. Therefore this course has a strong foundation of becoming proficient at actual \nattacks. A theoretical understanding is of course necessary, but without some practice the bad \nguys will run circles around the good guys. In 12 hours I can\u2019t bring you from zero to the stage \nwhere you can invent new attacks and countermeasures, but at least by practicing and \ndissecting common attacks (akin to \u201cstudying the classics\u201d) I\u2019ll give you a feeling for the kind of \nadversarial thinking that is required in this field. Large portions of this course are hands-on: you \nwill need to acquire the relevant skills rather than just reading about it. The recommended \ncourse textbook will be especially helpful to those with no prior low-level hacking experience. \n \nLectures \nIntroduction. \nThe adversarial nature of security; thinking like the attacker; confidentiality, integrity and \navailability; systems-level thinking; Trusted Computing Base; role of cryptography. \n \nFundamentals of access control (chapter 1). \nDiscretionary vs mandatory access control; discretionary access control in POSIX; file \npermissions, file ownership, groups, permission bits. \n \nSoftware security (spanning 4 book chapters) \nSetuid (chapter 2): privileged programs, attack surfaces, exploitable vulnerabilities, privilege \nescalation from regular user to root. \nBuffer overflow (chapter 4): stack memory layout, exploiting a buffer overflow vulnerability, \nguessing unknown addresses, payload, countermeasures and counter-countermeasures. \nReturn to libc and return-oriented programming (chapter 5): exploiting a non-executable stack, \nchaining function calls, chaining ROP gadgets. \nSQL injection (chapter 14): vulnerability and its exploitation, countermeasures, input filtering, \nprepared statements. \n \nAuthentication. \nSomething you know, have, are; passwords, their dominance, their shortcomings and the many \nattempts at replacing them; password cracking; tokens; biometrics; taxonomy of Single Sign-On \nsystems; password managers. \n \nWeb and internet security (spanning 3 book chapters). \nCross-Site Request Forgery (chapter 12): why cross-site requests, CSRF attacks on HTTP GET \nand HTTP POST, countermeasures. \n\nCross-Site Scripting (chapter 13): non-persistent vs persistent XSS, javascript, self-propagating \nXSS worm, countermeasures; \n \nHuman factors. \nUsers are not the enemy; Compliance budget; Prospect theory; 7 principles for systems \nsecurity. \n \nAdditional topics. \nViruses, worms and quines; lockpicking; privilege escalation in physical locks; conclusions. \n \nTo complete the course successfully, students must acquire the practical ability to carry out (as \nopposed to just describing) the exploits mentioned in the syllabus, given a vulnerable system. \nThe low-level hands-on portions of the course are supported by the very helpful course \ntextbook. Those who choose not to study on the recommended textbook will be severely \ndisadvantaged. \n \nRecommended textbook: Wenliang Du. Computer Security: A Hands-on Approach. 3rd Edition. \nISBN: 978-17330039-5-7. Published: 1 May 2022. \n \nhttps://www.handsonsecurity.net. Some chapters freely available online. \n \n \nOptional additional books (not substitutes): \n \nRoss Anderson, Security Engineering 3rd Ed, Wiley, 2020. ISBN: 978-1-119-64278-7. \nhttps://www.cl.cam.ac.uk/~rja14/book.html. Some chapters freely available online. \n \nPaul van Oorschot, Computer Security and the Internet, Springer, 2020. ISBN: \n978-3-030-33648-6 (hardcopy), 978-3-030-33649-3 (eBook). \nhttps://people.scs.carleton.ca/~paulv/toolsjewels.html. All chapters freely available online.\n\nFORMAL MODELS OF LANGUAGE \n \nAims \nThis course studies formal models of language and considers how they might be relevant to the \nprocessing and acquisition of natural languages. The course will extend knowledge of formal \nlanguage theory; introduce several new grammars; and use concepts from information theory to \ndescribe natural language. \n \nLectures \nNatural language and the Chomsky hierarchy 1. Recap classes of language. Closure properties \nof language classes. Recap pumping lemma for regular languages. Discussion of relevance (or \nnot) to natural languages (example embedded clauses in English). \nNatural language and the Chomsky hierarchy 2. Pumping lemma for context free languages. \nDiscussion of relevance (or not) to natural languages (example Swiss-German cross serial \ndependancies). Properties of minimally context sensitive languages. Introduction to tree \nadjoining grammars. \nLanguage processing and context free grammar parsing 1. Recap of context free grammar \nparsing. Language processing predictions based on top down parsing models (example Yngve\u2019s \nlanguage processing predictions). Language processing predictions based on probabilistic \nparsing (example Halle\u2019s language processing predictions). \nLanguage processing and context free grammar parsing 2. Introduction to context free grammar \nequivalent dependancy grammars. Language processing predictions based on Shift-Reduce \nparsing (examples prosodic look-ahead parsers, Parsey McParseface). \nGrammar induction of language classes. Introduction to grammar induction. Discussion of \nrelevance (or not) to natural language acquisition. Gold\u2019s theorem. Introduction to context free \ngrammar equivalent categorial grammars and their learnable classes. \nNatural language and information theory 1. Entropy and natural language typology. Uniform \ninformation density as a predictor for language processing. \nNatural language and information theory 2. Noisy channel encoding as a model for spelling \nerror, translation and language processing. \nVector space models and word vectors. Introduction to word vectors (example Word2Vec). Word \nvectors as predictors for semantic language processing. \nObjectives \nAt the end of the course students should \n \nunderstand how known natural languages relate to formal languages in the Chomsky hierarchy; \nhave knowledge of several context free grammars equivalents; \nunderstand how we might make predictions about language processing and language \nacquisition from formal models; \nknow how to use information theoretic concepts to describe aspects of natural language. \nRecommended reading \n* Jurafsky, D. and Martin, J. (2008). Speech and language processing. Prentice Hall. \nManning, C.D. and Schutze, H. (1999) Foundations of statistical natural language processing. \nMIT Press. \n\nRuslan, M. (2003) The Oxford handbook of computational linguistics. Oxford University Press. \nClark, A., Fox, C. and Lappin, S. (2010) The handbook of computational linguistics and natural \nlanguage processing. Wiley-Blackwell. \nKozen, D. (1997) Automata and computibility. Springer.\n \n\n",
        "5th Semester \nBIOINFORMATICS \nAims \nThis course focuses on algorithms used in Bioinformatics and System Biology. Most of the \nalgorithms are general and can be applied in other fields on multidimensional and noisy data. All \nthe necessary biological terms and concepts useful for the course and the examination will be \ngiven in the lectures. The most important software implementing the described algorithms will be \ndemonstrated. \n \nLectures \nIntroduction to biological data: Bioinformatics as an interesting field in computer science. \nComputing and storing information with DNA (including Adleman\u2019s experiment). \nDynamic programming. Longest common subsequence, DNA global and local alignment, linear \nspace alignment, Nussinov algorithm for RNA, heuristics for multiple alignment. (Vol. 1, chapter \n5) \nSequence database search. Blast. (see notes and textbooks) \nGenome sequencing. De Bruijn graph. (Vol. 1, chapter 3) \nPhylogeny. Distance based algorithms (UPGMA, Neighbour-Joining). Parsimony-based \nalgorithms. Examples in Computer Science. (Vol. 2, chapter 7) \nClustering. Hard and soft K-means clustering, use of Expectation Maximization in clustering, \nHierarchical clustering, Markov clustering algorithm. (Vol. 2, chapter 8) \nGenomics Pattern Matching. Suffix Tree String Compression and the Burrows-Wheeler \nTransform. (Vol. 2, chapter 9) \nHidden Markov Models. The Viterbi algorithm, profile HMMs for sequence alignment, classifying \nproteins with profile HMMs, soft decoding problem, Baum-Welch learning. (Vol. 2, chapter 10) \nObjectives \nAt the end of this course students should \n \nunderstand Bioinformatics terminology; \nhave mastered the most important algorithms in the field; \nbe able to work with bioinformaticians and biologists; \nbe able to find data and literature in repositories. \nRecommended reading \n* Compeau, P. and Pevzner, P.A. (2015). Bioinformatics algorithms: an active learning approach. \nActive Learning Publishers. \nDurbin, R., Eddy, S., Krough, A. and Mitchison, G. (1998). Biological sequence analysis: \nprobabilistic models of proteins and nucleic acids. Cambridge University Press. \nJones, N.C. and Pevzner, P.A. (2004). An introduction to bioinformatics algorithms. MIT Press. \nFelsenstein, J. (2003). Inferring phylogenies. Sinauer Associates.\n \n\nBUSINESS STUDIES \n \nAims \nHow to start and run a computer company; the aims of this course are to introduce students to \nall the things that go to making a successful project or product other than just the programming. \nThe course will survey some of the issues that students are likely to encounter in the world of \ncommerce and that need to be considered when setting up a new computer company. \n \nSee also Business Seminars in the Easter Term. \n \nLectures \nSo you\u2019ve got an idea? Introduction. Why are you doing it and what is it? Types of company. \nMarket analysis. The business plan. \nMoney and tools for its management. Introduction to accounting: profit and loss, cash flow, \nbalance sheet, budgets. Sources of finance. Stocks and shares. Options and futures. \nSetting up: legal aspects. Company formation. Brief introduction to business law; duties of \ndirectors. Shares, stock options, profit share schemes and the like. Intellectual Property Rights, \npatents, trademarks and copyright. Company culture and management theory. \nPeople. Motivating factors. Groups and teams. Ego. Hiring and firing: employment law. \nInterviews. Meeting techniques. \nProject planning and management. Role of a manager. PERT and GANTT charts, and critical \npath analysis. Estimation techniques. Monitoring. \nQuality, maintenance and documentation. Development cycle. Productization. Plan for quality. \nPlan for maintenance. Plan for documentation. \nMarketing and selling. Sales and marketing are different. Marketing; channels; marketing \ncommunications. Stages in selling. Control and commissions. \nGrowth and exit routes. New markets: horizontal and vertical expansion. Problems of growth; \nsecond system effects. Management structures. Communication. Exit routes: acquisition, \nfloatation, MBO or liquidation. Futures: some emerging ideas for new computer businesses. \nSummary. Conclusion: now you do it! \nObjectives \nAt the end of the course students should \n \nbe able to write and analyse a business plan; \nknow how to construct PERT and GANTT diagrams and perform critical path analysis; \nappreciate the differences between profitability and cash flow, and have some notion of budget \nestimation; \nhave an outline view of company formation, share structure, capital raising, growth and exit \nroutes; \nhave been introduced to concepts of team formation and management; \nknow about quality documentation and productization processes; \nunderstand the rudiments of marketing and the sales process. \nRecommended reading \n\nLang, J. (2001). The high-tech entrepreneur\u2019s handbook: how to start and run a high-tech \ncompany. FT.COM/Prentice Hall. \n \nStudents will be expected to be able to use Microsoft Excel and Microsoft Project. \n \nFor additional reading on a lecture-by-lecture basis, please see the course website. \n \nStudents are strongly recommended to enter the CU Entrepreneurs Business Ideas Competition \nhttp://www.cue.org.uk/\n \n\nDENOTATIONAL SEMANTICS \n \nAims \nThe aims of this course are to introduce domain theory and denotational semantics, and to \nshow how they provide a mathematical basis for reasoning about the behaviour of programming \nlanguages. \n \nLectures \nIntroduction.The denotational approach to the semantics of programming \nlanguages.Recursively defined objects as limits of successive approximations. \nLeast fixed points.Complete partial orders (cpos) and least elements.Continuous functions and \nleast fixed points. \nConstructions on domains.Flat domains.Product domains. Function domains. \nScott induction.Chain-closed and admissible subsets of cpos and domains.Scott\u2019s fixed-point \ninduction principle. \nPCF.The Scott-Plotkin language PCF.Evaluation. Contextual equivalence. \nDenotational semantics of PCF.Denotation of types and terms. Compositionality. Soundness \nwith respect to evaluation. [2 lectures]. \nRelating denotational and operational semantics.Formal approximation relation and its \nfundamental property.Computational adequacy of the PCF denotational semantics with respect \nto evaluation. Extensionality properties of contextual equivalence. [2 lectures]. \nFull abstraction.Failure of full abstraction for the domain model. PCF with parallel or. \nObjectives \nAt the end of the course students should \n \nbe familiar with basic domain theory: cpos, continuous functions, admissible subsets, least fixed \npoints, basic constructions on domains; \nbe able to give denotational semantics to simple programming languages with simple types; \nbe able to apply denotational semantics; in particular, to understand the use of least fixed points \nto model recursive programs and be able to reason about least fixed points and simple \nrecursive programs using fixed point induction; \nunderstand the issues concerning the relation between denotational and operational semantics, \nadequacy and full abstraction, especially with respect to the language PCF. \nRecommended reading \nWinskel, G. (1993). The formal semantics of programming languages: an introduction. MIT \nPress. \nGunter, C. (1992). Semantics of programming languages: structures and techniques. MIT Press. \nTennent, R. (1991). Semantics of programming languages. Prentice Hall.\n \n\nINFORMATION THEORY \n \nAims \nThis course introduces the principles and applications of information theory: how information is \nmeasured in terms of probability and various entropies, how these are used to calculate the \ncapacity of communication channels, with or without noise, and to measure how much random \nvariables reveal about each other. Coding schemes including error correcting codes are studied \nalong with data compression, spectral analysis, transforms, and wavelet coding. Applications of \ninformation theory are reviewed, from astrophysics to pattern recognition. \n \nLectures \nInformation, probability, uncertainty, and surprise. How concepts of randomness and uncertainty \nare related to information. How the metrics of information are grounded in the rules of \nprobability. Shannon Information. Weighing problems and other examples. \nEntropy of discrete variables. Definition and link to Shannon information. Joint entropy, Mutual \ninformation. Visual depictions of the relationships between entropy types. Why entropy gives \nfundamental measures of information content. \nSource coding theorem and data compression; prefix, variable-, and fixed-length codes. \nInformation rates; Asymptotic equipartition principle; Symbol codes; Huffman codes and the \nprefix property. Binary symmetric channels. Capacity of a noiseless discrete channel. Stream \ncodes. \nNoisy discrete channel coding. Joint distributions; mutual information; Conditional Entropy; \nError-correcting codes; Capacity of a discrete channel. Noisy channel coding theorem. \nEntropy of continuous variables. Differential entropy; Mutual information; Channel Capacity; \nGaussian channels. \nEntropy for comparing probability distributions and for machine learning. Relative entropy/KL \ndivergence; cross-entropy; use as loss function. \nApplications of information theory in other sciences.  \nObjectives \nAt the end of the course students should be able to \n \ncalculate the information content of a random variable from its probability distribution; \nrelate the joint, conditional, and marginal entropies of variables in terms of their coupled \nprobabilities; \ndefine channel capacities and properties using Shannon\u2019s Theorems; \nconstruct efficient codes for data on imperfect communication channels; \ngeneralize the discrete concepts to continuous signals on continuous channels; \nunderstand encoding and communication schemes in terms of the spectral properties of signals \nand channels; \ndescribe compression schemes, and efficient coding using wavelets and other representations \nfor data. \nRecommended reading \n  Mackay's Information Theory, inference and Learning Algorithms \n \n\n Stone's Information Theory: A Tutorial Introduction.\n \n\nLATEX AND JULIA \n \nAims \nIntroduction to two widely-used languages for typesetting dissertations and scientific \npublications, for prototyping numerical algorithms and to visualize results. \n \nLectures \nLATEX. Workflow example, syntax, typesetting conventions, non-ASCII characters, document \nstructure, packages, mathematical typesetting, graphics and figures, cross references, build \ntools. \nJulia. Tools for technical computing and visualization. The Array type and its operators, 2D/3D \nplotting, functions and methods, notebooks, packages, vectorized audio demonstration. \nObjectives \nStudents should be able to avoid the most common LATEX mistakes, to prototype simple image \nand signal-processing algorithms in Julia, and to visualize the results. \n \nRecommended reading \n* Lamport, L. (1994). LATEX \u2013 a documentation preparation system user\u2019s guide and reference \nmanual. Addison-Wesley (2nd ed.). \nMittelbach, F., et al. (2023). The LATEX companion. Addison-Wesley (3rd ed.).\n \n\nPRINCIPLES OF COMMUNICATIONS \n \nAims \nThis course aims to provide a detailed understanding of the underlying principles for how \ncommunications systems operate. Practical examples (from wired and wireless \ncommunications, the Internet, and other communications systems) are used to illustrate the \nprinciples. \n \nLectures \nIntroduction. Course overview. Abstraction, layering. Review of structure of real networks, links, \nend systems and switching systems. [1 lecture] \nRouting. Central versus Distributed Routing Policy Routing. Multicast Routing Circuit Routing [6 \nlectures] \nFlow control and resource optimisation. 1[Control theory] is a branch of engineering familiar to \npeople building dynamic machines. It can be applied to network traffic. Stemming the flood, at \nsource, sink, or in between? Optimisation as a model of networkand user. TCP in the wild. [3 \nlectures] \nPacket Scheduling. Design choices for scheduling and queue management algorithms for \npacket forwarding, and fairness. [2 lectures] \nThe big picture for managing traffic. Economics and policy are relevant to networks in many \nways. Optimisation and game theory are both relevant topics discussed here. [2 lectures] \nSystem Structures and Summary. Abstraction, layering. The structure of real networks, links, \nend systems and switching. [2 lectures] \n1 Control theory was not taught and will not be the subject of any exam question. \n \nObjectives \nAt the end of the course students should be able to explain the underlying design and behaviour \nof protocols and networks, including capacity, topology, control and use. Several specific \nmathematical approaches are covered (control theory, optimisation). \n \nRecommended reading \n* Keshav, S. (2012). Mathematical Foundations of Computer Networking. Addison Wesley. ISBN \n9780321792105 \nBackground reading: \nKeshav, S. (1997). An engineering approach to computer networking. Addison-Wesley (1st ed.). \nISBN 0201634422 \nStevens, W.R. (1994). TCP/IP illustrated, vol. 1: the protocols. Addison-Wesley (1st ed.). ISBN \n0201633469\n \n\nTYPES \n \nAims \nThe aim of this course is to show by example how type systems for programming languages can \nbe defined and their properties developed, using techniques that were introduced in the Part IB \ncourse on Semantics of Programming Languages. The emphasis is on type systems for \nfunctional languages and their connection to constructive logic. \n \nLectures \nIntroduction. The role of type systems in programming languages. Review of rule-based \nformalisation of type systems. [1 lecture] \nPropositions as types. The Curry-Howard correspondence between intuitionistic propositional \ncalculus and simply-typed lambda calculus. Inductive types and iteration. Consistency and \ntermination. [2 lectures] \nPolymorphic lambda calculus (PLC). PLC syntax and reduction semantics. Examples of \ndatatypes definable in the polymorphic lambda calculus. Type inference. [3 lectures] \nMonads and effects. Explicit versus implicit effects. Using monadic types to control effects. \nReferences and polymorphism. Recursion and looping. [2 lectures] \nContinuations and classical logic. First-class continuations and control operators. Continuations \nas Curry-Howard for classical logic. Continuation-passing style. [2 lectures] \nDependent types. Dependent function types. Indexed datatypes. Equality types and combining \nproofs with programming. [2 lectures] \nObjectives \nAt the end of the course students should \n \nbe able to use a rule-based specification of a type system to carry out type checking and type \ninference; \nunderstand by example the Curry-Howard correspondence between type systems and logics; \nunderstand how types can be used to control side-effects in programming; \nappreciate the expressive power of parametric polymorphism and dependent types. \nRecommended reading \n* Pierce, B.C. (2002). Types and programming languages. MIT Press. \nPierce, B. C. (Ed.) (2005). Advanced Topics in Types and Programming Languages. MIT Press. \nGirard, J-Y. (tr. Taylor, P. and Lafont, Y.) (1989). Proofs and types. Cambridge University Press.\n\nADVANCED DATA SCIENCE \n \nPracticals \nThe module will have a practical session for each of the three stages of the pipeline. Each of the \nparts will be tied into an aspect of the final project. \n \nObjectives \nAt the end of the course students will be familiar with the purpose of data science, how it differs \nfrom the closely related fields of machine learning, statistics and artificial intelligence and what a \ntypical data analysis pipeline looks like in practice. As well as emphasising the importance of \nanalysis methods we will introduce a formalism for organising how data science is done in \npractice and what the different aspects the data scientist faces when giving data-driven answers \nto questions of interest. \n \nRecommended reading \nSimon Rogers et al. (2016). A First Course in Machine Learning, Second Edition. [] Chapman \nand Hall/CRC, nil \nShai Shalev-Shwartz et al. (2014). Understanding Machine Learning: From Theory to \nAlgorithms. New York, NY, USA: Cambridge  University Press \nChristopher M. Bishop (2006). Pattern Recognition and Machine Learning (Information Science \nand Statistics). Secaucus, NJ, USA: Springer-Verlag New York, Inc. \nAssessment \nPractical There will be four practicals contributing 20% to the final module mark. Data science is \na topic where there is rarely a single \ncorrect answer therefore the important thing is that an informed attempt has been made to \naddress the questions that can be supported and motivated. \nReport The course will be concluded by an individual report covering the material in the course \nthrough \npractical exercise working with real data. This report will make up 80% of the mark for the \ncourse. \n \n \n\nAFFECTIVE ARTIFICIAL INTELLIGENCE \n \nSynopsis \nAffective Artificial Intelligence (Affective AI) aims to imbue machines with social and emotional \nintelligence (EQ). More specifically, Affective AI aims to create artificially intelligent systems and \nmachines that can recognize, interpret, process, and simulate human social signals and \nbehaviours, expressions, and emotions, to enhance human-AI interaction and communication. \n\\r\\n\\r\\nTo achieve this goal, Affective AI draws upon various scientific disciplines, including \nmachine learning, computer vision, speech / natural language / signal processing, psychology \nand cognitive science, and ethics and social sciences. \n \n  \nBackground & Aims \nAffective Artificial Intelligence (Affective AI) aims to imbue machines with social and emotional \nintelligence (EQ). More specifically, Affective AI aims to create artificially intelligent systems and \nmachines that can recognize, interpret, process, and simulate human social signals and \nbehaviours, expressions, and emotions, to enhance human-AI interaction and communication.  \n \nTo achieve this goal, Affective AI draws upon various scientific disciplines, including machine \nlearning, computer vision, speech / natural language / signal processing, psychology and \ncognitive science, and ethics and social sciences. \n \nAffective AI has direct applications in and implications for the design of innovative interactive \ntechnology (e.g., interaction with chat bots, virtual agents, robots), single and multi-user smart \nenvironments ( e.g., in-car/ virtual / augmented / mixed reality, serious games), public speaking \nand cognitive training, and clinical and biomedical studies (e.g., autism, depression, pain). \n \nThe aim of this module is to impart knowledge and ability needed to make informed choices of \nmodels, data, and machine learning techniques for sensing, recognition, and generation of \naffective and social behaviour (e.g., smile, frown, head nodding/shaking, \nagreement/disagreement) in order to create Affectively intelligent AI systems, with a \nconsideration for various ethical issues (e.g., privacy, bias) arising from the real-world \ndeployment of these systems. \n \n  \n \nSyllabus \nThe following list provides a representative list of topics: \n \nIntroduction, definitions, and overview \nTheories from various disciplines \nSensing from multiple modalities (e.g., vision, audio, bio signals, text) \nData acquisition and annotation \nSignal processing / feature extraction \n\nLearning / prediction / recognition and evaluation \nBehaviour synthesis / generation (e.g., for embodied agents / robots) \nAdvanced topics and ethical considerations (e.g., bias and fairness) \nCross-disciplinary applications (via seminar presentations and discussions) \nGuest lectures (diverse topics \u2013 changes each year) \nHands-on research and programming work (i.e., mini project & report)  \n \n  \n \nObjectives \nOn completion of this module, students will: \n \nunderstand and demonstrate knowledge in key characteristics of affectively intelligent AI, which \ninclude: \nRecognition: How to equip Affective AI systems with capabilities of analysing facial expressions, \nvocal intonations, gestures, and other physiological signals to infer human affective states? \nGeneration: How to enable affectively intelligent AI simulate expressions of emotions in \nmachines, allowing them to respond in a more human-like manner, such as virtual agents and \nhumanoid robots, showing empathy or sympathy? \nAdaptation and Personalization: How to enable affectively intelligent AI adapt system responses \nbased on users' affective states and/or needs, or tailor interactions and experiences to individual \nusers based on their expressivity / emotional profiles, personalities, and past interactions? \nEmpathetic Communication: How to design Affective AI systems to communicate with users in a \nway that demonstrates empathy, understanding, and sensitivity to their affective states and \nneeds? \nEthical and societal considerations: What are the various human differences, ethical guidelines, \nand societal impacts that need to be considered when designing and deploying Affective AI to \nensure that these systems respect users' privacy, autonomy, and well-being? \ncomprehend and apply (appropriate) methods for collection, analysis, representation, and \nevaluation of human affective and communicative behavioural data. \nenhance programming skills for creating and implementing (components of) Affective AI \nsystems. \ndemonstrate critical thinking, analysis and synthesis while deciding on 'when' and 'how' to \nincorporate human affect and social signals in a specific AI system context and gain practical \nexperience in proposing and justifying computational solution(s) of suitable nature and scope. \n  \n \nAssessment - Part II Students \nSeminar presentation: 20% \nParticipating in Q&A and discussions: 10% \nMid-term mini project report and presentation (as a team of 2): 10%  \nFinal mini project report & code (as a team of 2): 60% \n \n\nCATEGORY THEORY \n \nAims \nCategory theory provides a unified treatment of mathematical properties and constructions that \ncan be expressed in terms of 'morphisms' between structures. It gives a precise framework for \ncomparing one branch of mathematics (organized as a category) with another and for the \ntransfer of problems in one area to another. Since its origins in the 1940s motivated by \nconnections between algebra and geometry, category theory has been applied to diverse fields, \nincluding computer science, logic and linguistics. This course introduces the basic notions of \ncategory theory: adjunction, natural transformation, functor and category. We will use category \ntheory to organize and develop the kinds of structure that arise in models and semantics for \nlogics and programming languages. \n \nSyllabus \nIntroduction; some history. Definition of category. The category of sets and functions. \nCommutative diagrams. Examples of categories: preorders and monotone functions; monoids \nand monoid homomorphisms; a preorder as a category; a monoid as a category. Definition of \nisomorphism. Informal notion of a 'category-theoretic' property. \nTerminal objects. The opposite of a category and the duality principle. Initial objects. Free \nmonoids as initial objects. \nBinary products and coproducts. Cartesian categories. \nExponential objects: in the category of sets and in general. Cartesian closed categories: \ndefinition and examples. \nIntuitionistic Propositional Logic (IPL) in Natural Deduction style. Semantics of IPL in a cartesian \nclosed preorder. \nSimply Typed Lambda Calculus (STLC). The typing relation. Semantics of STLC types and \nterms in a cartesian closed category (ccc). The internal language of a ccc. The \nCurry-Howard-Lawvere correspondence. \nFunctors. Contravariance. Identity and composition for functors. \nSize: small categories and locally small categories. The category of small categories. Finite \nproducts of categories. \nNatural transformations. Functor categories. The category of small categories is cartesian \nclosed. \nHom functors. Natural isomorphisms. Adjunctions. Examples of adjoint functors. Theorem \ncharacterising the existence of right (respectively left) adjoints in terms of a universal property. \nDependent types. Dependent product sets and dependent function sets as adjoint functors. \nEquivalence of categories. Example: the category of I-indexed sets and functions is equivalent \nto the slice category Set/I. \nPresheaves. The Yoneda Lemma. Categories of presheaves are cartesian closed. \nMonads. Modelling notions of computation as monads. Moggi's computational lambda calculus. \nObjectives \nOn completion of this module, students should: \n \n\nbe familiar with some of the basic notions of category theory and its connections with logic and \nprogramming language semantics \nAssessment \na graded exercise sheet (25% of the final mark), and \na take-home test (75%) \nRecommended reading \nAwodey, S. (2010). Category theory. Oxford University Press (2nd ed.). \n \nCrole, R. L. (1994). Categories for types. Cambridge University Press. \n \nLambek, J. and Scott, P. J. (1986). Introduction to higher order categorical logic. Cambridge \nUniversity Press. \n \nPitts, A. M. (2000). Categorical Logic. Chapter 2 of S. Abramsky, D. M. Gabbay and T. S. E. \nMaibaum (Eds) Handbook of Logic in Computer Science, Volume 5. Oxford University Press. \n(Draft copy available here.) \n \nClass Size \nThis module can accommodate upto 15 Part II students plus 15 MPhil / Part III students.\n\nDIGITAL SIGNAL PROCESSING \n \nAims \nThis course teaches basic signal-processing principles necessary to understand many modern \nhigh-tech systems, with examples from audio processing, image coding, radio communication, \nradar, and software-defined radio. Students will gain practical experience from numerical \nexperiments in programming assignments (in Julia, MATLAB or NumPy). \n \nLectures \nSignals and systems. Discrete sequences and systems: types and properties. Amplitude, \nphase, frequency, modulation, decibels, root-mean square. Linear time-invariant systems, \nconvolution. Some examples from electronics, optics and acoustics. \nPhasors. Eigen functions of linear time-invariant systems. Review of complex arithmetic. \nPhasors as orthogonal base functions. \nFourier transform. Forms and properties of the Fourier transform. Convolution theorem. Rect \nand sinc. \nDirac\u2019s delta function. Fourier representation of sine waves, impulse combs in the time and \nfrequency domain. Amplitude-modulation in the frequency domain. \nDiscrete sequences and spectra. Sampling of continuous signals, periodic signals, aliasing, \ninterpolation, sampling and reconstruction, sample-rate conversion, oversampling, spectral \ninversion. \nDiscrete Fourier transform. Continuous versus discrete Fourier transform, symmetry, linearity, \nFFT, real-valued FFT, FFT-based convolution, zero padding, FFT-based resampling, \ndeconvolution exercise. \nSpectral estimation. Short-time Fourier transform, leakage and scalloping phenomena, \nwindowing, zero padding. Audio and voice examples. DTFM exercise. \nFinite impulse-response filters. Properties of filters, implementation forms, window-based FIR \ndesign, use of frequency-inversion to obtain high-pass filters, use of modulation to obtain \nband-pass filters. \nInfinite impulse-response filters. Sequences as polynomials, z-transform, zeros and poles, some \nanalog IIR design techniques (Butterworth, Chebyshev I/II, elliptic filters, second-order cascade \nform). \nBand-pass signals. Band-pass sampling and reconstruction, IQ up and down conversion, \nsuperheterodyne receivers, software-defined radio front-ends, IQ representation of AM and FM \nsignals and their demodulation. \nDigital communication. Pulse-amplitude modulation. Matched-filter detector. Pulse shapes, \ninter-symbol interference, equalization. IQ representation of ASK, BSK, PSK, QAM and FSK \nsignals. [2 hours] \nRandom sequences and noise. Random variables, stationary and ergodic processes, \nautocorrelation, cross-correlation, deterministic cross-correlation sequences, filtered random \nsequences, white noise, periodic averaging. \nCorrelation coding. Entropy, delta coding, linear prediction, dependence versus correlation, \nrandom vectors, covariance, decorrelation, matrix diagonalization, eigen decomposition, \n\nKarhunen-Lo\u00e8ve transform, principal component analysis. Relation to orthogonal transform \ncoding using fixed basis vectors, such as DCT. \nLossy versus lossless compression. What information is discarded by human senses and can \nbe eliminated by encoders? Perceptual scales, audio masking, spatial resolution, colour \ncoordinates, some demonstration experiments. \nQuantization, image coding standards. Uniform and logarithmic quantization, A/\u00b5-law coding, \ndithering, JPEG. \nObjectives \napply basic properties of time-invariant linear systems; \nunderstand sampling, aliasing, convolution, filtering, the pitfalls of spectral estimation; \nexplain the above in time and frequency domain representations; \nuse filter-design software; \nvisualize and discuss digital filters in the z-domain; \nuse the FFT for convolution, deconvolution, filtering; \nimplement, apply and evaluate simple DSP applications; \nfamiliarity with a number of signal-processing concepts used in digital communication systems \nRecommended reading \nLyons, R.G. (2010). Understanding digital signal processing. Prentice Hall (3rd ed.). \nOppenheim, A.V. and Schafer, R.W. (2007). Discrete-time digital signal processing. Prentice \nHall (3rd ed.). \nStein, J. (2000). Digital signal processing \u2013 a computer science perspective. Wiley. \n \nClass size \nThis module can accommodate a maximum of 24 students (16 Part II students and 8 MPhil \nstudents) \n \nAssessment - Part II students \nThree homework programming assignments, each comprising 20% of the mark \nWritten test, comprising 40% of the total mark.\n \n\nMACHINE VISUAL PERCEPTION \n \nAims \nThis course aims at introducing the theoretical foundations and practical techniques for machine \nperception, the capability of computers to interpret data resulting from sensor measurements. \nThe course will teach the fundamentals and modern techniques of machine perception, i.e. \nreconstructing the real world starting from sensor measurements with a focus on machine \nperception for visual data. The topics covered will be image/geometry representations for \nmachine perception, semantic segmentation, object detection and recognition, geometry \ncapture, appearance modeling and acquisition, motion detection and estimation, \nhuman-in-the-loop machine perception, select topics in applied machine perception. \n \nMachine perception/computer vision is a rapidly expanding area of research with real-world \napplications. An understanding of machine perception is also important for robotics, interactive \ngraphics (especially AR/VR), applied machine learning, and several other fields and industries. \nThis course will provide a fundamental understanding of and practical experience with the \nrelevant techniques. \n \nLearning outcomes \nStudents will understand the theoretical underpinnings of the modern machine perception \ntechniques for reconstructing models of reality starting from an incomplete and imperfect view of \nreality. \nStudents will be able to apply machine perception theory to solve practical problems, e.g. \nclassification of images, geometry capture. \nStudents will gain an understanding of which machine perception techniques are appropriate for \ndifferent tasks and scenarios. \nStudents will have hands-on experience with some of these techniques via developing a \nfunctional machine perception system in their projects. \nStudents will have practical experience with the current prominent machine perception \nframeworks. \nSyllabus \nThe fundamentals of machine learning for machine perception \nDeep neural networks and frameworks for machine perception \nSemantic segmentation of objects and humans \nObject detection and recognition \nMotion estimation, tracking and recognition \n3D geometry capture \nAppearance modeling and acquisition \nSelect topics in applied machine perception \nAssessment \nA practical exercise, worth 20% of the mark. This will cover the basics and theory of machine \nperception and some of the practical techniques the students will likely use in their projects. This \nis individual work. No GPU hours will be needed for the practical work. \nA machine perception project worth 80% of the marks: \n\nCourse projects will be selected by the students following the possible project themes proposed \nby the lecturer, and will be checked by the lecturer for appropriateness.  \nThe students will form groups of 2-3 to design, implement, report, and present a project to tackle \na given task in machine perception. \nThe final mark will be composed of an implementation/report mark (60%) and a presentation \nmark (20%). Each team member will be evaluated based on her/his contribution. \nEach project will have extensions to be completed only by the ACS students. Each student will \nwrite a different part of the report, whose author will be clearly marked. Each student will further \nsummarise her/his contributions to the project in the same report. \nRecommended Reading List \nComputer Vision: Algorithms and Applications, Richard Szeliski, Springer, 2010. \nDeep Learning, Ian Goodfellow, Yoshua Bengio, and Aaron Courville, MIT Press, 2016. \nMachine Learning and Visual Perception, Baochang Zhang, De Gruyter, 2020.\n \n\nNATURAL LANGUAGE PROCESSING \n \nAims \nThis course introduces the fundamental techniques of natural language processing. It aims to \nexplain the potential and the main limitations of these techniques. Some current research issues \nare introduced and some current and potential applications discussed and evaluated. Students \nwill also be introduced to practical experimentation in natural language processing. \n \nLectures \nOverview. Brief history of NLP research, some current applications, components of NLP \nsystems. \nMorphology and Finite State Techniques. Morphology in different languages, importance of \nmorphological analysis in NLP, finite-state techniques in NLP. \nPart-of-Speech Tagging and Log-Linear Models. Lexical categories, word tagging, corpora and \nannotations, empirical evaluation. \nPhrase Structure and Structure Prediction. Phrase structures, structured prediction, context-free \ngrammars, weights and probabilities. Some limitations of context-free grammars. \nDependency Parsing. Dependency structure, grammar-free parsing, incremental processing.  \nGradient Descent and Neural Nets. Parameter optimisation by gradient descent. Non-linear \nfunctions with neural network layers. Log-linear model as softmax layer. Current findings of \nNeural NLP. \nWord representations. Representing words with vectors, count-based and prediction-based \napproaches, similarity metrics. \nRecurrent Neural Networks. Modelling sequences, parameter sharing in recurrent neural \nnetworks, neural language models, word prediction. \nCompositional Semantics. Logical representations, compositional semantics, lambda calculus, \ninference and robust entailment. \nLexical Semantics. Semantic relations, WordNet, word senses. \nDiscourse. Discourse relations, anaphora resolution, summarization. \nNatural Language Generation. Challenges of natural language generation (NLG), tasks in NLG, \nsurface realisation. \nPractical and assignments. Students will build a natural language processing system which will \nbe trained and evaluated on supplied data. The system will be built from existing components, \nbut students will be expected to compare approaches and some programming will be required \nfor this. Several assignments will be set during the practicals for assessment. \nObjectives \nBy the end of the course students should: \n \nbe able to discuss the current and likely future performance of several NLP applications; \nbe able to describe briefly a fundamental technique for processing language for several \nsubtasks, such as morphological processing, parsing, word sense disambiguation etc.; \nunderstand how these techniques draw on and relate to other areas of computer science. \nRecommended reading \n\nJurafsky, D. & Martin, J. (2023). Speech and language processing. Prentice Hall (3rd ed. draft, \nonline). \n \nAssessment - Part II Students \nAssignment 1 - 10% of marks \nAssignment 2 - 60% of marks \nAssignment 3 - 30% of marks\n \n\nPRACTICAL RESEARCH IN HUMAN-CENTRED AI \n \nAims \nThis is an advanced course in human-computer interaction, with a specialist focus on intelligent \nuser interfaces and interaction with machine-learning and artificial intelligence technologies. The \nformat will be largely Practical, with students carrying out a mini-project involving empirical \nresearch investigation. These studies will investigate human interaction with some kind of \nmodel-based system for planning, decision-making, automation etc. Possible study formats \nmight include: System evaluation, Field observation, Hypothesis testing experiment, Design \nintervention or Corpus analysis, following set examples from recent research publications. \nProject work will be formally evaluated through a report and presentation. \n \nLectures \n(note that Lectures 2-7 also include one hour class discussion of practical work) \n        \u2022 Current research themes in intelligent user interfaces \n        \u2022 Program synthesis \n        \u2022 Mixed initiative interaction \n        \u2022 Interpretability / explainable AI \n        \u2022 Labelling as a fundamental problem \n        \u2022 Machine learning risks and bias \n        \u2022 Visualisation and visual analytics \n        \u2022 Student research presentations \n \nObjectives \nBy the end of the course students should: \n        \u2022 be familiar with current state of the art in intelligent interactive systems \n        \u2022 understand the human factors that are most critical in the design of such systems \n        \u2022 be able to evaluate evidence for and against the utility of novel systems \n        \u2022 have experience of conducting user studies meeting the quality criteria of this field \n        \u2022 be able to write up and present user research in a professional manner \n \nClass Size \nThis module can accommodate upto 20 Part II, Part III and MPhil students. \n \nRecommended reading \nBrad A. Myers and Richard McDaniel (2000). Demonstrational Interfaces: Sometimes You Need \na Little Intelligence, Sometimes You Need a Lot. \n \nAlan Blackwell (2024). Moral Codes: Designing alternatives to AI \n \nAssessment - Part II Students \nThe format will be largely practical, with students carrying out an individual mini-project involving \nempirical research investigation. \n \n\nAssignment 1: six incremental submissions which together contribute 20% to the final module \nmark. \n \nAssignment 2: Final report - 80% of the final module mark \n \n\n",
        "6th Semester \nADVANCED COMPUTER ARCHITECTURE \nAims \nThis course examines the techniques and underlying principles that are used to design \nhigh-performance computers and processors. Particular emphasis is placed on understanding \nthe trade-offs involved when making design decisions at the architectural level. A range of \nprocessor architectures are explored and contrasted. In each case we examine their merits and \nlimitations and how ultimately the ability to scale performance is restricted. \n \nLectures \nIntroduction. The impact of technology scaling and market trends. \nFundamentals of Computer Design. Amdahl\u2019s law, energy/performance trade-offs, ISA design. \nAdvanced pipelining. Pipeline hazards; exceptions; optimal pipeline depth; branch prediction; \nthe branch target buffer [2 lectures] \nSuperscalar techniques. Instruction-Level Parallelism (ILP); superscalar processor architecture \n[2 lectures] \nSoftware approaches to exploiting ILP. VLIW architectures; local and global instruction \nscheduling techniques; predicated instructions and support for speculative compiler \noptimisations. \nMultithreaded processors. Coarse-grained, fine-grained, simultaneous multithreading \nThe memory hierarchy. Caches; programming for caches; prefetching [2 lectures] \nVector processors. Vector machines; short vector/SIMD instruction set extensions; stream \nprocessing \nChip multiprocessors. The communication model; memory consistency models; false sharing; \nmultiprocessor memory hierarchies; cache coherence protocols; synchronization [2 lectures] \nOn-chip interconnection networks. Bus-based interconnects; on-chip packet switched networks \nSpecial-purpose architectures. Converging approaches to computer design \nObjectives \nAt the end of the course students should \n \nunderstand what determines processor design goals; \nappreciate what constrains the design process and how architectural trade-offs are made within \nthese constraints; \nbe able to describe the architecture and operation of pipelined and superscalar processors, \nincluding techniques such as branch prediction, register renaming and out-of-order execution; \nhave an understanding of vector, multithreaded and multi-core processor architectures; \nfor the architectures discussed, understand what ultimately limits their performance and \napplication domain. \nRecommended reading \n* Hennessy, J. and Patterson, D. (2012). Computer architecture: a quantitative approach. \nElsevier (5th ed.) ISBN 9780123838728. (the 3rd and 4th editions are also good)\n \n\nCRYPTOGRAPHY \n \nAims \nThis course provides an overview of basic modern cryptographic techniques and covers \nessential concepts that users of cryptographic standards need to understand to achieve their \nintended security goals. \n \nLectures \nCryptography. Overview, private vs. public-key ciphers, MACs vs. signatures, certificates, \ncapabilities of adversary, Kerckhoffs\u2019 principle. \nClassic ciphers. Attacks on substitution and transposition ciphers, Vigen\u00e9re. Perfect secrecy: \none-time pads. \nPrivate-key encryption. Stream ciphers, pseudo-random generators, attacking \nlinear-congruential RNGs and LFSRs. Semantic security definitions, oracle queries, advantage, \ncomputational security, concrete-security proofs. \nBlock ciphers. Pseudo-random functions and permutations. Birthday problem, random \nmappings. Feistel/Luby-Rackoff structure, DES, TDES, AES. \nChosen-plaintext attack security. Security with multiple encryptions, randomized encryption. \nModes of operation: ECB, CBC, OFB, CNT. \nMessage authenticity. Malleability, MACs, existential unforgeability, CBC-MAC, ECBC-MAC, \nCMAC, birthday attacks, Carter-Wegman one-time MAC. \nAuthenticated encryption. Chosen-ciphertext attack security, ciphertext integrity, \nencrypt-and-authenticate, authenticate-then-encrypt, encrypt-then-authenticate, padding oracle \nexample, GCM. \nSecure hash functions. One-way functions, collision resistance, padding, Merkle-Damg\u00e5rd \nconstruction, sponge function, duplex construct, entropy pool, SHA standards. \nApplications of secure hash functions. HMAC, stream authentication, Merkle tree, commitment \nprotocols, block chains, Bitcoin. \nKey distribution problem. Needham-Schroeder protocol, Kerberos, hardware-security modules, \npublic-key encryption schemes, CPA and CCA security for asymmetric encryption. \nNumber theory, finite groups and fields. Modular arithmetic, Euclid\u2019s algorithm, inversion, \ngroups, rings, fields, GF(2n), subgroup order, cyclic groups, Euler\u2019s theorem, Chinese \nremainder theorem, modular roots, quadratic residues, modular exponentiation, easy and \ndifficult problems. [2 lectures] \nDiscrete logarithm problem. Baby-step-giant-step algorithm, computational and decision \nDiffie-Hellman problem, DH key exchange, ElGamal encryption, hybrid cryptography, Schnorr \ngroups, elliptic-curve systems, key sizes. [2 lectures] \nTrapdoor permutations. Security definition, turning one into a public-key encryption scheme, \nRSA, attacks on \u201ctextbook\u201d RSA, RSA as a trapdoor permutation, optimal asymmetric \nencryption padding, common factor attacks. \nDigital signatures. One-time signatures, RSA signatures, Schnorr identification scheme, \nElGamal signatures, DSA, PS3 hack, certificates, PKI. \nObjectives \nBy the end of the course students should \n\n \nbe familiar with commonly used standardized cryptographic building blocks; \nbe able to match application requirements with concrete security definitions and identify their \nabsence in naive schemes; \nunderstand various adversarial capabilities and basic attack algorithms and how they affect key \nsizes; \nunderstand and compare the finite groups most commonly used with discrete-logarithm \nschemes; \nunderstand the basic number theory underlying the most common public-key schemes, and \nsome efficient implementation techniques. \nRecommended reading \nKatz, J., Lindell, Y. (2015). Introduction to modern cryptography. Chapman and Hall/CRC (2nd \ned.).\n \n\nE-COMMERCE \n \nAims \nThis course aims to give students an outline of the issues involved in setting up an e-commerce \nsite. \n \nLectures \nThe history of electronic commerce. Mail order; EDI; web-based businesses, credit card \nprocessing, PKI, identity and other hot topics. \nNetwork economics. Real and virtual networks, supply-side versus demand-side scale \neconomies, Metcalfe\u2019s law, the dominant firm model, the differentiated pricing model Data \nProtection Act, Distance Selling regulations, business models. \nWeb site design. Stock and price control; domain names, common mistakes, dynamic pages, \ntransition diagrams, content management systems, multiple targets. \nWeb site implementation. Merchant systems, system design and sizing, enterprise integration, \npayment mechanisms, CRM and help desks. Personalisation and internationalisation. \nThe law and electronic commerce. Contract and tort; copyright; binding actions; liabilities and \nremedies. Legislation: RIP; Data Protection; EU Directives on Distance Selling and Electronic \nSignatures. \nPutting it into practice. Search engine interaction, driving and analysing traffic; dynamic pricing \nmodels. Integration with traditional media. Logs and audit, data mining modelling the user. \ncollaborative filtering and affinity marketing brand value, building communities, typical behaviour. \nFinance. How business plans are put together. Funding Internet ventures; the recent hysteria; \nmaximising shareholder value. Future trends. \nUK and International Internet Regulation. Data Protection Act and US Privacy laws; HIPAA, \nSarbanes-Oxley, Security Breach Disclosure, RIP Act 2000, Electronic Communications Act \n2000, Patriot Act, Privacy Directives, data retention; specific issues: deep linking, Inlining, brand \nmisuse, phishing. \nObjectives \nAt the end of the course students should know how to apply their computer science skills to the \nconduct of e-commerce with some understanding of the legal, security, commercial, economic, \nmarketing and infrastructure issues involved. \n \nRecommended reading \nShapiro, C. and Varian, H. (1998). Information rules. Harvard Business School Press. \n \nAdditional reading: \n \nStandage, T. (1999). The Victorian Internet. Phoenix Press. Klemperer, P. (2004). Auctions: \ntheory and practice. Princeton Paperback ISBN 0-691-11925-2.\n \n\nMACHINE LEARNING AND BAYESIAN INFERENCE \nAims \nThe Part 1B course Artificial Intelligence introduced simple neural networks for supervised \nlearning, and logic-based methods for knowledge representation and reasoning. This course \nhas two aims. First, to provide a rigorous introduction to machine learning, moving beyond the \nsupervised case and ultimately presenting state-of-the-art methods. Second, to provide an \nintroduction to the wider area of probabilistic methods for representing and reasoning with \nknowledge. \n \nLectures \nIntroduction to learning and inference. Supervised, unsupervised, semi-supervised and \nreinforcement learning. Bayesian inference in general. What the naive Bayes method actually \ndoes. Review of backpropagation. Other kinds of learning and inference. [1 lecture] \nHow to classify optimally. Treating learning probabilistically. Bayesian decision theory and Bayes \noptimal classification. Likelihood functions and priors. Bayes theorem as applied to supervised \nlearning. The maximum likelihood and maximum a posteriori hypotheses. What does this teach \nus about the backpropagation algorithm? [2 lectures] \nLinear classifiers I. Supervised learning via error minimization. Iterative reweighted least \nsquares. The maximum margin classifier. [2 lectures] \nGaussian processes. Learning and inference for regression using Gaussian process models. [2 \nlectures] \nSupport vector machines (SVMs). The kernel trick. Problem formulation. Constrained \noptimization and the dual problem. SVM algorithm. [2 lectures] \nPractical issues. Hyperparameters. Measuring performance. Cross-validation. Experimental \nmethods. [1 lecture] \nLinear classifiers II. The Bayesian approach to neural networks. [1 lecture] \nUnsupervised learning I. The k-means algorithm. Clustering as a maximum likelihood problem. \n[1 lecture] \nUnsupervised learning II. The EM algorithm and its application to clustering. [1 lecture] \nBayesian networks I. Representing uncertain knowledge using Bayesian networks. Conditional \nindependence. Exact inference in Bayesian networks. [2 lectures] \nBayesian networks II. Markov random fields. Approximate inference. Markov chain Monte Carlo \nmethods. [1 lecture] \nObjectives \nAt the end of this course students should: \n \nUnderstand how learning and inference can be captured within a probabilistic framework, and \nknow how probability theory can be applied in practice as a means of handling uncertainty in AI \nsystems. \nUnderstand several algorithms for machine learning and apply those methods in practice with \nproper regard for good experimental practice. \nRecommended reading \nIf you are going to buy a single book for this course I recommend: \n \n\n* Bishop, C.M. (2006). Pattern recognition and machine learning. Springer. \n \nThe course text for Artificial Intelligence I: \n \nRussell, S. and Norvig, P. (2010). Artificial intelligence: a modern approach. Prentice Hall (3rd \ned.). \n \ncovers some relevant material but often in insufficient detail. Similarly: \n \nMitchell, T.M. (1997). Machine Learning. McGraw-Hill. \n \ngives a gentle introduction to some of the course material, but only an introduction. \n \nRecently a few new books have appeared that cover a lot of relevant ground well. For example: \n \nBarber, D. (2012). Bayesian Reasoning and Machine Learning. Cambridge University Press. \nFlach, P. (2012). Machine Learning: The Art and Science of Algorithms that Make Sense of \nData. Cambridge University Press. \nMurphy, K.P. (2012). Machine Learning: A Probabilistic Perspective. MIT Press.\n \n\nOPTIMISING COMPILERS \n \nAims \nThe aims of this course are to introduce the principles of program optimisation and related \nissues in decompilation. The course will cover optimisations of programs at the abstract syntax, \nflowgraph and target-code level. It will also examine how related techniques can be used in the \nprocess of decompilation. \n \nLectures \nIntroduction and motivation. Outline of an optimising compiler. Optimisation partitioned: analysis \nshows a property holds which enables a transformation. The flow graph; representation of \nprogramming concepts including argument and result passing. The phase-order problem. \nKinds of optimisation. Local optimisation: peephole optimisation, instruction scheduling. Global \noptimisation: common sub-expressions, code motion. Interprocedural optimisation. The call \ngraph. \nClassical dataflow analysis. Graph algorithms, live and avail sets. Register allocation by register \ncolouring. Common sub-expression elimination. Spilling to memory; treatment of \nCSE-introduced temporaries. Data flow anomalies. Static Single Assignment (SSA) form. \nHigher-level optimisations. Abstract interpretation, Strictness analysis. Constraint-based \nanalysis, Control flow analysis for lambda-calculus. Rule-based inference of program properties, \nTypes and effect systems. Points-to and alias analysis. \nTarget-dependent optimisations. Instruction selection. Instruction scheduling and its phase-order \nproblem. \nDecompilation. Legal/ethical issues. Some basic ideas, control flow and type reconstruction. \nObjectives \nAt the end of the course students should \n \nbe able to explain program analyses as dataflow equations on a flowgraph; \nknow various techniques for high-level optimisation of programs at the abstract syntax level; \nunderstand how code may be re-scheduled to improve execution speed; \nknow the basic ideas of decompilation. \nRecommended reading \n* Nielson, F., Nielson, H.R. and Hankin, C.L. (1999). Principles of program analysis. Springer. \nGood on part A and part B. \nAppel, A. (1997). Modern compiler implementation in Java/C/ML (3 editions). \nMuchnick, S. (1997). Advanced compiler design and implementation. Morgan Kaufmann. \nWilhelm, R. (1995). Compiler design. Addison-Wesley. \nAho, A.V., Sethi, R. and Ullman, J.D. (2007). Compilers: principles, techniques and tools. \nAddison-Wesley (2nd ed.).\n \n\nQUANTUM COMPUTING \n \nAims \nThe principal aim of the course is to introduce students to the basics of the quantum model of \ncomputation. The model will be used to study algorithms for searching, factorisation and \nquantum chemistry as well as other important topics in quantum information such as \ncryptography and super-dense coding. Issues in the complexity of computation will also be \nexplored. A second aim of the course is to introduce student to near-term quantum computing. \nTo this end, error-correction and adiabatic quantum computing are studied. \n \nLectures \nBits and qubits. Introduction to quantum states and measurements with motivating examples. \nComparison with discrete classical states. \nLinear algebra. Review of linear algebra: vector spaces, linear operators, Dirac notation, the \ntensor product. \nThe postulates of quantum mechanics. Postulates of quantum mechanics, incl. evolution and \nmeasurement. \nImportant concepts in quantum mechanics. Entanglement, distinguishing orthogonal and \nnon-orthogonal quantum states, no-cloning and no signalling. \nThe quantum circuit model. The circuit model of quantum computation. Quantum gates and \ncircuits. Universality of the quantum circuit model, and efficient simulation of arbitrary two-qubit \ngates with a standard universal set of gates. \nSome applications of quantum information. Applications of quantum information (other than \nquantum computation): quantum key distribution, superdense coding and quantum teleportation. \nDeutsch-Jozsa algorithm. Introducing Deutsch\u2019s problem and Deutsch\u2019s algorithm leading onto \nits generalisation, the Deutsch-Jozsa algorithm. \nQuantum search. Grover\u2019s search algorithm: analysis and lower bounds. \nQuantum Fourier Transform and Quantum Phase Estimation. Definition of the Quantum Fourier \nTransform (QFT), and efficient representation thereof as a quantum circuit. Application of the \nQFT to enable Quantum Phase Estimation (QPE). \nApplication 1 of QFT / QPE: Factoring. Shor\u2019s algorithm: reduction of factoring to period finding \nand then using the QFT for period finding. \nApplication 2 of QFT / QPE: Quantum Chemistry. Efficient simulation of quantum systems, and \napplications to real-world problems in quantum chemistry. \nQuantum complexity. Quantum complexity classes and their relationship to classical complexity. \nComparison with probabilistic computation. \nQuantum error correction. Introducing the concept of quantum error correction required for the \nfollowing lecture on fault-tolerance. \nFault tolerant quantum computing. Elements of fault tolerant computing; the threshold theorem \nfor efficient suppression of errors. \nAdiabatic quantum computing and quantum optimisation. The quantum adiabatic theorem, and \nadiabatic optimisation. Quantum annealing and D-Wave. \nCase studies in near-term quantum computation. Examples of state-of-the-art quantum \nalgorithms and computers, including superconducting and networked quantum computers. \n\nObjectives \nAt the end of the course students should: \n \nunderstand the quantum model of computation and the basic principles of quantum mechanics; \nbe familiar with basic quantum algorithms and their analysis; \nbe familiar with basic quantum protocols such as teleportation and superdense coding; \nsee how the quantum model relates to classical models of deterministic and probabilistic \ncomputation. \nappreciate the importance of efficient error-suppression if quantum computation is to yield an \nadvantage over classical computation. \ngain a general understanding of the important topics in near-term quantum computing, including \nadiabatic quantum computing. \nRecommended reading \nBooks: \nNielsen M.A., Chuang I.L. (2010). Quantum Computation and Quantum Information. Cambridge \nUniversity Press. \nMermin N.D. (2007). Quantum Computer Science: An Introduction. Cambridge University Press. \nMcGeoch, C. (2014). Adiabatic Quantum Computation and Quantum Annealing Theory and \nPractice. Morgan and Claypool. https://ieeexplore.ieee.org/document/7055969 \n \nPapers: \n \nBraunstein S.L. (2003). Quantum computation tutorial. Available at: \nhttps://www-users.cs.york.ac.uk/~schmuel/comp/comp_best.pdf \nAharonov D., Quantum computation [arXiv:quant-ph/9812037] \nAlbash T., Adiabatic Quantum Computing https://arxiv.org/pdf/1611.04471.pdf \nMcCardle S. et al, Quantum computational chemistry https://arxiv.org/abs/1808.10402 \n \nOther lecture notes: \n \nAndrew Childs (University of Maryland): http://cs.umd.edu/~amchilds/qa/ \n \n\nRANDOMISED ALGORITHMS \n \nAims: \nThe aim of this course is to introduce advanced techniques in the design and analysis \nalgorithms, with a strong focus on randomised algorithms. It covers essential tools and ideas \nfrom probability, optimisation and graph theory, and develops this knowledge through a variety \nof examples of algorithms and processes. This course will demonstrate that randomness is not \nonly an important design technique which often leads to simpler and more elegant algorithms, \nbut may also be essential in settings where time and space are restricted.   \n \nLectures: \nIntroduction. Course Overview. Why Randomised Algorithms? A first Randomised Algorithm for \nthe MAX-CUT problem. [1 Lecture]  \n \nConcentration Inequalities. Moment-Generating Functions and Chernoff Bounds. Extension: \nMethod of Bounded Independent Differences. Applications: Balls-into-Bins, Quick-Sort and Load \nBalancing. [approx. 2 Lectures] \n \nMarkov Chains and Mixing Times. Random Walks on Graphs. Application: Randomised \nAlgorithm for the 2-SAT problem. Mixing Times of Markov Chains. Application: Load Balancing \non Networks. [approx. 2 Lectures] \n \nLinear Programming and Applications. Definitions and Applications. Formulating Linear \nPrograms. The Simplex Algorithm. Finding Initial Solutions. How to use Linear Programs and \nBranch & Bound to Solve a Classical TSP instance. [approx. 3 Lectures] \n \nRandomised Approximation Algorithms. Randomised Approximation Schemes. Linearity of \nExpectations, Derandomisation. Deterministic and Randomised Rounding of Linear Programs. \nApplications: MAX3-SAT problem, Weighted Vertex Cover, Weighted Set Cover, MAX-SAT. \n[approx. 2 Lectures] \n \nSpectral Graph Theory and Clustering. Eigenvalues of Graphs and Matrices: Relations between \nEigenvalues and Graph Properties, Spectral Graph Drawing. Spectral Clustering: Conductance, \nCheeger's Inequality. Spectral Partitioning Algorithm [approx. 2 Lectures] \n \nObjectives: \nBy the end of the course students should be able to: \n \nlearn how to use randomness in the design of algorithms, in particular, approximation \nalgorithms; \nlearn the basics of linear programming, integer programming and randomised rounding; \napply randomisation to various problems coming from optimisation, machine learning and data \nscience; \nuse results from probability theory to analyse the performance of randomised algorithms. \n\nRecommended reading \n* Michael Mitzenmacher and Eli Upfal. Probability and Computing: Randomized Algorithms and \nProbabilistic Analysis., Cambridge University Press, 2nd edition. \n \n* David P. Williamson and David B. Shmoys. The Design of Approximation Algorithms, \nCambridge University Press, 2011 \n \n* Cormen, T.H., Leiserson, C.D., Rivest, R.L. and Stein, C. (2009). Introduction to Algorithms. \nMIT Press (3rd ed.). ISBN 978-0-262-53305-8 \n \n \n\nCLOUD COMPUTING \nAims \nThis module aims to teach students the fundamentals of Cloud Computing covering topics such \nas virtualization, data centres, cloud resource management, cloud storage and popular cloud \napplications including batch and data stream processing. Emphasis is given on the different \nbackend technologies to build and run efficient clouds and the way clouds are used by \napplications to realise computing on demand. The course will include practical tutorials on cloud \ninfrastructure technologies. Students will be assessed via a Cloud-based coursework project. \n \nLectures \nIntroduction to Cloud Computing \nData centres \nVirtualization I \nVirtualization II \nMapReduce \nMapReduce advanced \nResource management for virtualized data centres \nCloud storage \nCloud-based data stream processing \nObjectives \nBy the end of the course students should: \n \nunderstand how modern clouds operate and provide computing on demand; \nunderstand about cloud availability, performance, scalability and cost; \nknow about cloud infrastructure technologies including virtualization, data centres, resource \nmanagement and storage; \nknow how popular applications such as batch and data stream processing run efficiently on \nclouds; \nAssessment \nAssignment 1, worth 55% of the final mark. Develop batch processing applications running on a \ncluster assessed through automatic testing, code inspection and a report. \nAssignment 2, worth 30% of the final mark: Design and develop a framework for running the \nbatch processing applications from Assignment 1 on a cluster according to certain resource \nallocation policies. This assigment will be assessed by an expert, testing, code inspection and a \nreport. \nAssignment 3, worth 15% of the final mark: Write two individual project reports describing your \nwork for assignments 1 and 2. Write two sets of scripts to manage and run your code. Reports \nand scripts will be submitted with each assignment. \nRecommended reading \nMarinescu, D.C. Cloud Computing, Theory and Practice. Morgan Kaufmann. \nBarham, P., et. al. (2003). \u201cXen and the Art of Virtualization\u201d. In Proceedings of SOSP 2003. \nCharkasova, L., Gupta, D. and Vahdat, A. (2007). \u201cComparison of the Three CPU Schedulers in \nXen\u201d. In SIGMETRICS 2007. \n\nDean, J. and Ghemawat, S. (2004). \u201cMapReduce: Simplified Data Processing on Large \nClusters\u201d. In Proceedings of OSDI 2004. \nZaharia, M, et al. (2008). \u201cImproving MapReduce Performance in Heterogeneous \nEnvironments\u201d. In Proceedings of OSDI 2008. \nHindman, A., et al. (2011). \u201cMesos: A Platform for Fine-Grained Resource Sharing in Data \nCenter\u201d. In Proceedings of NSDI 2011. \nSchwarzkopf, M., et al. (2013). \u201cOmega: Flexible, Scalable Schedulers for Large Compute \nClusters\u201d. In EuroSys 2013. \nGhemawat, S. (2003). \u201cThe Google File System\u201d. In Proceedings of SOSP 2003. \nChang, F. (2006). \u201cBigtable: A Distributed Storage System for Structured Data\u201d. In Proceedings \nof OSDI 2006. \nFernandez, R.C., et al. (2013). \u201cIntegrating Scale Out and Fault Tolerance in Stream Processing \nusing Operator State Management\u201d. In SIGMOD 2013.\n \n\nCOMPUTER SYSTEMS MODELLING \n \nAims \n \nThe aims of this course are to introduce the concepts and principles of mathematical modelling \n \nand simulation, with particular emphasis on using queuing theory and control theory for \nunderstanding the behaviour of computer and communications systems. \n \nSyllabus \n \n1.     Overview of computer systems modeling using both analytic techniques and simulation. \n \n2.     Introduction to discrete event simulation. \n \na.     Simulation techniques \n \nb.     Random number generation methods \n \nc.     Statistical aspects: confidence intervals, stopping criteria \n \nd.     Variance reduction techniques. \n \n3.     Stochastic processes (builds on starred material in Foundations of Data Science) \n \na.     Discrete and continuous stochastic processes. \n \nb.     Markov processes and Chapman-Kolmogorov equations. \n \nc.     Discrete time Markov chains. \n \nd.     Ergodicity and the stationary distribution. \n \ne.     Continuous time Markov chains. \n \nf.      Birth-death processes, flow balance equations. \n \ng.     The Poisson process. \n \n4.     Queueing theory \n \na.     The M/M/1 queue in detail. \n \n\nb.     The equilibrium distribution with conditions for existence and common performance \nmetrics. \n \nc.     Extensions of the M/M/1 queue: the M/M/k queue, the M/M/infinity queue. \n \nd.     Queueing networks. Jacksonian networks. \n \ne.     The M/G/1 queue. \n \n5.     Signals, systems, and transforms \n \na.     Discrete- and continuous-time convolution. \n \nb.     Signals. The complex exponential signal. \n \nc.     Linear Time-Invariant Systems. Modeling practical systems as an LTI system. \n \nd.     Fourier and Laplace transforms. \n \n6.     Control theory. \n \na.     Controlled systems. Modeling controlled systems. \n \nb.     State variables. The transfer function model. \n \nc.     First-order and second-order systems. \n \nd.     Feedback control. PID control. \n \ne.     Stability. BIBO stability. Lyapunov stability. \n \nf.      Introduction to Model Predictive Control. \n \nObjectives \n \nAt the end of the course students should \n \n\u00b7       Be aware of different approaches to modeling a computer system; their pros and cons \n \n\u00b7       Be aware of the issues in building a simulation of a computer system and analysing the \nresults obtained \n \n\u00b7       Understand the concept of a stochastic process and how they arise in practice \n \n\n\u00b7       Be able to build simple Markov models and understand the critical modelling assumptions \n \n\u00b7       Be able to solve simple birth-death processes \n \n\u00b7       Understand and use M/M/1 queues to model computer systems \n \n\u00b7       Be able to model a computer system as a linear time-invariant system \n \n\u00b7       Understand the dynamics of a second-order controlled system \n \n\u00b7       Design a PID control for an LTI system \n \n\u00b7       Understand what is meant by BIBO and Lyapunov stability \n \nAssessment \n \nThere will be one programming assignment to build a discrete event simulator and using it to \nsimulate an M/M/1 and an M/D/1 queue (worth 15% of the final marks). There will also be two \none-hour in-person assessments for the course on: Stochastic processes and Queueing theory \n(40%) and Signals, Systems, Transforms, and Control Theory (45%). \n \nReference books \n \nKeshav, S. (2012)*. Mathematical Foundations of Computer Networking. Addison-Wesley. A \ncustomized PDF will be made available to class participants. \n \nKleinrock, L. (1975). Queueing systems, vol. 1. Theory. Wiley. \n \nKraniauskas, Peter. Transforms in signals and systems. Addison-Wesley Longman, 1992. \n \nJain, R. (1991). The art of computer systems performance analysis. Wiley. \n \n \n\nCOMPUTING EDUCATION \n \nAims \nThis module considers various aspects of the teaching and learning of computer science in \nschool, including practical experience, and provides an introduction to the field of computing \neducation research. It is offered by the Raspberry Pi Computing Education Research Centre, \npart of the Department of Computer Science and Technology, in conjunction with local schools \nin the Cambridge area. Students will have the opportunity to observe, plan and deliver a lesson, \nand explore the teaching of computing in a secondary education setting. The lesson taught will \nbe on a topic within the computing/computer science curriculum in England but will be \nnegotiated with the teacher mentoring the school placement. In the sessions in the Department, \nstudents will be introduced to elements of pedagogy and assessment alongside current issues \nin computing education. The course can also serve as a useful introduction to research in \ncomputing education for those interested in this area. Assessment will include lesson planning, \npresentations, an in-person viva and a 3000-word report which will include evidence of \nengagement with relevant research underpinning the lessons and activities undertaken. An \ninformation session relating to the module will be available in the preceding Easter Term. The \narrangement of school placements and DBS checks will be carried out in Michaelmas Term, so \nstudents should be prepared to engage in these preparatory activities. \n \nLectures \nThis is not a traditional lecture course and the term will be structured as follows: \n \nWeek 1: Introduction to computing education & visit to school placement \nWeek 2: In school (observation, supporting teacher) \nWeek 3: In school (observation, supporting teacher) \nWeek 4: In school (co-teach / small group teaching) \nWeek 5: Computing pedagogy and assessment (school half-term) \nWeek 6: In school (co-teach / small group teaching) \nWeek 7: In school (teach part/whole lesson) \nWeek 8: Presentations \nObjectives \nBy the end of the course students should: \n \ngain experience of computing education in practice \ndemonstrate an ability to communicate an aspect of computer science clearly and effectively \nbe able to plan and design a teaching activity/lesson with consideration of the audience \nbe able to thoroughly evaluate the design and implementation of a teaching/activity lesson \nbe able to demonstrate an understanding of relevant theories of learning and pedagogy from the \nliterature and how they apply to the learning/teaching of the topic under consideration \ngain an understanding of the breadth and depth of the field of computing education research \nClass Size \nThis module can accommodate up to 10 Part II students. \n \n\nRecommended reading \nBrown, N. C., Sentance, S., Crick, T., & Humphreys, S. (2014). Restart: The resurgence of \ncomputer science in UK schools. ACM Transactions on Computing Education (TOCE), 14(2), \n1-22. https://doi.org/10.1145/2602484 \n \nSentance, S., Barendsen, E., Howard, N. R., & Schulte, C. (Eds.). (2023). Computer science \neducation: Perspectives on teaching and learning in school. Bloomsbury Publishing. \n \nGrover, S. (Ed). 2020. Computer Science in K-12: An A-to-Z Handbook on Teaching \nProgramming. Edfinity. https://www.shuchigrover.com/atozk12cs/ \n \nRaspberry Pi Foundation (2022). Hello World Big Book of Pedagogy. \nhttps://www.raspberrypi.org/hello-world/issues/the-big-book-of-computing-pedagogy \n \nAssessment \nWeek 2: Reflections on an observed lesson (10%, graded out of 20) \nWeek 4: Submission of a lesson plan outline (10% graded out of 20) \nWeek 8: Delivery of an oral presentation relating to the lesson delivery (10% graded out of 20) \nAfter course completion: 3000 word report (60%), graded out of 20 \nViva with assessor (10%) (pass/fail) \nFurther Information \nWe will find placements for students in secondary schools within cycling or bus distance from \nthe Department/centre of Cambridge. There will be a timetabled slot for the course that students \ncan use to go to their school placement, but students may need to be flexible and liaise with the \nschool to find the best time for working with a specific class. \n \n \n\nDEEP NEURAL NETWORKS \n \nObjectives \nYou will gain detailed knowledge of \n \nCurrent understanding of generalization in neural networks vs classical statistical models. \nOptimization procedures for neural network models such as stochastic gradient descent and \nADAM. \nAutomatic differentiation and at least one software framework (PyTorch, TensorFolow) as well as \nan overview of other software approaches. \nArchitectures that are deployed to deal with different data types such as images or sequences \nincluding a. convolutional networks b. recurrent networks and c. transformers. \nIn addition you will gain knowledge of more advanced topics reflecting recent research in \nmachine learning. These change from year to year, examples include: \n \nApproaches to unsupervised learning including autoencoders and self-supervised learning.. \nTechniques for deploying models in low data regimes such as transfer learning and \nmeta-learning. \nTechniques for propagating uncertainty such as Bayesian neural networks. \nReinforcement learning and its applications to robotics or games. \nGraph Neural Networks and Geometric Deep Learning. \nTeaching Style \nThe start of the course will focus on the latest undertanding of current theory of neural networks, \ncontrasting with previous classical understandings of generalization performance. Then we will \nmove to practical examples of network architectures and deployment. We will end with more \nadvanced topics reflecting current research, mostly delivered by guest lecturers from the \nDepartment and beyond. \n \nSchedule \nWeek 1 \nTwo lectures: Generalization and Approximation with Neural Networks \n \nWeek 2 \nTwo lectures: Optimization: Stochastic Gradient Descent and ADAM \n \nWeek 3 \nTwo lectures: Hardware Acceleration and Convolutional Neural Networks \n \nWeek 4 \nTwo lectures: Vanishing Gradients, Recurrent and Residual Networks, Sequence Modeling. \n \nWeek 5 \nTwo lectures: Attention, The Transformer Architecture, Large Language Models \n \n\nWeek 6-8 \nLectures and guest lectures from the special topics below. \n \nSpecial topics \nSelf-Supervised Learning, AutoEncoders, Generative Modeling of Images \nHardware Implementations \nReinforcement learning \nTransfer learning and meta-learning \nUncertainty and Bayesian Neural Networks \nGraph Neural Networks \nAssessment \nAssessment involves solving a number of exercises in a jupyter notebook environment. \nFamiliarity with the python programming language is assumed. The exercises rely on the \npytorch deep learning framework, the syntax of which we don\u2019t cover in detail in the lectures, \nstudents are referred to documentation and tutorials to learn this component of the assessment. \n \nAssignment 1 \n \n30% of the total marks, with guided exercises. \n \nAssignment 2 \n \n70% of the total marks, a combination of guided exercises and a more exploratory mini-project.\n\nEXTENDED REALITY \n \nSyllabus & Schedule \nWeek 1 \nIntroduction \nCourse schedule and concept \nThe history of AR/VR/XR \nApplications of AR/VR/XR \nOverview of the XR pipeline \u2013 3 main components of XR \nDisplay technologies and rendering \nMachine perception and input interfaces \nContent generation \u2013 geometry, appearance, motion \nOverview of the AR/VR/XR frameworks \nPractical 1 \nProject structure \nProject themes \nIntroduction to the XR programming platform \nWeek 2 \n3D scene representations and rendering \nObject representations with polygonal meshes \nEfficient rendering via rasterization \nPractical XR rendering of textured geometries \nTracking and pose estimation for XR \nRigid transforms \nCamera pose estimation \nBody/ hand tracking \nPractical 2 \nProject draft proposal due \nProject feedback \nRendering and displaying a first object \nWeek 3 \n3D scene capture and understanding \nDepth estimation \n3D geometry capture \nAppearance acquisition \nLighting estimation and relighting \nLighting models for 3D scenes \nEstimating lighting in a scene \nRelighting rendered models \nPractical 3 \nAcquiring and utilizing depth from sensors \nRelighting a displayed object \nWeek 4 \nPhysically-based simulation of rigid objects \n\nRigid-body dynamics \nCollision detection \nTime integration \nPhysically-based simulation of non-rigid volumetric phenomena \nParticle systems \nBasic fluid dynamics \nPractical 4 \nPractical due \nProject prototype due \nProject check-in \nSimulating and rendering a simple object \nWeek 5 \nAdvanced topics in XR I: AR/VR displays technologies \nMicro LED-based displays \nWaveguide-based displays \nNext generation displays \nWeek 6 \nAdvanced topics in XR II \u2013 Mobile AR \nComputer vision on limited hardware \nUX/UI challenges \nContent challenges \nProject due \n  \n \nAssessment \nA practical exercise, worth 20% of the mark. This will cover the basics of AR/VR development \nand some of the practical techniques the students will use in their projects. This is individual \nwork. No mobile device or GPU hours are needed. \nAn AR/VR project worth 80% of the marks: \nCourse projects will be designed by the students following the possible project themes proposed \nby the lecturer and will be checked by the lecturer for appropriateness. \nThe students will form groups of 2-3 to design, implement, report, and present the course \nproject. \nThe final mark will be an implementation mark (40%), a report mark (20%), and a \npresentation/demo mark (20%). Each team member will be evaluated based on their \ncontribution. \nEach project will have extensions to be completed only by the ACS students. Each student will \nwrite a different part of the report, whose author will be clearly marked. Each student will further \nsummarise her/his contributions to the project in the same report. \nNote: Submission is by one submission per group but work by each individual must be clearly \nlabelled.\n \n\nFEDERATED LEARNING: THEORY AND PRACTICE \n \nObjectives \nThis course aims to extend the machine learning knowledge available to students in Part I (or \npresent in typical undergraduate degrees in other universities), and allow them to understand \nhow these concepts can manifest in a decentralized setting. The course will consider both \ntheoretical (e.g., decentralized optimization) and practical (e.g., networking efficiency) aspects \nthat combine to define this growing area of machine learning.  \n \nAt the end of the course students should: \n \nUnderstand popular methods used in federated learning \nBe able to construct and scale a simple federated system \nHave gained an appreciation of the core limitations to existing methods, and the approaches \navailable to cope with these issues \nDeveloped an intuition for related technologies like differential privacy and secure aggregation, \nand are able to use them within typical federated settings  \nCan reason about the privacy and security issues with federated systems \nLectures \nCourse Overview. Introduction to Federated Learning. \nDecentralized Optimization. \nStatistical and Systems Heterogeneity. \nVariations of Federated Aggregation. \nSecure Aggregation. \nDifferential Privacy within Federated Systems. \nExtensions to Federated Analytics. \nApplications to Speech, Video, Images and Robotics. \nLab sessions \nFederating a Centralized ML Classifier. \nBehaviour under Heterogeneity. \nScaling a Federated Implementation. \nExploring Privacy with Federated Settings \nRecommended Reading \nReadings will be assigned for each lecture. Readings will be taken from either research papers, \ntutorials, source code or blogs that provide more comprehensive treatment of taught concepts. \n \nAssessment - Part II Students \nFour labs are performed during the course, and students receive 10% of their total grade for \nwork done as part of each lab. (For a total of 40% of the total grade from lab work alone). Labs \nwill primarily provide hands-on teaching opportunities, that are then utilized within the lab \nassignment which is completed outside of the lab contact time. MPhil and Part III students will \nbe given additional questions to answer within their version of the lab assignment which will \ndiffer from the assignment given to Part II CST students. \n \n\nThe remainder of the course grade (60%) will be given based on a hands-on project that applies \nthe concepts taught in lectures and labs. This hands-on project will be assessed based on upon \na combination of source code, related documentation and brief 8-minute pre-recorded talk that \nsummarizes key project elements (any slides used are also submitted as part of the project). \nPlease note, that in the case of Part II CST students, the talk itself is not examinable -- as such \nwill be made optional to those students. \n \nA range of possible practical projects will be described and offered to students to select from, or \nalternatively students may propose their own. MPhil and Part III students will select from a \nproject pool that is separate from those offered to Part II CST students. MPhil and Part III \nprojects will contain a greater emphasis on a research element, and the pre-recorded talks \nprovided by this student group will focus on this research contribution. The project will be \nassessed on the level of student understanding demonstrated, the degree of difficulty, \ncorrectness of implementation -- and for Part III/MPhil students the additional criteria of the \nquality and execution of the research methodology, and depth and quality of results analysis. \n \nThis project can be done individually or in groups -- although individual projects will be strongly \nencouraged. It will be required the project is performed using a code repository that also will \ncontain all documentation -- access to this repository will be shared with course staff (e.g., \nlecturer and TAs). Where needed, marks assigned to students within a group will be \ndifferentiated using this repository as an input. Furthermore if groups are formed, members must \nbe either entirely from Part III/MPhil students or Part II CST, i.e., these two student groups \nshould not mix to form a project group. \n \nProjects will be made available publicly. A maximum word count for written contributions for the \nproject will be enforced.\n \n\nMOBILE HEALTH \n \nAims \nThe course aims to explore how wearable and mobile systems sensors can be used to gather \ndata relevant to understand health, how the data can be analysed with advanced signal \nprocessing and machine learning and the performance of these systems in terms of diagnostics \nand disease progression detection. \n \nSyllabus \nCourse Overview. Introduction to Mobile Health. Evaluation metrics and methodology. Basics of \nSignal Processing. \nInertial Measurement Units, Human Activity Recognition (HAR) and Gait Analysis and Machine \nLearning for IMU data. \nRadios, Bluetooth, GPS and Cellular. Epidemiology and contact tracing, Social interaction \nsensing and applications. Location tracking in health monitoring and public health. \nAudio Signal Processing. Voice and Speech Analysis: concepts and data analysis. Body \nSounds analysis. \nPhotoplethysmogram and Light sensing for health (heart and sleep) \nContactless and wireless behaviour and physiological monitoring \nGenerative Models for Wearable Health Data \nTopical Guest Lectures \nObjectives \nThe course aims to explore how wearable and mobile systems sensors can be used to gather \ndata relevant to understand health, how the data can be analysed with advanced signal \nprocessing and machine learning and the performance of these systems in terms of diagnostics \nand disease progression detection. \n \nRoughly, each lecture contains a theory part about the working of \u201csensor signals\u201d or \u201cdata \nanalysis methods\u201d and an application part which contextualises the concepts. \n \nAt the end of the course students should: Understand how mobile/wearable sensors capture \ndata and their working. Understand different approaches to acquiring and analysing sensor data \nfrom different types of sensors. Understand the concept of signal processing applied to time \nseries data and their practical application in health. Be able to extract sensor data and analyse it \nwith basic signal processing and machine learning techniques. Be aware of the different health \napplications of the various sensor techniques. The course will also touch on privacy and ethics \nimplications of the approaches developed in an orthogonal fashion. \n \nRecommended Reading \nPlease see Course Materials for recommended reading for each session. \n \nAssessment - Part II students \nTwo assignments will be based on two datasets which will be provided to the students: \n \n\nAssignment 1 (shared for Part II and Part III/MPhil): this will be based on a dataset and will be \nworth 40% of the final mark. The task of the assessment will be to perform pre-processing and \nbasic data analysis in a \"colab\" and an answer sheet of no more than 1000 words. \n \nAssignment 2 (Part II): This assignment (worth 60% of the final mark) will be a fuller analysis of \na dataset focusing on machine learning algorithms and metrics. Discussion and interpretation of \nthe findings will be reported in a colab and a report of no more than 1200 words.\n \n\nMULTICORE SEMANTICS AND PROGRAMMING \n \nAims \nIn recent years multiprocessors have become ubiquitous, but building reliable concurrent \nsystems with good performance remains very challenging. The aim of this module is to \nintroduce some of the theory and the practice of concurrent programming, from hardware \nmemory models and the design of high-level programming languages to the correctness and \nperformance properties of concurrent algorithms. \n \nLectures \nPart 1: Introduction and relaxed-memory concurrency [Professor P. Sewell] \n \nIntroduction. Sequential consistency, atomicity, basic concurrent problems. [1 block] \nConcurrency on real multiprocessors: the relaxed memory model(s) for x86, ARM, and IBM \nPower, and theoretical tools for reasoning about x86-TSO programs. [2 blocks] \nHigh-level languages. An introduction to C/C++11 and Java shared-memory concurrency. [1 \nblock] \nPart 2: Concurrent algorithms [Dr T. Harris] \n \nConcurrent programming. Simple algorithms (readers/writers, stacks, queues) and correctness \ncriteria (linearisability and progress properties). Advanced synchronisation patterns (e.g. some \nof the following: optimistic and lazy list algorithms, hash tables, double-checked locking, RCU, \nhazard pointers), with discussion of performance and on the interaction between algorithm \ndesign and the underlying relaxed memory models. [3 blocks] \nResearch topics, likely to include one hour on transactional memory and one guest lecture. [1 \nblock] \nObjectives \nBy the end of the course students should: \n \nhave a good understanding of the semantics of concurrent programs, both at the multprocessor \nlevel and the C/Java programming language level; \nhave a good understanding of some key concurrent algorithms, with practical experience. \nAssessment - Part II Students \nTwo assignments each worth 50% \n \nRecommended reading \nHerlihy, M. and Shavit, N. (2008). The art of multiprocessor programming. Morgan Kaufmann.\n\nBUSINESS STUDIES SEMINARS \nAims \nThis course is a series of seminars by former members and friends of the Laboratory about their \nreal-world experiences of starting and running high technology companies. It is a follow on to \nthe Business Studies course in the Michaelmas Term. It provides practical examples and case \nstudies, and the opportunity to network with and learn from actual entrepreneurs. \n \nLectures \nEight lectures by eight different entrepreneurs. \n \nObjectives \nAt the end of the course students should have a better knowledge of the pleasures and pitfalls \nof starting a high tech company. \n \nRecommended reading \nLang, J. (2001). The high-tech entrepreneur\u2019s handbook: how to start and run a high-tech \ncompany. FT.COM/Prentice Hall. \nMaurya, A. (2012). Running Lean: Iterate from Plan A to a Plan That Works. O\u2019Reilly. \nOsterwalder, A. and Pigneur, Y. (2010). Business Model Generation: A Handbook for \nVisionaires, Game Changers, and Challengers. Wiley. \nKim, W. and Mauborgne, R. (2005). Blue Ocean Strategy. Harvard Business School Press. \n \nSee also the additional reading list on the Business Studies web page.\n \n\nHOARE LOGIC AND MODEL CHECKING \n \nAims \nThe course introduces two verification methods, Hoare Logic and Temporal Logic, and uses \nthem to formally specify and verify imperative programs and systems. \n \nThe first aim is to introduce Hoare logic for a simple imperative language and then to show how \nit can be used to formally specify programs (along with discussion of soundness and \ncompleteness), and also how to use it in a mechanised program verifier. \n \nThe second aim is to introduce model checking: to show how temporal models can be used to \nrepresent systems, how temporal logic can describe the behaviour of systems, and finally to \nintroduce model-checking algorithms to determine whether properties hold, and to find \ncounter-examples. \n \nCurrent research trends also will be outlined. \n \nLectures \nPart 1: Hoare logic. Formal versus informal methods. Specification using preconditions and \npostconditions. \nAxioms and rules of inference. Hoare logic for a simple language with assignments, sequences, \nconditionals and while-loops. Syntax-directedness. \nLoops and invariants. Various examples illustrating loop invariants and how they can be found. \nPartial and total correctness. Hoare logic for proving termination. Variants. \nSemantics and metatheory Mathematical interpretation of Hoare logic. Semantics and \nsoundness of Hoare logic. \nSeparation logic Separation logic as a resource-aware reinterpretation of Hoare logic to deal \nwith aliasing in programs with pointers. \nPart 2: Model checking. Models. Representation of state spaces. Reachable states. \nTemporal logic. Linear and branching time. Temporal operators. Path quantifiers. CTL, LTL, and \nCTL*. \nModel checking. Simple algorithms for verifying that temporal properties hold. \nApplications and more recent developments Simple software and hardware examples. CEGAR \n(counter-example guided abstraction refinement). \nObjectives \nAt the end of the course students should \n \nbe able to prove simple programs correct by hand and implement a simple program verifier; \nbe familiar with the theory and use of separation logic; \nbe able to write simple models and specify them using temporal logic; \nbe familiar with the core ideas of model checking, and be able to implement a simple model \nchecker. \nRecommended reading \n\nHuth, M. and Ryan M. (2004). Logic in Computer Science: Modelling and Reasoning about \nSystems. Cambridge University Press (2nd ed.).\n \n\n",
        "7th Semester \nADVANCED TOPICS IN COMPUTER ARCHITECTURE \nAims \nThis course aims to provide students with an introduction to a range of advanced topics in \ncomputer architecture. It will explore the current and future challenges facing the architects of \nmodern computers. These will also be used to illustrate the many different influences and \ntrade-offs involved in computer architecture. \n \nObjectives \nOn completion of this module students should: \n \nunderstand the challenges of designing and verifying modern microprocessors \nbe familiar with recent research themes and emerging challenges \nappreciate the complex trade-offs at the heart of computer architecture \nSyllabus \nEach seminar will focus on a different topic. The proposed topics are listed below but there may \nbe some minor changes to this: \n \nTrends in computer architecture \nState-of-the-art microprocessor design \nMemory system design \nHardware reliability \nSpecification, verification and test (may be be replaced with a different topic) \nHardware security (2) \nHW accelerators and accelerators for machine learning \nEach two hour seminar will include three student presentations (15mins) questions (5mins) and \na broader discussion of the topics (around 30mins). The last part of the seminar will include a \nshort scene setting lecture (around 20mins) to introduce the following week's topic. \n \nAssessment \nEach week students will compare and contrast two of the main papers and submit a written \nsummary and review in advance of each seminar (except when presenting). \n \nStudents will be expected to give a number of 15 minute presentations. \n \nEssays and presentations will be marked out of 10. After dropping the lowest mark, the \nremaining marks will be scaled to give a final score out of 100. \n \nStudents will give at least one presentation during the course. They will not be required to \nsubmit an essay during the weeks they are presenting. \n \nEach presentation will focus on a single paper from the reading list. Marks will be awarded for \nclarity and the communication of the paper's key ideas, an analysis of the work's strengths and \nweaknesses and the work\u2019s relationship to related work and broader trends and constraints. \n\n \nRecommended prerequisite reading \nPatterson, D. A., Hennessy, J. L. (2017). Computer organization and design: The \nHardware/software interface RISC-V edition Morgan Kaufmann. ISBN 978-0-12-812275-4. \n \nHennessy, J. and Patterson, D. (2012). Computer architecture: a quantitative approach. Elsevier \n(5th ed.) ISBN 9780123838728. (the 3rd and 4th editions are also relevant)\n \n\nADVANCED TOPICS IN PROGRAMMING LANGUAGES \nAims \nThis module explores various topics in programming languages beyond the scope of \nundergraduate courses. It aims to introduce students to ideas, results and techniques found in \nthe literature and prepare them for research in the field. \n \nSyllabus and structure \nThe module consists of eight two-hour seminars, each on a particular topic. Topics will vary from \nyear to year, but may include, for example, \n \nAbstract interpretation \nVerified software \nMetaprogramming \nBehavioural types \nProgram synthesis \nVerified compilation \nPartial evaluation \nGarbage collection \nDependent types \nAutomatic differentiation \nDelimited continuations \nModule systems \nThere will be three papers assigned for each topic, which students are expected to read before \nthe seminar. \nEach seminar will include three 20 minute student presentations (15 minutes + 5 minutes \nquestions), time for general discussion of the topic, and a brief overview lecture for the following \nweek\u2019s topic. \nBefore each seminar, except in weeks in which they give presentations, students will submit a \nshort essay about two of the papers. \n \n \nObjectives \nOn completion of this module, students should \n \nbe able to identify some major themes in programming language research \nbe familiar with some classic papers and recent advances \nhave an understanding of techniques used in the field \n \nAssessment \nAssessment consists of: \n \nPresentation of one of the papers from the reading list (typically once or twice in total for each \nstudent, depending on class numbers) \nOne essay per week (except on the first week and on presentation weeks) \n\nAll essays and presentations carry equal numbers of marks. \n \nEssay marks are awarded for understanding, for insight and analysis, and for writing quality. \nEssays should be around 1500 words (with a lower limit of 1450 and upper limit of 1650). \nPresentation marks are awarded for clarity, for effective communication, and for selection and \norganisation of topics. \n \nThere will be seven submissions (essays or presentations) in total and, as in other courses, the \nlowest mark for each student will be disregarded when computing the final mark. \n \nMarking, deadlines and extensions will be handled in accordance with the MPhil Assessment \nGuidelines. \n \nRecommended reading material and resources \nResearch and survey papers from programming language conferences and journals (e.g. \nPOPL, PLDI, TOPLAS, FTPL) will be assigned each week. General background material may \nbe found in: \n \n\u2022 Types and Programming Languages (Benjamin C. Pierce) \n   The MIT Press \n   ISBN 0-262-16209-1 \n \n\u2022 Advanced Topics in Types and Programming Languages (ed. Benjamin C. Pierce) \n   The MIT Press \n   ISBN 0-262-16228-8 \n \n\u2022 Practical Foundations for Programming Languages (Robert Harper) \n   Cambridge University Press \n   9781107150300\n \n\nAFFECTIVE ARTIFICIAL INTELLIGENCE \n \nSynopsis \nAffective Artificial Intelligence (Affective AI) aims to imbue machines with social and emotional \nintelligence (EQ). More specifically, Affective AI aims to create artificially intelligent systems and \nmachines that can recognize, interpret, process, and simulate human social signals and \nbehaviours, expressions, and emotions, to enhance human-AI interaction and communication. \n\\r\\n\\r\\nTo achieve this goal, Affective AI draws upon various scientific disciplines, including \nmachine learning, computer vision, speech / natural language / signal processing, psychology \nand cognitive science, and ethics and social sciences. \n \n  \nBackground & Aims \nAffective Artificial Intelligence (Affective AI) aims to imbue machines with social and emotional \nintelligence (EQ). More specifically, Affective AI aims to create artificially intelligent systems and \nmachines that can recognize, interpret, process, and simulate human social signals and \nbehaviours, expressions, and emotions, to enhance human-AI interaction and communication.  \n \nTo achieve this goal, Affective AI draws upon various scientific disciplines, including machine \nlearning, computer vision, speech / natural language / signal processing, psychology and \ncognitive science, and ethics and social sciences. \n \nAffective AI has direct applications in and implications for the design of innovative interactive \ntechnology (e.g., interaction with chat bots, virtual agents, robots), single and multi-user smart \nenvironments ( e.g., in-car/ virtual / augmented / mixed reality, serious games), public speaking \nand cognitive training, and clinical and biomedical studies (e.g., autism, depression, pain). \n \nThe aim of this module is to impart knowledge and ability needed to make informed choices of \nmodels, data, and machine learning techniques for sensing, recognition, and generation of \naffective and social behaviour (e.g., smile, frown, head nodding/shaking, \nagreement/disagreement) in order to create Affectively intelligent AI systems, with a \nconsideration for various ethical issues (e.g., privacy, bias) arising from the real-world \ndeployment of these systems. \n \n  \n \nSyllabus \nThe following list provides a representative list of topics: \n \nIntroduction, definitions, and overview \nTheories from various disciplines \nSensing from multiple modalities (e.g., vision, audio, bio signals, text) \nData acquisition and annotation \nSignal processing / feature extraction \n\nLearning / prediction / recognition and evaluation \nBehaviour synthesis / generation (e.g., for embodied agents / robots) \nAdvanced topics and ethical considerations (e.g., bias and fairness) \nCross-disciplinary applications (via seminar presentations and discussions) \nGuest lectures (diverse topics \u2013 changes each year) \nHands-on research and programming work (i.e., mini project & report)  \n \n  \n \nObjectives \nOn completion of this module, students will: \n \nunderstand and demonstrate knowledge in key characteristics of affectively intelligent AI, which \ninclude: \nRecognition: How to equip Affective AI systems with capabilities of analysing facial expressions, \nvocal intonations, gestures, and other physiological signals to infer human affective states? \nGeneration: How to enable affectively intelligent AI simulate expressions of emotions in \nmachines, allowing them to respond in a more human-like manner, such as virtual agents and \nhumanoid robots, showing empathy or sympathy? \nAdaptation and Personalization: How to enable affectively intelligent AI adapt system responses \nbased on users' affective states and/or needs, or tailor interactions and experiences to individual \nusers based on their expressivity / emotional profiles, personalities, and past interactions? \nEmpathetic Communication: How to design Affective AI systems to communicate with users in a \nway that demonstrates empathy, understanding, and sensitivity to their affective states and \nneeds? \nEthical and societal considerations: What are the various human differences, ethical guidelines, \nand societal impacts that need to be considered when designing and deploying Affective AI to \nensure that these systems respect users' privacy, autonomy, and well-being? \ncomprehend and apply (appropriate) methods for collection, analysis, representation, and \nevaluation of human affective and communicative behavioural data. \nenhance programming skills for creating and implementing (components of) Affective AI \nsystems. \ndemonstrate critical thinking, analysis and synthesis while deciding on 'when' and 'how' to \nincorporate human affect and social signals in a specific AI system context and gain practical \nexperience in proposing and justifying computational solution(s) of suitable nature and scope. \n  \n \nAssessment - Part II Students \nSeminar presentation: 20% \nParticipating in Q&A and discussions: 10% \nMid-term mini project report and presentation (as a team of 2): 10%  \nFinal mini project report & code (as a team of 2): 60% \n  \n \n\nAssessment - MPhil / Part III Students \nSeminar presentation: 20% \nParticipating in Q&A and discussions: 10% \nMid-term mini project report and presentation: 10%  \nFinal mini project report & code: 60% \nFurther Information \nCurrent Cambridge undergraduate students who are continuing onto Part III or the MPhil in \nAdvanced Computer Science may only take this module if they did NOT take it as a Unit of \nAssessment in Part II. \n \nThis module is shared with ACS. Assessment will be adjusted for the two groups of students to \nbe at an appropriate level for whichever course the student is enrolled on. More information will \nfollow at the first lecture. \n \nThis module is shared with Part II of the Computer Science Tripos. Assessment will be adjusted \nfor the two groups of students to be at an appropriate level for whichever course the student is \nenrolled on. Further information about assessment and practicals will follow at the first lecture.\n\nCATEGORY THEORY \n \nAims \nCategory theory provides a unified treatment of mathematical properties and constructions that \ncan be expressed in terms of 'morphisms' between structures. It gives a precise framework for \ncomparing one branch of mathematics (organized as a category) with another and for the \ntransfer of problems in one area to another. Since its origins in the 1940s motivated by \nconnections between algebra and geometry, category theory has been applied to diverse fields, \nincluding computer science, logic and linguistics. This course introduces the basic notions of \ncategory theory: adjunction, natural transformation, functor and category. We will use category \ntheory to organize and develop the kinds of structure that arise in models and semantics for \nlogics and programming languages. \n \nSyllabus \nIntroduction; some history. Definition of category. The category of sets and functions. \nCommutative diagrams. Examples of categories: preorders and monotone functions; monoids \nand monoid homomorphisms; a preorder as a category; a monoid as a category. Definition of \nisomorphism. Informal notion of a 'category-theoretic' property. \nTerminal objects. The opposite of a category and the duality principle. Initial objects. Free \nmonoids as initial objects. \nBinary products and coproducts. Cartesian categories. \nExponential objects: in the category of sets and in general. Cartesian closed categories: \ndefinition and examples. \nIntuitionistic Propositional Logic (IPL) in Natural Deduction style. Semantics of IPL in a cartesian \nclosed preorder. \nSimply Typed Lambda Calculus (STLC). The typing relation. Semantics of STLC types and \nterms in a cartesian closed category (ccc). The internal language of a ccc. The \nCurry-Howard-Lawvere correspondence. \nFunctors. Contravariance. Identity and composition for functors. \nSize: small categories and locally small categories. The category of small categories. Finite \nproducts of categories. \nNatural transformations. Functor categories. The category of small categories is cartesian \nclosed. \nHom functors. Natural isomorphisms. Adjunctions. Examples of adjoint functors. Theorem \ncharacterising the existence of right (respectively left) adjoints in terms of a universal property. \nDependent types. Dependent product sets and dependent function sets as adjoint functors. \nEquivalence of categories. Example: the category of I-indexed sets and functions is equivalent \nto the slice category Set/I. \nPresheaves. The Yoneda Lemma. Categories of presheaves are cartesian closed. \nMonads. Modelling notions of computation as monads. Moggi's computational lambda calculus. \nObjectives \nOn completion of this module, students should: \n \n\nbe familiar with some of the basic notions of category theory and its connections with logic and \nprogramming language semantics \nAssessment \na graded exercise sheet (25% of the final mark), and \na take-home test (75%) \nRecommended reading \nAwodey, S. (2010). Category theory. Oxford University Press (2nd ed.). \n \nCrole, R. L. (1994). Categories for types. Cambridge University Press. \n \nLambek, J. and Scott, P. J. (1986). Introduction to higher order categorical logic. Cambridge \nUniversity Press. \n \nPitts, A. M. (2000). Categorical Logic. Chapter 2 of S. Abramsky, D. M. Gabbay and T. S. E. \nMaibaum (Eds) Handbook of Logic in Computer Science, Volume 5. Oxford University Press. \n(Draft copy available here.) \n \nClass Size \nThis module can accommodate upto 15 Part II students plus 15 MPhil / Part III students. \n \nPre-registration evaluation form  \nStudents who are interested in taking this module will be asked to complete a pre-registration \nevaluation form to determine whether they have enough mathematical background to take the \nmodule.  \n \nThis will take place during the week Monday 12 August - Friday 16 August.  \n \nCoursework \nThe coursework in this module will consist of a graded exercise sheet. \n \nFurther Information \nCurrent Cambridge undergraduate students who are continuing onto Part III or the MPhil in \nAdvanced Computer Science may only take this module if they did NOT take 'Catgeory Theory' \nas a Unit of Assessment in Part II. \n \nThis module is shared with Part II of the Computer Science Tripos. Assessment will be adjusted \nfor the two groups of students to be at an appropriate level for whichever course the student is \nenrolled on. Further information about assessment and practicals will follow at the first lecture.\n\nDIGITAL MONEY AND DECENTRALISED FINANCE \n \nAims \nOur society is evolving towards digital payments and the elimination of physical cash. Digital \nalternatives to cash require trade-offs between various properties including unforgeability, \ntraceability, divisibility, transferability without intermediaries, privacy, redeemability, control over \nthe money supply and many more, often in conflict with each other. Since the 1980s, \ncryptographers have proposed a variety of clever technical solutions addressing some of these \nissues but it is only with the appearance of Bitcoin, blockchain and a plethora of copycat \ncryptocurrencies that a new asset class has now emerged, worth in excess of two trillion dollars \n(although plagued by extreme volatility). Meanwhile, most major countries have been planning \nthe introduction of Central Bank Digital Currencies in an attempt to retain control. This \nseminar-style interactive module encourages the students to understand what's happening, \nwhat the underlying problems and opportunities are (technical, social and economic) and where \nwe should go from here. We are at a historical turning point where an enterprising candidate \nmight disrupt the status quo and truly make a difference. \n \nSyllabus \nUnderstanding money: from gold standard to digital cash \nIntro to the Decentralised Finance (DeFi) ecosystem: Bitcoin, blockchain, wallets, smart \ncontracts \nStablecoins and Central Bank Digital Currencies \nDecentralised Autonomous Organisations and Automated Market Makers \nYield Farming, Perpetual Futures and Maximal Extractable Value \nWeb3, Non Fungible Tokens \nCryptocurrency crashes; crimes facilitated by cryptocurrencies \nNew areas of development. Where from here? \nThe module is structured as a reading club with a high degree of interactivity. Aside from the first \nand last session, which are treated differently, the regular two-hour sessions have the following \nstructure (not necessarily in this order). \n \nTwo half-hour debates on each of two papers \nTwo rapid-fire presentations on open questions previously assigned by the lecturers. \nHalf an hour of presentation by the lecturers. \nAbout ten minutes of buffer that can account for switchover time or be absorbed into the \nlecturers' presentation. \nThe first session is scene-setting by the lecturers, with no student presentations. The last \nsession has no paper debates, only one rapid-fire presentation on an open question from each \nof the students. \n \nObjectives \nOn completion of this module, students will gain an in-depth understanding of digital money \nalternatives as well as many modern applications and components of Decentralised Finance. By \ncomparing DeFi with current processes in traditional banking, students will be able to discuss \n\nthe merits and limitations of these new technologies as well as to identify potential new areas of \ndevelopment. \n \nThrough having to write, present and defend very brief extended abstracts, the students will \nimprove their communication skills and in particular the ability to distil and convey the most \nimportant points forcefully and concisely. \n \n \nAssessment \nThe module is an interactive reading club. Each student produces four output triplets throughout \nthe course. Each output triplet consists of three parts: a 1200-word essay, a 200-word extended \nabstract and a 7-minute presentation. \n \nThe essay and abstract must be submitted in advance, using the LaTeX templates provided. \nThe abstract must consist of full sentences, not bullet points, and must fit on a single slide. \nDuring the presentation, the slide with the abstract will be projected on screen as the only visual \naid; the essay will not be accessible to either presenter or audience. Reading out the slide \nverbatim will obviously not count as a great presentation. Presenters must be able to expand \nand defend their ideas live, and answer questions from lecturers and audience. \n \nThe essays, the abstracts and the presentations are graded separately, each out of 10, and \nassigned weights of 60%, 20%, 20%, respectively, within the triplet. \n \nIn a regular 2-hour session (all but the first and last) there will be two paper slots and two \ninvestigation slots, plus a lecturer slot, described next. \n \nEach paper slot (30 min) involves a \u201csupporter\u201d and a \u201cdissenter\u201d for the paper, each \ncontributing one output triplet, and then a panel Q&A where all the other participants quiz the \nsupporter and dissenter. \n \nEach investigation slot (10 min) involves a single student contributing an output triplet on a \ngiven theme. \n \nRoles, papers and themes are assigned by the lecturers the previous week. \n \nIn the first session there will be no student presentations, only lecturer-led exploration. In the \nlast session there will be 12 investigation slots and no paper slots. \n \nOverall, each student will be supporter once, dissenter once and investigator twice, for a total of \n4 output triplets. The output triplets of the last session will be weighed twice as much as the \nothers, i.e. 40% each, while the others 20% each. \n \nAttendance is required at all sessions. Missing a session in which the candidate was due to \npresent will result in the loss of the corresponding presentation marks, while the written work will \n\nstill have to be submitted and will be marked as usual. Missing a session in which the candidate \nwas not due to present will result in the loss of 3%. \n \nRecommended reading \nThere isn't a textbook covering all the material in this course. The following textbook, also \nadopted by R47 Distributed Ledger Technologies (also freely available online), is a thorough \nintroduction to Bitcoin and Blockchain, but we will complement it with research papers and \nindustry white papers for each lecture. \n \nNarayanan, A. , Bonneau, J., Felten, E., Miller, A. and Goldfeder, S. (2016). Bitcoin and \nCryptocurrency Technologies: A Comprehensive Introduction. Princeton University Press. \n(2016 Draft available here: \nhttps://d28rh4a8wq0iu5.cloudfront.net/bitcointech/readings/princeton_bitcoin_book.pdf)\n \n\nDIGITAL SIGNAL PROCESSING \n \nAims \nThis course teaches basic signal-processing principles necessary to understand many modern \nhigh-tech systems, with examples from audio processing, image coding, radio communication, \nradar, and software-defined radio. Students will gain practical experience from numerical \nexperiments in programming assignments (in Julia, MATLAB or NumPy). \n \nLectures \nSignals and systems. Discrete sequences and systems: types and properties. Amplitude, \nphase, frequency, modulation, decibels, root-mean square. Linear time-invariant systems, \nconvolution. Some examples from electronics, optics and acoustics. \nPhasors. Eigen functions of linear time-invariant systems. Review of complex arithmetic. \nPhasors as orthogonal base functions. \nFourier transform. Forms and properties of the Fourier transform. Convolution theorem. Rect \nand sinc. \nDirac\u2019s delta function. Fourier representation of sine waves, impulse combs in the time and \nfrequency domain. Amplitude-modulation in the frequency domain. \nDiscrete sequences and spectra. Sampling of continuous signals, periodic signals, aliasing, \ninterpolation, sampling and reconstruction, sample-rate conversion, oversampling, spectral \ninversion. \nDiscrete Fourier transform. Continuous versus discrete Fourier transform, symmetry, linearity, \nFFT, real-valued FFT, FFT-based convolution, zero padding, FFT-based resampling, \ndeconvolution exercise. \nSpectral estimation. Short-time Fourier transform, leakage and scalloping phenomena, \nwindowing, zero padding. Audio and voice examples. DTFM exercise. \nFinite impulse-response filters. Properties of filters, implementation forms, window-based FIR \ndesign, use of frequency-inversion to obtain high-pass filters, use of modulation to obtain \nband-pass filters. \nInfinite impulse-response filters. Sequences as polynomials, z-transform, zeros and poles, some \nanalog IIR design techniques (Butterworth, Chebyshev I/II, elliptic filters, second-order cascade \nform). \nBand-pass signals. Band-pass sampling and reconstruction, IQ up and down conversion, \nsuperheterodyne receivers, software-defined radio front-ends, IQ representation of AM and FM \nsignals and their demodulation. \nDigital communication. Pulse-amplitude modulation. Matched-filter detector. Pulse shapes, \ninter-symbol interference, equalization. IQ representation of ASK, BSK, PSK, QAM and FSK \nsignals. [2 hours] \nRandom sequences and noise. Random variables, stationary and ergodic processes, \nautocorrelation, cross-correlation, deterministic cross-correlation sequences, filtered random \nsequences, white noise, periodic averaging. \nCorrelation coding. Entropy, delta coding, linear prediction, dependence versus correlation, \nrandom vectors, covariance, decorrelation, matrix diagonalization, eigen decomposition, \n\nKarhunen-Lo\u00e8ve transform, principal component analysis. Relation to orthogonal transform \ncoding using fixed basis vectors, such as DCT. \nLossy versus lossless compression. What information is discarded by human senses and can \nbe eliminated by encoders? Perceptual scales, audio masking, spatial resolution, colour \ncoordinates, some demonstration experiments. \nQuantization, image coding standards. Uniform and logarithmic quantization, A/\u00b5-law coding, \ndithering, JPEG. \nObjectives \napply basic properties of time-invariant linear systems; \nunderstand sampling, aliasing, convolution, filtering, the pitfalls of spectral estimation; \nexplain the above in time and frequency domain representations; \nuse filter-design software; \nvisualize and discuss digital filters in the z-domain; \nuse the FFT for convolution, deconvolution, filtering; \nimplement, apply and evaluate simple DSP applications; \nfamiliarity with a number of signal-processing concepts used in digital communication systems \nRecommended reading \nLyons, R.G. (2010). Understanding digital signal processing. Prentice Hall (3rd ed.). \nOppenheim, A.V. and Schafer, R.W. (2007). Discrete-time digital signal processing. Prentice \nHall (3rd ed.). \nStein, J. (2000). Digital signal processing \u2013 a computer science perspective. Wiley. \n \nClass size \nThis module can accommodate a maximum of 24 students (16 Part II students and 8 MPhil \nstudents) \n \nAssessment - Part II students \nThree homework programming assignments, each comprising 20% of the mark \nWritten test, comprising 40% of the total mark. \nAssessment - Part III and MPhil students \nThree homework programming exercises, each comprising 10% of the mark. \nWritten test, comprising 20% of the total mark. \nIndividual implementation project with 8-page written report, topic agreed with lecturer, \ncomprising 50% of the mark. \nFurther Information \nCurrent Cambridge undergraduate students who are continuing onto Part III or the MPhil in \nAdvanced Computer Science may only take this module if they did NOT take it as a Unit of \nAssessment in Part II. \n \nThis module is shared with Part II of the Computer Science Tripos. Assessment will be adjusted \nfor the two groups of students to be at an appropriate level for whichever course the student is \nenrolled on. Further information about assessment and practicals will follow at the first lecture.\n\nINTRODUCTION TO COMPUTATIONAL SEMANTICS \n \nAims \nThis is a lecture-style course that introduces students to various aspects of the semantics of \nNatural Languages (mainly English): \n \nLexical Semantics, with an emphasis on theory and phenomenology  \nCompositional Semantics \nDiscourse and pragmatics-related aspects of semantics \nObjectives \nGive an operational definition of what is meant by \u201cmeaning\u201d (for instance, above and beyond \nsyntax); \nName the types of phenomena in language that require semantic consideration, in terms of \nlexical, compositional and discourse/pragmatic aspects, in other words, argue why semantics is \nimportant; \nDemonstrate an understanding of the basics of various semantic representations, including \nlogic-based and graph-based semantic representations, their properties, how they are used and \nwhy they are important, and how they are different from syntactic representations; \nKnow how such semantic representations are derived during or after parsing, and how they can \nbe analysed and mapped to surface strings; \nUnderstand applications of semantic representations e.g. reasoning, validation, and methods \nhow these are approached. \nWhen designing NL tasks that clearly require semantic processing (e.g. knowledge-based QA), \nto be aware of and reuse semantic representations and algorithms when designing the task, \nrather than reinventing the wheel. \nPractical advantages of this course for NLP students \nKnowledge of underlying semantic effects helps improve NLP evaluation, for instance by \nproviding more meaningful error analysis. You will be able to link particular errors to design \ndecisions inside your system. \nYou will learn methods for better benchmarking of your system, whatever the task may be. \nSupervised ML systems (in particular black-box systems such as Deep Learning) are only as \nclever as the datasets they are based on. In this course, you will learn to design datasets so that \nthey are harder to trick without real understanding, or critique existing datasets. \nYou will be able to design tests for ML systems that better pinpoint which aspects of language \nan end-to-end system has \u201cunderstood\u201d. \nYou will learn to detect ambiguity and ill-formed semantics in human-human communication. \nThis can serve to write more clearly and logically. \nYou will learn about decomposing complex semantics-reliant tasks sensibly so that you can \nreuse the techniques underlying semantic analyzers in a modular way. In this way, rather than \nbeing forced to treat complex tasks in an end-to-end manner, you will be able to profit from \npartial explanations and a better error analysis already built into the system. \nSyllabus \nOverview of the course \nEvents and semantic role labelling \n\nReferentiality and coreference resolution \nTruth-conditional semantics \nGraph-based meaning representation \nCompositional semantics \nContext-free Graph Rewriting \nSurface realisation \nNegation and psychological approach to semantics \nDynamic semantics \nGricean pragmatics \nVector space models \nCross-modality \nAcquisition of semantics \nDiachronic change of semantics \nSummary of the course \n  \n \nAssessment \n5 take-home exercises worth 20% each: \n \nTake-home assignment 1 is given in Lecture 4 and due is Lecture 6 \n \nStudents are given 10 English sentences and asked to provide their semantic analysis \naccording to truth-conditions. Students will be expected to return 10 logical expressions as their \nanswers. No word limit. Assessment criteria: correctness of the 10 logical expressions; 2 points \nfor each logical expression. \n \nTake-home assignment 2 is given in Lecture 7 and due is Lecture 9  \n \nStudents are given 10 English sentences and asked to provide their syntactico-semantic \nderivations according to the compositionality principle. Students will be expected to return \nderivation graphs as their answers. No word limit. Assessment criteria: correctness of the 10 \nderivation graphs; 2 points for each derivation. \n \nTake-home assignment 3 is given in Lecture 11 and due is Lecture 13  \n \nAll students are assigned with a paper on modelling common ground in dialogue system. \nStudents will receive related but different papers. Each student will write a review of their \nassigned paper, including a comprehensive summary and their own thoughts. Word limit: 1000 \nwords. Assessment criteria: 15 points on whether a student understands the paper correctly; 5 \npoints on whether a student is able to think critically. \n \nTake-home assignment 4 is given in Lecture 13 and due is Lecture 15 \n \n\nAll students are assigned with a paper on language-vision interaction. Students will receive \nrelated but different papers. Each student will write a review of their assigned paper, including a \ncomprehensive summary and their own thoughts. Word limit: 1000 words. Assessment criteria: \n15 points on whether a student understands the paper correctly; 5 points on whether a student \nis able to think critically. \n \nTake-home assignment 5 is given in Lecture 14 and due is two weeks later. \n \nContent: All students are assigned with a paper on bootstrapping language acquisition. All \nstudents will receive the same paper. Each student will write a review of the paper, including a \ncomprehensive summary and their own thoughts. Word limit: 1000 words. Assessment criteria: \n15 points on whether a student understands the paper correctly; 5 points on whether a student \nis able to think critically.\n \n\nINTRODUCTION TO NATURAL LANGUAGE SYNTAX AND PARSING \n \nAims \nThis module aims to provide a brief introduction to linguistics for computer scientists and then \ngoes on to cover some of the core tasks in natural language processing (NLP), focussing on \nstatistical parsing of sentences to yield syntactic representations. We will look at how to \nevaluate parsers and see how well state-of-the-art tools perform given current techniques. \n \nSyllabus \nLinguistics for NLP focusing on morphology and syntax \nGrammars and representations \nStatistical and neural parsing \nTreebanks and evaluation \nObjectives \nOn completion of this module, students should: \n \nunderstand the basic properties of human languages and be familiar with descriptive and \ntheoretical frameworks for handling these properties; \nunderstand the design of tools for NLP tasks such as parsing and be able to apply them to text \nand evaluate their performance; \nPractical work \nWeek 1-7: There will be non-assessed practical exercises between sessions and during \nsessions. \nWeek 8: Download and apply two parsers to a designated text. Evaluate the performance of the \ntools quantitatively and qualitatively. \nAssessment \nThere will be one presentation worth 10% of the final mark; presentation topics will be allocated \nat the start of term. \nAn assessed practical report of not more 8 pages in ACL conference format. It will contribute \n90% of the final mark. \nRecommended reading \nJurafsky, D. and Martin, J. (2008). Speech and Language Processing. Prentice-Hall (2nd ed.). \n(See also 3rd ed. available online.)\n \n\nINTRODUCTION TO NETWORKING AND SYSTEMS MEASUREMENTS \n \nAims \nSystems research refers to the study of a broad range of behaviours arising from complex \nsystem design, including: resource sharing and scheduling; interactions between hardware and \nsoftware; network topology, protocol and device design and implementation; low-level operating \nsystems; Interconnect, storage and more. This module will: \n \nTeach performance characteristics and performance measurement methodology and practice \nthrough profiling experiments; \nExpose students to real-world systems artefacts evident through different measurement tools; \nDevelop scientific writing skills through a series of laboratory reports; \nProvide research skills for characterization and modelling of systems and networks using \nmeasurements. \nPrerequisites \nIt is strongly recommended that students have previously (and successfully) completed an \nundergraduate networking course -- or have equivalent experience through project or \nopen-source work. \n \nSyllabus \nIntroduction to performance measurements, performance characteristics [1 lecture] \nPerformance measurements tools and techniques [2 lectures + 2 lab sessions] \nReproducible experiments [1 lecture + 1 lab session] \nCommon pitfalls in measurements [1 lecture] \nDevice and system characterisation [1 lecture + 2 lab sessions] \nObjectives \nOn completion of this module, students should: \n \nDescribe the objectives of measurements, and what they can achieve; \nCharacterise and model a system using measurements; \nPerform reproducible measurements experiments; \nEvaluate the performance of a system using measurements; \nOperate measurements tools and be aware of their limitations; \nDetect anomalies in the network and avoid common measurements pitfalls; \nWrite system-style performance evaluations. \nPractical work \nFive 2-hour in-classroom labs will ask students to develop and use skills in performance \nmeasurements as applied to real-world systems and networking artefacts. Results from these \nlabs (and follow-up work by students outside of the classroom) will by the primary input to lab \nreports. \n \nThe first three labs will provide an introduction and hands on experience with measurement \ntools and measurements methodologies, while the last two labs will focus on practical \nmeasurements and evaluation of specific platforms. Students may find it useful to work in pairs \n\nwithin the lab, but must prepare lab reports independently. The module lecturer will give a short \nintroductory at the start of each lab, and instructors will be on-hand throughout labs to provide \nassistance. \n \nLab participation is not directly included in the final mark, but lab work is a key input to lab \nreports that are assessed. Guided lab experiments resulting in practical write ups. \n \nAssessment \nEach student will write two lab reports. The first lab report will summarise the experiments done \nin the first three labs (20%). The second will be a lab report (5000 words) summarising the \nevaluation of a device or a system (80%). \n \nRecommended reading \nThe following list provides some background to the course materials, but is not mandatory. A \nreading list, including research papers, will be provided in the course materials. \n \nGeorge Varghese. Network algorithmics. Chapman and Hall/CRC, 2010. \nMark Crovella and Balachander Krishnamurthy. Internet measurement: infrastructure, traffic and \napplications. John Wiley and Sons, Inc., 2006. \nBrendan Gregg. Systems Performance: Enterprise and the Cloud, Prentice Hall Press, Upper \nSaddle River, NJ, USA, October 2013. \nRaj Jain, The Art of Computer Systems Performance Analysis: Techniques for Experimental \nDesign, Measurement, Simulation, and Modeling, Wiley - Interscience, New York, NY, USA, \nApril 1991.\n \n\nLARGE-SCALE DATA PROCESSING AND OPTIMISATION \n \nAims \nThis module provides an introduction to large-scale data processing, optimisation, and the \nimpact on computer system's architecture. Large-scale distributed applications with high volume \ndata processing such as training of machine learning will grow ever more in importance. \nSupporting the design and implementation of robust, secure, and heterogeneous large-scale \ndistributed systems is essential. To deal with distributed systems with a large and complex \nparameter space, tuning and optimising computer systems is becoming an important and \ncomplex task, which also deals with the characteristics of input data and algorithms used in the \napplications. Algorithm designers are often unaware of the constraints imposed by systems and \nthe best way to consider these when designing algorithms with massive volume of data. On the \nother hand, computer systems often miss advances in algorithm design that can be used to cut \ndown processing time and scale up systems in terms of the size of the problem they can \naddress. Integrating machine learning approaches (e.g. Bayesian Optimisation, Reinforcement \nLearning) for system optimisation will be explored in this course. \n \nSyllabus \nThis course provides perspectives on large-scale data processing, including data-flow \nprogramming, graph data processing, probabilistic programming and computer system \noptimisation, especially using machine learning approaches, thus providing a solid basis to work \non the next generation of distributed systems. \n \nThe module consists of 8 sessions, with 5 sessions on specific aspects of large-scale data \nprocessing research. Each session discusses 3-4 papers, led by the assigned students. One \nsession is a hands-on tutorial on on dataflow programming fundamentals. The first session \nadvises on how to read/review a paper together with a brief introduction on different \nperspectives in large-scale data \nprocessing and optimisation. The last session is dedicated to the student presentation of \nopensource project studies. \n \nIntroduction to large-scale data processing and optimisation \nData flow programming: Map/Reduce to TensorFlow \nLarge-scale graph data processing: storage, processing model and parallel processing \nDataflow programming hands-on tutorial \nProbabilistic Programming \nOptimisations in ML Compiler \nOptimisations of Computer systems using ML \nPresentation of Open Source Project Study \nObjectives \nOn completion of this module, students should: \n \nUnderstand key concepts of scalable data processing approaches in future computer systems. \n\nObtain a clear understanding of building distributed systems using data centric programming \nand large-scale data processing. \nUnderstand a large and complex parameter space in computer system's optimisation and \napplicability of Machine Learning approach. \nCoursework \nReading Club: \nThe preparation for the reading club will involve 1-3 papers every week. At each session, \naround 3-4 papers are selected under the given topic, and the students present their review \nwork. \nHands-on tutorial session of data flow programming including writing an application of \nprocessing streaming in Twitter data and/or Deep Neural Networks using Google TensorFlow \nusing cluster computing. \nReports \nThe following three reports are required, which could be extended from the assignment of the \nreading club, within the scope of data centric systems. \n \nReview report on a full length paper (max 1800 words) \nDescribe the contribution of the paper in depth with criticisms \nCrystallise the significant novelty in contrast to other related work \nSuggestions for future work \nSurvey report on sub-topic in large-scale data processing and optimisation (max 2000 words) \nPick up to 5 papers as core papers in the survey scope \nRead the above and expand reading through related work \nComprehend the view and finish an own survey paper \nProject study and exploration of a prototype (max 2500 words) \nWhat is the significance of the project in the research domain? \nCompare with similar and succeeding projects \nDemonstrate the project by exploring its prototype \nReports 1 should be handed in by the end of 5th week; Report 2 in the 8th week and Report 3 \non the first day of Lent Term. \n \nAssessment \nThe final grade for the course will be provided as a percentage, and the assessment will consist \nof two parts: \n \n25%: for reading club (participation, presentation) \n75%: for the three reports: \n15%: Intensive review report \n25%: Survey report \n35%: Project study \nRecommended reading \nM. Abadi et al. TensorFlow: A System for Large-Scale Machine Learning, OSDI, 2016. \nD. Aken et al.: Automatic Database Management System Tuning Through Large-scale Machine \nLearning, SIGMOD, 2017. \n\nJ. Ansel et al. Opentuner: an extensible framework for program autotuning. PACT, 2014. \nV. Dalibard, M. Schaarschmidt, E. Yoneki. BOAT: Building Auto-Tuners with Structured Bayesian \nOptimization, WWW, 2017. \nJ. Dean et al. Large scale distributed deep networks. NIPS, 2012. \nG. Malewicz, M. Austern, A. Bik, J. Dehnert, I. Horn, N. Leiser, G. Czajkowski. Pregel: A System \nfor Large-Scale Graph Processing, SIGMOD, 2010. \nA. Mirhoseini et al. Device Placement Optimization with Reinforcement Learning, ICML, 2017. \nD. Murray, F. McSherry, R. Isaacs, M. Isard, P. Barham, M. Abadi. Naiad: A Timely Dataflow \nSystem, SOSP, 2013. \nM. Schaarschmidt, S. Mika, K. Fricke and E. Yoneki: RLgraph: Modular Computation Graphs for \nDeep Reinforcement Learning, SysML, 2019. \nZ. Jia, O. Padon, J. Thomas, T. Warszawski, M. Zaharia,  A. Aiken: TASO: Optimizing Deep \nLearning Computation with Automated Generation of Graph Substitutions: SOSP, 2019. \nH. Mao et al.: Park: An Open Platform for Learning-Augmented Computer Systems, \nOpenReview, 2019. \nJ. Shao, et al.: Tensor Program Optimization with Probabilistic Programs, NeurIPS, 2022.  \nA complete list can be found on the course material web page. See also 2023-2024 course \nmaterial on the previous course Large-Scale Data Processing and Optimisation.\n \n\nMACHINE LEARNING AND THE PHYSICAL WORLD \nAims \nThe module \u201cMachine Learning and the Physical World\u201d is focused on machine learning \nsystems that interact directly with the real world. Building artificial systems that interact with the \nphysical world have significantly different challenges compared to the purely digital domain. In \nthe real world data is scares, often uncertain and decisions can have costly and irreversible \nconsequences. However, we also have the benefit of centuries of scientific knowledge that we \ncan draw from. This module will provide the methodological background to machine learning \napplied in this scenario. We will study how we can build models with a principled treatment of \nuncertainty, allowing us to leverage prior knowledge and provide decisions that can be \ninterrogated. \n \nThere are three principle points about machine learning in the real world that will concern us. \n \nWe often have a mechanistic understanding of the real world which we should be able to \nbootstrap to make decisions. For example, equations from physics or an understanding of \neconomics. \nReal world decisions have consequences which may have costs, and often these cost functions \nneed to be assimilated into our machine learning system. \nThe real world is surprising, it does things that you do not expect and accounting for these \nchallenges requires us to build more robust and or interpretable systems. \nDecision making in the real world hasn\u2019t begun only with the advent of machine learning \ntechnologies. There are other domains which take these areas seriously, physics, environmental \nscientists, econometricians, statisticians, operational researchers. This course identifies how \nmachine learning can contribute and become a tool within these fields. It will equip you with an \nunderstanding of methodologies based on uncertainty and decision making functions for \ndelivering on these challenges. \n \nObjectives \nYou will gain detailed knowledge of \n \nsurrogate models and uncertainty \nsurrogate-based optimization \nsensitivity analysis \nexperimental design \nYou will gain knowledge of \n \ncounterfactual analysis \nsurrogate-based quadrature \nSchedule \nWeek 1: Introduction to the unit and foundation of probabilistic modelling \n \nWeek 2: Gaussian processes and probablistic inference \n \n\nWeek 3: Simulation and Sequential decision making under uncertainty \n \nWeek 4: Emulation and Experimental Design \n \nWeek 5: Sensitivity Analysis and Multifidelity Modelling \n \nWeek 6-8: Case studies of applications and Projects \n \nPractical work \nDuring the first five weeks of the unit we will provide a weekly worksheet that will focus on \nimplementation and practical exploration of the material covered in the lectures. The worksheets \nwill allow you to build up a these methods without relying on extensive external libraries. You are \nfree to use any programming language of choice however we highly recommended the use of \n=Python=. \n \nAssessment \nTwo individual tasks (15% each) \nGroup Project (70%). Each group will work on an application of uncertainty that covers the \nmaterial of the first 5 weeks of lectures in the unit. Each group will submit a report which will \nform the basis of the assessment. In addition to the report each group will also attend a short \noral examination based on the material covered both in the report and the taught material. \nRecommended Reading \nRasmussen, C. E. and Williams, C. K. I. (2006). Gaussian Processes for Machine Learning. MIT \nPress \n \nBishop, C. (2006). Pattern recognition and machine learning. Springer. \nhttps://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-an\nd-Machine-Learning-2006.pdf \n \nLaplace, P. S. (1902). A Philosophical Essay on Probabilities. John Wiley & Sons. \nhttps://archive.org/details/philosophicaless00lapliala\n \n\nMACHINE VISUAL PERCEPTION \nAims \nThis course aims at introducing the theoretical foundations and practical techniques for machine \nperception, the capability of computers to interpret data resulting from sensor measurements. \nThe course will teach the fundamentals and modern techniques of machine perception, i.e. \nreconstructing the real world starting from sensor measurements with a focus on machine \nperception for visual data. The topics covered will be image/geometry representations for \nmachine perception, semantic segmentation, object detection and recognition, geometry \ncapture, appearance modeling and acquisition, motion detection and estimation, \nhuman-in-the-loop machine perception, select topics in applied machine perception. \n \nMachine perception/computer vision is a rapidly expanding area of research with real-world \napplications. An understanding of machine perception is also important for robotics, interactive \ngraphics (especially AR/VR), applied machine learning, and several other fields and industries. \nThis course will provide a fundamental understanding of and practical experience with the \nrelevant techniques. \n \nLearning outcomes \nStudents will understand the theoretical underpinnings of the modern machine perception \ntechniques for reconstructing models of reality starting from an incomplete and imperfect view of \nreality. \nStudents will be able to apply machine perception theory to solve practical problems, e.g. \nclassification of images, geometry capture. \nStudents will gain an understanding of which machine perception techniques are appropriate for \ndifferent tasks and scenarios. \nStudents will have hands-on experience with some of these techniques via developing a \nfunctional machine perception system in their projects. \nStudents will have practical experience with the current prominent machine perception \nframeworks. \nSyllabus \nThe fundamentals of machine learning for machine perception \nDeep neural networks and frameworks for machine perception \nSemantic segmentation of objects and humans \nObject detection and recognition \nMotion estimation, tracking and recognition \n3D geometry capture \nAppearance modeling and acquisition \nSelect topics in applied machine perception \nAssessment \nA practical exercise, worth 20% of the mark. This will cover the basics and theory of machine \nperception and some of the practical techniques the students will likely use in their projects. This \nis individual work. No GPU hours will be needed for the practical work. \nA machine perception project worth 80% of the marks: \n\nCourse projects will be selected by the students following the possible project themes proposed \nby the lecturer, and will be checked by the lecturer for appropriateness.  \nThe students will form groups of 2-3 to design, implement, report, and present a project to tackle \na given task in machine perception. \nThe final mark will be composed of an implementation/report mark (60%) and a presentation \nmark (20%). Each team member will be evaluated based on her/his contribution. \nEach project will have extensions to be completed only by the ACS students. Each student will \nwrite a different part of the report, whose author will be clearly marked. Each student will further \nsummarise her/his contributions to the project in the same report. \nRecommended Reading List \nComputer Vision: Algorithms and Applications, Richard Szeliski, Springer, 2010. \nDeep Learning, Ian Goodfellow, Yoshua Bengio, and Aaron Courville, MIT Press, 2016. \nMachine Learning and Visual Perception, Baochang Zhang, De Gruyter, 2020. \nFurther Information \nCurrent Cambridge undergraduate students who are continuing onto Part III or the MPhil in \nAdvanced Computer Science may only take this module if they did NOT take it as a Unit of \nAssessment in Part II. \n \nThis module is shared with Part II of the Computer Science Tripos. Assessment will be adjusted \nfor the two groups of students to be at an appropriate level for whichever course the student is \nenrolled on. Further information about assessment and practicals will follow at the first lecture.\n\nMOBILE, WEARABLE SYSTEMS AND MACHINE LEARNING \n \nAims \nThis module aims to introduce the latest research advancements in mobile systems and mobile \ndata machine learning, spanning a range of domains including systems, data gathering, \nanalytics and machine learning, on device machine learning and applications such as health, \ntransportation, behavior monitoring, cyber-physical systems, autonomous vehicles, drones. The \ncourse will cover current and seminal research papers in the area of research. \n \nSyllabus \nThe course will consist of one introductory lecture, seven two-hour, and one three-hour, \nsessions covering a variety of topics roughly including the following material (some variation in \nthe topics might happen from year to year): \n \nSystem, Energy and Security \nBackscatter Communication, Battery Free and Energy Harvesting Devices \nNew Sensing Modalities \nMachine Learning on Wearable Data \nOn Device Machine Learning \nMobile and Wearable Health \nMobile and Wearable Systems of Sustainability \nEach week, three class participants will be assigned to introduce assigned three papers via \n20-minute presentations, conference-style and highlighting critically its features. Each \npresentation will be followed by 10 minutes of questions. This will be followed by 10 minutes of \ngeneral discussion. Slides will be used for presentation. \n \nStudents will give one or more presentations each term. Each student will submit a paper review \neach week for one of the three papers presented except for the week they will be presenting \nslides. Each review will follow a template and be up to 1,000 words. Each review will receive a \nmaximum of 10 points. As a result, each student will produce 6-7 reviews and at least one \npresentation, probably two. All participants are expected to attend and participate in every class; \nthe instructor must be notified of any absences in advance. \n \nObjectives \nOn completion of this module students should have an understanding of the recent key research \nin mobile and sensor systems and mobile analytics as well as an improved critical thinking over \nresearch papers. \n \nAssessment \nAggregate mark for 7 assignments from 6-7 reports and 1-2 presentations. Each report or \npresentation will contribute one seventh of the course mark. \nA tick for presence and participation to each class will also be awarded. \nRecommended reading \n\nReadings from the most recent conferences such as AAAI, ACM KDD, ACM MobiCom, ACM \nMobiSys, ACM SenSys, ACM UbiComp, ICLR, ICML Neurips, and WWW pertinent to mobile \nsystems and data.\n \n\nNETWORK ARCHITECTURES \nAims \nThis module aims to provide the world with more network architects. The 2011-2012 version \nwas oriented around the evolution of IP to support new services like multicast, mobility, \nmultihoming, pub/sub and, in general, data oriented networking. The course is a paper reading \nwhich puts the onus on the student to do the work. \n \nSyllabus \nIPng [2 lectures, Jon Crowcroft] \nNew Architectures [2 lectures, Jon Crowcroft] \nMulticast [2 lectures, Jon Crowcroft] \nContent Distribution and Content Centric Networks [2 lectures, Jon Crowcroft] \nResource Pooling [2 lectures, Jon Crowcroft] \nGreen Networking [2 lectures, Jon Crowcroft] \nAlternative Router Implementions [2 lectures, Jon Crowcroft] \nData Center Networks [2 Lectures, Jon Crowcroft] \nObjectives \nOn completion of this module, students should be able to: \n \ncontribute to new network system designs; \nengineer evolutionary changes in network systems; \nidentify and repair architectural design flaws in networked systems; \nsee that there are no perfect solutions (aside from academic ones) for routing, addressing, \nnaming; \nunderstand tradeoffs in modularisation and other pressures on clean software systems \nimplementation, and see how the world is changing the proper choices in protocol layering, or \nnon layered or cross-layered. \nCoursework \nAssessment is through three graded essays (each chosen individually from a number of \nsuggested or student-chosen topics), as follows: \n \nAnalysis of two different architectures for a particular scenario in terms of cost/performance \ntradeoffs for some functionality and design dimension, for example: \nATM \u2013 e.g. for hardware versus software tradeoff \nIP \u2013 e.g. for mobility, multi-homing, multicast, multipath \n3GPP \u2013 e.g. for plain complexity versus complicatedness \nA discursive essay on a specific communications systems component, in a particular context, \nsuch as ad hoc routing, or wireless sensor networks. \nA bespoke network design for a narrow, well specified specialised target scenario, for example: \nA customer baggage tracking network for an airport. \nin-flight entertainment system. \nin-car network for monitoring and control. \ninter-car sensor/control network for automatic highways. \nPractical work \n\nThis course does not feature any implementation work due to time constraints. \n \nAssessment \nThree 1,200-word essays (worth 25% each), and \nan annotated bibliography (25%). \nRecommended reading \nPre-course reading: \n \nKeshav, S. (1997). An engineering approach to computer networking. Addison-Wesley (1st ed.). \nISBN 0201634422 \nPeterson, L.L. and Davie, B.S. (2007). Computer networks: a systems approach. Morgan \nKaufmann (4th ed.). \n \nDesign patterns: \n \nDay, John (2007). Patterns in network architecture: a return to fundamentals. Prentice Hall. \n \nExample systems: \n \nKrishnamurthy, B. and Rexford, J. (2001). Web protocols and practice: HTTP/1.1, Networking \nprotocols, caching, and traffic measurement. Addison-Wesley. \n \nEconomics and networks: \n \nFrank, Robert H. (2008). The economic naturalist: why economics explains almost everything. \n \nPapers: \n \nCertainly, a collection of papers (see ACM CCR which publishes notable network researchers' \nfavourite ten papers every 6 months or so).\n \n\nOVERVIEW OF NATURAL LANGUAGE PROCESSING \n \nAims \nThis course introduces the fundamental techniques of natural language processing. It aims to \nexplain the potential and the main limitations of these techniques. Some current research issues \nare introduced and some current and potential applications discussed and evaluated. Students \nwill also be introduced to practical experimentation in natural language processing. \n \nLectures \nOverview. Brief history of NLP research, some current applications, components of NLP \nsystems. \nMorphology and Finite State Techniques. Morphology in different languages, importance of \nmorphological analysis in NLP, finite-state techniques in NLP. \nPart-of-Speech Tagging and Log-Linear Models. Lexical categories, word tagging, corpora and \nannotations, empirical evaluation. \nPhrase Structure and Structure Prediction. Phrase structures, structured prediction, context-free \ngrammars, weights and probabilities. Some limitations of context-free grammars. \nDependency Parsing. Dependency structure, grammar-free parsing, incremental processing.  \nGradient Descent and Neural Nets. Parameter optimisation by gradient descent. Non-linear \nfunctions with neural network layers. Log-linear model as softmax layer. Current findings of \nNeural NLP. \nWord representations. Representing words with vectors, count-based and prediction-based \napproaches, similarity metrics. \nRecurrent Neural Networks. Modelling sequences, parameter sharing in recurrent neural \nnetworks, neural language models, word prediction. \nCompositional Semantics. Logical representations, compositional semantics, lambda calculus, \ninference and robust entailment. \nLexical Semantics. Semantic relations, WordNet, word senses. \nDiscourse. Discourse relations, anaphora resolution, summarization. \nNatural Language Generation. Challenges of natural language generation (NLG), tasks in NLG, \nsurface realisation. \nPractical and assignments. Students will build a natural language processing system which will \nbe trained and evaluated on supplied data. The system will be built from existing components, \nbut students will be expected to compare approaches and some programming will be required \nfor this. Several assignments will be set during the practicals for assessment. \nObjectives \nBy the end of the course students should: \n \nbe able to discuss the current and likely future performance of several NLP applications; \nbe able to describe briefly a fundamental technique for processing language for several \nsubtasks, such as morphological processing, parsing, word sense disambiguation etc.; \nunderstand how these techniques draw on and relate to other areas of computer science. \nRecommended reading \n\nJurafsky, D. & Martin, J. (2023). Speech and language processing. Prentice Hall (3rd ed. draft, \nonline). \n \nAssessment - Part II Students \nAssignment 1 - 10% of marks \nAssignment 2 - 60% of marks \nAssignment 3 - 30% of marks \nCoursework \nSubmit work for 3 assignments as part of practical sessions on information extraction. \n \nThese include an annotation exercise, a feature-based classifier along with code repository and \ndocumentation, and a 4,000-word report on results and analysis from an extended information \nextraction experiment. \n \nPractical work \nStudents will build a natural language processing system which will be trained and evaluated on \nsupplied data. The system will be built from existing components, but students will be expected \nto compare approaches and some programming will be required for this. \n \nAssessment - Part III and MPhil Students \nAssessment will be based on the practicals: \n \nFirst practical exercise (10%, ticked) \nSecond practical exercise (25%, code repository or notebook with documentation) \nFinal report (65%, 4,000 words, excluding references) \nFurther Information \nAlthough the lectures don't assume any exposure to linguistics, the course will be easier to \nfollow if students have some understanding of basic linguistic concepts. The following may be \nuseful for this: The Internet Grammar of English \n \nCurrent Cambridge undergraduate students who are continuing onto Part III or the MPhil in \nAdvanced Computer Science may only take this module if they did NOT take it as a Unit of \nAssessment in Part II. \n \nStudents from other departments may attend the lectures for this module if space allows. \nHowever students wanting to take it for credit will need to make arrangements for assessment \nwithin their own department. \n \nThis module is shared with Part II of the Computer Science Tripos. Assessment will be adjusted \nfor the two groups of students to be at an appropriate level for whichever course the student is \nenrolled on. Further information about assessment and practicals will follow at the first lecture.\n\nPRACTICAL RESEARCH IN HUMAN-CENTRED AI \n \nAims \nThis is an advanced course in human-computer interaction, with a specialist focus on intelligent \nuser interfaces and interaction with machine-learning and artificial intelligence technologies. The \nformat will be largely Practical, with students carrying out a mini-project involving empirical \nresearch investigation. These studies will investigate human interaction with some kind of \nmodel-based system for planning, decision-making, automation etc. Possible study formats \nmight include: System evaluation, Field observation, Hypothesis testing experiment, Design \nintervention or Corpus analysis, following set examples from recent research publications. \nProject work will be formally evaluated through a report and presentation. \n \nLectures \n(note that Lectures 2-7 also include one hour class discussion of practical work) \n        \u2022 Current research themes in intelligent user interfaces \n        \u2022 Program synthesis \n        \u2022 Mixed initiative interaction \n        \u2022 Interpretability / explainable AI \n        \u2022 Labelling as a fundamental problem \n        \u2022 Machine learning risks and bias \n        \u2022 Visualisation and visual analytics \n        \u2022 Student research presentations \n \nObjectives \nBy the end of the course students should: \n        \u2022 be familiar with current state of the art in intelligent interactive systems \n        \u2022 understand the human factors that are most critical in the design of such systems \n        \u2022 be able to evaluate evidence for and against the utility of novel systems \n        \u2022 have experience of conducting user studies meeting the quality criteria of this field \n        \u2022 be able to write up and present user research in a professional manner \n \nClass Size \nThis module can accommodate upto 20 Part II, Part III and MPhil students. \n \nRecommended reading \nBrad A. Myers and Richard McDaniel (2000). Demonstrational Interfaces: Sometimes You Need \na Little Intelligence, Sometimes You Need a Lot. \n \nAlan Blackwell (2024). Moral Codes: Designing alternatives to AI \n \nAssessment - Part II Students \nThe format will be largely practical, with students carrying out an individual mini-project involving \nempirical research investigation. \n \n\nAssignment 1: six incremental submissions which together contribute 20% to the final module \nmark. \n \nAssignment 2: Final report - 80% of the final module mark \n  \n \nAssessment - MPhil / Part III Students \nThere will be a minor assessment component (20%) in which students compile a reflective diary \nthroughout the term, reporting on the weekly sessions. Diary entries should include citations to \nany key references, notes of possible further reading, summary of key points, questions relevant \nto the personal project, and points of interest noted in relation to the work of other students. \n \nThe major assessment component (80%) will involve a report on research findings, in the style \nof a submission to the ACM CHI or IUI conferences. This work will be submitted incrementally \nthrough the term, in order that feedback can be provided before final assessment of the full \nreport. Phased submissions will cover the following aspects of the empirical study: \n \nResearch question \nMethod \nLiterature Review \nIntroduction \nResults \nDiscussion / Conclusion \nFeedback on each phased submission will include an indicative mark for guidance, but with the \nunderstanding that the final grade will be based on the final delivered report, and that this may \ngo up (or possibly down), depending on how well the student responds to earlier feedback. \n \nReflective diary entries will also be assessed, but graded at a relatively coarse granularity \ncorresponding to the ACS grading bands, and with minimal written feedback. Informal and \ngeneric feedback will be offered verbally in class, and also potentially supplemented with peer \nassessment. \n \nFurther Information \nCurrent Cambridge undergraduate students who are continuing onto Part III or the MPhil in \nAdvanced Computer Science may only take this module if they did NOT take it as a Unit of \nAssessment in Part II. \n \nThis module is shared with Part II of the Computer Science Tripos. Assessment will be adjusted \nfor the two groups of students to be at an appropriate level for whichever course the student is \nenrolled on. Further information about assessment and practicals will follow at the first lecture.\n\nPRINCIPLES OF MACHINE LEARNING SYSTEMS \n \nPrerequisites: It is recommended that students have successfully completed introductory \ncourses, at an undergraduate level, in: 1) operating systems, 2) computer architecture, and 3) \nmachine learning. In addition, this course will heavily focus on deep neural network \nmethodologies and assume familiarity with common neural architectures and related algorithms. \nIf these topics were not covered within the machine learning course taken, students should \nsupplement by reviewing material in a book like: \"Dive into Deep Learning\". Finally, students are \nassumed to be comfortable with programming in Python. \nMoodle, timetable \n \nAims \nThis course will examine the emerging principles and methodologies that underpin scalable and \nefficient machine learning systems. Primarily, the course will focus on an exciting cross-section \nof algorithms and system techniques that are used to support the training and inference of \nmachine learning models under a spectrum of computing systems that range from constrained \nembedded systems up to large-scale distributed systems. It will also touch upon the new \nengineering practices that are developing in support of such systems at scale. When needed to \nappreciate issues of scalability and efficiency, the course will drill down to certain aspects of \ncomputer architecture, systems software and distributed systems and explore how these \ninteract with the usage and deployment of state-of-the-art machine learning. \n \nSyllabus \nTopics covered may include the following, with confirmation a month before the course begins: \n \nSystem Performance Trade-offs \nDistributed Learning Algorithms  \nModel Compression  \nDeep Learning Compilers  \nFrameworks and Run-times  \nScalable Inference Serving  \nDevelopment Practices  \nAutomated Machine Learning  \nFederated Learning  \nPrimarily, topics are covered with conventional lectures. However, where appropriate, material \nwill be delivered through hands-on lab tutorials. Lab tutorials will make use of hardware \nincluding ARM microcontrollers and multi-GPU machines to explore forms of efficient machine \nlearning (any necessary equipment will be provided to students) \n \nAssessment \nEach student will be assessed on 3 labs which will be worth 30% of their grade. They will also \nundertake a written project report which will be worth 70% of the grade. This report will detail an \ninvestigation into a particular aspect of machine learning systems, this report will be made \navailable publicly.\n \n\nPROOF ASSISTANTS \nAims \nThis module introduces students to interactive theorem proving using Isabelle and Coq. It \nincludes techniques for specifying formal models of software and hardware systems and for \nderiving properties of these models. \n \n  \n \nSyllabus \nIntroduction to proof assistants and logic. \nReasoning in predicate logic and typed set theory. \nReasoning in dependent type theory. \nInductive definitions and recursive functions: modelling them in logic, reasoning about them. \nModelling operational semantics definitions and proving properties. \n  \n \nObjectives \nOn completion of this module, students should: \n \npossess good skills in the use of Isabelle and Coq; \nbe able to specify inductive definitions and perform proofs by induction; \nbe able to formalise and reason about a variety of specifications in a proof assistant. \nCoursework \nPractical sessions will allow students to develop skills. Some of the exercises will serve as the \nbasis for assessment. \n \nAssessment \nAssessment will be based on two marked assignments (due in weeks 5 and 8) with students \nperforming small formalisation and verification projects in a proof assistant together with a \nwrite-up explaining their work (word limit 2,500 per project for the write-up). Each of the \nassignments will be worth 100 marks, distributed as follows: \n \n50 marks for completing basic formalisation and verification tasks assessing grasp of the \nmaterial taught in the lecture. Students will submit their work as theory files for either the \nIsabelle or Coq proof assistant, and they will be assessed for correctness and completeness of \nthe specifications and proofs. \n20 marks for completing designated more challenging tasks, requiring the use of advanced \ntechniques or creative proof strategies. \n30 marks for a clear write-up explaining the design decisions made during the formalisation and \nthe strategy used for the proofs, where 10 of these marks will be reserved for write-ups of \nexceptional quality, e.g. demonstrating particular insight. \nThe main tasks in the assignments will be designed to assess the student's proficiency with the \nbasic material and techniques taught in the lectures and the practical sessions, while the more \nchallenging tasks will give exceptional students the opportunity to earn distinction marks \n\n \nRecommended reading \nNipkow, T., Klein, G. (2014). Concrete Semantics with Isabelle/HOL. (The first part of this book, \n\u201cProgramming and Proving in Isabelle/HOL\u201d, comes with the Isabelle distribution.)  \n \nNipkow, T., Paulson, L.C. and Wenzel, M. (2002). A proof assistant for higher-order logic. \nSpringer LNCS 2283.  \n \nThe Software Foundations series of books, in particular the first two: \n \nPierce, B., Azevedo de Amorim, A., Casinghino, C., Gaboardi, M., Greenberg, M., Hri\u0163cu, C., \nSj\u00f6berg, V., Yorgey, B. (2023). Logical Foundations  \nPierce, B., Azevedo de Amorim, A., Casinghino, C., Gaboardi, M., Greenberg, M., Hri\u0163cu, C., \nSj\u00f6berg, V., Tolmach, A., Yorgey, B. (2023). Programming Language Foundations  \nChlipala, A. (2022). Certified programming with dependent types: a pragmatic introduction to the \nCoq proof assistant. MIT Press.  \n \nSergey, I. (2014). Programs and Proofs: Mechanizing Mathematics with Dependent Types.  \n \nAll of these are freely available online.\n \n\nQUANTUM ALGORITHMS AND COMPLEXITY \n \nAims \nThis module is a research-focused introduction to the theory of quantum computing. The aim is \nto prepare the students to conduct research in quantum algorithms and quantum complexity \ntheory. \n \nSyllabus \nTopics will vary from year to year, based on developments in cutting edge research. \nRepresentative topics include: \n \nQuantum algorithms: \n \nQuantum learning theory \nQuantum property testing \nShadow tomography \nQuantum walks \nQuantum state/unitary synthesis. \nStructure vs randomness in quantum algorithms \nQuantum complexity theory: \n \nThe quantum PCP conjecture \nEntanglement and MIP \nState complexity, AdS/CFT, and quantum gravity \nQuantum locally testable codes \nPseudorandom states and unitaries \nQuantum zero-knowledge proofs \nObjectives \nOn completion of this module, students should: \n \nbe familiar with contemporary quantum algorithms \ndevelop an understanding of the power and limitations of quantum computation \nconduct research in quantum algorithms and complexity theory \nAssessment \nMini research project (70%) \nPresentation (20%) \nAttendance and participation (10%) \nTimelines for the assignment submissions:  \n \nsubmit questions/observations on a weekly basis (i.e., Weeks 2-8) \ndeliver talks in Weeks 5-8 \nsubmit their mini-project at the end of term \nRecommended reading material and resources \n\u201cQuantum Computing Since Democritus\u201d by Scott Aaronson \n\n \n\u201cIntroduction to Quantum Information Science\u201d by Scott Aaronson \n \n\u201cQuantum Computing: Lecture Notes\u201d by Ronald de Wolf \n \n\u201cQuantum Computation and Quantum Information\u201d by Nielsen and Chuang\n \n\nUNDERSTANDING QUANTUM ARCHITECTURE \nPrerequisites: Requires introductory linear algebra - concepts such as eigenvalues, Hermitian \nmatrices, unitary matrices. Taking the Part II Quantum Computing course (or similar) is helpful \nbut not required. Familiarity with computer architecture and compilers is helpful. \nMoodle, timetable \n \nAims \nThis course covers the architecture of a practical-scale quantum computer. We will examine the \nresource requirements of practical quantum applications, understand the different layers of the \nquantum stack, the techniques used in these layers and examine how these layers come \ntogether to enable practical quantum advantage over classical computing. \n \nSyllabus \nThe course has two parts: a series of lectures to cover important aspects of the quantum stack \nand a set of student presentations. The following are a list of representative topics: \n \n- Basics of quantum computing \n- The fault-tolerant quantum stack \n- Compilation \n- Instruction sets \n- Implementations of quantum error correction \n- Implementation of magic state distillation \n- Resource estimation \n \nStudent presentations will be based on a reading list of important papers in quantum \narchitecture.  \n  \n \nObjectives \nAt the end of the course, students will have a broad understanding of the quantum computing \nstack. They will understand how major qubit technologies work, design challenges in real \nquantum hardware, how quantum applications are mapped to a system and the importance of \nquantum error correction for scalability.  \n \nAssessment \nSeminar presentation: 20% \nRead one paper from a provided reading list \nPrepare a 20 minute presentation on it + 10 minutes of answering questions. \nThe student presentation should be similar to a conference presentation - convey the problem, \nwhat are prior solutions, what did the paper do, what are the results, what are future directions \nFor the 20% of the score, the score will be split as 15% and 5%. 15% based on how well the \nstudent understands and explains the paper to the rest of the audience. 5% based on the Q&A \nsession. \n \n\n  \n \nCourse project: 80% (split across a proposal, mid-term report, final report) \nA. Research proposal - at least 500 words (10% of total) \nB. Mid-term report - at least 1000 words (including aspects like the research problem, literature \nreview, what questions will be evaluated, any early ideas or methods (20% of total)  \nC. Final report - up to 4 pages double column in a conference paper style with 3000-4000 \nwords. In addition to the mid term report, should include aspects like results and future \ndirections. (50% of the total) \nD. Report on contributions (in case of group work by 2 students) - 1 paragraph on individual \ncontribution of the student. 1 paragraph on teammate's contribution. \nStudents may work alone in the project, in which case they need only components A-C. \nStudents may work in pairs of two, but they should in addition individually submit D. The \ninstructor may conduct a short viva in case contributions from both team members are not clear \nor imbalanced. \nRecommended reading \nNielsen M.A., Chuang I.L. (2010). Quantum Computation and Quantum Information. Cambridge \nUniversity Press. \n \nMermin N.D. (2007). Quantum Computer Science: An Introduction. Cambridge University Press. \n \nAssessing requirements to scale to practical quantum advantage (2022) Beverland et al.\n\n",
        "8th Semester \nADVANCED TOPICS IN CATEGORY THEORY \n \n \nTeaching \nThe teaching style will be lecture-based, but supported by a practical component where \nstudents will learn to use a proof assistant for higher category theory, and build a small portfolio \nof proofs. Towards the end of the course we will explore some of the exciting computer science \nresearch literature on monoidal and higher categories, and students will choose a paper and \npresent it to the class. \n \nAims \nThe module will introduce advanced topics in category theory. The aim is to train students to \nengage and start modern research on the mathematical foundations of higher categories, the \ngraphical calculus, monoids and representations, type theories, and their applications in \ntheoretical computer science, both classical and quantum. \n \nObjectives \nOn completion of this module, students should: \n \nBe familiar with the techniques of compositional category theory. \nHave a strong understanding of basic categorical semantic models. \nBegun exploring current research in monoidal categories and higher structures. \nSyllabus \nPart 1, lecture course: \nThe first part of the course introduces concepts from monoidal categories and higher categories, \nand explores their application in computer science. \n\u2010 Monoidal categories and the graphical calculus \n\u2010 The proof assistant homotopy.io \n\u2010 Coherence theorems and higher category theory \n\u2010 Linearity, superposition, duality, quantum entanglement \n\u2010 Monoids, Frobenius algebras and bialgebras \n\u2010 Type theory for higher category theory \n \nPart 2, exploring the research frontier: \nIn the second part of the course, students choose a research paper to study, and give a \npresentation to the class. \nThere is a nice varied literature related to the topics of the course, and the lecturer will supply a \nlist of suggested papers.  \n \nClasses \nThere will be three exercise sheets for homework, with accompanying classes by a teaching \nassistant to go over them. \n \n\nAssessment \nProblem sheets (50%) \nClass presentation (20%) \nPractical portfolio (30%) \nReading List \nChris Heunen and Jamie Vicary, \u201cCategory for Quantum Theory: An Introduction\u201d, Oxford \nUniversity Press\n \n\nADVANCED TOPICS IN COMPUTER SYSTEMS \n \nAims \nThis module will attempt to provide an overview of \u201csystems\u201d research. This is a very broad field \nwhich has existed for over 50 years and which has historically included areas such as operating \nsystems, database systems, file systems, distributed systems and networking, to name but a \nfew. The course will thus necessarily cover only a tiny subset of the field. \n \nMany good ideas in systems research are the result of discussing and debating previous work. \nA primary aim of this course therefore will be to educate students in the art of critical thinking: \nthe ability to argue for and/or against a particular approach or idea. This will be done by having \nstudents read and critique a set of papers each week. In addition, each week will include \npresentations from a number of participants which aim to advocate or criticise each piece of \nwork. \n \nSyllabus \nThe syllabus for this course will vary from year to year so as to cover a mixture of older and \nmore contemporary systems papers. Contemporary papers will be generally selected from the \npast 5 years, primarily drawn from high quality conferences such as SOSP, OSDI, ASPLOS, \nFAST, NSDI and EuroSys. Example topics might include: \n \nSystems Research and System Design \nOS Structure and Virtual Memory \nVirtualisation \nConsensus \nScheduling \nPrivacy \nData Intensive Computing \nBugs \nThe reading each week will involve a load equivalent to 3 full length papers. Students will be \nexpected to read these in detail and prepare a written summary and review. In addition, each \nweek will contain one or more short presentations by students for each paper. The types of \npresentation will include: \n \nOverview: a balanced presentation of the paper, covering both positive and negative aspects. \nAdvocacy: a positive spin on the paper, aiming to convince others of its value. \nCriticism: a negative take on the paper, focusing on its weak spots and omissions. \nThese presentation roles will be assigned in advance, regardless of the soi disant absolute merit \nof the paper or the preference of the student. Furthermore, all students \u2013 regardless of any \nassigned presentation role in a given week \u2013 will be expected to participate in the class by \nasking questions and generally entering into the debate. \n \nObjectives \n\nOn completion of this module students should have a broad understanding of some key papers \nand concepts in computer systems research, as well as an appreciation of how to argue for or \nagainst any particular idea. \n \nCoursework and practical work \nCoursework will be the production of the weekly paper reviews. Practical work will be presenting \npapers as appropriate, as well as ongoing participation in the class. \n \nAssessment \nAssessment consists of: \n \nOne essay per week for 7 weeks (10% each) \nPresentation (20%) \nParticipation in class over the term (10%) \nRecommended reading \nMost of the reading for this course will be in the form of the selected papers each week. \nHowever, the following may be useful background reading to refresh your knowledge from \nundergraduate courses: \n \nSilberschatz, A., Peterson, J.L. and Galvin, P.C. (2005). Operating systems concepts. \nAddison-Wesley (7th ed.). \n \nTanenbaum, A.S. (2008). Modern Operating Systems. Prentice-Hall (3rd ed.). \n \nBacon, J. and Harris, T. (2003). Operating systems. Addison-Wesley (3rd ed.). \n \nAnderson, T. and Dahlin, M. (2014). Operating Systems: Principles and Practice. Recursive \nBooks (2nd ed.). \n \nHennessy, J. and Patterson, D. (2006). Computer architecture: a quantitative approach. Elsevier \n(4th ed.). ISBN 978-0-12-370490-0. \n \nKleppmann M (2016) Designing Data-Intensive Applications, O'Reilly (1st ed.)\n \n\nADVANCED TOPICS IN MACHINE LEARNING \nAims \nThis course explores current research topics in machine learning in sufficient depth that, at the \nend of the course, participants will be in a position to contribute to research on their chosen \ntopics. Each topic will be introduced with a lecture which, building on the material covered in the \nprerequisite courses, will make the current research literature accessible. Each lecture will be \nfollowed by up to three sessions which will typically be run as a reading group with student \npresentations on recent papers from the literature followed by a discussion, or a practical, or \nsimilar. \n \nStructure \nEach student will attend 3 topics and each topic's sessions will be spread over 5 contact hours. \nStudents will be expected to undertake readings for their selected topics. There will be some \ngroup work. \n \nThere will be a briefing session in Michaelmas term. \n \nSyllabus \nStudents choose five topics in preferential order from a list to be published in Michaelmas term. \nThey will be assigned to three topics out of their list. Students are assessed on one of these \ntopics which may not necessarily be their first choice topic. \n \nThe topics to be offered in 2024-25 are yet to be decided but to give an indicative idea of the \ntypes of topics, the ones offered in 2023-24 were: \n \nImitation learning  Prof A. Vlachos \nThe Future of Large Language Models: data architectures ethics Dr M. Tomalin \nPhysics and Geometry in Machine Learning Dr C. Mishra \nDiffusion Models and SDEs Francisco Vargas, Dr C. H. Ek \nExplainable Artificial Intelligence M. Espinosa, Z. Shams, Prof M. Jamnik \nUnconventional approaches to AI Dr S. Banerjee \nNarratives in Artificial Intelligence and Machine Learning Prof N. Lawrence \nMultimodal Machine Learning K. Hemker, N. Simidjievski, Prof M. Jamnik \nDeep Reinforcement Learning S. Morad, Dr C. H. Ek \nAutomation in Proof Assistants A. Jiang, W. Li, Prof M. Jamnik \nOn completion of this module, students should: \n \nbe in a strong position to contribute to the research topics covered; \nunderstand the fundamental methods (algorithms, data analysis, specific tasks) underlying each \ntopic; \nand be familiar with recent research papers and advances in the field. \nCoursework \nStudents will typically work in groups to give a presentation on assigned papers. Alternatively, a \ntopic may include practical sessions. Each topic will typically consist of one preliminary lecture \n\nfollowed by 3 reading and discussion sessions, or several lectures followed by a practical \nsession. A typical topic can accommodate up to 9 students presenting papers. There will be at \nleast 10 minutes general discussion per session. \n \nFull coursework details will be published by October. \n \nAssessment \nCoursework will be marked by the topic leaders and second marked by the module conveners. \n \nParticipation in all assigned topics, 10% \nPresentation or practical work or similar (for one of the chosen topics), 20% \nTopic coursework (for one of the chosen topics), 70% \nIndividual topic coursework will be published late Michaelmas term. \n \nAssessment criteria for topic coursework will follow project assessment criteria here: \nhttps://www.cl.cam.ac.uk/teaching/exams/acs_project_marking.pdf \n \nPlease note that students will be assessed on one of their three chosen topics but this may not \nbe their first choice. \n \nRecommended reading \nTo be confirmed by each topic convenor.\n \n\nCOMPUTING FOR COLLECTIVE INTELLIGENCE \nPrerequisites: Familiarity with core machine learning paradigms - Basic calculus (analytical \nskills) - Basic discretre optimization (combinatorics) \nMoodle, timetable \n \nObjectives \nThere is a substantial body of academic work demonstrating that complex life is built by \ncooperation across scales: collections of genes cooperate to produce organisms, cells \ncooperate to produce multi-cellular organisms and multicellular animals cooperate to form \ncomplex social groups. Arguably, all intelligence is collective intelligence. Yet, to-date, many \nartificially intelligent agents (both embodied and virtual), are generally not conceived from the \nground up to interact with other intelligent agents (be it machines or humans). The canonical AI \nproblem is that of a monolithic and solitary machine confronting a non-social environment. This \ncourse aims to balance this trend by (i) equipping students with conceptual and practical \nknowledge on collective intelligence from a computational standpoint, and (ii) by conveying \nvarious computational paradigms by which collective intelligence can be modelled as well as \nsynthesized. \n \nAssessment \nThe assessment will be based on a reading-group paper presentation (individual or in pairs), \naccompanied by a 2-page summary, and a technical position paper (individual work) handed in \nafter the course. The position paper will be assessed based on its technical correctness, \nstrength or arguments, \nand clarity of research vision, and will be accompanied by a brief 5-minute pre-recorded talk that \nsummarizes key arguments (any slides used are also submitted as part of the project). \n \nPaper presentation and 1-page summary: 25% \nTechnical position paper: 75% (4000 word limit) \nRecommended reading \nSwarm Intelligence : From Natural to Artificial Systems, Bonabeau et al (1999) \nJoined-Up Thinking, Hannah Critchlow, (2024) \nSupercooperators, Martin Nowak (2011) \nA Course on Cooperative Game Theory, Chakravarty et al., (2015) \nGoverning the Commons: The Evolution of Institutions for Collective Action. Ostrom (1990).  \n \n\nCRYPTOGRAPHY AND PROTOCOL ENGINEERING \nAims \nWe all use cryptographic protocols every day: whenever we access an https:// website or send a \nmessage via WhatsApp or Signal, for example. In this module, students will get hands-on \nexperience of how those protocols are implemented. Students start by writing their own secure \nmessaging protocol from scratch, and then move on to more advanced examples of \ncryptographic protocols for security and privacy, such as private information retrieval (searching \nfor data without revealing what you\u2019re searching for). \n \nSyllabus \nThis is a practical course in which the first half of each of the 2-hour weekly sessions is a lecture \nthat introduces a topic, while the second half is lab time during which the students can work on \ntheir implementations, discuss the topics in small groups, and get help from the lecturers. The \nmodule is structured into three blocks as follows:  \n \nCryptographic primitives (2 weeks). Using common building blocks such as symmetric ciphers \nand hash functions. Implementing selected aspects of elliptic curve Diffie-Hellman and digital \nsignatures. Protocol security, Dolev-Yao model, side-channel attacks.  \nSecure communication (3 weeks). Authenticated key exchange (e.g. SIGMA, TLS), \npassword-authenticated key exchange (e.g. SPAKE2), forward secrecy, post-compromise \nsecurity, double ratchet, the Signal protocol.  \nPrivate database lookups (3 weeks). Lattice-based post-quantum cryptography, learning with \nerrors, homomorphic encryption, private information retrieval.  \nIn each block, students will be given a description of the algorithms and protocols covered (e.g. \na research paper or an RFC standards document), and a code template that the students \nshould use for their implementation. The implementation language will probably be Python (to \nbe confirmed after the practical materials have been developed ). Students will also be given a \nset of test cases to check their code, and will have the opportunity to test their implementation \ncommunicating with other students\u2019 implementations. Finally, for each block, students will write \nand submit a lab report explaining their approach and the findings from their implementation. \n \nObjectives \nOn completion of this module, students should: \n \nUnderstand that cryptography is not magic! The mathematics can look intimidating, but this \nmodule will show students that they needn\u2019t be afraid of cryptography. \nGain appreciation for the complexity and careful implementation of real-world cryptography \nlibraries ,and understand why one should prefer vetted implementations. \nBe familiar with the mathematical notation and concepts commonly used in descriptions of \ncryptographic protocols, and be able to understand and implement research papers and RFCs \nin this field. \nBe comfortable with cryptographic primitives commonly used in protocols, and able to correctly \nuse software libraries that implement them. \n\nHave gained hands-on experience of attacks on cryptographic protocols, and an appreciation \nfor the difficulty of making these protocols correct. \nHave been exposed to ideas and techniques from recent research, which may be useful for their \nown future research. \nHave practised technical writing through lab reports. \nAssessment \nStudents will be provided with code skeletons in one or two different programming languages \n(probably Python, maybe Rust) to avoid them struggling with setup issues. For each \nassignment, students are required to produce a working implementation of the protocols \ndiscussed in the course, using the programming language and skeleton provided. Here, \n\u201cworking\u201d means that the algorithms are functionally correct, but they are not production-quality \n(in particular, no side-channel countermeasures such as constant-time algorithms are required). \nFor some primitives (e.g. symmetric ciphers and hash functions) existing libraries should be \nused, whereas other cryptographic algorithms will be implemented from scratch. Students \nshould submit their code as a Docker container that can be run against specified test cases.  \n \nAfter each of the three blocks, students will be asked to submit a lab report of approximately \n2,000 words along with their code for that block. In the lab report, the students should explain \nhow their implementation works and why it is correct, as well as any findings from the work (e.g. \nany limitations or trade-offs they found with the protocols). The report structure and marking \nscheme will be specified in advance. The lab reports provide an opportunity for students to \nprovide critical insights and creativity. For instance, they might compare their implementation \nwith existing real-world implementations which provide additional features and guarantees.  \n \nEach of the three lab reports (along with the associated code) will be marked, forming the basis \nof assessment, and feedback on the reports will be given to the students before the next \nsubmission is due, so that students can take the feedback on board for their next submission. \nOf the three reports, the worst mark will be discarded, and the other two reports each contribute \n50% of the final mark. As such, students might decide to submit only two reports.  \n \nStudents may discuss their work with others, but the implementations and lab reports must be \nindividual work. \n \nRecommended reading \nTextbook: Jonathan Katz and Yehuda Lindell. Introduction to Modern Cryptography (3rd edition). \nCRC Press, 2020. \n \nThe textbook covers prerequisite knowledge on cryptographic primitives, such as ciphers, \nMACs, hash functions, and signatures. This book is also used in the Part II Cryptography \ncourse. For the protocols we cover in this course, we are not aware of a suitable textbook; \ninstead we will use research papers, lecture slides, and RFCs as resources, which will be \nprovided at the start of the module.\n \n\nDISTRIBUTED LEDGER TECHNOLOGIES: FOUNDATIONS AND APPLICATIONS \n \nAims \nThis reading group course examines foundations and current research into distributed ledger \n(blockchain) technologies and their applications. Students will read, review, and present seminal \nresearch papers in this area. Once completed, students should be able to integrate blockchain \ntechnologies into their own research and gain familiarity with a range of research skills. \n \nLectures \nIntroduction \nConsensus protocols \nBitcoin and its variants \nEthereum, smart contracts, and other permissionless DLTs \nHybrid and permissioned DLTs \nApplications \nLearning objectives \nThere are two broad objectives: to acquire familiarity with a body of work in the area of \ndistributed ledgers and to learn some specific research skills: \n \nHow to read a paper \nHow to review a paper \nHow to analyze a paper\u2019s strengths and weaknesses \nWritten and oral presentation skills \nAssessment \nYou are expected to read all assigned papers and submit paper reviews each week. Each \nreview must either follow the provided review form [PDF] [Latex source]. Each \u201creview\u201d is worth \n5% of your total mark, and is marked out of 100 with 60 a passing grade. Marks will be awarded \nand penalties for late submission applied according to ACS Assessment Guidelines.  \n \nOne paper review for the first week, then two paper reviews each week for 6 weeks (13 reviews, \n5% each) 65% (approx 600 words per review) \nSummative essay 25% (max 3000 words) \nPresentation 5% \n100% attendance in class 5% (2 marks deducted per missed class) \nRecommended Reading \nNarayanan, A. , Bonneau, J., Felten, E., Miller, A. and Goldfeder, S. (2016). Bitcoin and \nCryptocurrency Technologies: A Comprehensive Introduction. Princeton University Press. \n(2016 Draft available here: \nhttps://d28rh4a8wq0iu5.cloudfront.net/bitcointech/readings/princeton_bitcoin_book.pdf)\n \n\nEXPLAINABLE ARTIFICIAL INTELLIGENCE \nPrerequisites: A solid background in statistics, calculus and linear algebra. We strongly \nrecommend some experience with machine learning and deep neural networks (to the level of \nthe first chapters of Goodfellow et al.\u2019s \u201cDeep Learning\u201d). Students are expected to be \ncomfortable reading and writing Python code for the module\u2019s practical sessions. \nMoodle, timetable \n \nAims \nThe recent palpable introduction of Artificial Intelligence (AI) models to everyday \nconsumer-facing products, services, and tools brings forth several new technical challenges and \nethical considerations. Amongst these is the fact that most of these models are driven by Deep \nNeural Networks (DNNs), models that, although extremely expressive and useful, are \nnotoriously complex and opaque. This \u201cblack-box\u201d nature of DNNs limits their ability to be \nsuccessfully deployed in critical scenarios such as those in healthcare and law. Explainable \nArtificial Intelligence (XAI) is a fast-moving subfield of AI that aims to circumvent this crucial \nlimitation of DNNs by either  \n \n(i) constructing human-understandable explanations for their predictions,  \n \nor (ii) designing novel neural architectures that are interpretable by construction.  \n \nIn this module, we will introduce the key ideas behind XAI methods and discuss some of the \nimportant application areas of these methods (e.g., healthcare, scientific discovery, debugging, \nmodel auditing, etc.). We will approach this by focusing on the nature of what constitutes an \nexplanation, and discussing different ways in which explanations can be constructed or learnt to \nbe generated as a by-product of a model. The main aim of this module is to introduce students \nto several commonly used approaches in XAI, both theoretically in lectures and through \nhands-on exercises in practicals, while also bringing recent promising directions within this field \nto their attention. We hope that, by the end of this module, students will be able to directly \ncontribute to XAI research and will understand how methods discussed in this module may be \npowerful tools for their own work and research. \n \nSyllabus \nOverview and taxonomy of XAI (why is explainability needed, definition of terms, taxonomy of \nthe XAI space, etc.)  \nPerturbation-based feature attribution (e.g., LIME, Anchors, SHAP, etc.) \nPropagation-based feature attribution methods (e.g., Relevance Propagation, Saliency \nmethods, etc.) \nConcept-based explainability (Net2Vec, T-CAV, ACE, etc.) \nInterpretable architectures (CBMs and variants, Concept Whitening, SENNs, etc.) \nNeurosymbolic methods (DeepProbLog, Neural Reasoners, etc.) \nSample-based Explanations (Influence functions, ProtoPNets, etc.) \nCounterfactual explanations \nProposed Schedule \n\nThe 16 hours of lectures across 8 weeks will be divided as follows:  \nWeek 1: 1h lecture + 1h lecture  \nWeek 2: 1h lecture + 1h reading group & presentations  \nWeek 3: 1h lecture + 1h reading group & presentations  \nWeek 4: 2h practical  \nWeek 5: 1h lecture + 1h reading group & presentations  \nWeek 6: 2h practical  \nWeek 7: 1h lecture + 1h reading group & presentations  \nWeek 8: 1h reading group & presentation + 1h reading group & presentations \n \nIn weeks where lectures are planned, one-hour lecture slots will intercalate with one hour of \nstudent paper presentations. During each paper presentation session, three students will \npresent a paper related to the topic covered in the earlier lecture that week for about 10 minutes \neach + 5 minutes of questions. At the end of all paper presentations, there will be a discussion \non all the papers. The order of student presentations will be allocated randomly during the first \nweek so that students know in advance when they are expected to present. For the sake of \nfairness, we will release the paper to be presented by each student a week before their \npresentation slot. \n \nObjectives \nBy the end of this module, students should be able to: \n \nRecognise and identify key concepts in XAI together with their connection to related subfields in \nAI such as fairness, accountability, and trustworthy AI. \nUnderstand how to use, design, and deploy model-agnostic perturbation methods such as \nLIME, Anchors, and RISE. In particular, students should understand the connection between \nfeature importance and cooperative game theory, and its uses in methods such as SHAP. \nIdentify the uses and limitations of propagation-based feature importance methods such as \nSaliency, SmoothGrad, GradCAM, and Integrated Gradients. Students should be able to \nimplement each of these methods on their own and connect the theoretical ideas behind them \nto practical code, exploiting modern frameworks\u2019 auto-differentiation. \nUnderstand what concept learning is and what limitations it overcomes compared to traditional \nfeature-based methods. Specifically, students should understand how probing a DNN\u2019s latent \nspace may be exploited for learning useful concepts for explainability. \nReason about the key components of inherently interpretable architectures and neuro-symbolic \nmethods and understand how interpretable neural networks can be designed from first \nprinciples. \nElaborate on what sample-based explanations are and how influence functions and prototypical \narchitectures such as ProtoPNet can be used to construct such explanations. \nExplain what counterfactual explanations are, how they are related to causality, and under which \nconditions they may be useful. \nUpon completion of this module, students will have the technical background and tools to use \nXAI as part of their own research or partake in XAI research itself. Moreover, we hope that by \ndetailing a clear timeline of how this relatively young subfield has developed, students may be \n\nable to better understand what are some fundamental open questions in this area and what are \nsome promising directions that are currently actively being explored. \n  \n \nAssessment \n(10%) student presentation: each student will be randomly assigned a presentation slot at the \nbeginning of the course. We will then distribute a paper for them to present in their slot a week \nbefore the presentation\u2019s scheduled time. Students are expected to prepare a 10-minute \npresentation of their assigned paper where they will present the motivation of their assigned \nwork and discuss the main methodology and findings reported in that paper. We will encourage \nstudents to focus on fully communicating the intuition of the work in their assigned papers and \ntry and connect it with ideas that we have previously discussed in previous lectures. All students \nnot presenting each week will be asked to submit one question pertaining to each paper \npresented that week before the paper presentations. \n \n(20%) practical exercises: We will run two practical sessions where students will be asked to \nperform a series of exercises that require them to use concepts we have introduced in lecture \nup to that point. For each practical session, we will prepare a colab notebook to guide the \nstudent through exercises and we will ask the students to submit their solutions through this \ncolab notebook. We expect students to complete about \u2154 of the exercises in the practical class \nand complete the rest at home as homework. Each practical will be worth 10%. \n \n(70%) mini-project: At the end of week 3, we will hand out a list of papers for students to select \ntheir mini-projects from. Each mini-project will consist of a student selecting a paper from our list \nand reimplementing and expanding the key idea in the paper. We encourage students to be as \ncreative as they want with how they drive their mini-project once the paper has been selected. \nFor example, they can reimplement the technique in the paper and combine it with \nmethodologies from other works we discussed in lecture, or they can apply their paper\u2019s \nmethodology to a new domain, datasets, or setup, where the technique may offer interesting \nand potentially novel insights. We will ask all students to submit a report in a workshop format of \nup to 4,000 words. This report, due roughly a week after Lent term ends, should describe their \nmethodology, experiments, and results. A crucial aspect of this report involves explaining the \nrationale behind different choices in methodology and experiments, as well as elaborating on \nthe choices made and hypotheses tested throughout their mini-project (potentially showing a \ndeep understanding of the work they are basing their mini-project on). To aid students with \nselecting their projects and making progress on them, we will hold regular office hours when \nstudents can come to discuss their progress and questions with us. \n \nRecommended reading \nTextbooks \n \n* Christoph Molnar, \u201cInterpretable Machine Learning\u201d. (2022): \nhttps://christophm.github.io/interpretable-ml-book/ \n \n\nOnline Courses and Tutorial \n \n* Su-in Lee and Ian Cover, \u201cCSEP 590B Explainable AI\u201d University of Washington* Explaining \nMachine Learning Predictions: State-of-the-art, Challenges, and Opportunities \n \n* On Explainable AI: From Theory to Motivation, Industrial Applications, XAI Coding & \nEngineering Practices - AAAI 2022 Turorial \n \nSurvey papers \n \n* Arrieta, Alejandro Barredo, et al. \"Explainable Artificial Intelligence (XAI): Concepts, \ntaxonomies, opportunities and challenges toward responsible AI.\" Information fusion 58 (2020): \n82-115. \n \n* Rudin, Cynthia, et al. \"Interpretable machine learning: Fundamental principles and 10 grand \nchallenges.\" Statistics Surveys 16 (2022): 1-85.\n \n\nFEDERATED LEARNING: THEORY AND PRACTICE \nPrerequisites: It is strongly recommended that students have previously (and successfully) \ncompleted an undergraduate machine learning course - or have equivalent experience through \nopen-source material (e.g., lectures or similar). An example course would be: Part1B \"Artificial \nIntelligence\". Students should feel comfortable with SGD and optimization methods used to train \ncurrent popular neural networks and simple forms of neural network architectures. \nMoodle, timetable \n \nObjectives \nThis course aims to extend the machine learning knowledge available to students in Part I (or \npresent in typical undergraduate degrees in other universities), and allow them to understand \nhow these concepts can manifest in a decentralized setting. The course will consider both \ntheoretical (e.g., decentralized optimization) and practical (e.g., networking efficiency) aspects \nthat combine to define this growing area of machine learning.  \n \nAt the end of the course students should: \n \nUnderstand popular methods used in federated learning \nBe able to construct and scale a simple federated system \nHave gained an appreciation of the core limitations to existing methods, and the approaches \navailable to cope with these issues \nDeveloped an intuition for related technologies like differential privacy and secure aggregation, \nand are able to use them within typical federated settings  \nCan reason about the privacy and security issues with federated systems \nLectures \nCourse Overview. Introduction to Federated Learning. \nDecentralized Optimization. \nStatistical and Systems Heterogeneity. \nVariations of Federated Aggregation. \nSecure Aggregation. \nDifferential Privacy within Federated Systems. \nExtensions to Federated Analytics. \nApplications to Speech, Video, Images and Robotics. \nLab sessions \nFederating a Centralized ML Classifier. \nBehaviour under Heterogeneity. \nScaling a Federated Implementation. \nExploring Privacy with Federated Settings \nRecommended Reading \nReadings will be assigned for each lecture. Readings will be taken from either research papers, \ntutorials, source code or blogs that provide more comprehensive treatment of taught concepts. \n \nAssessment - Part II Students \n\nFour labs are performed during the course, and students receive 10% of their total grade for \nwork done as part of each lab. (For a total of 40% of the total grade from lab work alone). Labs \nwill primarily provide hands-on teaching opportunities, that are then utilized within the lab \nassignment which is completed outside of the lab contact time. MPhil and Part III students will \nbe given additional questions to answer within their version of the lab assignment which will \ndiffer from the assignment given to Part II CST students. \n \nThe remainder of the course grade (60%) will be given based on a hands-on project that applies \nthe concepts taught in lectures and labs. This hands-on project will be assessed based on upon \na combination of source code, related documentation and brief 8-minute pre-recorded talk that \nsummarizes key project elements (any slides used are also submitted as part of the project). \nPlease note, that in the case of Part II CST students, the talk itself is not examinable -- as such \nwill be made optional to those students. \n \nA range of possible practical projects will be described and offered to students to select from, or \nalternatively students may propose their own. MPhil and Part III students will select from a \nproject pool that is separate from those offered to Part II CST students. MPhil and Part III \nprojects will contain a greater emphasis on a research element, and the pre-recorded talks \nprovided by this student group will focus on this research contribution. The project will be \nassessed on the level of student understanding demonstrated, the degree of difficulty, \ncorrectness of implementation -- and for Part III/MPhil students the additional criteria of the \nquality and execution of the research methodology, and depth and quality of results analysis. \n \nThis project can be done individually or in groups -- although individual projects will be strongly \nencouraged. It will be required the project is performed using a code repository that also will \ncontain all documentation -- access to this repository will be shared with course staff (e.g., \nlecturer and TAs). Where needed, marks assigned to students within a group will be \ndifferentiated using this repository as an input. Furthermore if groups are formed, members must \nbe either entirely from Part III/MPhil students or Part II CST, i.e., these two student groups \nshould not mix to form a project group. \n \nProjects will be made available publicly. A maximum word count for written contributions for the \nproject will be enforced.  \n \n  \nAssessment - MPhil / Part III Students \n50% final mini-project. This hands-on project will be assessed based on upon a combination of \nsource code, related documentation and brief 8-minute pre-recorded talk that summarizes key \nproject elements (any slides used are also submitted as part of the project). \n \n30% midterm lab report (a written report of a bit more depth of thought than required in a lab, \nthe report provides the results of experiments -- with the skills to perform these experiments \ncoming from lab sessions),  \n \n\n20% for two individual hands-on labs. Labs will primarily provide hands-on teaching \nopportunities, that are then utilized within the lab assignment which is completed outside of the \nlab contact time. There will also be additional questions to answer within their version of the lab \nassignment. \n \nFor the mini-project, a range of possible practical projects will be described and offered to \nstudents to select from, or alternatively students may propose their own. The project will be \nassessed on the level of student understanding demonstrated, the degree of difficulty, \ncorrectness of implementation, the quality and execution of the research methodology, and \ndepth and quality of results analysis. \n \nThe project can be done individually or in groups -- although individual projects will be strongly \nencouraged. It will be required the project is performed using a code repository that also will \ncontain all documentation -- access to this repository will be shared with course staff (e.g., \nlecturer and TAs). Where needed, marks assigned to students within a group will be \ndifferentiated using this repository as an input. Furthermore if groups are formed, members must \nbe either entirely from Part III or entirely from MPhil students, i.e., these two student groups \nshould not mix to form a project group. \n \nProjects will be made available publicly. A maximum word count for written contributions for the \nproject will be enforced.   \n \nAdvanced Material sessions - MPhil / Part III Students \nThese sessions are mandatory for the MPhil and Part III students. Part II CST students may \nattend if they wish \n \nTopics and announced the first week of class. Selected to extend beyond topics covered in \nlectures and labs. Content is a mixture of sessions run in the lecture room the are either (1) \npresentations by guest lectures by an invited domain expert or (2) class-wide discussions \nregarding one or more related academic papers. Paper discussions will require students to read \nthe paper ahead of the lecture, and a brief discussion primer presentation will be given students \nbefore discussions begin.  \n \nThis module is shared with Part II of the Computer Science Tripos. Assessment will be adjusted \nfor the two groups of students to be at an appropriate level for whichever course the student is \nenrolled on. Further information about assessment and practicals will follow at the first lecture.\n\nGEOMETRIC DEEP LEARNING \n \nPrerequisites: Experience with machine learning and deep neural networks is recommended. \nKnowledge of concepts from graph theory and group theory is useful, although the relevant \nparts will be explicitly retaught. \nMoodle, timetable \n \nAims and Objectives \nMost of the patterns we see in nature can be elegantly reasoned about using spatial \nsymmetries\u2014transformations that leave underlying objects unchanged. This observation has \nhad direct implications for the development of modern deep learning architectures that are \nseemingly able to escape the \u201ccurse of dimensionality\u201d and fit complex high-dimensional tasks \nfrom noisy real-world data. Beyond this, one of the most generic symmetries\u2014the \npermutation\u2014will prove remarkably powerful in building models that reason over graph \nstructured-data, which is an excellent abstraction to reason about naturally-occurring, \nirregularly-structured data. Prominent examples include molecules (represented as graphs of \natoms and bonds, with three-dimensional coordinates provided), social networks and \ntransportation networks. Several already-impacted application areas include traffic forecasting, \ndrug discovery, social network analysis and recommender systems. The module will provide the \nstudents the capability to analyse irregularly- and nontrivially-structured data in an effective way, \nand position geometric deep learning in a proper context with related fields. The main aim of the \ncourse is to enable students to make direct contributions to the field, thoroughly assimilate the \nkey concepts in the area, and draw relevant connections to various other fields (such as NLP, \nFourier Analysis and Probabilistic Graphical Models). We assume only a basic background in \nmachine learning with deep neural networks. \n \nLearning outcomes \nThe framework of geometric deep learning, and its key building blocks: symmetries, \nrepresentations, invariance and equivariance \nFundamentals of processing data on graphs, as well as impactful application areas for graph \nrepresentation learning \nTheoretical principles of graph machine learning: permutation invariance and equivariance \nThe three \"flavours\" of spatial graph neural networks (GNNs) (convolutional, attentional, \nmessage passing) and their relative merits. The Transformer architecture as a special case. \nAttaching symmetries to graphs: CNNs on images, spheres and manifolds, Geometric Graphs \nand E(n)-equivariant GNNs \nRelevant connections of geometric deep learning to various other fields (such as NLP, Fourier \nAnalysis and Probabilistic Graphical Models) \nLectures \nThe lectures will cover the following topics: \n \nLearning with invariances and symmetries: geometric deep learning. Foundations of group \ntheory and representation theory. \n\nWhy study data on graphs? Success stories: drug screening, travel time estimation, \nrecommender systems. Fundamentals of graph data processing: network science, spectral \nclustering, node embeddings. \nPermutation invariance and equivariance on sets and graphs. The principal tasks of node, edge \nand graph classification. Neural networks for point clouds: Deep Sets, PointNet; universal \napproximation properties. \nThe three flavours of spatial GNNs: convolutional, attentional, message passing. Prominent \nexamples: GCN, SGC, ChebyNets, MoNet, GAT, GATv2, IN, MPNN, GraphNets. Tradeoffs of \nusing different GNN variants. \nGraph Rewiring: how to apply GNNs when there is no graph? Links to natural language \nprocessing---Transformers as a special case of attentional GNNs. Representative \nmethodologies for graph rewiring: GDC, SDRF, EGP, DGCNN. \nExpressive power of graph neural networks: the Weisfeiler-Lehman hierarchy. GINs as a \nmaximally expressive GNN. Links between GNNs and graph algorithms: neural algorithmic \nreasoning. \nCombining spatial symmetries with GNNs: E(n)-equivariant GNNs, TFNs, SE(3)-Transformer. A \ndeep dive into AlphaFold 2. \nWorked examples: Circulant matrices on grids, the discrete Fourier transform, and convolutional \nnetworks on spheres. Graph Fourier transform and the Laplacian eigenbasis. \nPracticals \nThe practical is designed to complement the knowledge learnt in lectures and teach students to \nderive additional important results and architectures not directly shown in lectures. The practical \nwill be given as a series of individual exercises (each either code implementation or \nproof/derivation). Each of these exercises can be individually assessed based on a specified \nmark budget. \n \nPossible practical topics include the study of higher-order GNNs and equivariant message \npassing. \n \nAssessment \n(60%) Group Mini-project (writeup) at the end of the course. The mini projects can either be \nself-proposed, or the students can express their preference for one of the provided topics, the \nlist of which will be announced at the start of term. The projects will consist of implementing \nand/or extending graph representation learning models in the literature, applying them to \npublicly available datasets. Students will undertake the project in pairs and submit a joint \nwriteup limited to 4,000 words (in line with other modules); appendix of work logs to be included \nbut ungraded; \n \n(10%) Short presentation and viva: students will give a short presentation to explain their \nindividual contribution to the mini-project and there will be a short viva following. \n \n(30%) Practical work completion. Completing the exercises specified in the practical to a \nsatisfactory standard. The practical assessor should be satisfied that the student derived their \nanswers using insight gained from the course; coupled with original thought, not by simple \n\ncopy-pasting of relevant related work. The students would submit code and a short report, which \nwould then be marked in line with the predetermined mark budget for each practical item. \nThe students will learn how to run advanced architectures on GPU but no specific need for \ndedicated GPU resources. Practicals will be made possible to do on CPU; if required, students \ncan use GPUs on publicly available free services (such as Colab) for their mini-project work. \n \nReferences \nThe course will be based on the following literature: \n \n\"Geometric Deep Learning: Grids, Graphs, Groups, Geodesics, and Gauges\", by Michael \nBronstein, Joan Bruna, Taco Cohen and Petar Veli\u010dkovi\u0107 \n\"Graph Representation Learning\", by Will Hamilton \n\"Deep Learning\", by Ian Goodfellow, Yoshua Bengio and Aaron Courville.\n \n\nMOBILE HEALTH \nAims \nThe course aims to explore how wearable and mobile systems sensors can be used to gather \ndata relevant to understand health, how the data can be analysed with advanced signal \nprocessing and machine learning and the performance of these systems in terms of diagnostics \nand disease progression detection. \n \nSyllabus \nCourse Overview. Introduction to Mobile Health. Evaluation metrics and methodology. Basics of \nSignal Processing. \nInertial Measurement Units, Human Activity Recognition (HAR) and Gait Analysis and Machine \nLearning for IMU data. \nRadios, Bluetooth, GPS and Cellular. Epidemiology and contact tracing, Social interaction \nsensing and applications. Location tracking in health monitoring and public health. \nAudio Signal Processing. Voice and Speech Analysis: concepts and data analysis. Body \nSounds analysis. \nPhotoplethysmogram and Light sensing for health (heart and sleep) \nContactless and wireless behaviour and physiological monitoring \nGenerative Models for Wearable Health Data \nTopical Guest Lectures \nObjectives \nThe course aims to explore how wearable and mobile systems sensors can be used to gather \ndata relevant to understand health, how the data can be analysed with advanced signal \nprocessing and machine learning and the performance of these systems in terms of diagnostics \nand disease progression detection. \n \nRoughly, each lecture contains a theory part about the working of \u201csensor signals\u201d or \u201cdata \nanalysis methods\u201d and an application part which contextualises the concepts. \n \nAt the end of the course students should: Understand how mobile/wearable sensors capture \ndata and their working. Understand different approaches to acquiring and analysing sensor data \nfrom different types of sensors. Understand the concept of signal processing applied to time \nseries data and their practical application in health. Be able to extract sensor data and analyse it \nwith basic signal processing and machine learning techniques. Be aware of the different health \napplications of the various sensor techniques. The course will also touch on privacy and ethics \nimplications of the approaches developed in an orthogonal fashion. \n \nRecommended Reading \nPlease see Course Materials for recommended reading for each session. \n \nAssessment - Part II students \nTwo assignments will be based on two datasets which will be provided to the students: \n \n\nAssignment 1 (shared for Part II and Part III/MPhil): this will be based on a dataset and will be \nworth 40% of the final mark. The task of the assessment will be to perform pre-processing and \nbasic data analysis in a \"colab\" and an answer sheet of no more than 1000 words. \n \nAssignment 2 (Part II): This assignment (worth 60% of the final mark) will be a fuller analysis of \na dataset focusing on machine learning algorithms and metrics. Discussion and interpretation of \nthe findings will be reported in a colab and a report of no more than 1200 words. \n \nAssessment  - Part III and MPhil students \nTwo assignments will be based on datasets which will be provided to the students: \n \nAssignment 1: this will be based on a dataset and will be worth 40% of the final mark. The task \nof the assessment will be to perform pre-processing and basic data analysis in a \"colab\" and an \nanswer sheet of no more than 1000 words. \n \nAssignment 2: The second assignment (worth 60% of the final mark) will be based on multiple \ndatasets. The students will be asked to compare and contract ML algorithms/solutions (trained \nand tested on the different data) and discuss the findings and interpretation in terms of health \ncontext. This will be in the form of a colab and a report of 1800 words. \n \nFurther Information \nCurrent Cambridge undergraduate students who are continuing onto Part III or the MPhil in \nAdvanced Computer Science may only take this module if they did NOT take it as a Unit of \nAssessment in Part II. \n \nThis module is shared with Part II of the Computer Science Tripos. Assessment will be adjusted \nfor the two groups of students to be at an appropriate level for whichever course the student is \nenrolled on. Further information about assessment and practicals will follow at the first lecture.\n\nMULTICORE SEMANTICS AND PROGRAMMING \nPrerequisites: Some familiarity with discrete mathematics (sets, partial orders, etc.) and with \nsequential Java programming will be assumed. Experience with operational semantics and with \nsome concurrent programming would be helpful. \nMoodle, timetable \n \nAims \nIn recent years multiprocessors have become ubiquitous, but building reliable concurrent \nsystems with good performance remains very challenging. The aim of this module is to \nintroduce some of the theory and the practice of concurrent programming, from hardware \nmemory models and the design of high-level programming languages to the correctness and \nperformance properties of concurrent algorithms. \n \nLectures \nPart 1: Introduction and relaxed-memory concurrency [Professor P. Sewell] \n \nIntroduction. Sequential consistency, atomicity, basic concurrent problems. [1 block] \nConcurrency on real multiprocessors: the relaxed memory model(s) for x86, ARM, and IBM \nPower, and theoretical tools for reasoning about x86-TSO programs. [2 blocks] \nHigh-level languages. An introduction to C/C++11 and Java shared-memory concurrency. [1 \nblock] \nPart 2: Concurrent algorithms [Dr T. Harris] \n \nConcurrent programming. Simple algorithms (readers/writers, stacks, queues) and correctness \ncriteria (linearisability and progress properties). Advanced synchronisation patterns (e.g. some \nof the following: optimistic and lazy list algorithms, hash tables, double-checked locking, RCU, \nhazard pointers), with discussion of performance and on the interaction between algorithm \ndesign and the underlying relaxed memory models. [3 blocks] \nResearch topics, likely to include one hour on transactional memory and one guest lecture. [1 \nblock] \nObjectives \nBy the end of the course students should: \n \nhave a good understanding of the semantics of concurrent programs, both at the multprocessor \nlevel and the C/Java programming language level; \nhave a good understanding of some key concurrent algorithms, with practical experience. \nAssessment - Part II Students \nTwo assignments each worth 50% \n \nRecommended reading \nHerlihy, M. and Shavit, N. (2008). The art of multiprocessor programming. Morgan Kaufmann. \n \nCoursework \nCoursework will consist of assessed exercises. \n\n \nPractical work \nPart 2 of the course will include a practical exercise sheet.  The practical exercises involve \nbuilding concurrent data structures and measuring their performance.  The work can be \ncompleted in C, Java, or similar languages. \n \nAssessment - Part III and MPhil Students \nAssignment 1, for 50% of the overall grade. Written coursework comprising a series of questions \non hardware and software relaxed-memory concurrency semantics. \nAssignment 2, for 50% of the overall grade. Written coursework comprising a series of questions \non the design of mutual exclusion locks and shared-memory data structures. \nFurther Information \nCurrent Cambridge undergraduate students who are continuing onto Part III or the MPhil in \nAdvanced Computer Science may only take this module if they did NOT take it as a Unit of \nAssessment in Part II. \n \nThis module is shared with Part II of the Computer Science Tripos. Assessment will be adjusted \nfor the two groups of students to be at an appropriate level for whichever course the student is \nenrolled on. Further information about assessment and practicals will follow at the first lecture.\n\nREINFORCEMENT LEARNING \n \nAims \nThe aim of this module is to introduce students to the foundations of the field of reinforcement \nlearning (RL), discuss state-of-the-art RL methods, incentivise students to produce critical \nanalysis of recent methods, and prompt students to propose novel solutions that address \nshortcomings of existing methods. If judged by the headlines, RL has seen an unprecedented \nsuccess in recent years. However, the vast majority of RL methods still have shortcomings that \nmight not be apparent at first glance. The aim of the course is to inspire students by \ncommunicating the promising aspects of RL, but ensure that students develop the ability to \nproduce a critical analysis of current RL limitations. \n \nObjectives \nStudents will learn the following technical concepts: \n \nfundamental RL terminology and mathematical formalism; a brief history of RL and its \nconnection to neuroscience and biological systems \nRL methods for discrete action spaces, e.g. deep Q-learning and large-scale Monte Carlo Tree \nSearch \nmethods for exploration, modelling uncertainty, and partial observability for RL \nmodern policy gradient and actor-critic methods \nconcepts needed to construct model-based RL and Model Predictive Control methods \napproaches to make RL data-efficient and ways to enable simulation-to-reality transfer \nexamples of fine-tuning foundation models and large language models (LLMs) with human \nfeedback; safe RL concepts; examples of using RL for safety validation \nexamples of using RL for scientific discovery \nStudents will also gain experience with analysing RL methods to uncover their strengths and \nshortcomings, as well as proposing extensions to improve performance. \n \nFinally, students will gain skills needed to create and deliver a successful presentation in a \nformat similar to that of conference presentations. \n \nWith all of the above, students who take part in this module will be well-prepared to start \nconducting research in the field of reinforcement learning. \n  \n \nSyllabus \nTopic 1: Introduction and Fundamentals \n \nOverview of RL: foundational ideas, history, and books; connection to neuroscience and \nbiological systems, recent industrial applications and research demonstrations \nMathematical fundamentals: Markov decision processes, Bellman equations, policy and value \niteration, temporal difference learning \nTopic 2: RL in Discrete Action Spaces \n\n \nQ-learning, function approximation and deep Q-learning; nonstationarity in RL and its \nimplications for deep learning; example applications (video games; initial example: Atari) \nMonte Carlo Tree Search; example applications (AlphaGo) \nTopic 3: Exploration, Uncertainty, Partial Observability \n \nMulti-armed bandits, Bayesian optimisation, regret analysis \nPartially observable Markov decision process; belief, memory, and sequence modelling \n(probabilistic methods, recurrent networks, transformers) \nTopic 4: Policy Gradient and Actor-critic Methods for Continuous Action Spaces \n \nImportance sampling, policy gradient theorem, actor-critic methods (SPG, DDPG) \nProximal policy optimisation; example applications \nTopic 5: Model-based RL and Model Predictive Control \n \nLearning dynamics models (graph networks, stochastic processes, diffusion models, \nphysics-based models, ensembles); planning with learned models \nModel predictive control; example applications (real-time control) \nTopic 6: Data-efficient RL and Simulation-to-reality Transfer \n \nData-efficient learning with probabilistic methods from real data (e.g. policy search in robotics), \nreal-to-sim inference and differentiable simulation, data-efficient simulation-to-reality transfer \nRL for physical systems (successful examples in locomotion, open problems in contact-rich \nmanipulation, applications to logistics, energy, and transport systems); examples of RL for \nhealthcare. \nTopic 7: RL with Human Feedback ; Safe RL and RL for Validation \n \nFine-tuning large language models (LLMs) and other foundation models with human feedback \n(TRLX,RL4LMs, a light-weight overview of RLHF) \nA review of SafeRL, example: optimising commercial HVAC systems using policy improvement \nwith constraints; improving safety using RL for validation: examples in autonomous driving and \nautonomous flying and aircraft collision avoidance \nTopic 8: RL for Scientific Discovery; Student Presentations \n \nExamples of RL for molecular design and drug discovery, active learning for synthesising new \nmaterials, RL for nuclear fusion experiments \nStudent presentations (based on essays and mini-projects) for other topics in RL, e.g. \nmulti-agent RL, hierarchical RL, RL for hyperparameter optimisation and NN architecture \nsearch, RL for multi-task transfer, lifelong RL, RL in biological systems, etc. \nAssessment \nThe assessment for this module consists of: \n \nEssay and mini-project (60%)  \n\nStudents will choose a concrete RL method from the literature, then complete a 2-part essay \ndescribed below: \nEssay Part 1 (1500 words, 20%): Introduce a formal description of the method and explain how \nthe method extended the state-of-the-art at the time of its publication \nEssay Part 2 or a mini-project (2500 words, 40%): Provide a critical analysis of the chosen RL \nmethod. \nPresentation of the essay and mini-project results (15%)  \nstudents will be expected to create and record a 15-minute video presentation of the analysis \ndescribed in their essay and mini-project. \nShort test of RL theory fundamentals (15%)  \nto test the understanding of RL fundamentals (~15 minutes, in-class, closed book) \nParticipation in seminar discussions (10%)  \ntake an active part in the seminars by asking clarifying questions and mentioning related works. \nRecommended Reading \nBooks \n \n[S&B] Reinforcement Learning: An Introduction (second print edition). Richard S. Sutton, \nAndrew G. Barto. [Available from the book\u2019s website as a free PDF updated in 2022] \n \n[CZ] Algorithms for Reinforcement Learning. Csaba Szepesvari. [Available from the book\u2019s \nwebsite as a free PDF updated in 2019] \n \n[MK] Algorithms for Decision Making. Mykel J. Kochenderfer Tim A. WheelerKyle H. Wray. \n[Available from the book\u2019s website as a free PDF updated in 2023] \n \n[DB] Reinforcement Learning and Optimal Control. Dimitri Bertsekas. [Available from the book\u2019s \nwebsite as a free PDF updated in 2023] \n \nPresentation guidelines \n \n[KN] Ten simple rules for effective presentation slides. Kristen M.Naegle. PLoS computational \nbiology 17, no. 12 (2021).\n \n\nTHEORIES OF SOCIO-DIGITAL DESIGN FOR HUMAN-CENTRED AI \n \nPrerequisites: This course is appropriate to any ACS student, and will assist in the development \nof critical thinking, argument and long-form writing skills. Prior experience of essay-based \nhumanities subjects at university or secondary school will be beneficial, but not required. \nMoodle, timetable \n \nAims \nThis module is a theoretically-oriented advanced introduction to the broad field of \nhuman-computer interaction, extending to consider topics such as intelligent user interfaces, \ncritical and speculative design, participatory design, cognitive models of users, human-centred, \nas well as more-than-human-centred design, and others. The course will not address purely \nengineering approaches to the development of user interfaces (unless there is a clear \ntheoretical question being addressed), but allow students to effectively link the political, social, \nand ethical considerations in HCI to specific technical and conceptual design challenges. The \nmodule builds on the Practical Research in Human-Centred AI course offered in Michaelmas \n(developed with researchers at Microsoft Research Cambridge) and a collaboration with the \nLeverhulme Centre for the Future of Intelligence at Cambridge. Participants may include visitors \nfrom these groups and/or interdisciplinary research students and academic guests from other \nUniversity departments, including Cambridge Digital Humanities. \n \nSyllabus \nThe syllabus will remain broadly within the area of human-computer interaction, including \ntheories of design practice and the social contexts of technology use. Individual seminar topics \nwill be selected in response to contemporary and recent research developments, in consultation \nwith members of the class and visiting contributors. \n \nRepresentative topics in the next year are likely to include: \n \nResponsible AI and HCI \nIntersectional design for social justice \nParticipatory design and co-design \nSustainable AI and more-than-human-centred design \nUsefulness and usability of ethical design toolkits \nThe EU AI Act in design practice \nAI and interface design for transparency \nDesigning otherwise: on anarchist HCI \n  \nObjectives \nOn completion of this module, students should have developed facility in discussing and \ncritiquing the aims of their research, especially for an audience drawn from other academic \ndisciplines, including the following skills: \n \nTheoretical motivation and defence of a research question. \n\nConsideration of a research proposal from one or more alternative theoretical perspectives. \nPotential critique of the theoretical basis for a programme of research. \nCoursework \nIn advance of each seminar, all participants must read the essential reading - generally one \nlong, plus one or two short papers. Further reading is suggested for some seminars. Some \nweeks, instead of shorter papers, students will be asked to explore specific design tools or \nresources. \n \nBefore seminars 2-8, each student will submit a written commentary on the paper, either \ngenerated using a large language model (LLM), or written in the style of an LLM, supplemented \nby a short original observation. \n \nEach member of the class must select one of the 8 seminar themes as the subject for a more \nextended critical review essay. The critical review should be written as an academic research \nessay, not in LLM style. \n \nEach seminar will include an informal design exercise to be carried out in small groups. The \npurpose of these exercises is to reflect on a variety of practical design resources, by applying \nthose resources to concrete case studies. \n \nThroughout the course, students will be expected to keep a \"reflective diary\", briefly reporting \nthemes that arise in discussion, reflections on the design exercises, and ways in which the \nreadings relate to their own research interests. \n  \n \nPractical work \nThere is no practical work element in this course. \n \nAssessment \nWritten critical review of a single discussion text (20%) \nReflective diary submitted at the end of the module (60%) \nIn advance of the session, each student will submit a written commentary on the paper, either \ngenerated using a large language model (LLM), or written in the style of an LLM. A short original \nobservation should be added (20%) \nRecommended reading \nTo be assigned during the module, as discussed above. Most set readings will be available \neither from the ACM Digital Library, or from institutional repositories of the authors. \n \nFurther Information \nStudents from the MPhil in the Ethics of AI, Data and Algorithms, MSt in AI, and MPhil in Digital \nHumanities courses also attend the lectures for this module.\n \n\nTHEORY OF DEEP LEARNING \nPrerequisites: A strong background in calculus, probability theory and linear algebra, familiarity \nwith differential equations, optimization and information theory is required. Students need to \nhave studied Deep Neural Networks, familiarity with deep learning terminology (e.g. \narchitectures, benchmark problems) as well as deep learning frameworks (pytorch, jax or \nsimilar) is assumed \nMoodle, timetable \n \nAims \nThe objectives of this course is to expose you to one of the most active contemporary research \ndirections within machine learning: the theory of deep learning (DL). While the first wave of \nmodern DL has focussed on empirical breakthroughs and ever more complex techniques, the \nattention is now shifting to building a solid mathematical understanding of why these techniques \nwork so well in the first place. The purpose of this course is to review this recent progress \nthrough a mixture of reading group sessions and invited talks by leading researchers in the \ntopic, and prepare you to embark on a PhD in modern deep learning research. Compared to \ntypical, non-mathematical courses on deep learning, this advanced module will appeal to those \nwho have strong foundations in mathematics and theoretical computer science. In a way, this \ncourse is our answer to the question \u201cWhat should the world\u2019s best computer science students \nknow about deep learning in 2023?\u201d \n \nLearning Outcomes \nThis course should prepare the best students to start a PhD in the theory and mathematics of \ndeep learning, and to start formulating their own hypotheses in this space. You will be \nintroduced to a range of empirical and mathematical tools developed in recent years for the \nstudy of deep learning behaviour, and you will build an awareness of the main open questions \nand current lines of attack. At the end of the course you will: \n \nbe able to explain why classical learning theory is insufficient to describe the phenomenon of \ngeneralization in DL \nbe able to design and interpret empirical studies aimed at understanding generalization \nbe able to explain the role of overparameterization: be able to use deep linear models as a \nmodel to study implicit regularisation of gradient-based learning \nbe able to state PAC-Bayes and Information-theoretic bounds, and apply them to DL \nbe able to explain the connection between Gaussian processes and neural networks, and will \nbe able to study learning dynamics in the neural tangent kernel (NTK) regime. \nbe able to formulate your own hypotheses about DL and choose tools to prove/test them \nleverage your deeper theoretical understanding to produce more robust, rigorous and \nreproducible solutions to practical machine learning problems. \nSyllabus \nEach week we'll have two to four student-lead presentations about a research paper chosen \nfrom a reading list. The reading list loosely follows the weekly breakdown below (but we adapt it \neach year based as this is an active research area): \n \n\nWeek 1: Introduction to the topic \nWeek 2: Empirical Studies of Deep Learning Phenomena \nWeek 3: Interpolation Regime and \u201cDouble Descent\u201d Phenomena \nWeek 4: Implicit Regularization in Deep Linear Models \nWeek 5: Approximation Theory \nWeek 6: Networks in the Infinite Width Limit \nWeek 7: PAC-Bayes and Information Theoretic Bounds for SGD \nWeek 8: Discussion and Coursework Spotlight Session \n \nAssessment \nStudents will be assessed on the following basis: \n \n20% for presentation/content contributed to the module: Each student will have an opportunity \nto present one of the recommended papers during Weeks 1-7 (30 minute slot including Q&A). \nFor the presentation, students should aim to communicate the core ideas behind the paper, and \nclearly present the results, conclusions, and future directions. Where possible, students are \nencouraged to comment on how the work itself fits into broader research goals. \n10% for active participation (regular attendance and contribution to discussions during the Q&A \nsessions). \n70% for a group project report, with a word limit of 4000. Either (1) an original research \nproposal/report with a hypothesis, review of related literature, and ideally preliminary findings, \nor, (2) reproduction and ideally extension of an existing relevant paper. \nCoursework reports are marked in line with general ACS guidelines, reports receiving top marks \nwill have have demonstrable research value (contain an original research idea, extension of \nexisting work, or a thorough reproduction effort which is valuable to the research community). \nAdditionally, some projects will be suggested during the first weeks of the course, although \nstudents are encouraged to come up with their own ideas. Students may be required to \nparticipate in group projects, with groups of size 2-3 (the class groups will be separated). For \nany given project, individual contributions would be noted for assessment though a viva \ncomponent. \nRelationship with related modules \nThis course can be considered as an advanced follow-up to the Part IIB course on Deep Neural \nNetworks. That course introduces some high level concepts that this course significantly \nexpands on. \n \nThis module complements L48: Machine Learning in the Physical World and L46: Principles of \nMachine Learning Systems, which focus on applications and hardware/systems aspects of ML \nrespectively. \n \nRecommended reading \nPNAS Colloquium on the Science of Deep Learning \nMathematics of Machine Learning book by Marc Deisenroth, Aldo Faisal and Cheng Soon Ong. \nProbabilistic Machine Learning: An Introduction book by Kevin Murphy \nMatus Telgarsky's lecture notes on deep learning theory  \n\nThese are in addition to the papers which will be discussed in the lectures.\n \n\nUNDERSTANDING NETWORKED-SYSTEMS PERFORMANCE \nPrerequisites: A student must possess knowledge comparable to the undergraduate subjects on \nUnix Tools, C/C++ programming and Computer Networks. Optionally; a student will be \nadvantaged by doing an introduction to Computer Systems Modelling; such as the Part 2 \nComputer Systems Modelling subject as well as having a background in measurement \nmethodologies such as that of L50: Introduction to networking and systems measurements \nprovided in the ACS/Part III curriculum. \nMoodle, timetable \n \nAims \nThis is a practical course, actively building and extending software tools to observe the detailed \nbehaviour of transaction-oriented datacenter-like software. Students will observe and \nunderstand sources of user-facing tail latency, including that stemming \nfrom resource contention, cross-program interference, bad software locking, and simple design \nerrors. \nA 2-hour weekly hybrid, lecture/lab format permits students continuous monitored progress with \ncomplexity of tasks building naturally upon the previous weeks learning. \n \nObjectives \nUpon successful completion of the course, students will be able to: \n \nMake order-of-magnitude estimates of software, hardware, and I/O speeds \nMake valid measurements of actual software, hardware, and I/O speeds \nCreate observation facilities, including logs and dashboards, as part of a software system \ndesign \nCreate tracing facilities to fully observe the execution of complex software \nTime-align traces across multiple computers \nDisplay dense tracing information in meaningful ways \nReason about the sources of real-time and transaction delays, including cross-program \nresource interference, remote procedure call delays, and software locking surprises \nFix programs based on the above reasoning, making their response times faster and more \nrobust \n \nSyllabus \nMeasurement \n   Week 1 Intro, Measuring CPU time, rdtsc, Measuring memory access times, Measuring disks, \n   Week 2 gettimeofday, logs Measuring networks, Remote Procedure Calls, Multi-threads, locks \nObservation \n   Week 3 RPC, logs, displaying traces, interference \n   Week 4 Antagonist programs, Logging, dashboards, profiling. \nKUtrace \n   Week 5 Kernel patches, hello world, post-processing \n   Week 6 KUtrace multi-CPU time display \n   Week 7 Client-server KUtrace, with antagonists and interference \n\n   Week 8 Other trace mysteries \n \n \nAssessment \nThis is a Lab-based module; assessment is based upon reports covering guided laboratory work \nperformed each week. Assessment will be via two submissions: \n \n20% assignment deadline week 4 based upon Lab work from Weeks 1-4; word target 1000, limit \n2000 \n80% assignment deadline week 8 based upon lab work from weeks 5-8; word target 2000, limit \n4000 \nThe intent of the first assessment point is to provide rich feedback to students based upon a \n20% assignment permitting focussed and improved work to be executedfor the final assignment. \n \nAll work in this module is expected to be the effort of the student. Enough equipment is \nprovisioned to allow each student an independent set of apparatus. While classmembers may \nfind sharing operational experience valuable all assessment is based upon a students sole \nsubmission based upon. their own experiments and findings. \n \nReading Material \nCore text \n \nRichard L. Sites, Understanding Software Dynamics, Addison-Wesley Professional Computing \nSeries, 2022 \nFurther reading: papers and presentations that you might find interesting. None are required \nreading \u2013 they are well-written and/or informative. \n \nJohn K. Ousterhout et al., A trace-driven analysis of the UNIX 4.2 BSD file system ACM \nSIGOPS Operating Systems Review, Vol. 19, No. 5, Dec, 1985 \nhttps://dl.acm.org/doi/pdf/10.1145/323627.323631 \nLuiz Andr\u00b4e Barroso, Jimmy Clidaras, Urs H\u00a8olzle, The Datacenter as a Computer: An \nIntroduction to the Design of Warehouse-Scale Machines, Second Edition \nJohn L. Hennessy, David A. Patterson, Computer Architecture, A Quantitative Approach 5th \nEdition \nGeorge Varghese, Network Algorithmics. Morgan Kaufmann \nActually keeping datacenter software up and running \u2013 how Google runs production systems \nSite Reliability Engineering Edited by Betsy Beyer, Chris Jones, Jennifer Petoff and Niall \nRichard Murphy free PDF: https://landing.google.com/sre/book.html \nHow NOT to run datacenters (27 minute video, funny/sad) dotScale 2014 - Robert Kennedy - \nLife in the Trenches of healthcare.gov https://www.youtube.com/watch?v=GLQyj-kBRdo \nAn extended (optional) reading list will also be provided. \n \n"
    ],
    "semesters_-8714839614001454533": [
        "1st Semester \n \nDATABASES \n \nAims \nThis course introduces basic concepts for database systems as seen from the perspective of \napplication designers. That is, the focus is on the abstractions supported by database \nmanagement systems and not on how those abstractions are implemented. \n \nThe database world is currently undergoing swift and dramatic transformations largely driven by \nInternet-oriented applications and services. Today many more options are available to database \napplication developers than in the past and so it is becoming increasingly difficult to sort fact \nfrom fiction. The course attempts to cut through the fog with a practical approach that \nemphasises engineering tradeoffs that underpin these recent developments and also guide our \nselection of \u201cthe right tool for the job.\u201d \n \nThis course covers three approaches. First, the traditional mainstay of the database industry -- \nthe relational approach -- is described with emphasis on eliminating logical redundancy in data. \nThen two representatives of recent trends are presented -- graph-oriented and \ndocument-oriented databases. The lectures are supported with two Help and Tick sessions, \nwhere students gain hands-on experience and guidance with the Assessed Exercises (ticks). \n \nLectures \nL1 Introduction. What is a database system? What is a data model? Typical DBMS \nconfigurations and variations (fast queries or reliable update). In-core, in secondary store or \ndistributed. \nL2 Conceptual modelling. Entities, relations, E/R diagrams and implementation-independent \nmodelling, weak entities, cardinality. \nL3+4 The relational database model. Implementing E/R models with relational tables. Relational \nalgebra and SQL. Basic query primitives. Update anomalies caused by redundancy. Minimising \nredundancy with normalised schemas. \nL5 Transactions. On-Line Transaction Processing. On-line Analytical Processing. Reliability, \nthroughput, normal forms, ACID, BASE, eventual consistency. \nL6 Documents and semi-structured data. The NoSQL, schema-free movement. XML/JSON. \nKey/value stores. Embracing data redundancy: representing data for fast, application-specific \naccess. Path queries (if time permits). \nL7 Further SQL. Multisets, NULL values, aggregates, transitive closure, expressibility (nested \nqueries and recursive SQL). \nL8 Graph databases. Optimised for processing enormous numbers of nodes and edges. \nImplementing E/R models in a graph-oriented database. Comparison of the presented models. \nObjectives \nAt the end of the course students should \n\n \nbe able to design entity-relationship diagrams to represent simple database application \nscenarios \nknow how to convert entity-relationship diagrams to relational- and graph-oriented \nimplementations \nunderstand the fundamental tradeoff between the ease of updating data and the response time \nof complex queries \nunderstand that no single data architecture can be used to meet all data management \nrequirements \nbe familiar with recent trends in the database area. \nRecommended reading \nLemahieu, W., Broucke, S. van den and Baesens, B. (2018) Principles of database \nmanagement. Cambridge University Press.\n \n\nDIGITAL ELECTRONICS \n \nAims \nThe aims of this course are to present the principles of combinational and sequential digital logic \ndesign and optimisation at a gate level. The use of n and p channel MOSFETs for building logic \ngates is also introduced. \n \nTopics \nIntroduction. Semiconductors to computers. Logic variables. Examples of simple logic. Logic \ngates. Boolean algebra. De Morgan\u2019s theorem. \nLogic minimisation. Truth tables and normal forms. Karnaugh maps. Quine-McCluskey method. \nBinary adders. Half adder, full adder, ripple carry adder, fast carry generation. \nCombinational logic design: further considerations. Multilevel logic. Gate propagation delay. An \nintroduction to timing diagrams. Hazards and hazard elimination. Other ways to implement \ncombinational logic. \nIntroduction to practical classes. Prototyping box. Breadboard and Dual in line (DIL) packages. \nWiring. \nSequential logic. Memory elements. RS latch. Transparent D latch. Master-slave D flip-flop. T \nand JK flip-flops. Setup and hold times. \nSequential logic. Counters: Ripple and synchronous. Shift registers. System timing - setup time \nconstraint, clock skew, metastability. \nSynchronous State Machines. Moore and Mealy finite state machines (FSMs). Reset and self \nstarting. State transition diagrams. Elimination of redundant states - row matching and state \nequivalence/implication table. \nFurther state machines. State assignment: sequential, sliding, shift register, one hot. \nImplementation of FSMs. \nIntroduction to microprocessors. Microarchitecture, fetch, register access, memory access, \nbranching, execution time. \nElectronics, Devices and Circuits. Current and voltage, conductors/insulators/semiconductors, \nresistance, basic circuit theory, the potential divider. Solving non-linear circuits. P-N junction \n(forward and reverse bias), N and p channel MOSFETs (operation and characteristics) and \nn-MOSFET logic, e.g., n-MOSFET inverter. Power consumption and switching time problems \nproblems in n-MOSFET logic. CMOS logic (NOT, NAND and NOR gates), logic families, noise \nmargin. \nObjectives \nAt the end of the course students should \n \nunderstand the relationships between combination logic and boolean algebra, and between \nsequential logic and finite state machines; \nbe able to design and minimise combinational logic; \nappreciate tradeoffs in complexity and speed of combinational designs; \nunderstand how state can be stored in a digital logic circuit; \nknow how to design a simple finite state machine from a specification and be able to implement \nthis in gates and edge triggered flip-flops; \n\nunderstand how to use MOSFETs to build digital logic circuits. \nRecommended reading \n* Harris, D.M. and Harris, S.L. (2013). Digital design and computer architecture. Morgan \nKaufmann (2nd ed.). The first edition is still relevant. \nKatz, R.H. (2004). Contemporary logic design. Benjamin/Cummings. The 1994 edition is more \nthan sufficient. \nHayes, J.P. (1993). Introduction to digital logic design. Addison-Wesley. \n \nBooks for reference: \n \nHorowitz, P. and Hill, W. (1989). The art of electronics. Cambridge University Press (2nd ed.) \n(more analog). \nWeste, N.H.E. and Harris, D. (2005). CMOS VLSI Design - a circuits and systems perspective. \nAddison-Wesley (3rd ed.). \nMead, C. and Conway, L. (1980). Introduction to VLSI systems. Addison-Wesley. \nCrowe, J. and Hayes-Gill, B. (1998). Introduction to digital electronics. Butterworth-Heinemann. \nGibson, J.R. (1992). Electronic logic circuits. Butterworth-Heinemann.\n \n\nDISCRETE MATHEMATICS \n \nAims \nThe course aims to introduce the mathematics of discrete structures, showing it as an essential \ntool for computer science that can be clever and beautiful. \n \nLectures \nProof [5 lectures]. \nProofs in practice and mathematical jargon. Mathematical statements: implication, bi-implication, \nuniversal quantification, conjunction, existential quantification, disjunction, negation. Logical \ndeduction: proof strategies and patterns, scratch work, logical equivalences. Proof by \ncontradiction. Divisibility and congruences. Fermat\u2019s Little Theorem. \n \nNumbers [5 lectures]. \nNumber systems: natural numbers, integers, rationals, modular integers. The Division Theorem \nand Algorithm. Modular arithmetic. Sets: membership and comprehension. The greatest \ncommon divisor, and Euclid\u2019s Algorithm and Theorem. The Extended Euclid\u2019s Algorithm and \nmultiplicative inverses in modular arithmetic. The Diffie-Hellman cryptographic method. \nMathematical induction: Binomial Theorem, Pascal\u2019s Triangle, Fundamental Theorem of \nArithmetic, Euclid\u2019s infinity of primes. \n \nSets [9 lectures]. \nExtensionality Axiom: subsets and supersets. Separation Principle: Russell\u2019s Paradox, the \nempty set. Powerset Axiom: the powerset Boolean algebra, Venn and Hasse diagrams. Pairing \nAxiom: singletons, ordered pairs, products. Union axiom: big unions, big intersections, disjoint \nunions. Relations: composition, matrices, directed graphs, preorders and partial orders. Partial \nand (total) functions. Bijections: sections and retractions. Equivalence relations and set \npartitions. Calculus of bijections: characteristic (or indicator) functions. Finite cardinality and \ncounting. Infinity axiom. Surjections. Enumerable and countable sets. Axiom of choice. \nInjections. Images: direct and inverse images. Replacement Axiom: set-indexed constructions. \nSet cardinality: Cantor-Schoeder-Bernstein Theorem, unbounded cardinality, diagonalisation, \nfixed-points. Foundation Axiom. \n \nFormal languages and automata [5 lectures]. \nIntroduction to inductive definitions using rules and proof by rule induction. Abstract syntax \ntrees. Regular expressions and their algebra. Finite automata and regular languages: Kleene\u2019s \ntheorem and the Pumping Lemma. \n \nObjectives \nOn completing the course, students should be able to \n \nprove and disprove mathematical statements using a variety of techniques; \napply the mathematical principle of induction; \nknow the basics of modular arithmetic and appreciate its role in cryptography; \n\nunderstand and use the language of set theory in applications to computer science; \ndefine sets inductively using rules and prove properties about them; \nconvert between regular expressions and finite automata; \nuse the Pumping Lemma to prove that a language is not regular. \nRecommended reading \nBiggs, N.L. (2002). Discrete mathematics. Oxford University Press (Second Edition). \nDavenport, H. (2008). The higher arithmetic: an introduction to the theory of numbers. \nCambridge University Press. \nHammack, R. (2013). Book of proof. Privately published (Second edition). Available at: \nhttp://www.people.vcu.edu/ rhammack/BookOfProof/index.html \nHouston, K. (2009). How to think like a mathematician: a companion to undergraduate \nmathematics. Cambridge University Press. \nKozen, D.C. (1997). Automata and computability. Springer. \nLehman, E.; Leighton, F.T.; Meyer, A.R. (2014). Mathematics for computer science. Available \non-line. \nVelleman, D.J. (2006). How to prove it: a structured approach. Cambridge University Press \n(Second Edition).\n \n\nFOUNDATIONS OF COMPUTER SCIENCE \nAims \nThe main aim of this course is to present the basic principles of programming. As the \nintroductory course of the Computer Science Tripos, it caters for students from all backgrounds. \nTo those who have had no programming experience, it will be comprehensible; to those \nexperienced in languages such as C, it will attempt to correct any bad habits that they have \nlearnt. \n \nA further aim is to introduce the principles of data structures and algorithms. The course will \nemphasise the algorithmic side of programming, focusing on problem-solving rather than on \nhardware-level bits and bytes. Accordingly it will present basic algorithms for sorting, searching, \netc., and discuss their efficiency using O-notation. Worked examples (such as polynomial \narithmetic) will demonstrate how algorithmic ideas can be used to build efficient applications. \n \nThe course will use a functional language (OCaml). OCaml is particularly appropriate for \ninexperienced programmers, since a faulty program cannot crash and OCaml\u2019s unobtrusive type \nsystem captures many program faults before execution. The course will present the elements of \nfunctional programming, such as curried and higher-order functions. But it will also introduce \ntraditional (procedural) programming, such as assignments, arrays and references. \n \nLectures \nIntroduction to Programming. The role of abstraction and representation. Introduction to integer \nand floating-point arithmetic. Declaring functions. Decisions and booleans. Example: integer \nexponentiation. \nRecursion and Efficiency. Examples: Exponentiation and summing integers. Iteration versus \nrecursion. Examples of growth rates. Dominance and O-Notation. The costs of some \nrepresentative functions. Cost estimation. \nLists. Basic list operations. Append. Na\u00efve versus efficient functions for length and reverse. \nStrings. \nMore on lists. The utilities take and drop. Pattern-matching: zip, unzip. A word on polymorphism. \nThe \u201cmaking change\u201d example. \nSorting. A random number generator. Insertion sort, mergesort, quicksort. Their efficiency. \nDatatypes and trees. Pattern-matching and case expressions. Exceptions. Binary tree traversal \n(conversion to lists): preorder, inorder, postorder. \nDictionaries and functional arrays. Functional arrays. Dictionaries: association lists (slow) versus \nbinary search trees. Problems with unbalanced trees. \nFunctions as values. Nameless functions. Currying. The \u201capply to all\u201d functional, map. \nExamples: The predicate functionals filter and exists. \nSequences, or lazy lists. Non-strict functions such as IF. Call-by-need versus call-by-name. Lazy \nlists. Their implementation in OCaml. Applications, for example Newton-Raphson square roots. \nQueues and search strategies. Depth-first search and its limitations. Breadth-first search (BFS). \nImplementing BFS using lists. An efficient representation of queues. Importance of efficient data \nrepresentation. \n\nElements of procedural programming. Address versus contents. Assignment versus binding. \nOwn variables. Arrays, mutable or not. Introduction to linked lists. \nObjectives \nAt the end of the course, students should \n \nbe able to write simple OCaml programs; \nunderstand the fundamentals of using a data structure to represent some mathematical \nabstraction; \nbe able to estimate the efficiency of simple algorithms, using the notions of average-case, \nworse-case and amortised costs; \nknow the comparative advantages of insertion sort, quick sort and merge sort; \nunderstand binary search and binary search trees; \nknow how to use currying and higher-order functions; \nunderstand how OCaml combines imperative and functional programming in a single language. \nRecommended reading \nJohn Whitington. OCaml from the Very Beginning. (http://ocaml-book.com). \n \nOkasaki, C. (1998). Purely functional data structures. Cambridge University Press.\n \n\nHARDWARE PRACTICAL CLASSES \nThe Hardware Practical Classes accompany the Digital Electronics series of lectures. The aim \nof the Practical Classes is to enable students to get hands-on experience of designing, building, \nand testing and debugging of digital electronic circuits. The labs will take place in the Intel Lab. \nlocated in the William Gates Building (WGB). \n \nThe Digital Electronics lecture series occupies 12 lectures in the first 4 weeks of the Michaelmas \nTerm, while the Practical Classes occupy the latter 6 weeks of the Michaelmas Term and the \nfirst 6 weeks of the Lent Term. If required, extra sessions will be available for any students \nneeding to 'catch-up' on missed sessions or to complete any remaining practical work. \n \nThe Practical Classes take the form of 4 workshops, specifically: \n \nWorkshop 1 \u2013 Electronic Die; \nWorkshop 2 \u2013 Shaft Position Encoder; \nWorkshop 3 \u2013 Debouncing a Switch; \nWorkshop 4 \u2013 Framestore for an LED Array. \nIn general, the workshops require some preparatory work to be done prior to the practical \nsession. These tasks are highlighted at the beginning of each Worksheet. Typically this involves \npreparing a design that you will then build, test and modify during the practical class. Note that \ninsufficient preparation prior to the practical classes may compromise effective use of your time \nin the Intel lab. \n \nIn the Practical Classes you will usually work on your own, and you are expected to complete \none Workshop in each of your scheduled sessions. Demonstrators are available during the \nsessions to assist you with any queries or problems you may have. \n \nImportant: Remember to get your work ticked.\n \n\nINTRODUCTION TO GRAPHICS \nAims \nTo introduce the necessary background, the basic algorithms, and the applications of computer \ngraphics and graphics hardware. \n \nLectures \nBackground. What is an image? Resolution and quantisation. Storage of images in memory. [1 \nlecture] \nRendering. Perspective. Reflection of light from surfaces and shading. Geometric models. Ray \ntracing. [2 lectures] \nGraphics pipeline. Polygonal mesh models. Transformations using matrices in 2D and 3D. \nHomogeneous coordinates. Projection: orthographic and perspective. [1 lecture] \nGraphics hardware and modern OpenGL. GPU rendering. GPU frameworks and APIs. Vertex \nprocessing. Rasterisation. Fragment processing. Working with meshes and textures. Z-buffer. \nDouble-buffering and frame synchronization. [2 lectures] \n  \nHuman vision, colour and tone mapping. Perception of colour. Tone mapping. Colour spaces. [2 \nlectures] \nObjectives \nBy the end of the course students should be able to: \n \nunderstand and apply in practice basic concepts of ray-tracing: ray-object intersection, \nreflections, refraction, shadow rays, distributed ray-tracing, direct and indirect illumination; \ndescribe and explain the following algorithms: z-buffer, texture mapping, double buffering, \nmip-map, and normal-mapping; \nuse matrices and homogeneous coordinates to represent and perform 2D and 3D \ntransformations; understand and use 3D to 2D projection, the viewing volume, and 3D clipping; \nimplement OpenGL code for rendering of polygonal objects, control camera and lighting, work \nwith vertex and fragment shaders; \ndescribe a number of colour spaces and their relative merits. \nexplain the need for tone mapping and colour processing in rendering pipeline. \nRecommended reading \n* Shirley, P. and Marschner, S. (2009). Fundamentals of Computer Graphics. CRC Press (3rd \ned.). \n \nFoley, J.D., van Dam, A., Feiner, S.K. and Hughes, J.F. (1990). Computer graphics: principles \nand practice. Addison-Wesley (2nd ed.). \n \nKessenich, J.M., Sellers, G. and Shreiner, D (2016). OpenGL Programming Guide: The Official \nGuide to Learning OpenGL, Version 4.5 with SPIR-V, [seventh edition and later]\n \n\nOBJECT-ORIENTED PROGRAMMING \n \nAims \nThe goal of this course is to provide students with an understanding of Object-Oriented \nProgramming. Concepts are demonstrated in multiple languages, but the primary language is \nJava. \n \nLecture syllabus \nTypes, Objects and Classes Moving from functional to imperative. Functions, methods. Control \nflow. values, variables and types. Primitive Types. Classes as custom types. Objects vs \nClasses. Class definition, constructors. Static data and methods. \nDesigning Classes Identifying classes. UML class diagrams. Modularity. Encapsulation/data \nhiding. Immutability. Access modifiers. Parameterised types (Generics). \nPointers, References and Memory Pointers and references. Reference types in Java. The call \nstack. The heap. Iteration and recursion. Pass-by-value and pass-by-reference. \nInheritance Inheritance. Casting. Shadowing. Overloading. Overriding. Abstract Methods and \nClasses. \nPolymorphism and Multiple Inheritance Polymorphism in ML and Java. Multiple inheritance. \nInterfaces in Java. \nLifecycle of an Object Constructors and chaining. Destructors. Finalizers. Garbage Collection: \nreference counting, tracing. \nJava Collections and Object Comparison Java Collection interface. Key classes. Collections \nclass. Iteration options and the use of Iterator. Comparing primitives and objects. Operator \noverloading. \nError Handling Types of errors. Limitations of return values. Deferred error handling. Exceptions. \nCustom exceptions. Checked vs unchecked. Inappropriate use of exceptions. Assertions. \nDesignLanguage evolution Need for languages to evolve. Generics in Java. Type erasure. \nIntroduction to Java 8: Lambda functions, functions as values, method references, streams. \nDesign Patterns Introduction to design patterns. Open-closed principle. Examples of Singleton, \nDecorator, State, Composite, Strategy, Observer. [2 lectures] \nObjectives \nAt the end of the course students should \n \nProvide an overview of key OOP concepts that are transferable in mainstream programming \nlanguages; \nGain an understanding of software development approaches adopted in the industry including \nmaintainability, testing, and software design patterns; \nIncrease programming familiarity with Java; \nRecommended reading \n1. Real-World Software Development: A Project-Driven Guide to Fundamentals in Java [Urma et \nal.] \n2. Modern Java in Action (2nd edition) [Urma et al.]. \n3. Java How to Program: early objects [Deitel et al.]\n \n\nOCAML PRACTICAL CLASSES \n \nThe Role of Tickers \nA ticking session is a short (5 minute) one-to-one session with a ticker conducted on Thursday \nafternoons. The role of the ticker is twofold: \n \nto check you understand your solution (and didn\u2019t just have some lucky guesses or copy code \nfrom elsewhere) \nto give you general feedback on ways to improve your code. \nTo those ends a Ticker will typically ask you questions related to the core material of the tick and \ndiscuss your code directly. \n \nDeadline Extensions \nDeadline extensions can be granted for illness or similar reasons. To obtain an extension please \nemail Jon Ludlam in the first instance, clearly stating your justification for an extension and \nCCing your Director of Studies, who will need to support your request.\n \n\nSCIENTIFIC COMPUTING \n \nAims \nThis course is a hands-on introduction to using computers to investigate scientific models and \ndata. \n \nSyllabus \nPython notebooks. Overview of the Python programming language. Use of notebooks for \nscientific computing. \nNumerical computation. Writing fast vectorized code in numpy. Optimization and fitting. \nSimulation. \nWorking with data. Data import. Common ways to summarize and plot data, for univariate and \nmultivariate analysis. \nObjectives \nAt the end of the course students should \n \nbe able to import data, plot it, and summarize it appropriately \nbe able to write fast vectorized code for scientific / data work\n \n\n",
        "2nd Semester \nALGORITHMS 1 \nAims \nThe aim of this course is to provide an introduction to computer algorithms and data structures, \nwith an emphasis on foundational material. \n \nLectures \nSorting. Review of complexity and O-notation. Trivial sorting algorithms of quadratic complexity. \nReview of merge sort and quicksort, understanding their memory behaviour on statically \nallocated arrays. Heapsort. Stability. Other sorting methods including sorting in linear time. \nMedian and order statistics. [Ref: CLRS3 chapters 1, 2, 3, 6, 7, 8, 9] [about 4 lectures] \nStrategies for algorithm design. Dynamic programming, divide and conquer, greedy algorithms \nand other useful paradigms. [Ref: CLRS3 chapters 4, 15, 16] [about 3 lectures] \nData structures. Elementary data structures: pointers, objects, stacks, queues, lists, trees. \nBinary search trees. Red-black trees. B-trees. Hash tables. Priority queues and heaps. [Ref: \nCLRS3 chapters 6, 10, 11, 12, 13, 18] [about 5 lectures]. \nObjectives \nAt the end of the course students should: \n \nhave a thorough understanding of several classical algorithms and data structures; \nbe able to analyse the space and time efficiency of most algorithms; \nhave a good understanding of how a smart choice of data structures may be used to increase \nthe efficiency of particular algorithms; \nbe able to design new algorithms or modify existing ones for new applications and reason about \nthe efficiency of the result. \nRecommended reading \n* Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest and Clifford Stein Introduction to \nAlgorithms, Fourth Edition ISBN 9780262046305 Published: April 5, 2022. \n \nSedgewick, R., Wayne, K. (2011). Algorithms. Addison-Wesley. ISBN 978-0-321-57351-3. \n \nKleinberg, J. and Tardos, \u00c9. (2006). Algorithm design. Addison-Wesley. ISBN \n978-0-321-29535-4. \n \nKnuth, D.A. (2011). The Art of Computer Programming. Addison-Wesley. ISBN \n978-0-321-75104-1.\n \n\nALGORITHMS 2 \n \nAims \nThe aim of this course is to provide an introduction to computer algorithms and data structures, \nwith an emphasis on foundational material. \n \nLectures \nGraphs and path-finding algorithms. Graph representations. Breadth-first and depth-first search. \nSingle-source shortest paths: Bellman-Ford and Dijkstra algorithms. All-pairs shortest paths: \ndynamic programming and Johnson\u2019s algorithms. [About 4 lectures] \nGraphs and subgraphs. Maximum flow: Ford-Fulkerson method, Max-flow min-cut theorem. \nMatchings in bipartite graphs. Minimum spanning tree: Kruskal and Prim algorithms. Topological \nsort. [About 4 lectures] \nAdvanced data structures. Binomial heap. Amortized analysis: aggregate analysis, potential \nmethod. Fibonacci heaps. Disjoint sets. [About 4 lectures] \nObjectives \nAt the end of the course students should: \n \nhave a thorough understanding of several classical algorithms and data structures; \nbe able to analyse the space and time efficiency of most algorithms; \nhave a good understanding of how a smart choice of data structures may be used to increase \nthe efficiency of particular algorithms; \nbe able to design new algorithms or modify existing ones for new applications and reason about \nthe efficiency of the result. \nRecommended reading \n* Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest and Clifford Stein Introduction to \nAlgorithms, Fourth Edition ISBN 9780262046305 Published: April 5, 2022. \n \nSedgewick, R., Wayne, K. (2011). Algorithms. Addison-Wesley. ISBN 978-0-321-57351-3. \n \nKleinberg, J. and Tardos, \u00c9. (2006). Algorithm design. Addison-Wesley. ISBN \n978-0-321-29535-4. \n \nKnuth, D.A. (2011). The Art of Computer Programming. Addison-Wesley. ISBN \n978-0-321-75104-1.\n \n\nMACHINE LEARNING AND REAL-WORLD DATA \n \nAims \nThis course introduces students to machine learning algorithms as used in real-world \napplications, and to the experimental methodology necessary to perform statistical analysis of \nlarge-scale data from unpredictable processes. Students will perform 3 extended practicals, as \nfollows: \n \nStatistical classification: Determining movie review sentiment using Naive Bayes; \nSequence Analysis: Hidden Markov Modelling and its application to a task from biology \n(predicting protein interactions with a cell membrane); \nAnalysis of social networks, including detection of cliques and central nodes. \nSyllabus \nTopic One: Statistical Classification \nIntroduction to sentiment classification. \nNaive Bayes parameter estimation. \nStatistical laws of language. \nStatistical tests for classification tasks. \nCross-validation and test sets. \nUncertainty and human agreement. \nTopic Two: Sequence Analysis  \nHidden Markov Models (HMM) and HMM training. \nThe Viterbi algorithm. \nUsing an HMM in a biological application. \nTopic Three: Social Networks  \nProperties of networks: Degree, Diameter. \nBetweenness Centrality. \nClustering using betweenness centrality. \nObjectives \nBy the end of the course students should be able to: \n \nunderstand and program two simple supervised machine learning algorithms; \nuse these algorithms in statistically valid experiments, including the design of baselines, \nevaluation metrics, statistical testing of results, and provision against overtraining; \nvisualise the connectivity and centrality in large networks; \nuse clustering (i.e., a type of unsupervised machine learning) for detection of cliques in \nunstructured networks. \nRecommended reading \nJurafsky, D. and Martin, J. (2008). Speech and language processing. Prentice Hall. \n \nEasley, D. and Kleinberg, J. (2010). Networks, crowds, and markets: reasoning about a highly \nconnected world. Cambridge University Press.\n \n\nOPERATING SYSTEMS \n \nAims \nThe overall aim of this course is to provide a general understanding of the structure and key \nfunctions of the operating system. Case studies will be used to illustrate and reinforce \nfundamental concepts. \n \nLectures \nIntroduction to operating systems. Elementary computer organisaton. Abstract view of an \noperating system. Key concepts: layering, multiplexing, performance, caching, buffering, policies \nversus mechanisms. OS evolution: multi-programming, time-sharing. Booting. [1 lecture] \nProtection. Dual-mode operation. Protecting CPU, memory, I/O, storage. Kernels, micro-kernels, \nand modules. System calls. Virtual machines and containers. Subjects and objects. \nAuthentication. Access matrix: ACLs and capabilities. Covert channels. [1 lecture] \nProcesses. Job/process concepts. Process lifecycle. Process management. Context switching. \nInter-process communication. [1 lecture] \nScheduling. CPU-I/O burst cycle. Scheduling basics: preemption and non-preemption. \nScheduling algorithms: FCFS, SJF, SRTF, Priority, and Round Robin. Multilevel scheduling. [2 \nlectures] \nMemory management. Processes in memory. Logical versus physical addresses. Partitions: \nstatic versus dynamic, free space management, external fragmentation. Segmented memory. \nPaged memory: concepts, internal fragmentation, page tables. Demand paging/segmentation. \nPage replacement strategies: FIFO, OPT, and LRU with approximations including NRU, LFU, \nMFU, MRU. Working set schemes. Thrashing. [3 lectures] \nI/O subsystem. General structure. Polled mode versus interrupt-driven I/O. Programmed I/O \n(PIO) versus Direct Memory Access (DMA). Application I/O interface: block and character \ndevices, buffering, blocking, non-blocking, asynchronous, and vectored I/O. Other issues: \ncaching, scheduling, spooling, performance. [1 lecture] \nFile management. File concept. Directory and storage services. File names and metadata. \nDirectory name-space: hierarchies, DAGs, hard and soft links. File operations. Access control. \nExistence and concurrency control. [1 lecture] \nUnix case study. History. General structure. Unix file system: file abstraction, directories, mount \npoints, implementation details. Processes: memory image, lifecycle, start of day. The shell: \nbasic operation, commands, standard I/O, redirection, pipes, signals. Character and block I/O. \nProcess scheduling. [2 lectures] \nObjectives \nAt the end of the course students should be able to: \n \ndescribe the general structure and purpose of an operating system; \nexplain the concepts of process, address space, and file; \ncompare and contrast various CPU scheduling algorithms; \ncompare and contrast mechanisms involved in memory and storage management; \ncompare and contrast polled, interrupt-driven, and DMA-based access to I/O devices. \nRecommended reading \n\n* Silberschatz, A., Galvin, P.C., and Gagne, G. (2018). Operating systems concepts. Wiley (10th \ned.). Prior editions, at least back to 8th ed., should also be acceptable. \nAnderson, T. and Dahlin, M. (2014). Operating Systems: Principles and Practice. Recursive \nBooks (2nd ed.). \nBacon, J. and Harris, T. (2003). Operating systems. Addison-Wesley (3rd ed.). Currently \nappears out-of-print. \nLeffler, S. (1989). The design and implementation of the 4.3BSD Unix operating system. \nAddison-Wesley. \nMcKusick, M.K., Neville-Neil, G.N. and Watson, R.N.M. (2014) The Design and Implementation \nof the FreeBSD Operating System. Pearson Education. (2nd ed.).\n \n\nINTERACTION DESIGN \nAims \nThe aim of this course is to provide an introduction to interaction design, with an emphasis on \nunderstanding and experiencing the user-centred design process, from conducting user \nresearch and requirements development to implementation and evaluation, while understanding \nthe background to human-computer interaction. \n \nLectures \nCourse overview and user research methods. Introduction to the course and practicals. \nUser-Centred Design. User research methods. \nKeeping usera in mind. User research data analysis. Identifying users and stakeholders. \nRepresenting user goals and activities. Identifying and establishing requirements. \nDesign and prototyping. Methods for exploring the design space. Prototyping and different kinds \nof prototypes. \nVisual and interaction design. Memory, perception, attention, and their implications for \ninteraction design. Modalities of interaction, interaction design patterns, information architecture, \nand their implications for interaction design. \nEvaluation. Practical methods for evaluating designs. Evaluation methods without users. \nEvaluation methods with users. \nCase studies from industry and research. Guest lectures (the topics of these lectures are \nsubject to change). \nObjectives \nBy the end of the course students should \n \nhave a thorough understanding of the user-centred design process and be able to apply it to \ninteraction design; \nbe able to design new user interfaces that are informed by principles of good design, and the \nprinciples of human visual perception, cognition and communication; \nbe able to prototype and implement interactive user interfaces with a strong emphasis on users, \nusability and appearance; \nbe able to evaluate existing or new user interfaces using multiple techniques; \nbe able to compare and contrast different design techniques and to critique their applicability to \nnew domains. \nRecommended reading \n* Preece, J., Rogers, Y. and Sharp, H. (2015). Interaction design. Wiley (4th ed.).\n \n\nINTRODUCTION TO PROBABILITY \n \nAims \nThis course provides an elementary introduction to probability and statistics with applications. \nProbability theory and the related field of statistical inference provide the foundations for \nanalysing and making sense of data. The focus of this course is to introduce the language and \ncore concepts of probability theory. The course will also cover some applications of statistical \ninference and algorithms in order to equip students with the ability to represent problems using \nthese concepts and analyse the data within these principles. \n \nLectures \nPart 1 - Introduction to Probability \n \nIntroduction. Counting/Combinatorics (revision), Probability Space, Axioms, Union Bound. \nConditional probability. Conditional Probabilities and Independence, Bayes\u2019Theorem, Partition \nTheorem \nPart 2 - Discrete Random Variables \n \nRandom variables. Definition of a Random Variable, Probability Mass Function, Cumulative \nDistribution, Expectation. \nProbability distributions. Definition and Properties of Expectation, Variance, different ways of \ncomputing them, Examples of important Distributions (Bernoulli, Binomial, Geometric, Poisson), \nPrimer on Continuous Distributions including Normal and Exponential Distributions. \nMultivariate distributions. Multiple Random Variables, Joint and Marginal Distributions, \nIndependence of Random Variables, Covariance. \nPart 3 - Moments and Limit Theorems \n \nIntroduction. Law of Average, Useful inequalities (Markov and Chebyshef), Weak Law of Large \nNumbers (including Proof using Chebyshef\u2019s inequality), Examples. \nMoments and Central Limit Theorem. Introduction to Moments of Random Variables, Central \nLimit Theorem (Proof using Moment Generating functions), Example. \nPart 4 - Applications/Statistics \n \nStatistics. Classical Parameter Estimation (Maximum-Likelihood-Estimation), bias, sample \nmean, sample variance), Examples (Collision-Sampling, Estimating Population Size). \nAlgorithms. Online Algorithms (Secretary Problem, Odd\u2019s Algorithm). \nObjectives \nAt the end of the course students should \n \nunderstand the basic principles of probability spaces and random variables \nbe able to formulate problems using concepts from probability theory and compute or estimate \nprobabilities \nbe familiar with more advanced concepts such as moments, limit theorems and applications \nsuch as parameter estimation \n\nRecommended reading \n* Ross, S.M. (2014). A First course in probability. Pearson (9th ed.). \n \nBertsekas, D.P. and Tsitsiklis, J.N. (2008). Introduction to probability. Athena Scientific. \n \nGrimmett, G. and Welsh, D. (2014). Probability: an Introduction. Oxford University Press (2nd \ned.). \n \nDekking, F.M., et. al. (2005) A modern introduction to probability and statistics. Springer.\n\nSOFTWARE AND SECURITY ENGINEERING \n \nAims \nThis course aims to introduce students to software and security engineering, and in particular to \nthe problems of building large systems, safety-critical systems and systems that must withstand \nattack by capable opponents. Case histories of failure are used to illustrate what can go wrong, \nwhile current software and security engineering practice is studied as a guide to how failures \ncan be avoided. \n \nLectures \n1. What is a security policy or a safety case? Definitions and examples; one-way flows for both \nconfidentiality and safety properties; separation of duties. Top-down and bottom-up analysis \nmethods. What architecture can do, versus benefits of decoupling policy from mechanism. \n \n2. Examples of safety and security policies. Safety and security usability; the pyramid of harms. \nPredicting and mitigating user errors. The prevention of fraud and error in accounting systems; \nthe safety usability of medical devices. \n \n3. Attitudes to risk: expected  utility, prospect theory, framing, status quo bias. Authority, \nconformity and gender; mental models, affordances and defaults. The characteristics of human \nmemory; forgetting passwords versus guessing them. \n \n4. Security protocols; how to enforce policy using  structured human interaction, cryptography or \nboth. Middleperson attacks.The role of verification and its limitations. \n \n5. Attacks on TLS, from rogue CAs through side channels to Heartbleed. Other types of \nsoftware bugs: syntactic, timing, concurrency, code injection, buffer overflows. Defensive \nprogramming: secure coding, contracts. Fuzzing. \n \n6. The software crisis. Examples of large-scale project failure, such as the London Ambulance \nService system and the NHS National Programme for IT. Intrinsic difficulties with complex \nsoftware. \n \n7. Software engineering as the management of complexity. The software life cycle; requirements \nanalysis methods; modular design; the role of prototyping; the waterfall, spiral and agile models. \n \n8. The economics of software as a Service (SaaS); the impact SaaS has on software \nengineering. Continuous integration, release engineering, behavioural analytics and experiment \nframeworks, rearchitecting systems while in operation. \n \n9. Critical systems: safety as an emergent system property. Examples of catastrophic failure: \nfrom Therac-25 to the Boeing 737Max. The problems of managing redundancy. The overall \nprocess of safety engineering. \n \n\n10. Managing the development of critical systems: tools and methods, individual versus group \nproductivity, economics of testing and agile development, measuring outcomes versus process, \nthe technical and human aspects of management, post-market surveillance and coordinated \ndisclosure. The sustainability of products with software components. \n \nAt the end of the course students should know how writing programs with tough assurance \ntargets, in large teams, or both, differs from the programming exercises they have engaged in \nso far. They should understand the different models of software development described in the \ncourse as well as the value of various development and management tools. They should \nunderstand the development life cycle and its basic economics. They should understand the \nvarious types of bugs, vulnerabilities and hazards, how to find them, and how to avoid \nintroducing them. Finally, they should be prepared for the organizational aspects of their Part IB \ngroup project. \n \nRecommended reading \nAnderson, R. (Third Edition 2020). Security engineering (Part 1 and Chapters 27-28). Wiley. \nAvailable at: http://www.cl.cam.ac.uk/users/rja14/book.html\n \n\n",
        "3rd Semester \nCONCURRENT AND DISTRIBUTED SYSTEMS \n \nAims \nThis course considers two closely related topics, Concurrent Systems and Distributed Systems, \nover 16 lectures. The aim of the first half of the course is to introduce concurrency control \nconcepts and their implications for system design and implementation. The aims of the latter \nhalf of the course are to study the fundamental characteristics of distributed systems, including \ntheir models and architectures; the implications for software design; some of the techniques that \nhave been used to build them; and the resulting details of good distributed algorithms and \napplications. \n \nLectures: Concurrency \nIntroduction to concurrency, threads, and mutual exclusion. Introduction to concurrent systems; \nthreads; interleaving; preemption; parallelism; execution orderings; processes and threads; \nkernel vs. user threads; M:N threads; atomicity; mutual exclusion; and mutual exclusion locks \n(mutexes). \nAutomata Composition. Synchronous and asynchronous parallelism; sequential consistency; \nrendezvous. Safety, liveness and deadlock; the Dining Philosophers; Hardware foundations for \natomicity: test-and-set, load-linked/store-conditional and fence instructions. Lamport bakery \nalgorithm. \nCommon design patterns: semaphores, producer-consumer, and MRSW. Locks and invariants; \nsemaphores; condition synchronisation; N-resource allocation; two-party and generalised \nproducer-consumer; Multi-Reader, Single-Write (MRSW) locks. \nCCR, monitors, and concurrency in practice. Conditional critical regions (CCR); monitors; \ncondition variables; signal-wait vs. signal-continue semantics; concurrency in practice (kernels, \npthreads, Java, Cilk, OpenMP). \nDeadlock and liveness guarantees Offline vs. online; model checking; resource allocation \ngraphs; lock order checking; deadlock prevention, avoidance, detection, and recovery; livelock; \npriority inversion; auto parallelisation. \nConcurrency without shared data; transactions. Active objects; message passing; tuple spaces; \nCSP; and actor models. Composite operations; transactions; ACID; isolation; and serialisability. \nFurther transactions History graphs; good and bad schedules; isolation vs. strict isolation; \n2-phase locking; rollback; timestamp ordering (TSO); and optimistic concurrency control (OCC). \nCrash recovery, lock-free programming, and transactional memory. Write-ahead logging, \ncheckpoints, and recovery. Lock-free programming. Hardware and software transactional \nmemories. \n  \n \nLectures: Distributed Systems \nIntroduction to distributed systems; RPC. Avantages and challenges of distributed systems; \nunbounded delay and partial failure; network protocols; transparency; client-server systems; \nremote procedure call (RPC); marshalling; interface definition languages (IDLs). \n\nSystem models and faults. Synchronous, partially synchronous, and asynchronous network \nmodels; crash-stop, crash-recovery, and Byzantine faults; failures, faults, and fault tolerance; \ntwo generals problem. \nTime, clocks, and ordering of events. Physical clocks; leap seconds; UTC; clock synchronisation \nand drift; Network Time Protocol (NTP). Causality; happens-before relation. \nLogical time; Lamport clocks; vector clocks. Broadcast (FIFO, causal, total order); gossip \nprotocols. \nReplication. Quorums; idempotence; replica consistency; read-after-write consistency. State \nmachine replication; leader-based replication. \nConsensus and total order broadcast. FLP result; leader election; the Raft consensus algorithm. \nReplica consistency. Two-phase commit; relationship between 2PC and consensus; \nlinearizability; ABD algorithm; eventual consistency; CAP theorem. \nCase studies. Conflict-free Replicated Data Types (CRDTs); collaborative text editing. Google's \nSpanner; TrueTime. \nObjectives \nAt the end of Concurrent Systems portion of the course, students should: \n \nunderstand the need for concurrency control in operating systems and applications, both mutual \nexclusion and condition synchronisation; \nunderstand how multi-threading can be supported and the implications of different approaches; \nbe familiar with the support offered by various programming languages for concurrency control \nand be able to judge the scope, performance implications and possible applications of the \nvarious approaches; \nbe aware that dynamic resource allocation can lead to deadlock; \nunderstand the concept of transaction; the properties of transactions, how they can be \nimplemented, and how their performance can be optimised based on optimistic assumptions; \nunderstand how the persistence properties of transactions are addressed through logging; and \nhave a high-level understanding of the evolution of software use of concurrency in the \noperating-system kernel case study. \nAt the end of the Distributed Systems portion of the course, students should: \n \nunderstand the difference between shared-memory concurrency and distributed systems; \nunderstand the fundamental properties of distributed systems and their implications for system \ndesign; \nunderstand notions of time, including logical clocks, vector clocks, and physical time \nsynchronisation; \nbe familiar with various approaches to data and service replication, as well as the concept of \nreplica consistency; \nunderstand the effects of large scale on the provision of fundamental services and the tradeoffs \narising from scale; \nappreciate the implications of individual node and network communications failures on \ndistributed computation; \nbe aware of a variety of programming models and abstractions for distributed systems, such as \nRPC, middleware, and total order broadcast; \n\nbe familiar with a range of distributed algorithms, such as consensus, causal broadcast, and \ntwo-phase commit. \nRecommended reading \nModern Operating Systems (free PDF available online) by Andrew S Tanenbaum, Herbert Bos \nJava Concurrency in Practice' (2006 but still highly relevant) by Brian Goetz \nKleppmann, M. (2017). Designing data-intensive applications. O\u2019Reilly. \nTanenbaum, A.S. and van Steen, M. (2017). Distributed systems, 3rd edition. available online. \nCachin, C., Guerraoui, R. and Rodrigues, L. (2011) Introduction to Reliable and Secure \nDistributed Programming. Springer (2nd edition).\n \n\nDATA SCIENCE \n \nAims \nThis course introduces fundamental tools for describing and reasoning about data. There are \ntwo themes: designing probability models to describe systems; and drawing conclusions based \non data generated by such systems. \n \nLectures \nSpecifying and fitting probability models. Random variables. Maximum likelihood estimation. \nGenerative and supervised models. Goodness of fit. \nFeature spaces. Vector spaces, bases, inner products, projection. Linear models. Model fitting \nas projection. Design of features. \nHandling probability models. Handling pdf and cdf. Bayes\u2019s rule. Monte Carlo estimation. \nEmpirical distribution. \nInference. Bayesianism. Frequentist confidence intervals, hypothesis testing. Bootstrap \nresampling. \nRandom processes. Markov chains. Stationarity, and drift analysis. Processes with memory. \nLearning a random process. \nObjectives \nAt the end of the course students should \n \nbe able to formulate basic probabilistic models, including discrete time Markov chains and linear \nmodels \nbe familiar with common random variables and their uses, and with the use of empirical \ndistributions rather than formulae \nunderstand different types of inference about noisy data, including model fitting, hypothesis \ntesting, and making predictions \nunderstand the fundamental properties of inner product spaces and orthonormal systems, and \ntheir application to modelling \nRecommended reading \n* F.M. Dekking, C. Kraaikamp, H.P. Lopuha\u00e4, L.E. Meester (2005). A modern introduction to \nprobability and statistics: understanding why and how. Springer. \n \nS.M. Ross (2002). Probability models for computer science. Harcourt / Academic Press. \n \nM. Mitzenmacher and E. Upfal (2005). Probability and computing: randomized algorithms and \nprobabilistic analysis. Cambridge University Press.\n \n\nECAD AND ARCHITECTURE PRACTICAL CLASSES \nAims \nThe aims of this course are to enable students to apply the concepts learned in the Computer \nDesign course. In particular a web based tutor is used to introduce the SystemVerilog hardware \ndescription language, while the remaining practical classes will then allow students to implement \nthe design of components in this language. \n \nPractical Classes \nWeb tutor The first class uses a web based tutor to rapidly teach the SystemVerilog language. \nFPGA design flow Test driven hardware development for FPGA including an embedded \nprocessor and peripherals [3 classes] \nEmbedded system implementation Embedded system implementation on FPGA [3-4 classes] \nObjectives \nGain experience in electronic computer aided design (ECAD) through learning a design-flow for \nfield programmable gate arrays (FPGAs). \nLearn how to interface to peripherals like a touch screen. \nLearn how to debug hardware and software systems in simulation. \nUnderstand how to construct and program a heterogeneous embedded system. \nRecommended reading \n* Harris, D.M. and Harris, S.L. (2007). Digital design and computer architecture: from gates to \nprocessors. Morgan Kaufmann. \n \nPointers to sources of more specialist information are included on the associated course web \npage. \n \nIntroduction \nThe ECAD and Architecture Laboratory sessions are a companion to the Introduction to \nComputer Architecture course. The objective is to provide experience of hardware design for \nFPGA including use of a small embedded processor. It covers hardware design in \nSystemVerilog, embedded software design in RISC-V assembly and C, and use of FPGA tools. \n \nPrerequisites \nThis course assumes familiarity with the material from Part IA Digital Electronics, although we \nwill use automated tools to perform many of the steps you might have done there by hand (for \nexample, logic minimisation). \n \nWe will be using a Linux command-line environment. We provide scripts and guidance on what \nyou need, however you may find it useful to review Unix Tools, in particular basic navigation \nsuch as ls, cd, mkdir, cp, mv. We'll be using the GCC compiler and Makefiles, described in parts \n31-32. We will also be using git for revision control and submission of work for ticking - you \nshould review at least parts 23-25 and 29. \n \nPracticalities \n\nThe course runs over the 8 weeks of Michaelmas term. The course has been designed so you \ncan do most of the work at home or at any time you wish. You should be able to run all the \nnecessary tools on your own machine, through the use of virtual machines and Docker \ncontainers. We have a number of routes to do this in case some of them aren't suitable for the \nmachine you have. \n \nWe're running the course in a hybrid mode this year. We will provide online help sessions via \nMicrosoft Teams as well as in-person lab sessions on Fridays and Tuesdays 2-5pm during term. \n \nThe core components of the course, being the RISC-V architecture and software and ticked \nexercise, can be done entirely online, without needing access to hardware. They can be \nundertaken with any of the platforms we support, including MCS Linux (on the Intel Lab \nworkstations and remotely) if your own machine isn't suitable. We expect everyone to complete \nthese parts. \n \nThe rest of the course is optional. The hardware simulation and FPGA compilation can be done \non your machine without access to hardware, if you are able to run the virtual machine. \n \nThe parts that require access to an FPGA board will need the ability to run the virtual machine \nand you be able to collect and return an FPGA board from the Department. That process will be \ndescribed when you come to those parts. There is an optional starred tick available for \ncompletion of this part, which will need assessment by a demonstrator in person. \n \nWe'll do our best to support all the different platforms and variations, but please bear with us - \nit's quite possible we'll come across a problem we haven't seen before! \n \nWe have provided four routes you can use different tools to complete the course. Some routes \nnecessarily exclude some material but this material is optional. \n \nAssessment \nIn a similar way to other practical courses, this course carries one 'tick'. Everyone should expect \nto receive this tick, and those who do not should expect to lose marks on their final exams. \n \nThis year we have adopted the 'chime' autoticker used by Further Java. You will check out the \ninitial files as a git repository from the chime server, commit your work, and push the repository \nback to submit it. You can then press a button to run tests against your code and the autoticker \nwill tell you whether you have passed. Additionally you may be selected for a (real/video) \ninterview with a ticker to discuss your code and understanding of the material. \n \nTicking procedures will only be available during weeks 1-8 of Michaelmas term. The deadline for \nsubmitting Tick 1 is 5pm on the Tuesday of week 6 (19 November 2024). After passing the \nautoticker you may be asked for a tick interview with a demonstrator (online or in person). You \ncan gain Tick 1* during the timetabled lab sessions, assuming demonstrators are available. \n \n\nIf you do not submit a successful Tick 1 to the autoticker by 5pm on the Tuesday of week 6 you \ncan self-certify an extension. If you need more time than this you will need to ask your Directory \nof Studies to organise a further extension. The hard deadline by which all ticks must be \ncompleted is noon on Friday 31 January 2025 (the Head of Department Notices is the definitive \ndocument). Extensions beyond this date due to exceptional circumstances require a formal \napplication from your college to the Examiners. \n \nScheduling \nWhile the full course contains 8 exercises, there is not a tight binding to doing one exercise per \nweek. Students should expect to spend about three hours per week on the course, however it is \nrecommended that you make a start on the next exercise if you have time in hand. \n \nDemonstrator help will be focused on the Tuesday and Friday 2-5pm slots during term the lab \nwould normally operate in, so you may find it helpful to work in these times as that will provide \nthe quickest response to questions. \n \nCollaboration \nWhen we have done these labs in person, we have often found they worked well when doing \nthem collaboratively. While your work must be your own, students can work together and help \neach other, and demonstrators contribute advice and experience with the tools. \n \nFor the timetabled sessions, which are Tuesdays and Fridays 2-5pm UK time during term, there \nwill be demonstrators available in the lab and at the same time we'll use the ECAD group on \nMicrosoft Teams. With this we can provide online audio/video help, screensharing and (for the \nWindows and possibly Mac Teams apps) optional remote control of your development \nenvironment. You will be added to the Team in week 1 - if you believe you have been missed \nplease post in the Moodle forum. In Teams there are a number of Helpdesk Channels (A-C) - if \nyou need help during lab times, post in Help Centre and a demonstrator can ask you to go to a \nspecific channel. There are also Student Breakout Channels (D-F) which are available for \nstudents to chat amongst themselves. \n \nFor help outside of timetabled sessions we've set up a Moodle forum where you can post \nquestions - we'll keep an eye on this at other times, but students are encouraged to use it to \nsupport each other. \n \nStudents are of course free to use other platforms to help each other. If you find anything useful \nthat we might use in future, please let us know! \n \nIf you are having difficulty accessing both Moodle and Teams, please email theo.markettos at \ncl.cam.ac.uk. \n \nFeedback \nWe revise the course each year, which may cause new bugs in the code or the notes. The \ncreators (Theo Markettos and Simon Moore) appreciate constructive feedback. \n\n \nCredits \nThis course was written by Theo Markettos and Simon Moore, with portions developed by \nRobert Eady, Jonas Fiala, Paul Fox, Vlad Gavrila, Brian Jones and Roy Spliet. We would like to \nthank Altera, Intel and Terasic for funding and other contributions.\n \n\nECONOMICS, LAW AND ETHICS \n \nAims \nThis course aims to give students an introduction to some basic concepts in economics, law and \nethics. \n \nLectures \nClassical economics and consumer theory. Prices and markets; Pareto efficiency; preferences; \nutility; supply and demand; the marginalist revolution; elasticity; the welfare theorems; \ntransaction costs. \nInformation economics. The discriminating monopolist; marginal costs; effects of technology on \nsupply and demand; competition and information; lock in; real and virtual networks; Metcalfe\u2019s \nlaw; the dominant firm model; price discrimination; bundling; income distribution. \nMarket failure and behavioural economics. Market failure: the business cycle; recession and \ntechnology; tragedy of the commons; externalities; monopoly rents; asymmetric information: the \nmarket for lemons; adverse selection; moral hazard; signalling. Behavioural economics: \nbounded rationality, heuristics and biases; nudge theory; the power of defaults; agency effects. \nAuction theory and game theory. Auction theory: types of auctions; strategic equivalence; the \nrevenue equivalence theorem; the winner\u2019s curse; problems with real auctions; mechanism \ndesign and the combinatorial auction; applicability of auction mechanisms in computer science; \nadvertising auctions. Game theory: the choice between cooperation and conflict; strategic \nforms; dominant strategy equilibrium; Nash equilibrium; the prisoners\u2019 dilemma; evolution of \nstrategies; stag hunt; volunteer\u2019s dilemma; chicken; iterated games; hawk-dove; application to \ncomputer science. \nPrinciples of law. Criminal and civil law; contract law; choice of law and jurisdiction; arbitration; \ntort; negligence; defamation; intellectual property rights. \nLaw and the Internet. Computer evidence; the General Data Protection Regulation; UK laws that \nspecifically affect the Internet; e-commerce regulations; privacy and electronic communications. \nPhilosophies of ethics. Authority, intuitionist, egoist and deontological theories; utilitarian and \nRawlsian models; morality; insights from evolutionary psychology, neurology, and experimental \nethics; professional codes of ethics; research ethics. \nContemporary ethical issues. The Internet and social policy; current debates on privacy, \nsurveillance, and censorship; responsible vulnerability disclosure; algorithmic bias; predictive \npolicing; gamification and engagement; targeted political advertising; environmental impacts. \nObjectives \nOn completion of this course, students should be able to: \n \nReflect on and discuss professional, economic, social, environmental, moral and ethical issues \nrelating to computer science \nDefine and explain economic and legal terminology and arguments \nApply the philosophies and theories covered to computer science problems and scenarios \nReflect on the main constraints that market, legislation and ethics place on firms dealing in \ninformation goods and services \nRecommended reading \n\n* Shapiro, C. & Varian, H. (1998). Information rules. Harvard Business School Press. \nHare, S. (2022). Technology is not neutral: A short guide to technology ethics. London \nPublishing Partnership. \n \nFurther reading: \nSmith, A. (1776). An inquiry into the nature and causes of the wealth of nations, available at    \nhttp://www.econlib.org/library/Smith/smWN.html \nThaler, R.H. (2016). Misbehaving. Penguin. \nGalbraith, J.K. (1991). A history of economics. Penguin. \nPoundstone, W. (1992). Prisoner\u2019s dilemma. Anchor Books. \nPinker, S (2011). The Better Angels of our Nature. Penguin. \nAnderson, R. (2008). Security engineering (Chapter 7). Wiley. \nVarian, H. (1999). Intermediate microeconomics - a modern approach. Norton. \nNuffield Council on Bioethics (2015) The collection, linking and use of data in biomedical \nresearch and health care.\n \n\nFURTHER GRAPHICS \n \nAims \nThe course introduces fundamental concepts of modern graphics pipelines. \n \nLectures \nThe order and content of lectures is provisional and subject to change. \n \nGeometry Representations. Parametric surfaces, implicit surfaces, meshes, point-set surfaces, \ngeometry processing pipeline. [1 lecture] \nDiscrete Differential Geometry. Surface normal and curvature, Laplace-Beltrami operator, heat \ndiffusion. [1 lecture] \nGeometry Processing.  Parametrization, filtering, 3D capture. [1 lecture] \nAnimation I. Animation types, animation pipeline, rigging/skinning, character animation. [1 \nlecture] \nAnimation II. Blending transformations, pose-space animation, controllers. [1 lecture] \nThe Rendering Equation. Radiosity, reflection models, BRDFs, local vs. global illumination. [1 \nlecture] \nDistributed Ray Tracing. Quadrature, importance sampling, recursive ray tracing. [1 lecture] \nInverse Rendering. Differentiable rendering, inverse problems. [1 lecture] \nObjectives \nOn completing the course, students should be able to \n \nunderstand and use fundamental 3D geometry/scene representations and operations; \nlearn animation techniques and controls; \nlearn how light transport is modeled and simulated in rendering; \nunderstand how graphics and rendering can be used to solve computer perception problems. \nRecommended reading \nStudents should expect to refer to one or more of these books, but should not find it necessary \nto purchase any of them. \n \nShirley, P. and Marschner, S. (2009). Fundamentals of Computer Graphics. CRC Press (3rd \ned.). \nWatt, A. (2000). 3D Computer Graphics. Addison-Wesley (3rd ed). \nHughes, van Dam, McGuire, Skalar, Foley, Feiner and Akeley (2013). Computer Graphics: \nPrinciples and Practice. Addison-Wesley (3rd edition) \nAkenine-M\u00f6ller, et. al. (2018). Real-time rendering. CRC Press (4th ed.).\n \n\nFURTHER JAVA \n \nAims \nThe goal of this course is to provide students with the ability to understand the advanced \nprogramming features available in the Java programming language, completing the coverage of \nthe language started in the Programming in Java course. The course is designed to \naccommodate students with diverse programming backgrounds; consequently Java is taught \nfrom first principles in a practical class setting where students can work at their own pace from a \ncourse handbook. \n \nObjectives \n \nAt the end of the course students should \n \nunderstand different mechanisms for communication between distributed applications and be \nable to evaluate their trade-offs; \nbe able to use Java generics and annotations to improve software usability, readability and \nsafety; \nunderstand and be able to exploit the Java class-loading mechansim; \nunderstand and be able to use concurrency control correctly; \nbe able to implement a vector clock algorithm and the happens-before relation. \nRecommended reading \n* Goetz, B. (2006). Java concurrency in practice. Addison-Wesley. \nGosling, J., Joy, B., Steele, G., Bracha, G. and Buckley, A. (2014). The Java language \nspecification, Java SE 8 Edition. Addison-Wesley. \nhttp://docs.oracle.com/javase/specs/jls/se8/html/\n \n\nGROUP PROJECTS \nExample Risk Report \nThe risk report should focus on the risks to your project succeeding on time (and not, say, on \ngeneral health and safety points). It is only 50 words. Example: \n \n\"Identified risks: \n * Training data requires access authorisation that has not been approved, and we don\u2019t know if \nclient can do this \n * Two group members have unreliable network connections and may miss meetings \n * One group member is in timezone UTC+8, and will have to attend meetings late at night \n * Bug reports on Github for a library we plan to use suggest maintenance updates will be \nnecessary for recent Android release\"\n \n\nINTRODUCTION TO COMPUTER ARCHITECTURE \n \nAims \nThe aims of this course are to introduce a hardware description language (SystemVerilog) and \ncomputer architecture concepts in order to design computer systems. The parallel ECAD+Arch \npractical classes will allow students to apply the concepts taught in lectures. \n \nLectures \nPart 1 - Gates to processors  \n \nTechnology trends and design challenges. Current technology, technology trends, ECAD trends, \nchallenges. [1 lecture] \nDigital system design. Practicalities of mapping SystemVerilog descriptions of hardware \n(including a processor) onto an FPGA board. Tips and pitfalls when generating larger modular \ndesigns. [1 lecture] \nEight great ideas in computer architecture. [1 lecture] \nReduced instruction set computers and RISC-V. Introduction to the RISC-V processor design. [1 \nlecture] \nExecutable and synthesisable models. [1 lecture] \nPipelining. [2 lectures] \nMemory hierarchy and caching. Caching, etc. [1 lecture] \nSupport for operating systems. Memory protection, exceptions, interrupts, etc. [1 lecture] \nOther instruction set architectures. CISC, stack, accumulator. [1 lecture] \nPart 2  \n \nOverview of Systems-on-Chip (SoCs) and DRAM. [1 lecture] High-level SoCs, DRAM storage \nand accessing. \nMulticore Processors. [2 lectures] Communication, cache coherence, barriers and \nsynchronisation primitives. \nGraphics processing units (GPUs) [2 lectures] Basic GPU architecture and programming. \nFuture Directions [1 lecture] Where is computer architecture heading? \nObjectives \nAt the end of the course students should \n \nbe able to read assembler given a guide to the instruction set and be able to write short pieces \nof assembler if given an instruction set or asked to invent an instruction set; \nunderstand the differences between RISC and CISC assembler; \nunderstand what facilities a processor provides to support operating systems, from memory \nmanagement to software interrupts; \nunderstand memory hierarchy including different cache structures and coherency needed for \nmulticore systems; \nunderstand how to implement a processor in SystemVerilog; \nappreciate the use of pipelining in processor design; \nhave an appreciation of control structures used in processor design; \n\nhave an appreciation of system-on-chips and their components; \nunderstand how DRAM stores data; \nunderstand how multicore processors communicate; \nunderstand how GPUs work and have an appreciation of how to program them. \nRecommended reading \n* Patterson, D.A. and Hennessy, J.L. (2017). Computer organization and design: The \nhardware/software interface RISC-V edition. Morgan Kaufmann. ISBN 978-0-12-812275-4. \n \nRecommended further reading: \n \nHarris, D.M. and Harris, S.L. (2012). Digital design and computer architecture. Morgan \nKaufmann. ISBN 978-0-12-394424-5. \nHennessy, J. and Patterson, D. (2006). Computer architecture: a quantitative approach. Elsevier \n(4th ed.). ISBN 978-0-12-370490-0. (Older versions of the book are also still generally relevant.) \n \nPointers to sources of more specialist information are included in the lecture notes and on the \nassociated course web page\n \n\nPROGRAMMING IN C AND C++ \n \nAims \nThis course aims \n \nto provide a solid introduction to programming in C and to provide an overview of the principles \nand constraints that affect the way in which the C programming language has been designed \nand is used; \nto introduce the key additional features of C++. \n  \n \nLectures \nIntroduction to the C language. Background and goals of C. Types, expressions, control flow \nand strings. \n(continued) Functions. Multiple compilation units. Scope. Segments. Incremental compilation. \nPreprocessor. \n(continued) Pointers and pointer arithmetic. Function pointers. Structures and Unions. \n(continued) Const and volatile qualifiers. Typedefs. Standard input/output. Heap allocation. \nMiscellaneous Features, Hints and Tips. \nC semantics and tools. Undefined vs implementation-defined behaviour. Buffer and integer \noverflows. ASan, MSan, UBsan, Valgrind checkers. \nMemory allocation, data structures and aliasing. Malloc/free, tree and DAG examples and their \ndeallocation using a memory arena. \nFurther memory management. Reference Counting and Garbage Collection \nMemory hierarchy and cache optimization. Data structure layouts. Intrusive lists. Array-of-structs \nvs struct-of-arrays representations. Loop blocking. \nDebugging. Using print statements, assertions and an interactive debugger. Unit testing and \nregression. \nIntroduction to C++. Goals of C++. Differences between C and C++. References versus \npointers. Overloaded functions. \nObjects in C++. Classes and structs. Destructors. Resource Acquisition is Initialisation. Operator \noverloading. Virtual functions. Casts. Multiple inheritance. Virtual base classes. \nOther C++ concepts. Templates and meta-programming. \nObjectives \nAt the end of the course students should \n \nbe able to read and write C programs; \nunderstand the interaction between C programs and the host operating system; \nbe familiar with the structure of C program execution in machine memory; \nunderstand the potential dangers of writing programs in C; \nunderstand the object model and main additional features of C++. \nRecommended reading \n* Kernighan, B.W. and Ritchie, D.M. (1988). The C programming language. Prentice Hall (2nd \ned.).\n \n\nSEMANTICS OF PROGRAMMING LANGUAGES \n \nAims \nThe aim of this course is to introduce the structural, operational approach to programming \nlanguage semantics. It will show how to specify the meaning of typical programming language \nconstructs, in the context of language design, and how to reason formally about semantic \nproperties of programs. \n \nLectures \nIntroduction. Transition systems. The idea of structural operational semantics. Transition \nsemantics of a simple imperative language. Language design options. [2 lectures] \nTypes. Introduction to formal type systems. Typing for the simple imperative language. \nStatements of desirable properties. [2 lectures] \nInduction. Review of mathematical induction. Abstract syntax trees and structural induction. \nRule-based inductive definitions and proofs. Proofs of type safety properties. [2 lectures] \nFunctions. Call-by-name and call-by-value function application, semantics and typing. Local \nrecursive definitions. [2 lectures] \nData. Semantics and typing for products, sums, records, references. [1 lecture] \nSubtyping. Record subtyping and simple object encoding. [1 lecture] \nSemantic equivalence. Semantic equivalence of phrases in a simple imperative language, \nincluding the congruence property. Examples of equivalence and non-equivalence. [1 lecture] \nConcurrency. Shared variable interleaving. Semantics for simple mutexes; a serializability \nproperty. [1 lecture] \nObjectives \nAt the end of the course students should \n \nbe familiar with rule-based presentations of the operational semantics and type systems for \nsome simple imperative, functional and interactive program constructs; \nbe able to prove properties of an operational semantics using various forms of induction \n(mathematical, structural, and rule-based); \nbe familiar with some operationally-based notions of semantic equivalence of program phrases \nand their basic properties. \nRecommended reading \n* Pierce, B.C. (2002). Types and programming languages. MIT Press. \nHennessy, M. (1990). The semantics of programming languages. Wiley. Out of print, but \navailable on the web at \nhttp://www.cs.tcd.ie/matthew.hennessy/splexternal2015/resources/sembookWiley.pdf \nWinskel, G. (1993). The formal semantics of programming languages. MIT Press.\n \n\nUNIX TOOLS \n \nAims \nThis video-lecture course gives students who have already basic Unix/Linux experience some \nadditional practical software-engineering knowledge: how to use the shell and related tools as \nan efficient working environment, how to automate routine tasks, and how version-control and \nautomated-build tools can help to avoid confusion and accidents, especially when working in \nteams. These are essential skills, both in industrial software development and student projects. \n \nLectures \nUnix concepts. Brief review of Unix history and design philosophy, documentation, terminals, \ninter-process communication mechanisms and conventions, shell, command-line arguments, \nenvironment variables, file descriptors. \nShell concepts. Program invocation, redirection, pipes, file-system navigation, argument \nexpansion, quoting, job control, signals, process groups, variables, locale, history and alias \nfunctions, security considerations. \nScripting. Plain-text formats, executables, #!, shell control structures and functions. Startup \nscripts. \nText, file and networking tools. sed, grep, chmod, find, ssh, rsync, tar, zip, etc. \nVersion control. diff, patch, RCS, Subversion, git. \nSoftware development tools. C compiler, linker, debugger, make. \nPerl. Introduction to a powerful scripting and text-manipulation language. [2 lectures] \nObjectives \nAt the end of the course students should \n \nbe confident in performing routine user tasks on a POSIX system, understand command-line \nuser-interface conventions and know how to find more detailed documentation; \nappreciate how simple tools can be combined to perform a large variety of tasks; \nbe familiar with the most common tools, file formats and configuration practices; \nbe able to understand, write, and maintain shell scripts and makefiles; \nappreciate how using a version-control system and fully automated build processes help to \nmaintain reproducibility and audit trails during software development; \nknow enough about basic development tools to be able to install, modify and debug C source \ncode; \nhave understood the main concepts of, and gained initial experience in, writing Perl scripts \n(excluding the facilities for object-oriented programming). \nRecommended reading \nRobbins, A. (2005). Unix in a nutshell. O\u2019Reilly (4th ed.). \nSchwartz, R.L., Foy, B.D. and Phoenix, T. (2011). Learning Perl. O\u2019Reilly (6th ed.).\n \n\n",
        "4th Semester \n \nCOMPILER CONSTRUCTION \n \nAims \nThis course aims to cover the main concepts associated with implementing compilers for \nprogramming languages. We use a running example called SLANG (a Small LANGuage) \ninspired by the languages described in 1B Semantics. A toy compiler (written in ML) is provided, \nand students are encouraged to extend it in various ways. \n \nLectures \nOverview of compiler structure The spectrum of interpreters and compilers; compile-time and \nrun-time. Compilation as a sequence of translations from higher-level to lower-level intermediate \nlanguages, where each translation preserves semantics. The structure of a simple compiler: \nlexical analysis and syntax analysis, type checking, intermediate representations, optimisations, \ncode generation. Overview of run-time data structures: stack and heap. Virtual machines. [1 \nlecture] \nLexical analysis and syntax analysis. Lexical analysis based on regular expressions and finite \nstate automata. Using LEX-tools. How does LEX work? Parsing based on context-free \ngrammars and push-down automata. Grammar ambiguity, left- and right-associativity and \noperator precedence. Using YACC-like tools. How does YACC work? LL(k) and LR(k) parsing \ntheory. [3 lectures] \nCompiler Correctness Recursive functions can be transformed into iterative functions using the \nContinuation-Passing Style (CPS) transformation. CPS applied to a (recursive) SLANG \ninterpreter to derive, in a step-by-step manner, a correct stack-based compiler. [3 lectures] \nData structures, procedures/functions Representing tuples, arrays, references. Procedures and \nfunctions: calling conventions, nested structure, non-local variables. Functions as first-class \nvalues represented as closures. Simple optimisations: inline expansion, constant folding, \nelimination of tail recursion, peephole optimisation. [5 lectures] \nAdvanced topics Run-time memory management (garbage collection). Static and dynamic \nlinking. Objects and inheritance; implementation of method dispatch. Try-catch exception \nmechanisms. Compiling a compiler via bootstrapping. [4 lectures] \nObjectives \nAt the end of the course students should understand the overall structure of a compiler, and will \nknow significant details of a number of important techniques commonly used. They will be \naware of the way in which language features raise challenges for compiler builders. \n \nRecommended reading \n* Aho, A.V., Sethi, R. and Ullman, J.D. (2007). Compilers: principles, techniques and tools. \nAddison-Wesley (2nd ed.). \nMogensen, T. \u00c6. (2011). Introduction to compiler design. Springer. http://www.diku.dk/ \ntorbenm/Basics.\n \n\nCOMPUTATION THEORY \n \nAims \nThe aim of this course is to introduce several apparently different formalisations of the informal \nnotion of algorithm; to show that they are equivalent; and to use them to demonstrate that there \nare uncomputable functions and algorithmically undecidable problems. \n \nLectures \nIntroduction: algorithmically undecidable problems. Decision problems. The informal notion of \nalgorithm, or effective procedure. Examples of algorithmically undecidable problems. [1 lecture] \nRegister machines. Definition and examples; graphical notation. Register machine computable \nfunctions. Doing arithmetic with register machines. [1 lecture] \nUniversal register machine. Natural number encoding of pairs and lists. Coding register machine \nprograms as numbers. Specification and implementation of a universal register machine. [2 \nlectures] \nUndecidability of the halting problem. Statement and proof. Example of an uncomputable partial \nfunction. Decidable sets of numbers; examples of undecidable sets of numbers. [1 lecture] \nTuring machines. Informal description. Definition and examples. Turing computable functions. \nEquivalence of register machine computability and Turing computability. The Church-Turing \nThesis. [2 lectures] \nPrimitive and partial recursive functions. Definition and examples. Existence of a recursive, but \nnot primitive recursive function. A partial function is partial recursive if and only if it is \ncomputable. [2 lectures] \nLambda-Calculus. Alpha and beta conversion. Normalization. Encoding data. Writing recursive \nfunctions in the lambda-calculus. The relationship between computable functions and \nlambda-definable functions. [3 lectures] \nObjectives \nAt the end of the course students should \n \nbe familiar with the register machine, Turing machine and lambda-calculus models of \ncomputability; \nunderstand the notion of coding programs as data, and of a universal machine; \nbe able to use diagonalisation to prove the undecidability of the Halting Problem; \nunderstand the mathematical notion of partial recursive function and its relationship to \ncomputability. \nRecommended reading \n* Hopcroft, J.E., Motwani, R. and Ullman, J.D. (2001). Introduction to automata theory, \nlanguages, and computation. Addison-Wesley (2nd ed.). \n* Hindley, J.R. and Seldin, J.P. (2008). Lambda-calculus and combinators, an introduction. \nCambridge University Press (2nd ed.). \nCutland, N.J. (1980). Computability: an introduction to recursive function theory. Cambridge \nUniversity Press. \nDavis, M.D., Sigal, R. and Weyuker, E.J. (1994). Computability, complexity and languages. \nAcademic Press (2nd ed.). \n\nSudkamp, T.A. (2005). Languages and machines. Addison-Wesley (3rd ed.).\n \n\nCOMPUTER NETWORKING \n \nAims \nThe aim of this course is to introduce key concepts and principles of computer networks. The \ncourse will use a top-down approach to study the Internet and its protocol stack. Instances of \narchitecture, protocol, application-examples will include email, web and media-streaming. We \nwill cover communications services (e.g., TCP/IP) required to support such network \napplications. The implementation and deployment of communications services in practical \nnetworks: including wired and wireless LAN environments, will be followed by a discussion of \nissues of network-management. Throughout the course, the Internet\u2019s architecture and \nprotocols will be used as the primary examples to illustrate the fundamental principles of \ncomputer networking. \n \nLectures \nIntroduction. Overview of networking using the Internet as an example. LANs and WANs. OSI \nreference model, Internet TCP/IP Protocol Stack. Circuit-switching, packet-switching, Internet \nstructure, networking delays and packet loss. [3 lectures] \nLink layer and local area networks. Link layer services, error detection and correction, Multiple \nAccess Protocols, link layer addressing, Ethernet, hubs and switches, Point-to-Point Protocol. [2 \nlectures] \nWireless and mobile networks. Wireless links and network characteristics, Wi-Fi: IEEE 802.11 \nwireless LANs. [1 lecture] \nNetwork layer addressing. Network layer services, IP, IP addressing, IPv4, DHCP, NAT, ICMP, \nIPv6. [3 lectures] \nNetwork layer routing. Routing and forwarding, routing algorithms, routing in the Internet, \nmulticast. [3 lectures] \nTransport layer. Service models, multiplexing/demultiplexing, connection-less transport (UDP), \nprinciples of reliable data transfer, connection-oriented transport (TCP), TCP congestion control, \nTCP variants. [6 lectures] \nApplication layer. Client/server paradigm, WWW, HTTP, Domain Name System, P2P. [1.5 \nlectures] \nMultimedia networking. Networked multimedia applications, multimedia delivery requirements, \nmultimedia protocols (SIP), content distribution networks. [0.5 lecture] \nObjectives \nAt the end of the course students should \n \nbe able to analyse a communication system by separating out the different functions provided \nby the network; \nunderstand that there are fundamental limits to any communications system; \nunderstand the general principles behind multiplexing, addressing, routing, reliable transmission \nand other stateful protocols as well as specific examples of each; \nunderstand what FEC is; \nbe able to compare communications systems in how they solve similar problems; \n\nhave an informed view of both the internal workings of the Internet and of a number of common \nInternet applications and protocols. \nRecommended reading \n* Peterson, L.L. and Davie, B.S. (2011). Computer networks: a systems approach. Morgan \nKaufmann (5th ed.). ISBN 9780123850591 \nKurose, J.F. and Ross, K.W. (2009). Computer networking: a top-down approach. \nAddison-Wesley (5th ed.). \nComer, D. and Stevens, D. (2005). Internetworking with TCP-IP, vol. 1 and 2. Prentice Hall (5th \ned.). \nStevens, W.R., Fenner, B. and Rudoff, A.M. (2003). UNIX network programming, Vol.I: The \nsockets networking API. Prentice Hall (3rd ed.).\n \n\nFURTHER HUMAN\u2013COMPUTER INTERACTION \n \nAims \nThis aim of this course is to provide an introduction to the theoretical foundations of Human \nComputer Interaction, and an understanding of how these can be applied to the design of \ncomplex technologies. \n \nLectures \nTheory driven approaches to HCI. What is a theory in HCI? Why take a theory driven approach \nto HCI? \nDesign of visual displays. Segmentation and variables of the display plane. Modes of \ncorrespondence. \nGoal-oriented interaction. Using cognitive theories of planning, learning and understanding to \nunderstand user behaviour, and what they find hard. \nDesigning smart systems. Using statistical methods to anticipate user needs and actions with \nBayesian strategies. \nDesigning efficient systems. Measuring and optimising human performance through quantitative \nexperimental methods. \nDesigning meaningful systems. Qualitative research methods to understand social context and \nrequirements of user experience. \nEvaluating interactive system designs. Approaches to evaluation in systems research and \nengineering, including Part II Projects. \nDesigning complex systems. Worked case studies of applying the theories to a hard HCI \nproblem. Research directions in HCI. \nObjectives \nAt the end of the course students should be able to apply theories of human performance and \ncognition to system design, including selection of appropriate techniques to analyse, observe \nand improve the usability of a wide range of technologies. \n \nRecommended reading \n* Preece, J., Sharp, H. and Rogers, Y. (2015). Interaction design: beyond human-computer \ninteraction. Wiley (Currently in 4th edition, but earlier editions will suffice). \n \nFurther reading: \n \nCarroll, J.M. (ed.) (2003). HCI models, theories and frameworks: toward a multi-disciplinary \nscience. Morgan Kaufmann.\n \n\nLOGIC AND PROOF \n \nAims \nThis course will teach logic, especially the predicate calculus. It will present the basic principles \nand definitions, then describe a variety of different formalisms and algorithms that can be used \nto solve problems in logic. Putting logic into the context of Computer Science, the course will \nshow how the programming language Prolog arises from the automatic proof method known as \nresolution. It will introduce topics that are important in mechanical verification, such as binary \ndecision diagrams (BDDs), SAT solvers and modal logic. \n \nLectures \nIntroduction to logic. Schematic statements. Interpretations and validity. Logical consequence. \nInference. \nPropositional logic. Basic syntax and semantics. Equivalences. Normal forms. Tautology \nchecking using CNF. \nThe sequent calculus. A simple (Hilbert-style) proof system. Natural deduction systems. \nSequent calculus rules. Sample proofs. \nFirst order logic. Basic syntax. Quantifiers. Semantics (truth definition). \nFormal reasoning in FOL. Free versus bound variables. Substitution. Equivalences for \nquantifiers. Sequent calculus rules. Examples. \nClausal proof methods. Clause form. A SAT-solving procedure. The resolution rule. Examples. \nRefinements. \nSkolem functions, Unification and Herbrand\u2019s theorem. Prenex normal form. Skolemisation. \nMost general unifiers. A unification algorithm. Herbrand models and their properties. \nResolution theorem-proving and Prolog. Binary resolution. Factorisation. Example of Prolog \nexecution. Proof by model elimination. \nSatisfiability Modulo Theories. Decision problems and procedures. How SMT solvers work. \nBinary decision diagrams. General concepts. Fast canonical form algorithm. Optimisations. \nApplications. \nModal logics. Possible worlds semantics. Truth and validity. A Hilbert-style proof system. \nSequent calculus rules. \nTableaux methods. Simplifying the sequent calculus. Examples. Adding unification. \nSkolemisation. The world\u2019s smallest theorem prover? \nObjectives \nAt the end of the course students should \n \nbe able to manipulate logical formulas accurately; \nbe able to perform proofs using the presented formal calculi; \nbe able to construct a small BDD; \nunderstand the relationships among the various calculi, e.g. SAT solving, resolution and Prolog; \nunderstand the concept of a decision procedure and the basic principles of \u201csatisfiability modulo \ntheories\u201d. \nbe able to apply the unification algorithm and to describe its uses. \nRecommended reading \n\n* Huth, M. and Ryan, M. (2004). Logic in computer science: modelling and reasoning about \nsystems. Cambridge University Press (2nd ed.). \nBen-Ari, M. (2001). Mathematical logic for computer science. Springer (2nd ed.).\n \n\nPROLOG \n \nAims \nThe aim of this course is to introduce programming in the Prolog language. Prolog encourages \na different programming style to Java or ML and particular focus is placed on programming to \nsolve real problems that are suited to this style. Practical experimentation with the language is \nstrongly encouraged. \n \nLectures \nIntroduction to Prolog. The structure of a Prolog program and how to use the Prolog interpreter. \nUnification. Some simple programs. \nArithmetic and lists. Prolog\u2019s support for evaluating arithmetic expressions and lists. The space \ncomplexity of program evaluation discussed with reference to last-call optimisation. \nBacktracking, cut, and negation. The cut operator for controlling backtracking. Negation as \nfailure and its uses. \nSearch and cut. Prolog\u2019s search method for solving problems. Graph searching exploiting \nProlog\u2019s built-in search mechanisms. \nDifference structures. Difference lists: introduction and application to example programs. \nBuilding on Prolog. How particular limitations of Prolog programs can be addressed by \ntechniques such as Constraint Logic Programming (CLP) and tabled resolution. \nObjectives \nAt the end of the course students should \n \nbe able to write programs in Prolog using techniques such as accumulators and difference \nstructures; \nknow how to model the backtracking behaviour of program execution; \nappreciate the unique perspective Prolog gives to problem solving and algorithm design; \nunderstand how larger programs can be created using the basic programming techniques used \nin this course. \nRecommended reading \n* Bratko, I. (2001). PROLOG programming for artificial intelligence. Addison-Wesley (3rd or 4th \ned.). \nSterling, L. and Shapiro, E. (1994). The art of Prolog. MIT Press (2nd ed.). \n \nFurther reading: \n \nO\u2019Keefe, R. (1990). The craft of Prolog. MIT Press. [This book is beyond the scope of this \ncourse, but it is very instructive. If you understand its contents, you\u2019re more than prepared for \nthe examination.]\n \n\nARTIFICIAL INTELLIGENCE \n \nPrerequisites: Algorithms 1, Algorithms 2. In addition the course requires some mathematics, in \nparticular some use of vectors and some calculus. Part IA Natural Sciences Mathematics or \nequivalent and Discrete Mathematics are likely to be helpful although not essential. Similarly, \nelements of Machine Learning and Real World Data, Foundations of Data Science, Logic and \nProof, Prolog and Complexity Theory are likely to be useful. This course is a prerequisite for the \nPart II courses Machine Learning and Bayesian Inference and Natural Language Processing. \nThis course is a prerequisite for: Natural Language Processing \nExam: Paper 7 Question 1, 2 \nPast exam questions, Moodle, timetable \n \nAims \nThe aim of this course is to provide an introduction to some fundamental issues and algorithms \nin artificial intelligence (AI). The course approaches AI from an algorithmic, computer \nscience-centric perspective; relatively little reference is made to the complementary \nperspectives developed within psychology, neuroscience or elsewhere. The course aims to \nprovide some fundamental tools and algorithms required to produce AI systems able to exhibit \nlimited human-like abilities, particularly in the form of problem solving by search, game-playing, \nrepresenting and reasoning with knowledge, planning, and learning. \n \nLectures \nIntroduction. Alternate ways of thinking about AI. Agents as a unifying view of AI systems. [1 \nlecture] \nSearch I. Search as a fundamental paradigm for intelligent problem-solving. Simple, uninformed \nsearch algorithms. Tree search and graph search. [1 lecture] \nSearch II. More sophisticated heuristic search algorithms. The A* algorithm and its properties. \nImproving memory efficiency: the IDA* and recursive best first search algorithms. Local search \nand gradient descent. [1 lecture] \nGame-playing. Search in an adversarial environment. The minimax algorithm and its \nshortcomings. Improving minimax using alpha-beta pruning. [1 lecture] \nConstraint satisfaction problems (CSPs). Standardising search problems to a common format. \nThe backtracking algorithm for CSPs. Heuristics for improving the search for a solution. Forward \nchecking, constraint propagation and arc consistency. [1 lecture] \nBackjumping in CSPs. Backtracking, backjumping using Gaschnig\u2019s algorithm, graph-based \nbackjumping. [1 lecture] \nKnowledge representation and reasoning I. How can we represent and deal with commonsense \nknowledge and other forms of knowledge? Semantic networks, frames and rules. How can we \nuse inference in conjunction with a knowledge representation scheme to perform reasoning \nabout the world and thereby to solve problems? Inheritance, forward and backward chaining. [1 \nlecture] \nKnowledge representation and reasoning II. Knowledge representation and reasoning using first \norder logic. The frame, qualification and ramification problems. The situation calculus. [1 lecture] \n\nPlanning I. Methods for planning in advance how to solve a problem. The STRIPS language. \nAchieving preconditions, backtracking and fixing threats by promotion or demotion: the \npartial-order planning algorithm. [1 lecture] \nPlanning II. Incorporating heuristics into partial-order planning. Planning graphs. The \nGRAPHPLAN algorithm. Planning using propositional logic. Planning as a constraint satisfaction \nproblem. [1 lecture] \nNeural Networks I. A brief introduction to supervised learning from examples. Learning as fitting \na curve to data. The perceptron. Learning by gradient descent. [1 lecture] \nNeural Networks II. Multilayer perceptrons and the backpropagation algorithm. [1 lecture] \nObjectives \nAt the end of the course students should: \n \nappreciate the distinction between the popular view of the field and the actual research results; \nappreciate the fact that the computational complexity of most AI problems requires us regularly \nto deal with approximate techniques; \nbe able to design basic problem solving methods based on AI-based search, knowledge \nrepresentation, reasoning, planning, and learning algorithms. \nRecommended reading \nThe recommended text is: \n \n* Russell, S. and Norvig, P. (2010). Artificial intelligence: a modern approach. Prentice Hall (3rd \ned.). \n \nThere are many good books available on artificial intelligence; one alternative is: \n \nPoole, D. L. and Mackworth, A. K. (2017). Artificial intelligence: foundations of computational \nagents. Cambridge University Press (2nd ed.). \n \nFor some of the material you might find it useful to consult more specialised texts, in particular: \n \nDechter, R. (2003). Constraint processing. Morgan Kaufmann. \n \nCawsey, A. (1998). The essence of artificial intelligence. Prentice Hall. \n \nGhallab, M., Nau, D. and Traverso, P. (2004). Automated planning: theory and practice. Morgan \nKaufmann. \n \nBishop, C.M. (2006). Pattern recognition and machine learning. Springer. \n \nBrachman, R.J and Levesque, H.J. (2004). Knowledge Representation and Reasoning. Morgan \nKaufmann.\n \n\nCOMPLEXITY THEORY \n \nAims \nThe aim of the course is to introduce the theory of computational complexity. The course will \nexplain measures of the complexity of problems and of algorithms, based on time and space \nused on abstract models. Important complexity classes will be defined, and the notion of \ncompleteness established through a thorough study of NP-completeness. Applications to \ncryptography will be considered. \n \nSyllabus \nIntroduction to Complexity Theory. Decidability and complexity; lower and upper bounds; the \nChurch-Turing hypothesis. \nTime complexity. Time complexity classes; polynomial time computation and the class P. \nNon-determinism. Non-deterministic machines; the class NP; the problem 3SAT. \nNP-completeness. Reductions and completeness; the Cook-Levin theorem; graph-theoretic \nproblems. \nMore NP-complete problems. 3-colourability; scheduling, matching, set covering, and knapsack.  \ncoNP. Complement classes. NP \u2229 coNP; primality and factorisation. \nCryptography. One-way functions; the class UP. \nSpace complexity. Space complexity classes; Savitch\u2019s theorem. \nHierarchy theorems. Time and space hierarchy theorems. \nRandomised algorithms. The class BPP; derandomisation; error-correcting codes, \ncommunication complexity. \nQuantum Complexity. The classes BQP and QMA. \nInteractive Proofs. IP=PSPACE; Zero Knowledge Proofs; Probabilistically Checkable Proofs \n(PCPs).  \n  \nObjectives \nAt the end of the course students should \n \nbe able to analyse practical problems and classify them according to their complexity; \nbe familiar with the phenomenon of NP-completeness, and be able to identify problems that are \nNP-complete; \nbe aware of a variety of complexity classes and their interrelationships; \nunderstand the role of complexity analysis in cryptography. \nRecommended reading \nA Survey of P vs. NP by Scott Aaronson. \nComputational Complexity: A Modern Approach by Arora and Barak \nComputational Complexity: A Conceptual Perspective by Oded Goldreich \nMathematics and Computation by Avi Wigderson\n \n\nCYBERSECURITY \n \nAims \nIn today\u2019s digital society, computer security is vital for commercial competitiveness, national \nsecurity and privacy of individuals. Protection against both criminal and state-sponsored attacks \nwill need a large cohort of skilled individuals with an understanding of the principles of security \nand with practical experience of the application of these principles. We want you to become one \nof them. In this adversarial game, to defeat the bad guys, the good guys have to be at least as \nskilled at them. Therefore this course has a strong foundation of becoming proficient at actual \nattacks. A theoretical understanding is of course necessary, but without some practice the bad \nguys will run circles around the good guys. In 12 hours I can\u2019t bring you from zero to the stage \nwhere you can invent new attacks and countermeasures, but at least by practicing and \ndissecting common attacks (akin to \u201cstudying the classics\u201d) I\u2019ll give you a feeling for the kind of \nadversarial thinking that is required in this field. Large portions of this course are hands-on: you \nwill need to acquire the relevant skills rather than just reading about it. The recommended \ncourse textbook will be especially helpful to those with no prior low-level hacking experience. \n \nLectures \nIntroduction. \nThe adversarial nature of security; thinking like the attacker; confidentiality, integrity and \navailability; systems-level thinking; Trusted Computing Base; role of cryptography. \n \nFundamentals of access control (chapter 1). \nDiscretionary vs mandatory access control; discretionary access control in POSIX; file \npermissions, file ownership, groups, permission bits. \n \nSoftware security (spanning 4 book chapters) \nSetuid (chapter 2): privileged programs, attack surfaces, exploitable vulnerabilities, privilege \nescalation from regular user to root. \nBuffer overflow (chapter 4): stack memory layout, exploiting a buffer overflow vulnerability, \nguessing unknown addresses, payload, countermeasures and counter-countermeasures. \nReturn to libc and return-oriented programming (chapter 5): exploiting a non-executable stack, \nchaining function calls, chaining ROP gadgets. \nSQL injection (chapter 14): vulnerability and its exploitation, countermeasures, input filtering, \nprepared statements. \n \nAuthentication. \nSomething you know, have, are; passwords, their dominance, their shortcomings and the many \nattempts at replacing them; password cracking; tokens; biometrics; taxonomy of Single Sign-On \nsystems; password managers. \n \nWeb and internet security (spanning 3 book chapters). \nCross-Site Request Forgery (chapter 12): why cross-site requests, CSRF attacks on HTTP GET \nand HTTP POST, countermeasures. \n\nCross-Site Scripting (chapter 13): non-persistent vs persistent XSS, javascript, self-propagating \nXSS worm, countermeasures; \n \nHuman factors. \nUsers are not the enemy; Compliance budget; Prospect theory; 7 principles for systems \nsecurity. \n \nAdditional topics. \nViruses, worms and quines; lockpicking; privilege escalation in physical locks; conclusions. \n \nTo complete the course successfully, students must acquire the practical ability to carry out (as \nopposed to just describing) the exploits mentioned in the syllabus, given a vulnerable system. \nThe low-level hands-on portions of the course are supported by the very helpful course \ntextbook. Those who choose not to study on the recommended textbook will be severely \ndisadvantaged. \n \nRecommended textbook: Wenliang Du. Computer Security: A Hands-on Approach. 3rd Edition. \nISBN: 978-17330039-5-7. Published: 1 May 2022. \n \nhttps://www.handsonsecurity.net. Some chapters freely available online. \n \n \nOptional additional books (not substitutes): \n \nRoss Anderson, Security Engineering 3rd Ed, Wiley, 2020. ISBN: 978-1-119-64278-7. \nhttps://www.cl.cam.ac.uk/~rja14/book.html. Some chapters freely available online. \n \nPaul van Oorschot, Computer Security and the Internet, Springer, 2020. ISBN: \n978-3-030-33648-6 (hardcopy), 978-3-030-33649-3 (eBook). \nhttps://people.scs.carleton.ca/~paulv/toolsjewels.html. All chapters freely available online.\n\nFORMAL MODELS OF LANGUAGE \n \nAims \nThis course studies formal models of language and considers how they might be relevant to the \nprocessing and acquisition of natural languages. The course will extend knowledge of formal \nlanguage theory; introduce several new grammars; and use concepts from information theory to \ndescribe natural language. \n \nLectures \nNatural language and the Chomsky hierarchy 1. Recap classes of language. Closure properties \nof language classes. Recap pumping lemma for regular languages. Discussion of relevance (or \nnot) to natural languages (example embedded clauses in English). \nNatural language and the Chomsky hierarchy 2. Pumping lemma for context free languages. \nDiscussion of relevance (or not) to natural languages (example Swiss-German cross serial \ndependancies). Properties of minimally context sensitive languages. Introduction to tree \nadjoining grammars. \nLanguage processing and context free grammar parsing 1. Recap of context free grammar \nparsing. Language processing predictions based on top down parsing models (example Yngve\u2019s \nlanguage processing predictions). Language processing predictions based on probabilistic \nparsing (example Halle\u2019s language processing predictions). \nLanguage processing and context free grammar parsing 2. Introduction to context free grammar \nequivalent dependancy grammars. Language processing predictions based on Shift-Reduce \nparsing (examples prosodic look-ahead parsers, Parsey McParseface). \nGrammar induction of language classes. Introduction to grammar induction. Discussion of \nrelevance (or not) to natural language acquisition. Gold\u2019s theorem. Introduction to context free \ngrammar equivalent categorial grammars and their learnable classes. \nNatural language and information theory 1. Entropy and natural language typology. Uniform \ninformation density as a predictor for language processing. \nNatural language and information theory 2. Noisy channel encoding as a model for spelling \nerror, translation and language processing. \nVector space models and word vectors. Introduction to word vectors (example Word2Vec). Word \nvectors as predictors for semantic language processing. \nObjectives \nAt the end of the course students should \n \nunderstand how known natural languages relate to formal languages in the Chomsky hierarchy; \nhave knowledge of several context free grammars equivalents; \nunderstand how we might make predictions about language processing and language \nacquisition from formal models; \nknow how to use information theoretic concepts to describe aspects of natural language. \nRecommended reading \n* Jurafsky, D. and Martin, J. (2008). Speech and language processing. Prentice Hall. \nManning, C.D. and Schutze, H. (1999) Foundations of statistical natural language processing. \nMIT Press. \n\nRuslan, M. (2003) The Oxford handbook of computational linguistics. Oxford University Press. \nClark, A., Fox, C. and Lappin, S. (2010) The handbook of computational linguistics and natural \nlanguage processing. Wiley-Blackwell. \nKozen, D. (1997) Automata and computibility. Springer.\n \n\n",
        "5th Semester \nBIOINFORMATICS \nAims \nThis course focuses on algorithms used in Bioinformatics and System Biology. Most of the \nalgorithms are general and can be applied in other fields on multidimensional and noisy data. All \nthe necessary biological terms and concepts useful for the course and the examination will be \ngiven in the lectures. The most important software implementing the described algorithms will be \ndemonstrated. \n \nLectures \nIntroduction to biological data: Bioinformatics as an interesting field in computer science. \nComputing and storing information with DNA (including Adleman\u2019s experiment). \nDynamic programming. Longest common subsequence, DNA global and local alignment, linear \nspace alignment, Nussinov algorithm for RNA, heuristics for multiple alignment. (Vol. 1, chapter \n5) \nSequence database search. Blast. (see notes and textbooks) \nGenome sequencing. De Bruijn graph. (Vol. 1, chapter 3) \nPhylogeny. Distance based algorithms (UPGMA, Neighbour-Joining). Parsimony-based \nalgorithms. Examples in Computer Science. (Vol. 2, chapter 7) \nClustering. Hard and soft K-means clustering, use of Expectation Maximization in clustering, \nHierarchical clustering, Markov clustering algorithm. (Vol. 2, chapter 8) \nGenomics Pattern Matching. Suffix Tree String Compression and the Burrows-Wheeler \nTransform. (Vol. 2, chapter 9) \nHidden Markov Models. The Viterbi algorithm, profile HMMs for sequence alignment, classifying \nproteins with profile HMMs, soft decoding problem, Baum-Welch learning. (Vol. 2, chapter 10) \nObjectives \nAt the end of this course students should \n \nunderstand Bioinformatics terminology; \nhave mastered the most important algorithms in the field; \nbe able to work with bioinformaticians and biologists; \nbe able to find data and literature in repositories. \nRecommended reading \n* Compeau, P. and Pevzner, P.A. (2015). Bioinformatics algorithms: an active learning approach. \nActive Learning Publishers. \nDurbin, R., Eddy, S., Krough, A. and Mitchison, G. (1998). Biological sequence analysis: \nprobabilistic models of proteins and nucleic acids. Cambridge University Press. \nJones, N.C. and Pevzner, P.A. (2004). An introduction to bioinformatics algorithms. MIT Press. \nFelsenstein, J. (2003). Inferring phylogenies. Sinauer Associates.\n \n\nBUSINESS STUDIES \n \nAims \nHow to start and run a computer company; the aims of this course are to introduce students to \nall the things that go to making a successful project or product other than just the programming. \nThe course will survey some of the issues that students are likely to encounter in the world of \ncommerce and that need to be considered when setting up a new computer company. \n \nSee also Business Seminars in the Easter Term. \n \nLectures \nSo you\u2019ve got an idea? Introduction. Why are you doing it and what is it? Types of company. \nMarket analysis. The business plan. \nMoney and tools for its management. Introduction to accounting: profit and loss, cash flow, \nbalance sheet, budgets. Sources of finance. Stocks and shares. Options and futures. \nSetting up: legal aspects. Company formation. Brief introduction to business law; duties of \ndirectors. Shares, stock options, profit share schemes and the like. Intellectual Property Rights, \npatents, trademarks and copyright. Company culture and management theory. \nPeople. Motivating factors. Groups and teams. Ego. Hiring and firing: employment law. \nInterviews. Meeting techniques. \nProject planning and management. Role of a manager. PERT and GANTT charts, and critical \npath analysis. Estimation techniques. Monitoring. \nQuality, maintenance and documentation. Development cycle. Productization. Plan for quality. \nPlan for maintenance. Plan for documentation. \nMarketing and selling. Sales and marketing are different. Marketing; channels; marketing \ncommunications. Stages in selling. Control and commissions. \nGrowth and exit routes. New markets: horizontal and vertical expansion. Problems of growth; \nsecond system effects. Management structures. Communication. Exit routes: acquisition, \nfloatation, MBO or liquidation. Futures: some emerging ideas for new computer businesses. \nSummary. Conclusion: now you do it! \nObjectives \nAt the end of the course students should \n \nbe able to write and analyse a business plan; \nknow how to construct PERT and GANTT diagrams and perform critical path analysis; \nappreciate the differences between profitability and cash flow, and have some notion of budget \nestimation; \nhave an outline view of company formation, share structure, capital raising, growth and exit \nroutes; \nhave been introduced to concepts of team formation and management; \nknow about quality documentation and productization processes; \nunderstand the rudiments of marketing and the sales process. \nRecommended reading \n\nLang, J. (2001). The high-tech entrepreneur\u2019s handbook: how to start and run a high-tech \ncompany. FT.COM/Prentice Hall. \n \nStudents will be expected to be able to use Microsoft Excel and Microsoft Project. \n \nFor additional reading on a lecture-by-lecture basis, please see the course website. \n \nStudents are strongly recommended to enter the CU Entrepreneurs Business Ideas Competition \nhttp://www.cue.org.uk/\n \n\nDENOTATIONAL SEMANTICS \n \nAims \nThe aims of this course are to introduce domain theory and denotational semantics, and to \nshow how they provide a mathematical basis for reasoning about the behaviour of programming \nlanguages. \n \nLectures \nIntroduction.The denotational approach to the semantics of programming \nlanguages.Recursively defined objects as limits of successive approximations. \nLeast fixed points.Complete partial orders (cpos) and least elements.Continuous functions and \nleast fixed points. \nConstructions on domains.Flat domains.Product domains. Function domains. \nScott induction.Chain-closed and admissible subsets of cpos and domains.Scott\u2019s fixed-point \ninduction principle. \nPCF.The Scott-Plotkin language PCF.Evaluation. Contextual equivalence. \nDenotational semantics of PCF.Denotation of types and terms. Compositionality. Soundness \nwith respect to evaluation. [2 lectures]. \nRelating denotational and operational semantics.Formal approximation relation and its \nfundamental property.Computational adequacy of the PCF denotational semantics with respect \nto evaluation. Extensionality properties of contextual equivalence. [2 lectures]. \nFull abstraction.Failure of full abstraction for the domain model. PCF with parallel or. \nObjectives \nAt the end of the course students should \n \nbe familiar with basic domain theory: cpos, continuous functions, admissible subsets, least fixed \npoints, basic constructions on domains; \nbe able to give denotational semantics to simple programming languages with simple types; \nbe able to apply denotational semantics; in particular, to understand the use of least fixed points \nto model recursive programs and be able to reason about least fixed points and simple \nrecursive programs using fixed point induction; \nunderstand the issues concerning the relation between denotational and operational semantics, \nadequacy and full abstraction, especially with respect to the language PCF. \nRecommended reading \nWinskel, G. (1993). The formal semantics of programming languages: an introduction. MIT \nPress. \nGunter, C. (1992). Semantics of programming languages: structures and techniques. MIT Press. \nTennent, R. (1991). Semantics of programming languages. Prentice Hall.\n \n\nINFORMATION THEORY \n \nAims \nThis course introduces the principles and applications of information theory: how information is \nmeasured in terms of probability and various entropies, how these are used to calculate the \ncapacity of communication channels, with or without noise, and to measure how much random \nvariables reveal about each other. Coding schemes including error correcting codes are studied \nalong with data compression, spectral analysis, transforms, and wavelet coding. Applications of \ninformation theory are reviewed, from astrophysics to pattern recognition. \n \nLectures \nInformation, probability, uncertainty, and surprise. How concepts of randomness and uncertainty \nare related to information. How the metrics of information are grounded in the rules of \nprobability. Shannon Information. Weighing problems and other examples. \nEntropy of discrete variables. Definition and link to Shannon information. Joint entropy, Mutual \ninformation. Visual depictions of the relationships between entropy types. Why entropy gives \nfundamental measures of information content. \nSource coding theorem and data compression; prefix, variable-, and fixed-length codes. \nInformation rates; Asymptotic equipartition principle; Symbol codes; Huffman codes and the \nprefix property. Binary symmetric channels. Capacity of a noiseless discrete channel. Stream \ncodes. \nNoisy discrete channel coding. Joint distributions; mutual information; Conditional Entropy; \nError-correcting codes; Capacity of a discrete channel. Noisy channel coding theorem. \nEntropy of continuous variables. Differential entropy; Mutual information; Channel Capacity; \nGaussian channels. \nEntropy for comparing probability distributions and for machine learning. Relative entropy/KL \ndivergence; cross-entropy; use as loss function. \nApplications of information theory in other sciences.  \nObjectives \nAt the end of the course students should be able to \n \ncalculate the information content of a random variable from its probability distribution; \nrelate the joint, conditional, and marginal entropies of variables in terms of their coupled \nprobabilities; \ndefine channel capacities and properties using Shannon\u2019s Theorems; \nconstruct efficient codes for data on imperfect communication channels; \ngeneralize the discrete concepts to continuous signals on continuous channels; \nunderstand encoding and communication schemes in terms of the spectral properties of signals \nand channels; \ndescribe compression schemes, and efficient coding using wavelets and other representations \nfor data. \nRecommended reading \n  Mackay's Information Theory, inference and Learning Algorithms \n \n\n Stone's Information Theory: A Tutorial Introduction.\n \n\nLATEX AND JULIA \n \nAims \nIntroduction to two widely-used languages for typesetting dissertations and scientific \npublications, for prototyping numerical algorithms and to visualize results. \n \nLectures \nLATEX. Workflow example, syntax, typesetting conventions, non-ASCII characters, document \nstructure, packages, mathematical typesetting, graphics and figures, cross references, build \ntools. \nJulia. Tools for technical computing and visualization. The Array type and its operators, 2D/3D \nplotting, functions and methods, notebooks, packages, vectorized audio demonstration. \nObjectives \nStudents should be able to avoid the most common LATEX mistakes, to prototype simple image \nand signal-processing algorithms in Julia, and to visualize the results. \n \nRecommended reading \n* Lamport, L. (1994). LATEX \u2013 a documentation preparation system user\u2019s guide and reference \nmanual. Addison-Wesley (2nd ed.). \nMittelbach, F., et al. (2023). The LATEX companion. Addison-Wesley (3rd ed.).\n \n\nPRINCIPLES OF COMMUNICATIONS \n \nAims \nThis course aims to provide a detailed understanding of the underlying principles for how \ncommunications systems operate. Practical examples (from wired and wireless \ncommunications, the Internet, and other communications systems) are used to illustrate the \nprinciples. \n \nLectures \nIntroduction. Course overview. Abstraction, layering. Review of structure of real networks, links, \nend systems and switching systems. [1 lecture] \nRouting. Central versus Distributed Routing Policy Routing. Multicast Routing Circuit Routing [6 \nlectures] \nFlow control and resource optimisation. 1[Control theory] is a branch of engineering familiar to \npeople building dynamic machines. It can be applied to network traffic. Stemming the flood, at \nsource, sink, or in between? Optimisation as a model of networkand user. TCP in the wild. [3 \nlectures] \nPacket Scheduling. Design choices for scheduling and queue management algorithms for \npacket forwarding, and fairness. [2 lectures] \nThe big picture for managing traffic. Economics and policy are relevant to networks in many \nways. Optimisation and game theory are both relevant topics discussed here. [2 lectures] \nSystem Structures and Summary. Abstraction, layering. The structure of real networks, links, \nend systems and switching. [2 lectures] \n1 Control theory was not taught and will not be the subject of any exam question. \n \nObjectives \nAt the end of the course students should be able to explain the underlying design and behaviour \nof protocols and networks, including capacity, topology, control and use. Several specific \nmathematical approaches are covered (control theory, optimisation). \n \nRecommended reading \n* Keshav, S. (2012). Mathematical Foundations of Computer Networking. Addison Wesley. ISBN \n9780321792105 \nBackground reading: \nKeshav, S. (1997). An engineering approach to computer networking. Addison-Wesley (1st ed.). \nISBN 0201634422 \nStevens, W.R. (1994). TCP/IP illustrated, vol. 1: the protocols. Addison-Wesley (1st ed.). ISBN \n0201633469\n \n\nTYPES \n \nAims \nThe aim of this course is to show by example how type systems for programming languages can \nbe defined and their properties developed, using techniques that were introduced in the Part IB \ncourse on Semantics of Programming Languages. The emphasis is on type systems for \nfunctional languages and their connection to constructive logic. \n \nLectures \nIntroduction. The role of type systems in programming languages. Review of rule-based \nformalisation of type systems. [1 lecture] \nPropositions as types. The Curry-Howard correspondence between intuitionistic propositional \ncalculus and simply-typed lambda calculus. Inductive types and iteration. Consistency and \ntermination. [2 lectures] \nPolymorphic lambda calculus (PLC). PLC syntax and reduction semantics. Examples of \ndatatypes definable in the polymorphic lambda calculus. Type inference. [3 lectures] \nMonads and effects. Explicit versus implicit effects. Using monadic types to control effects. \nReferences and polymorphism. Recursion and looping. [2 lectures] \nContinuations and classical logic. First-class continuations and control operators. Continuations \nas Curry-Howard for classical logic. Continuation-passing style. [2 lectures] \nDependent types. Dependent function types. Indexed datatypes. Equality types and combining \nproofs with programming. [2 lectures] \nObjectives \nAt the end of the course students should \n \nbe able to use a rule-based specification of a type system to carry out type checking and type \ninference; \nunderstand by example the Curry-Howard correspondence between type systems and logics; \nunderstand how types can be used to control side-effects in programming; \nappreciate the expressive power of parametric polymorphism and dependent types. \nRecommended reading \n* Pierce, B.C. (2002). Types and programming languages. MIT Press. \nPierce, B. C. (Ed.) (2005). Advanced Topics in Types and Programming Languages. MIT Press. \nGirard, J-Y. (tr. Taylor, P. and Lafont, Y.) (1989). Proofs and types. Cambridge University Press.\n\nADVANCED DATA SCIENCE \n \nPracticals \nThe module will have a practical session for each of the three stages of the pipeline. Each of the \nparts will be tied into an aspect of the final project. \n \nObjectives \nAt the end of the course students will be familiar with the purpose of data science, how it differs \nfrom the closely related fields of machine learning, statistics and artificial intelligence and what a \ntypical data analysis pipeline looks like in practice. As well as emphasising the importance of \nanalysis methods we will introduce a formalism for organising how data science is done in \npractice and what the different aspects the data scientist faces when giving data-driven answers \nto questions of interest. \n \nRecommended reading \nSimon Rogers et al. (2016). A First Course in Machine Learning, Second Edition. [] Chapman \nand Hall/CRC, nil \nShai Shalev-Shwartz et al. (2014). Understanding Machine Learning: From Theory to \nAlgorithms. New York, NY, USA: Cambridge  University Press \nChristopher M. Bishop (2006). Pattern Recognition and Machine Learning (Information Science \nand Statistics). Secaucus, NJ, USA: Springer-Verlag New York, Inc. \nAssessment \nPractical There will be four practicals contributing 20% to the final module mark. Data science is \na topic where there is rarely a single \ncorrect answer therefore the important thing is that an informed attempt has been made to \naddress the questions that can be supported and motivated. \nReport The course will be concluded by an individual report covering the material in the course \nthrough \npractical exercise working with real data. This report will make up 80% of the mark for the \ncourse. \n \n \n\nAFFECTIVE ARTIFICIAL INTELLIGENCE \n \nSynopsis \nAffective Artificial Intelligence (Affective AI) aims to imbue machines with social and emotional \nintelligence (EQ). More specifically, Affective AI aims to create artificially intelligent systems and \nmachines that can recognize, interpret, process, and simulate human social signals and \nbehaviours, expressions, and emotions, to enhance human-AI interaction and communication. \n\\r\\n\\r\\nTo achieve this goal, Affective AI draws upon various scientific disciplines, including \nmachine learning, computer vision, speech / natural language / signal processing, psychology \nand cognitive science, and ethics and social sciences. \n \n  \nBackground & Aims \nAffective Artificial Intelligence (Affective AI) aims to imbue machines with social and emotional \nintelligence (EQ). More specifically, Affective AI aims to create artificially intelligent systems and \nmachines that can recognize, interpret, process, and simulate human social signals and \nbehaviours, expressions, and emotions, to enhance human-AI interaction and communication.  \n \nTo achieve this goal, Affective AI draws upon various scientific disciplines, including machine \nlearning, computer vision, speech / natural language / signal processing, psychology and \ncognitive science, and ethics and social sciences. \n \nAffective AI has direct applications in and implications for the design of innovative interactive \ntechnology (e.g., interaction with chat bots, virtual agents, robots), single and multi-user smart \nenvironments ( e.g., in-car/ virtual / augmented / mixed reality, serious games), public speaking \nand cognitive training, and clinical and biomedical studies (e.g., autism, depression, pain). \n \nThe aim of this module is to impart knowledge and ability needed to make informed choices of \nmodels, data, and machine learning techniques for sensing, recognition, and generation of \naffective and social behaviour (e.g., smile, frown, head nodding/shaking, \nagreement/disagreement) in order to create Affectively intelligent AI systems, with a \nconsideration for various ethical issues (e.g., privacy, bias) arising from the real-world \ndeployment of these systems. \n \n  \n \nSyllabus \nThe following list provides a representative list of topics: \n \nIntroduction, definitions, and overview \nTheories from various disciplines \nSensing from multiple modalities (e.g., vision, audio, bio signals, text) \nData acquisition and annotation \nSignal processing / feature extraction \n\nLearning / prediction / recognition and evaluation \nBehaviour synthesis / generation (e.g., for embodied agents / robots) \nAdvanced topics and ethical considerations (e.g., bias and fairness) \nCross-disciplinary applications (via seminar presentations and discussions) \nGuest lectures (diverse topics \u2013 changes each year) \nHands-on research and programming work (i.e., mini project & report)  \n \n  \n \nObjectives \nOn completion of this module, students will: \n \nunderstand and demonstrate knowledge in key characteristics of affectively intelligent AI, which \ninclude: \nRecognition: How to equip Affective AI systems with capabilities of analysing facial expressions, \nvocal intonations, gestures, and other physiological signals to infer human affective states? \nGeneration: How to enable affectively intelligent AI simulate expressions of emotions in \nmachines, allowing them to respond in a more human-like manner, such as virtual agents and \nhumanoid robots, showing empathy or sympathy? \nAdaptation and Personalization: How to enable affectively intelligent AI adapt system responses \nbased on users' affective states and/or needs, or tailor interactions and experiences to individual \nusers based on their expressivity / emotional profiles, personalities, and past interactions? \nEmpathetic Communication: How to design Affective AI systems to communicate with users in a \nway that demonstrates empathy, understanding, and sensitivity to their affective states and \nneeds? \nEthical and societal considerations: What are the various human differences, ethical guidelines, \nand societal impacts that need to be considered when designing and deploying Affective AI to \nensure that these systems respect users' privacy, autonomy, and well-being? \ncomprehend and apply (appropriate) methods for collection, analysis, representation, and \nevaluation of human affective and communicative behavioural data. \nenhance programming skills for creating and implementing (components of) Affective AI \nsystems. \ndemonstrate critical thinking, analysis and synthesis while deciding on 'when' and 'how' to \nincorporate human affect and social signals in a specific AI system context and gain practical \nexperience in proposing and justifying computational solution(s) of suitable nature and scope. \n  \n \nAssessment - Part II Students \nSeminar presentation: 20% \nParticipating in Q&A and discussions: 10% \nMid-term mini project report and presentation (as a team of 2): 10%  \nFinal mini project report & code (as a team of 2): 60% \n \n\nCATEGORY THEORY \n \nAims \nCategory theory provides a unified treatment of mathematical properties and constructions that \ncan be expressed in terms of 'morphisms' between structures. It gives a precise framework for \ncomparing one branch of mathematics (organized as a category) with another and for the \ntransfer of problems in one area to another. Since its origins in the 1940s motivated by \nconnections between algebra and geometry, category theory has been applied to diverse fields, \nincluding computer science, logic and linguistics. This course introduces the basic notions of \ncategory theory: adjunction, natural transformation, functor and category. We will use category \ntheory to organize and develop the kinds of structure that arise in models and semantics for \nlogics and programming languages. \n \nSyllabus \nIntroduction; some history. Definition of category. The category of sets and functions. \nCommutative diagrams. Examples of categories: preorders and monotone functions; monoids \nand monoid homomorphisms; a preorder as a category; a monoid as a category. Definition of \nisomorphism. Informal notion of a 'category-theoretic' property. \nTerminal objects. The opposite of a category and the duality principle. Initial objects. Free \nmonoids as initial objects. \nBinary products and coproducts. Cartesian categories. \nExponential objects: in the category of sets and in general. Cartesian closed categories: \ndefinition and examples. \nIntuitionistic Propositional Logic (IPL) in Natural Deduction style. Semantics of IPL in a cartesian \nclosed preorder. \nSimply Typed Lambda Calculus (STLC). The typing relation. Semantics of STLC types and \nterms in a cartesian closed category (ccc). The internal language of a ccc. The \nCurry-Howard-Lawvere correspondence. \nFunctors. Contravariance. Identity and composition for functors. \nSize: small categories and locally small categories. The category of small categories. Finite \nproducts of categories. \nNatural transformations. Functor categories. The category of small categories is cartesian \nclosed. \nHom functors. Natural isomorphisms. Adjunctions. Examples of adjoint functors. Theorem \ncharacterising the existence of right (respectively left) adjoints in terms of a universal property. \nDependent types. Dependent product sets and dependent function sets as adjoint functors. \nEquivalence of categories. Example: the category of I-indexed sets and functions is equivalent \nto the slice category Set/I. \nPresheaves. The Yoneda Lemma. Categories of presheaves are cartesian closed. \nMonads. Modelling notions of computation as monads. Moggi's computational lambda calculus. \nObjectives \nOn completion of this module, students should: \n \n\nbe familiar with some of the basic notions of category theory and its connections with logic and \nprogramming language semantics \nAssessment \na graded exercise sheet (25% of the final mark), and \na take-home test (75%) \nRecommended reading \nAwodey, S. (2010). Category theory. Oxford University Press (2nd ed.). \n \nCrole, R. L. (1994). Categories for types. Cambridge University Press. \n \nLambek, J. and Scott, P. J. (1986). Introduction to higher order categorical logic. Cambridge \nUniversity Press. \n \nPitts, A. M. (2000). Categorical Logic. Chapter 2 of S. Abramsky, D. M. Gabbay and T. S. E. \nMaibaum (Eds) Handbook of Logic in Computer Science, Volume 5. Oxford University Press. \n(Draft copy available here.) \n \nClass Size \nThis module can accommodate upto 15 Part II students plus 15 MPhil / Part III students.\n\nDIGITAL SIGNAL PROCESSING \n \nAims \nThis course teaches basic signal-processing principles necessary to understand many modern \nhigh-tech systems, with examples from audio processing, image coding, radio communication, \nradar, and software-defined radio. Students will gain practical experience from numerical \nexperiments in programming assignments (in Julia, MATLAB or NumPy). \n \nLectures \nSignals and systems. Discrete sequences and systems: types and properties. Amplitude, \nphase, frequency, modulation, decibels, root-mean square. Linear time-invariant systems, \nconvolution. Some examples from electronics, optics and acoustics. \nPhasors. Eigen functions of linear time-invariant systems. Review of complex arithmetic. \nPhasors as orthogonal base functions. \nFourier transform. Forms and properties of the Fourier transform. Convolution theorem. Rect \nand sinc. \nDirac\u2019s delta function. Fourier representation of sine waves, impulse combs in the time and \nfrequency domain. Amplitude-modulation in the frequency domain. \nDiscrete sequences and spectra. Sampling of continuous signals, periodic signals, aliasing, \ninterpolation, sampling and reconstruction, sample-rate conversion, oversampling, spectral \ninversion. \nDiscrete Fourier transform. Continuous versus discrete Fourier transform, symmetry, linearity, \nFFT, real-valued FFT, FFT-based convolution, zero padding, FFT-based resampling, \ndeconvolution exercise. \nSpectral estimation. Short-time Fourier transform, leakage and scalloping phenomena, \nwindowing, zero padding. Audio and voice examples. DTFM exercise. \nFinite impulse-response filters. Properties of filters, implementation forms, window-based FIR \ndesign, use of frequency-inversion to obtain high-pass filters, use of modulation to obtain \nband-pass filters. \nInfinite impulse-response filters. Sequences as polynomials, z-transform, zeros and poles, some \nanalog IIR design techniques (Butterworth, Chebyshev I/II, elliptic filters, second-order cascade \nform). \nBand-pass signals. Band-pass sampling and reconstruction, IQ up and down conversion, \nsuperheterodyne receivers, software-defined radio front-ends, IQ representation of AM and FM \nsignals and their demodulation. \nDigital communication. Pulse-amplitude modulation. Matched-filter detector. Pulse shapes, \ninter-symbol interference, equalization. IQ representation of ASK, BSK, PSK, QAM and FSK \nsignals. [2 hours] \nRandom sequences and noise. Random variables, stationary and ergodic processes, \nautocorrelation, cross-correlation, deterministic cross-correlation sequences, filtered random \nsequences, white noise, periodic averaging. \nCorrelation coding. Entropy, delta coding, linear prediction, dependence versus correlation, \nrandom vectors, covariance, decorrelation, matrix diagonalization, eigen decomposition, \n\nKarhunen-Lo\u00e8ve transform, principal component analysis. Relation to orthogonal transform \ncoding using fixed basis vectors, such as DCT. \nLossy versus lossless compression. What information is discarded by human senses and can \nbe eliminated by encoders? Perceptual scales, audio masking, spatial resolution, colour \ncoordinates, some demonstration experiments. \nQuantization, image coding standards. Uniform and logarithmic quantization, A/\u00b5-law coding, \ndithering, JPEG. \nObjectives \napply basic properties of time-invariant linear systems; \nunderstand sampling, aliasing, convolution, filtering, the pitfalls of spectral estimation; \nexplain the above in time and frequency domain representations; \nuse filter-design software; \nvisualize and discuss digital filters in the z-domain; \nuse the FFT for convolution, deconvolution, filtering; \nimplement, apply and evaluate simple DSP applications; \nfamiliarity with a number of signal-processing concepts used in digital communication systems \nRecommended reading \nLyons, R.G. (2010). Understanding digital signal processing. Prentice Hall (3rd ed.). \nOppenheim, A.V. and Schafer, R.W. (2007). Discrete-time digital signal processing. Prentice \nHall (3rd ed.). \nStein, J. (2000). Digital signal processing \u2013 a computer science perspective. Wiley. \n \nClass size \nThis module can accommodate a maximum of 24 students (16 Part II students and 8 MPhil \nstudents) \n \nAssessment - Part II students \nThree homework programming assignments, each comprising 20% of the mark \nWritten test, comprising 40% of the total mark.\n \n\nMACHINE VISUAL PERCEPTION \n \nAims \nThis course aims at introducing the theoretical foundations and practical techniques for machine \nperception, the capability of computers to interpret data resulting from sensor measurements. \nThe course will teach the fundamentals and modern techniques of machine perception, i.e. \nreconstructing the real world starting from sensor measurements with a focus on machine \nperception for visual data. The topics covered will be image/geometry representations for \nmachine perception, semantic segmentation, object detection and recognition, geometry \ncapture, appearance modeling and acquisition, motion detection and estimation, \nhuman-in-the-loop machine perception, select topics in applied machine perception. \n \nMachine perception/computer vision is a rapidly expanding area of research with real-world \napplications. An understanding of machine perception is also important for robotics, interactive \ngraphics (especially AR/VR), applied machine learning, and several other fields and industries. \nThis course will provide a fundamental understanding of and practical experience with the \nrelevant techniques. \n \nLearning outcomes \nStudents will understand the theoretical underpinnings of the modern machine perception \ntechniques for reconstructing models of reality starting from an incomplete and imperfect view of \nreality. \nStudents will be able to apply machine perception theory to solve practical problems, e.g. \nclassification of images, geometry capture. \nStudents will gain an understanding of which machine perception techniques are appropriate for \ndifferent tasks and scenarios. \nStudents will have hands-on experience with some of these techniques via developing a \nfunctional machine perception system in their projects. \nStudents will have practical experience with the current prominent machine perception \nframeworks. \nSyllabus \nThe fundamentals of machine learning for machine perception \nDeep neural networks and frameworks for machine perception \nSemantic segmentation of objects and humans \nObject detection and recognition \nMotion estimation, tracking and recognition \n3D geometry capture \nAppearance modeling and acquisition \nSelect topics in applied machine perception \nAssessment \nA practical exercise, worth 20% of the mark. This will cover the basics and theory of machine \nperception and some of the practical techniques the students will likely use in their projects. This \nis individual work. No GPU hours will be needed for the practical work. \nA machine perception project worth 80% of the marks: \n\nCourse projects will be selected by the students following the possible project themes proposed \nby the lecturer, and will be checked by the lecturer for appropriateness.  \nThe students will form groups of 2-3 to design, implement, report, and present a project to tackle \na given task in machine perception. \nThe final mark will be composed of an implementation/report mark (60%) and a presentation \nmark (20%). Each team member will be evaluated based on her/his contribution. \nEach project will have extensions to be completed only by the ACS students. Each student will \nwrite a different part of the report, whose author will be clearly marked. Each student will further \nsummarise her/his contributions to the project in the same report. \nRecommended Reading List \nComputer Vision: Algorithms and Applications, Richard Szeliski, Springer, 2010. \nDeep Learning, Ian Goodfellow, Yoshua Bengio, and Aaron Courville, MIT Press, 2016. \nMachine Learning and Visual Perception, Baochang Zhang, De Gruyter, 2020.\n \n\nNATURAL LANGUAGE PROCESSING \n \nAims \nThis course introduces the fundamental techniques of natural language processing. It aims to \nexplain the potential and the main limitations of these techniques. Some current research issues \nare introduced and some current and potential applications discussed and evaluated. Students \nwill also be introduced to practical experimentation in natural language processing. \n \nLectures \nOverview. Brief history of NLP research, some current applications, components of NLP \nsystems. \nMorphology and Finite State Techniques. Morphology in different languages, importance of \nmorphological analysis in NLP, finite-state techniques in NLP. \nPart-of-Speech Tagging and Log-Linear Models. Lexical categories, word tagging, corpora and \nannotations, empirical evaluation. \nPhrase Structure and Structure Prediction. Phrase structures, structured prediction, context-free \ngrammars, weights and probabilities. Some limitations of context-free grammars. \nDependency Parsing. Dependency structure, grammar-free parsing, incremental processing.  \nGradient Descent and Neural Nets. Parameter optimisation by gradient descent. Non-linear \nfunctions with neural network layers. Log-linear model as softmax layer. Current findings of \nNeural NLP. \nWord representations. Representing words with vectors, count-based and prediction-based \napproaches, similarity metrics. \nRecurrent Neural Networks. Modelling sequences, parameter sharing in recurrent neural \nnetworks, neural language models, word prediction. \nCompositional Semantics. Logical representations, compositional semantics, lambda calculus, \ninference and robust entailment. \nLexical Semantics. Semantic relations, WordNet, word senses. \nDiscourse. Discourse relations, anaphora resolution, summarization. \nNatural Language Generation. Challenges of natural language generation (NLG), tasks in NLG, \nsurface realisation. \nPractical and assignments. Students will build a natural language processing system which will \nbe trained and evaluated on supplied data. The system will be built from existing components, \nbut students will be expected to compare approaches and some programming will be required \nfor this. Several assignments will be set during the practicals for assessment. \nObjectives \nBy the end of the course students should: \n \nbe able to discuss the current and likely future performance of several NLP applications; \nbe able to describe briefly a fundamental technique for processing language for several \nsubtasks, such as morphological processing, parsing, word sense disambiguation etc.; \nunderstand how these techniques draw on and relate to other areas of computer science. \nRecommended reading \n\nJurafsky, D. & Martin, J. (2023). Speech and language processing. Prentice Hall (3rd ed. draft, \nonline). \n \nAssessment - Part II Students \nAssignment 1 - 10% of marks \nAssignment 2 - 60% of marks \nAssignment 3 - 30% of marks\n \n\nPRACTICAL RESEARCH IN HUMAN-CENTRED AI \n \nAims \nThis is an advanced course in human-computer interaction, with a specialist focus on intelligent \nuser interfaces and interaction with machine-learning and artificial intelligence technologies. The \nformat will be largely Practical, with students carrying out a mini-project involving empirical \nresearch investigation. These studies will investigate human interaction with some kind of \nmodel-based system for planning, decision-making, automation etc. Possible study formats \nmight include: System evaluation, Field observation, Hypothesis testing experiment, Design \nintervention or Corpus analysis, following set examples from recent research publications. \nProject work will be formally evaluated through a report and presentation. \n \nLectures \n(note that Lectures 2-7 also include one hour class discussion of practical work) \n        \u2022 Current research themes in intelligent user interfaces \n        \u2022 Program synthesis \n        \u2022 Mixed initiative interaction \n        \u2022 Interpretability / explainable AI \n        \u2022 Labelling as a fundamental problem \n        \u2022 Machine learning risks and bias \n        \u2022 Visualisation and visual analytics \n        \u2022 Student research presentations \n \nObjectives \nBy the end of the course students should: \n        \u2022 be familiar with current state of the art in intelligent interactive systems \n        \u2022 understand the human factors that are most critical in the design of such systems \n        \u2022 be able to evaluate evidence for and against the utility of novel systems \n        \u2022 have experience of conducting user studies meeting the quality criteria of this field \n        \u2022 be able to write up and present user research in a professional manner \n \nClass Size \nThis module can accommodate upto 20 Part II, Part III and MPhil students. \n \nRecommended reading \nBrad A. Myers and Richard McDaniel (2000). Demonstrational Interfaces: Sometimes You Need \na Little Intelligence, Sometimes You Need a Lot. \n \nAlan Blackwell (2024). Moral Codes: Designing alternatives to AI \n \nAssessment - Part II Students \nThe format will be largely practical, with students carrying out an individual mini-project involving \nempirical research investigation. \n \n\nAssignment 1: six incremental submissions which together contribute 20% to the final module \nmark. \n \nAssignment 2: Final report - 80% of the final module mark \n \n\n",
        "6th Semester \nADVANCED COMPUTER ARCHITECTURE \nAims \nThis course examines the techniques and underlying principles that are used to design \nhigh-performance computers and processors. Particular emphasis is placed on understanding \nthe trade-offs involved when making design decisions at the architectural level. A range of \nprocessor architectures are explored and contrasted. In each case we examine their merits and \nlimitations and how ultimately the ability to scale performance is restricted. \n \nLectures \nIntroduction. The impact of technology scaling and market trends. \nFundamentals of Computer Design. Amdahl\u2019s law, energy/performance trade-offs, ISA design. \nAdvanced pipelining. Pipeline hazards; exceptions; optimal pipeline depth; branch prediction; \nthe branch target buffer [2 lectures] \nSuperscalar techniques. Instruction-Level Parallelism (ILP); superscalar processor architecture \n[2 lectures] \nSoftware approaches to exploiting ILP. VLIW architectures; local and global instruction \nscheduling techniques; predicated instructions and support for speculative compiler \noptimisations. \nMultithreaded processors. Coarse-grained, fine-grained, simultaneous multithreading \nThe memory hierarchy. Caches; programming for caches; prefetching [2 lectures] \nVector processors. Vector machines; short vector/SIMD instruction set extensions; stream \nprocessing \nChip multiprocessors. The communication model; memory consistency models; false sharing; \nmultiprocessor memory hierarchies; cache coherence protocols; synchronization [2 lectures] \nOn-chip interconnection networks. Bus-based interconnects; on-chip packet switched networks \nSpecial-purpose architectures. Converging approaches to computer design \nObjectives \nAt the end of the course students should \n \nunderstand what determines processor design goals; \nappreciate what constrains the design process and how architectural trade-offs are made within \nthese constraints; \nbe able to describe the architecture and operation of pipelined and superscalar processors, \nincluding techniques such as branch prediction, register renaming and out-of-order execution; \nhave an understanding of vector, multithreaded and multi-core processor architectures; \nfor the architectures discussed, understand what ultimately limits their performance and \napplication domain. \nRecommended reading \n* Hennessy, J. and Patterson, D. (2012). Computer architecture: a quantitative approach. \nElsevier (5th ed.) ISBN 9780123838728. (the 3rd and 4th editions are also good)\n \n\nCRYPTOGRAPHY \n \nAims \nThis course provides an overview of basic modern cryptographic techniques and covers \nessential concepts that users of cryptographic standards need to understand to achieve their \nintended security goals. \n \nLectures \nCryptography. Overview, private vs. public-key ciphers, MACs vs. signatures, certificates, \ncapabilities of adversary, Kerckhoffs\u2019 principle. \nClassic ciphers. Attacks on substitution and transposition ciphers, Vigen\u00e9re. Perfect secrecy: \none-time pads. \nPrivate-key encryption. Stream ciphers, pseudo-random generators, attacking \nlinear-congruential RNGs and LFSRs. Semantic security definitions, oracle queries, advantage, \ncomputational security, concrete-security proofs. \nBlock ciphers. Pseudo-random functions and permutations. Birthday problem, random \nmappings. Feistel/Luby-Rackoff structure, DES, TDES, AES. \nChosen-plaintext attack security. Security with multiple encryptions, randomized encryption. \nModes of operation: ECB, CBC, OFB, CNT. \nMessage authenticity. Malleability, MACs, existential unforgeability, CBC-MAC, ECBC-MAC, \nCMAC, birthday attacks, Carter-Wegman one-time MAC. \nAuthenticated encryption. Chosen-ciphertext attack security, ciphertext integrity, \nencrypt-and-authenticate, authenticate-then-encrypt, encrypt-then-authenticate, padding oracle \nexample, GCM. \nSecure hash functions. One-way functions, collision resistance, padding, Merkle-Damg\u00e5rd \nconstruction, sponge function, duplex construct, entropy pool, SHA standards. \nApplications of secure hash functions. HMAC, stream authentication, Merkle tree, commitment \nprotocols, block chains, Bitcoin. \nKey distribution problem. Needham-Schroeder protocol, Kerberos, hardware-security modules, \npublic-key encryption schemes, CPA and CCA security for asymmetric encryption. \nNumber theory, finite groups and fields. Modular arithmetic, Euclid\u2019s algorithm, inversion, \ngroups, rings, fields, GF(2n), subgroup order, cyclic groups, Euler\u2019s theorem, Chinese \nremainder theorem, modular roots, quadratic residues, modular exponentiation, easy and \ndifficult problems. [2 lectures] \nDiscrete logarithm problem. Baby-step-giant-step algorithm, computational and decision \nDiffie-Hellman problem, DH key exchange, ElGamal encryption, hybrid cryptography, Schnorr \ngroups, elliptic-curve systems, key sizes. [2 lectures] \nTrapdoor permutations. Security definition, turning one into a public-key encryption scheme, \nRSA, attacks on \u201ctextbook\u201d RSA, RSA as a trapdoor permutation, optimal asymmetric \nencryption padding, common factor attacks. \nDigital signatures. One-time signatures, RSA signatures, Schnorr identification scheme, \nElGamal signatures, DSA, PS3 hack, certificates, PKI. \nObjectives \nBy the end of the course students should \n\n \nbe familiar with commonly used standardized cryptographic building blocks; \nbe able to match application requirements with concrete security definitions and identify their \nabsence in naive schemes; \nunderstand various adversarial capabilities and basic attack algorithms and how they affect key \nsizes; \nunderstand and compare the finite groups most commonly used with discrete-logarithm \nschemes; \nunderstand the basic number theory underlying the most common public-key schemes, and \nsome efficient implementation techniques. \nRecommended reading \nKatz, J., Lindell, Y. (2015). Introduction to modern cryptography. Chapman and Hall/CRC (2nd \ned.).\n \n\nE-COMMERCE \n \nAims \nThis course aims to give students an outline of the issues involved in setting up an e-commerce \nsite. \n \nLectures \nThe history of electronic commerce. Mail order; EDI; web-based businesses, credit card \nprocessing, PKI, identity and other hot topics. \nNetwork economics. Real and virtual networks, supply-side versus demand-side scale \neconomies, Metcalfe\u2019s law, the dominant firm model, the differentiated pricing model Data \nProtection Act, Distance Selling regulations, business models. \nWeb site design. Stock and price control; domain names, common mistakes, dynamic pages, \ntransition diagrams, content management systems, multiple targets. \nWeb site implementation. Merchant systems, system design and sizing, enterprise integration, \npayment mechanisms, CRM and help desks. Personalisation and internationalisation. \nThe law and electronic commerce. Contract and tort; copyright; binding actions; liabilities and \nremedies. Legislation: RIP; Data Protection; EU Directives on Distance Selling and Electronic \nSignatures. \nPutting it into practice. Search engine interaction, driving and analysing traffic; dynamic pricing \nmodels. Integration with traditional media. Logs and audit, data mining modelling the user. \ncollaborative filtering and affinity marketing brand value, building communities, typical behaviour. \nFinance. How business plans are put together. Funding Internet ventures; the recent hysteria; \nmaximising shareholder value. Future trends. \nUK and International Internet Regulation. Data Protection Act and US Privacy laws; HIPAA, \nSarbanes-Oxley, Security Breach Disclosure, RIP Act 2000, Electronic Communications Act \n2000, Patriot Act, Privacy Directives, data retention; specific issues: deep linking, Inlining, brand \nmisuse, phishing. \nObjectives \nAt the end of the course students should know how to apply their computer science skills to the \nconduct of e-commerce with some understanding of the legal, security, commercial, economic, \nmarketing and infrastructure issues involved. \n \nRecommended reading \nShapiro, C. and Varian, H. (1998). Information rules. Harvard Business School Press. \n \nAdditional reading: \n \nStandage, T. (1999). The Victorian Internet. Phoenix Press. Klemperer, P. (2004). Auctions: \ntheory and practice. Princeton Paperback ISBN 0-691-11925-2.\n \n\nMACHINE LEARNING AND BAYESIAN INFERENCE \nAims \nThe Part 1B course Artificial Intelligence introduced simple neural networks for supervised \nlearning, and logic-based methods for knowledge representation and reasoning. This course \nhas two aims. First, to provide a rigorous introduction to machine learning, moving beyond the \nsupervised case and ultimately presenting state-of-the-art methods. Second, to provide an \nintroduction to the wider area of probabilistic methods for representing and reasoning with \nknowledge. \n \nLectures \nIntroduction to learning and inference. Supervised, unsupervised, semi-supervised and \nreinforcement learning. Bayesian inference in general. What the naive Bayes method actually \ndoes. Review of backpropagation. Other kinds of learning and inference. [1 lecture] \nHow to classify optimally. Treating learning probabilistically. Bayesian decision theory and Bayes \noptimal classification. Likelihood functions and priors. Bayes theorem as applied to supervised \nlearning. The maximum likelihood and maximum a posteriori hypotheses. What does this teach \nus about the backpropagation algorithm? [2 lectures] \nLinear classifiers I. Supervised learning via error minimization. Iterative reweighted least \nsquares. The maximum margin classifier. [2 lectures] \nGaussian processes. Learning and inference for regression using Gaussian process models. [2 \nlectures] \nSupport vector machines (SVMs). The kernel trick. Problem formulation. Constrained \noptimization and the dual problem. SVM algorithm. [2 lectures] \nPractical issues. Hyperparameters. Measuring performance. Cross-validation. Experimental \nmethods. [1 lecture] \nLinear classifiers II. The Bayesian approach to neural networks. [1 lecture] \nUnsupervised learning I. The k-means algorithm. Clustering as a maximum likelihood problem. \n[1 lecture] \nUnsupervised learning II. The EM algorithm and its application to clustering. [1 lecture] \nBayesian networks I. Representing uncertain knowledge using Bayesian networks. Conditional \nindependence. Exact inference in Bayesian networks. [2 lectures] \nBayesian networks II. Markov random fields. Approximate inference. Markov chain Monte Carlo \nmethods. [1 lecture] \nObjectives \nAt the end of this course students should: \n \nUnderstand how learning and inference can be captured within a probabilistic framework, and \nknow how probability theory can be applied in practice as a means of handling uncertainty in AI \nsystems. \nUnderstand several algorithms for machine learning and apply those methods in practice with \nproper regard for good experimental practice. \nRecommended reading \nIf you are going to buy a single book for this course I recommend: \n \n\n* Bishop, C.M. (2006). Pattern recognition and machine learning. Springer. \n \nThe course text for Artificial Intelligence I: \n \nRussell, S. and Norvig, P. (2010). Artificial intelligence: a modern approach. Prentice Hall (3rd \ned.). \n \ncovers some relevant material but often in insufficient detail. Similarly: \n \nMitchell, T.M. (1997). Machine Learning. McGraw-Hill. \n \ngives a gentle introduction to some of the course material, but only an introduction. \n \nRecently a few new books have appeared that cover a lot of relevant ground well. For example: \n \nBarber, D. (2012). Bayesian Reasoning and Machine Learning. Cambridge University Press. \nFlach, P. (2012). Machine Learning: The Art and Science of Algorithms that Make Sense of \nData. Cambridge University Press. \nMurphy, K.P. (2012). Machine Learning: A Probabilistic Perspective. MIT Press.\n \n\nOPTIMISING COMPILERS \n \nAims \nThe aims of this course are to introduce the principles of program optimisation and related \nissues in decompilation. The course will cover optimisations of programs at the abstract syntax, \nflowgraph and target-code level. It will also examine how related techniques can be used in the \nprocess of decompilation. \n \nLectures \nIntroduction and motivation. Outline of an optimising compiler. Optimisation partitioned: analysis \nshows a property holds which enables a transformation. The flow graph; representation of \nprogramming concepts including argument and result passing. The phase-order problem. \nKinds of optimisation. Local optimisation: peephole optimisation, instruction scheduling. Global \noptimisation: common sub-expressions, code motion. Interprocedural optimisation. The call \ngraph. \nClassical dataflow analysis. Graph algorithms, live and avail sets. Register allocation by register \ncolouring. Common sub-expression elimination. Spilling to memory; treatment of \nCSE-introduced temporaries. Data flow anomalies. Static Single Assignment (SSA) form. \nHigher-level optimisations. Abstract interpretation, Strictness analysis. Constraint-based \nanalysis, Control flow analysis for lambda-calculus. Rule-based inference of program properties, \nTypes and effect systems. Points-to and alias analysis. \nTarget-dependent optimisations. Instruction selection. Instruction scheduling and its phase-order \nproblem. \nDecompilation. Legal/ethical issues. Some basic ideas, control flow and type reconstruction. \nObjectives \nAt the end of the course students should \n \nbe able to explain program analyses as dataflow equations on a flowgraph; \nknow various techniques for high-level optimisation of programs at the abstract syntax level; \nunderstand how code may be re-scheduled to improve execution speed; \nknow the basic ideas of decompilation. \nRecommended reading \n* Nielson, F., Nielson, H.R. and Hankin, C.L. (1999). Principles of program analysis. Springer. \nGood on part A and part B. \nAppel, A. (1997). Modern compiler implementation in Java/C/ML (3 editions). \nMuchnick, S. (1997). Advanced compiler design and implementation. Morgan Kaufmann. \nWilhelm, R. (1995). Compiler design. Addison-Wesley. \nAho, A.V., Sethi, R. and Ullman, J.D. (2007). Compilers: principles, techniques and tools. \nAddison-Wesley (2nd ed.).\n \n\nQUANTUM COMPUTING \n \nAims \nThe principal aim of the course is to introduce students to the basics of the quantum model of \ncomputation. The model will be used to study algorithms for searching, factorisation and \nquantum chemistry as well as other important topics in quantum information such as \ncryptography and super-dense coding. Issues in the complexity of computation will also be \nexplored. A second aim of the course is to introduce student to near-term quantum computing. \nTo this end, error-correction and adiabatic quantum computing are studied. \n \nLectures \nBits and qubits. Introduction to quantum states and measurements with motivating examples. \nComparison with discrete classical states. \nLinear algebra. Review of linear algebra: vector spaces, linear operators, Dirac notation, the \ntensor product. \nThe postulates of quantum mechanics. Postulates of quantum mechanics, incl. evolution and \nmeasurement. \nImportant concepts in quantum mechanics. Entanglement, distinguishing orthogonal and \nnon-orthogonal quantum states, no-cloning and no signalling. \nThe quantum circuit model. The circuit model of quantum computation. Quantum gates and \ncircuits. Universality of the quantum circuit model, and efficient simulation of arbitrary two-qubit \ngates with a standard universal set of gates. \nSome applications of quantum information. Applications of quantum information (other than \nquantum computation): quantum key distribution, superdense coding and quantum teleportation. \nDeutsch-Jozsa algorithm. Introducing Deutsch\u2019s problem and Deutsch\u2019s algorithm leading onto \nits generalisation, the Deutsch-Jozsa algorithm. \nQuantum search. Grover\u2019s search algorithm: analysis and lower bounds. \nQuantum Fourier Transform and Quantum Phase Estimation. Definition of the Quantum Fourier \nTransform (QFT), and efficient representation thereof as a quantum circuit. Application of the \nQFT to enable Quantum Phase Estimation (QPE). \nApplication 1 of QFT / QPE: Factoring. Shor\u2019s algorithm: reduction of factoring to period finding \nand then using the QFT for period finding. \nApplication 2 of QFT / QPE: Quantum Chemistry. Efficient simulation of quantum systems, and \napplications to real-world problems in quantum chemistry. \nQuantum complexity. Quantum complexity classes and their relationship to classical complexity. \nComparison with probabilistic computation. \nQuantum error correction. Introducing the concept of quantum error correction required for the \nfollowing lecture on fault-tolerance. \nFault tolerant quantum computing. Elements of fault tolerant computing; the threshold theorem \nfor efficient suppression of errors. \nAdiabatic quantum computing and quantum optimisation. The quantum adiabatic theorem, and \nadiabatic optimisation. Quantum annealing and D-Wave. \nCase studies in near-term quantum computation. Examples of state-of-the-art quantum \nalgorithms and computers, including superconducting and networked quantum computers. \n\nObjectives \nAt the end of the course students should: \n \nunderstand the quantum model of computation and the basic principles of quantum mechanics; \nbe familiar with basic quantum algorithms and their analysis; \nbe familiar with basic quantum protocols such as teleportation and superdense coding; \nsee how the quantum model relates to classical models of deterministic and probabilistic \ncomputation. \nappreciate the importance of efficient error-suppression if quantum computation is to yield an \nadvantage over classical computation. \ngain a general understanding of the important topics in near-term quantum computing, including \nadiabatic quantum computing. \nRecommended reading \nBooks: \nNielsen M.A., Chuang I.L. (2010). Quantum Computation and Quantum Information. Cambridge \nUniversity Press. \nMermin N.D. (2007). Quantum Computer Science: An Introduction. Cambridge University Press. \nMcGeoch, C. (2014). Adiabatic Quantum Computation and Quantum Annealing Theory and \nPractice. Morgan and Claypool. https://ieeexplore.ieee.org/document/7055969 \n \nPapers: \n \nBraunstein S.L. (2003). Quantum computation tutorial. Available at: \nhttps://www-users.cs.york.ac.uk/~schmuel/comp/comp_best.pdf \nAharonov D., Quantum computation [arXiv:quant-ph/9812037] \nAlbash T., Adiabatic Quantum Computing https://arxiv.org/pdf/1611.04471.pdf \nMcCardle S. et al, Quantum computational chemistry https://arxiv.org/abs/1808.10402 \n \nOther lecture notes: \n \nAndrew Childs (University of Maryland): http://cs.umd.edu/~amchilds/qa/ \n \n\nRANDOMISED ALGORITHMS \n \nAims: \nThe aim of this course is to introduce advanced techniques in the design and analysis \nalgorithms, with a strong focus on randomised algorithms. It covers essential tools and ideas \nfrom probability, optimisation and graph theory, and develops this knowledge through a variety \nof examples of algorithms and processes. This course will demonstrate that randomness is not \nonly an important design technique which often leads to simpler and more elegant algorithms, \nbut may also be essential in settings where time and space are restricted.   \n \nLectures: \nIntroduction. Course Overview. Why Randomised Algorithms? A first Randomised Algorithm for \nthe MAX-CUT problem. [1 Lecture]  \n \nConcentration Inequalities. Moment-Generating Functions and Chernoff Bounds. Extension: \nMethod of Bounded Independent Differences. Applications: Balls-into-Bins, Quick-Sort and Load \nBalancing. [approx. 2 Lectures] \n \nMarkov Chains and Mixing Times. Random Walks on Graphs. Application: Randomised \nAlgorithm for the 2-SAT problem. Mixing Times of Markov Chains. Application: Load Balancing \non Networks. [approx. 2 Lectures] \n \nLinear Programming and Applications. Definitions and Applications. Formulating Linear \nPrograms. The Simplex Algorithm. Finding Initial Solutions. How to use Linear Programs and \nBranch & Bound to Solve a Classical TSP instance. [approx. 3 Lectures] \n \nRandomised Approximation Algorithms. Randomised Approximation Schemes. Linearity of \nExpectations, Derandomisation. Deterministic and Randomised Rounding of Linear Programs. \nApplications: MAX3-SAT problem, Weighted Vertex Cover, Weighted Set Cover, MAX-SAT. \n[approx. 2 Lectures] \n \nSpectral Graph Theory and Clustering. Eigenvalues of Graphs and Matrices: Relations between \nEigenvalues and Graph Properties, Spectral Graph Drawing. Spectral Clustering: Conductance, \nCheeger's Inequality. Spectral Partitioning Algorithm [approx. 2 Lectures] \n \nObjectives: \nBy the end of the course students should be able to: \n \nlearn how to use randomness in the design of algorithms, in particular, approximation \nalgorithms; \nlearn the basics of linear programming, integer programming and randomised rounding; \napply randomisation to various problems coming from optimisation, machine learning and data \nscience; \nuse results from probability theory to analyse the performance of randomised algorithms. \n\nRecommended reading \n* Michael Mitzenmacher and Eli Upfal. Probability and Computing: Randomized Algorithms and \nProbabilistic Analysis., Cambridge University Press, 2nd edition. \n \n* David P. Williamson and David B. Shmoys. The Design of Approximation Algorithms, \nCambridge University Press, 2011 \n \n* Cormen, T.H., Leiserson, C.D., Rivest, R.L. and Stein, C. (2009). Introduction to Algorithms. \nMIT Press (3rd ed.). ISBN 978-0-262-53305-8 \n \n \n\nCLOUD COMPUTING \nAims \nThis module aims to teach students the fundamentals of Cloud Computing covering topics such \nas virtualization, data centres, cloud resource management, cloud storage and popular cloud \napplications including batch and data stream processing. Emphasis is given on the different \nbackend technologies to build and run efficient clouds and the way clouds are used by \napplications to realise computing on demand. The course will include practical tutorials on cloud \ninfrastructure technologies. Students will be assessed via a Cloud-based coursework project. \n \nLectures \nIntroduction to Cloud Computing \nData centres \nVirtualization I \nVirtualization II \nMapReduce \nMapReduce advanced \nResource management for virtualized data centres \nCloud storage \nCloud-based data stream processing \nObjectives \nBy the end of the course students should: \n \nunderstand how modern clouds operate and provide computing on demand; \nunderstand about cloud availability, performance, scalability and cost; \nknow about cloud infrastructure technologies including virtualization, data centres, resource \nmanagement and storage; \nknow how popular applications such as batch and data stream processing run efficiently on \nclouds; \nAssessment \nAssignment 1, worth 55% of the final mark. Develop batch processing applications running on a \ncluster assessed through automatic testing, code inspection and a report. \nAssignment 2, worth 30% of the final mark: Design and develop a framework for running the \nbatch processing applications from Assignment 1 on a cluster according to certain resource \nallocation policies. This assigment will be assessed by an expert, testing, code inspection and a \nreport. \nAssignment 3, worth 15% of the final mark: Write two individual project reports describing your \nwork for assignments 1 and 2. Write two sets of scripts to manage and run your code. Reports \nand scripts will be submitted with each assignment. \nRecommended reading \nMarinescu, D.C. Cloud Computing, Theory and Practice. Morgan Kaufmann. \nBarham, P., et. al. (2003). \u201cXen and the Art of Virtualization\u201d. In Proceedings of SOSP 2003. \nCharkasova, L., Gupta, D. and Vahdat, A. (2007). \u201cComparison of the Three CPU Schedulers in \nXen\u201d. In SIGMETRICS 2007. \n\nDean, J. and Ghemawat, S. (2004). \u201cMapReduce: Simplified Data Processing on Large \nClusters\u201d. In Proceedings of OSDI 2004. \nZaharia, M, et al. (2008). \u201cImproving MapReduce Performance in Heterogeneous \nEnvironments\u201d. In Proceedings of OSDI 2008. \nHindman, A., et al. (2011). \u201cMesos: A Platform for Fine-Grained Resource Sharing in Data \nCenter\u201d. In Proceedings of NSDI 2011. \nSchwarzkopf, M., et al. (2013). \u201cOmega: Flexible, Scalable Schedulers for Large Compute \nClusters\u201d. In EuroSys 2013. \nGhemawat, S. (2003). \u201cThe Google File System\u201d. In Proceedings of SOSP 2003. \nChang, F. (2006). \u201cBigtable: A Distributed Storage System for Structured Data\u201d. In Proceedings \nof OSDI 2006. \nFernandez, R.C., et al. (2013). \u201cIntegrating Scale Out and Fault Tolerance in Stream Processing \nusing Operator State Management\u201d. In SIGMOD 2013.\n \n\nCOMPUTER SYSTEMS MODELLING \n \nAims \n \nThe aims of this course are to introduce the concepts and principles of mathematical modelling \n \nand simulation, with particular emphasis on using queuing theory and control theory for \nunderstanding the behaviour of computer and communications systems. \n \nSyllabus \n \n1.     Overview of computer systems modeling using both analytic techniques and simulation. \n \n2.     Introduction to discrete event simulation. \n \na.     Simulation techniques \n \nb.     Random number generation methods \n \nc.     Statistical aspects: confidence intervals, stopping criteria \n \nd.     Variance reduction techniques. \n \n3.     Stochastic processes (builds on starred material in Foundations of Data Science) \n \na.     Discrete and continuous stochastic processes. \n \nb.     Markov processes and Chapman-Kolmogorov equations. \n \nc.     Discrete time Markov chains. \n \nd.     Ergodicity and the stationary distribution. \n \ne.     Continuous time Markov chains. \n \nf.      Birth-death processes, flow balance equations. \n \ng.     The Poisson process. \n \n4.     Queueing theory \n \na.     The M/M/1 queue in detail. \n \n\nb.     The equilibrium distribution with conditions for existence and common performance \nmetrics. \n \nc.     Extensions of the M/M/1 queue: the M/M/k queue, the M/M/infinity queue. \n \nd.     Queueing networks. Jacksonian networks. \n \ne.     The M/G/1 queue. \n \n5.     Signals, systems, and transforms \n \na.     Discrete- and continuous-time convolution. \n \nb.     Signals. The complex exponential signal. \n \nc.     Linear Time-Invariant Systems. Modeling practical systems as an LTI system. \n \nd.     Fourier and Laplace transforms. \n \n6.     Control theory. \n \na.     Controlled systems. Modeling controlled systems. \n \nb.     State variables. The transfer function model. \n \nc.     First-order and second-order systems. \n \nd.     Feedback control. PID control. \n \ne.     Stability. BIBO stability. Lyapunov stability. \n \nf.      Introduction to Model Predictive Control. \n \nObjectives \n \nAt the end of the course students should \n \n\u00b7       Be aware of different approaches to modeling a computer system; their pros and cons \n \n\u00b7       Be aware of the issues in building a simulation of a computer system and analysing the \nresults obtained \n \n\u00b7       Understand the concept of a stochastic process and how they arise in practice \n \n\n\u00b7       Be able to build simple Markov models and understand the critical modelling assumptions \n \n\u00b7       Be able to solve simple birth-death processes \n \n\u00b7       Understand and use M/M/1 queues to model computer systems \n \n\u00b7       Be able to model a computer system as a linear time-invariant system \n \n\u00b7       Understand the dynamics of a second-order controlled system \n \n\u00b7       Design a PID control for an LTI system \n \n\u00b7       Understand what is meant by BIBO and Lyapunov stability \n \nAssessment \n \nThere will be one programming assignment to build a discrete event simulator and using it to \nsimulate an M/M/1 and an M/D/1 queue (worth 15% of the final marks). There will also be two \none-hour in-person assessments for the course on: Stochastic processes and Queueing theory \n(40%) and Signals, Systems, Transforms, and Control Theory (45%). \n \nReference books \n \nKeshav, S. (2012)*. Mathematical Foundations of Computer Networking. Addison-Wesley. A \ncustomized PDF will be made available to class participants. \n \nKleinrock, L. (1975). Queueing systems, vol. 1. Theory. Wiley. \n \nKraniauskas, Peter. Transforms in signals and systems. Addison-Wesley Longman, 1992. \n \nJain, R. (1991). The art of computer systems performance analysis. Wiley. \n \n \n\nCOMPUTING EDUCATION \n \nAims \nThis module considers various aspects of the teaching and learning of computer science in \nschool, including practical experience, and provides an introduction to the field of computing \neducation research. It is offered by the Raspberry Pi Computing Education Research Centre, \npart of the Department of Computer Science and Technology, in conjunction with local schools \nin the Cambridge area. Students will have the opportunity to observe, plan and deliver a lesson, \nand explore the teaching of computing in a secondary education setting. The lesson taught will \nbe on a topic within the computing/computer science curriculum in England but will be \nnegotiated with the teacher mentoring the school placement. In the sessions in the Department, \nstudents will be introduced to elements of pedagogy and assessment alongside current issues \nin computing education. The course can also serve as a useful introduction to research in \ncomputing education for those interested in this area. Assessment will include lesson planning, \npresentations, an in-person viva and a 3000-word report which will include evidence of \nengagement with relevant research underpinning the lessons and activities undertaken. An \ninformation session relating to the module will be available in the preceding Easter Term. The \narrangement of school placements and DBS checks will be carried out in Michaelmas Term, so \nstudents should be prepared to engage in these preparatory activities. \n \nLectures \nThis is not a traditional lecture course and the term will be structured as follows: \n \nWeek 1: Introduction to computing education & visit to school placement \nWeek 2: In school (observation, supporting teacher) \nWeek 3: In school (observation, supporting teacher) \nWeek 4: In school (co-teach / small group teaching) \nWeek 5: Computing pedagogy and assessment (school half-term) \nWeek 6: In school (co-teach / small group teaching) \nWeek 7: In school (teach part/whole lesson) \nWeek 8: Presentations \nObjectives \nBy the end of the course students should: \n \ngain experience of computing education in practice \ndemonstrate an ability to communicate an aspect of computer science clearly and effectively \nbe able to plan and design a teaching activity/lesson with consideration of the audience \nbe able to thoroughly evaluate the design and implementation of a teaching/activity lesson \nbe able to demonstrate an understanding of relevant theories of learning and pedagogy from the \nliterature and how they apply to the learning/teaching of the topic under consideration \ngain an understanding of the breadth and depth of the field of computing education research \nClass Size \nThis module can accommodate up to 10 Part II students. \n \n\nRecommended reading \nBrown, N. C., Sentance, S., Crick, T., & Humphreys, S. (2014). Restart: The resurgence of \ncomputer science in UK schools. ACM Transactions on Computing Education (TOCE), 14(2), \n1-22. https://doi.org/10.1145/2602484 \n \nSentance, S., Barendsen, E., Howard, N. R., & Schulte, C. (Eds.). (2023). Computer science \neducation: Perspectives on teaching and learning in school. Bloomsbury Publishing. \n \nGrover, S. (Ed). 2020. Computer Science in K-12: An A-to-Z Handbook on Teaching \nProgramming. Edfinity. https://www.shuchigrover.com/atozk12cs/ \n \nRaspberry Pi Foundation (2022). Hello World Big Book of Pedagogy. \nhttps://www.raspberrypi.org/hello-world/issues/the-big-book-of-computing-pedagogy \n \nAssessment \nWeek 2: Reflections on an observed lesson (10%, graded out of 20) \nWeek 4: Submission of a lesson plan outline (10% graded out of 20) \nWeek 8: Delivery of an oral presentation relating to the lesson delivery (10% graded out of 20) \nAfter course completion: 3000 word report (60%), graded out of 20 \nViva with assessor (10%) (pass/fail) \nFurther Information \nWe will find placements for students in secondary schools within cycling or bus distance from \nthe Department/centre of Cambridge. There will be a timetabled slot for the course that students \ncan use to go to their school placement, but students may need to be flexible and liaise with the \nschool to find the best time for working with a specific class. \n \n \n\nDEEP NEURAL NETWORKS \n \nObjectives \nYou will gain detailed knowledge of \n \nCurrent understanding of generalization in neural networks vs classical statistical models. \nOptimization procedures for neural network models such as stochastic gradient descent and \nADAM. \nAutomatic differentiation and at least one software framework (PyTorch, TensorFolow) as well as \nan overview of other software approaches. \nArchitectures that are deployed to deal with different data types such as images or sequences \nincluding a. convolutional networks b. recurrent networks and c. transformers. \nIn addition you will gain knowledge of more advanced topics reflecting recent research in \nmachine learning. These change from year to year, examples include: \n \nApproaches to unsupervised learning including autoencoders and self-supervised learning.. \nTechniques for deploying models in low data regimes such as transfer learning and \nmeta-learning. \nTechniques for propagating uncertainty such as Bayesian neural networks. \nReinforcement learning and its applications to robotics or games. \nGraph Neural Networks and Geometric Deep Learning. \nTeaching Style \nThe start of the course will focus on the latest undertanding of current theory of neural networks, \ncontrasting with previous classical understandings of generalization performance. Then we will \nmove to practical examples of network architectures and deployment. We will end with more \nadvanced topics reflecting current research, mostly delivered by guest lecturers from the \nDepartment and beyond. \n \nSchedule \nWeek 1 \nTwo lectures: Generalization and Approximation with Neural Networks \n \nWeek 2 \nTwo lectures: Optimization: Stochastic Gradient Descent and ADAM \n \nWeek 3 \nTwo lectures: Hardware Acceleration and Convolutional Neural Networks \n \nWeek 4 \nTwo lectures: Vanishing Gradients, Recurrent and Residual Networks, Sequence Modeling. \n \nWeek 5 \nTwo lectures: Attention, The Transformer Architecture, Large Language Models \n \n\nWeek 6-8 \nLectures and guest lectures from the special topics below. \n \nSpecial topics \nSelf-Supervised Learning, AutoEncoders, Generative Modeling of Images \nHardware Implementations \nReinforcement learning \nTransfer learning and meta-learning \nUncertainty and Bayesian Neural Networks \nGraph Neural Networks \nAssessment \nAssessment involves solving a number of exercises in a jupyter notebook environment. \nFamiliarity with the python programming language is assumed. The exercises rely on the \npytorch deep learning framework, the syntax of which we don\u2019t cover in detail in the lectures, \nstudents are referred to documentation and tutorials to learn this component of the assessment. \n \nAssignment 1 \n \n30% of the total marks, with guided exercises. \n \nAssignment 2 \n \n70% of the total marks, a combination of guided exercises and a more exploratory mini-project.\n\nEXTENDED REALITY \n \nSyllabus & Schedule \nWeek 1 \nIntroduction \nCourse schedule and concept \nThe history of AR/VR/XR \nApplications of AR/VR/XR \nOverview of the XR pipeline \u2013 3 main components of XR \nDisplay technologies and rendering \nMachine perception and input interfaces \nContent generation \u2013 geometry, appearance, motion \nOverview of the AR/VR/XR frameworks \nPractical 1 \nProject structure \nProject themes \nIntroduction to the XR programming platform \nWeek 2 \n3D scene representations and rendering \nObject representations with polygonal meshes \nEfficient rendering via rasterization \nPractical XR rendering of textured geometries \nTracking and pose estimation for XR \nRigid transforms \nCamera pose estimation \nBody/ hand tracking \nPractical 2 \nProject draft proposal due \nProject feedback \nRendering and displaying a first object \nWeek 3 \n3D scene capture and understanding \nDepth estimation \n3D geometry capture \nAppearance acquisition \nLighting estimation and relighting \nLighting models for 3D scenes \nEstimating lighting in a scene \nRelighting rendered models \nPractical 3 \nAcquiring and utilizing depth from sensors \nRelighting a displayed object \nWeek 4 \nPhysically-based simulation of rigid objects \n\nRigid-body dynamics \nCollision detection \nTime integration \nPhysically-based simulation of non-rigid volumetric phenomena \nParticle systems \nBasic fluid dynamics \nPractical 4 \nPractical due \nProject prototype due \nProject check-in \nSimulating and rendering a simple object \nWeek 5 \nAdvanced topics in XR I: AR/VR displays technologies \nMicro LED-based displays \nWaveguide-based displays \nNext generation displays \nWeek 6 \nAdvanced topics in XR II \u2013 Mobile AR \nComputer vision on limited hardware \nUX/UI challenges \nContent challenges \nProject due \n  \n \nAssessment \nA practical exercise, worth 20% of the mark. This will cover the basics of AR/VR development \nand some of the practical techniques the students will use in their projects. This is individual \nwork. No mobile device or GPU hours are needed. \nAn AR/VR project worth 80% of the marks: \nCourse projects will be designed by the students following the possible project themes proposed \nby the lecturer and will be checked by the lecturer for appropriateness. \nThe students will form groups of 2-3 to design, implement, report, and present the course \nproject. \nThe final mark will be an implementation mark (40%), a report mark (20%), and a \npresentation/demo mark (20%). Each team member will be evaluated based on their \ncontribution. \nEach project will have extensions to be completed only by the ACS students. Each student will \nwrite a different part of the report, whose author will be clearly marked. Each student will further \nsummarise her/his contributions to the project in the same report. \nNote: Submission is by one submission per group but work by each individual must be clearly \nlabelled.\n \n\nFEDERATED LEARNING: THEORY AND PRACTICE \n \nObjectives \nThis course aims to extend the machine learning knowledge available to students in Part I (or \npresent in typical undergraduate degrees in other universities), and allow them to understand \nhow these concepts can manifest in a decentralized setting. The course will consider both \ntheoretical (e.g., decentralized optimization) and practical (e.g., networking efficiency) aspects \nthat combine to define this growing area of machine learning.  \n \nAt the end of the course students should: \n \nUnderstand popular methods used in federated learning \nBe able to construct and scale a simple federated system \nHave gained an appreciation of the core limitations to existing methods, and the approaches \navailable to cope with these issues \nDeveloped an intuition for related technologies like differential privacy and secure aggregation, \nand are able to use them within typical federated settings  \nCan reason about the privacy and security issues with federated systems \nLectures \nCourse Overview. Introduction to Federated Learning. \nDecentralized Optimization. \nStatistical and Systems Heterogeneity. \nVariations of Federated Aggregation. \nSecure Aggregation. \nDifferential Privacy within Federated Systems. \nExtensions to Federated Analytics. \nApplications to Speech, Video, Images and Robotics. \nLab sessions \nFederating a Centralized ML Classifier. \nBehaviour under Heterogeneity. \nScaling a Federated Implementation. \nExploring Privacy with Federated Settings \nRecommended Reading \nReadings will be assigned for each lecture. Readings will be taken from either research papers, \ntutorials, source code or blogs that provide more comprehensive treatment of taught concepts. \n \nAssessment - Part II Students \nFour labs are performed during the course, and students receive 10% of their total grade for \nwork done as part of each lab. (For a total of 40% of the total grade from lab work alone). Labs \nwill primarily provide hands-on teaching opportunities, that are then utilized within the lab \nassignment which is completed outside of the lab contact time. MPhil and Part III students will \nbe given additional questions to answer within their version of the lab assignment which will \ndiffer from the assignment given to Part II CST students. \n \n\nThe remainder of the course grade (60%) will be given based on a hands-on project that applies \nthe concepts taught in lectures and labs. This hands-on project will be assessed based on upon \na combination of source code, related documentation and brief 8-minute pre-recorded talk that \nsummarizes key project elements (any slides used are also submitted as part of the project). \nPlease note, that in the case of Part II CST students, the talk itself is not examinable -- as such \nwill be made optional to those students. \n \nA range of possible practical projects will be described and offered to students to select from, or \nalternatively students may propose their own. MPhil and Part III students will select from a \nproject pool that is separate from those offered to Part II CST students. MPhil and Part III \nprojects will contain a greater emphasis on a research element, and the pre-recorded talks \nprovided by this student group will focus on this research contribution. The project will be \nassessed on the level of student understanding demonstrated, the degree of difficulty, \ncorrectness of implementation -- and for Part III/MPhil students the additional criteria of the \nquality and execution of the research methodology, and depth and quality of results analysis. \n \nThis project can be done individually or in groups -- although individual projects will be strongly \nencouraged. It will be required the project is performed using a code repository that also will \ncontain all documentation -- access to this repository will be shared with course staff (e.g., \nlecturer and TAs). Where needed, marks assigned to students within a group will be \ndifferentiated using this repository as an input. Furthermore if groups are formed, members must \nbe either entirely from Part III/MPhil students or Part II CST, i.e., these two student groups \nshould not mix to form a project group. \n \nProjects will be made available publicly. A maximum word count for written contributions for the \nproject will be enforced.\n \n\nMOBILE HEALTH \n \nAims \nThe course aims to explore how wearable and mobile systems sensors can be used to gather \ndata relevant to understand health, how the data can be analysed with advanced signal \nprocessing and machine learning and the performance of these systems in terms of diagnostics \nand disease progression detection. \n \nSyllabus \nCourse Overview. Introduction to Mobile Health. Evaluation metrics and methodology. Basics of \nSignal Processing. \nInertial Measurement Units, Human Activity Recognition (HAR) and Gait Analysis and Machine \nLearning for IMU data. \nRadios, Bluetooth, GPS and Cellular. Epidemiology and contact tracing, Social interaction \nsensing and applications. Location tracking in health monitoring and public health. \nAudio Signal Processing. Voice and Speech Analysis: concepts and data analysis. Body \nSounds analysis. \nPhotoplethysmogram and Light sensing for health (heart and sleep) \nContactless and wireless behaviour and physiological monitoring \nGenerative Models for Wearable Health Data \nTopical Guest Lectures \nObjectives \nThe course aims to explore how wearable and mobile systems sensors can be used to gather \ndata relevant to understand health, how the data can be analysed with advanced signal \nprocessing and machine learning and the performance of these systems in terms of diagnostics \nand disease progression detection. \n \nRoughly, each lecture contains a theory part about the working of \u201csensor signals\u201d or \u201cdata \nanalysis methods\u201d and an application part which contextualises the concepts. \n \nAt the end of the course students should: Understand how mobile/wearable sensors capture \ndata and their working. Understand different approaches to acquiring and analysing sensor data \nfrom different types of sensors. Understand the concept of signal processing applied to time \nseries data and their practical application in health. Be able to extract sensor data and analyse it \nwith basic signal processing and machine learning techniques. Be aware of the different health \napplications of the various sensor techniques. The course will also touch on privacy and ethics \nimplications of the approaches developed in an orthogonal fashion. \n \nRecommended Reading \nPlease see Course Materials for recommended reading for each session. \n \nAssessment - Part II students \nTwo assignments will be based on two datasets which will be provided to the students: \n \n\nAssignment 1 (shared for Part II and Part III/MPhil): this will be based on a dataset and will be \nworth 40% of the final mark. The task of the assessment will be to perform pre-processing and \nbasic data analysis in a \"colab\" and an answer sheet of no more than 1000 words. \n \nAssignment 2 (Part II): This assignment (worth 60% of the final mark) will be a fuller analysis of \na dataset focusing on machine learning algorithms and metrics. Discussion and interpretation of \nthe findings will be reported in a colab and a report of no more than 1200 words.\n \n\nMULTICORE SEMANTICS AND PROGRAMMING \n \nAims \nIn recent years multiprocessors have become ubiquitous, but building reliable concurrent \nsystems with good performance remains very challenging. The aim of this module is to \nintroduce some of the theory and the practice of concurrent programming, from hardware \nmemory models and the design of high-level programming languages to the correctness and \nperformance properties of concurrent algorithms. \n \nLectures \nPart 1: Introduction and relaxed-memory concurrency [Professor P. Sewell] \n \nIntroduction. Sequential consistency, atomicity, basic concurrent problems. [1 block] \nConcurrency on real multiprocessors: the relaxed memory model(s) for x86, ARM, and IBM \nPower, and theoretical tools for reasoning about x86-TSO programs. [2 blocks] \nHigh-level languages. An introduction to C/C++11 and Java shared-memory concurrency. [1 \nblock] \nPart 2: Concurrent algorithms [Dr T. Harris] \n \nConcurrent programming. Simple algorithms (readers/writers, stacks, queues) and correctness \ncriteria (linearisability and progress properties). Advanced synchronisation patterns (e.g. some \nof the following: optimistic and lazy list algorithms, hash tables, double-checked locking, RCU, \nhazard pointers), with discussion of performance and on the interaction between algorithm \ndesign and the underlying relaxed memory models. [3 blocks] \nResearch topics, likely to include one hour on transactional memory and one guest lecture. [1 \nblock] \nObjectives \nBy the end of the course students should: \n \nhave a good understanding of the semantics of concurrent programs, both at the multprocessor \nlevel and the C/Java programming language level; \nhave a good understanding of some key concurrent algorithms, with practical experience. \nAssessment - Part II Students \nTwo assignments each worth 50% \n \nRecommended reading \nHerlihy, M. and Shavit, N. (2008). The art of multiprocessor programming. Morgan Kaufmann.\n\nBUSINESS STUDIES SEMINARS \nAims \nThis course is a series of seminars by former members and friends of the Laboratory about their \nreal-world experiences of starting and running high technology companies. It is a follow on to \nthe Business Studies course in the Michaelmas Term. It provides practical examples and case \nstudies, and the opportunity to network with and learn from actual entrepreneurs. \n \nLectures \nEight lectures by eight different entrepreneurs. \n \nObjectives \nAt the end of the course students should have a better knowledge of the pleasures and pitfalls \nof starting a high tech company. \n \nRecommended reading \nLang, J. (2001). The high-tech entrepreneur\u2019s handbook: how to start and run a high-tech \ncompany. FT.COM/Prentice Hall. \nMaurya, A. (2012). Running Lean: Iterate from Plan A to a Plan That Works. O\u2019Reilly. \nOsterwalder, A. and Pigneur, Y. (2010). Business Model Generation: A Handbook for \nVisionaires, Game Changers, and Challengers. Wiley. \nKim, W. and Mauborgne, R. (2005). Blue Ocean Strategy. Harvard Business School Press. \n \nSee also the additional reading list on the Business Studies web page.\n \n\nHOARE LOGIC AND MODEL CHECKING \n \nAims \nThe course introduces two verification methods, Hoare Logic and Temporal Logic, and uses \nthem to formally specify and verify imperative programs and systems. \n \nThe first aim is to introduce Hoare logic for a simple imperative language and then to show how \nit can be used to formally specify programs (along with discussion of soundness and \ncompleteness), and also how to use it in a mechanised program verifier. \n \nThe second aim is to introduce model checking: to show how temporal models can be used to \nrepresent systems, how temporal logic can describe the behaviour of systems, and finally to \nintroduce model-checking algorithms to determine whether properties hold, and to find \ncounter-examples. \n \nCurrent research trends also will be outlined. \n \nLectures \nPart 1: Hoare logic. Formal versus informal methods. Specification using preconditions and \npostconditions. \nAxioms and rules of inference. Hoare logic for a simple language with assignments, sequences, \nconditionals and while-loops. Syntax-directedness. \nLoops and invariants. Various examples illustrating loop invariants and how they can be found. \nPartial and total correctness. Hoare logic for proving termination. Variants. \nSemantics and metatheory Mathematical interpretation of Hoare logic. Semantics and \nsoundness of Hoare logic. \nSeparation logic Separation logic as a resource-aware reinterpretation of Hoare logic to deal \nwith aliasing in programs with pointers. \nPart 2: Model checking. Models. Representation of state spaces. Reachable states. \nTemporal logic. Linear and branching time. Temporal operators. Path quantifiers. CTL, LTL, and \nCTL*. \nModel checking. Simple algorithms for verifying that temporal properties hold. \nApplications and more recent developments Simple software and hardware examples. CEGAR \n(counter-example guided abstraction refinement). \nObjectives \nAt the end of the course students should \n \nbe able to prove simple programs correct by hand and implement a simple program verifier; \nbe familiar with the theory and use of separation logic; \nbe able to write simple models and specify them using temporal logic; \nbe familiar with the core ideas of model checking, and be able to implement a simple model \nchecker. \nRecommended reading \n\nHuth, M. and Ryan M. (2004). Logic in Computer Science: Modelling and Reasoning about \nSystems. Cambridge University Press (2nd ed.).\n \n\n",
        "7th Semester \nADVANCED TOPICS IN COMPUTER ARCHITECTURE \nAims \nThis course aims to provide students with an introduction to a range of advanced topics in \ncomputer architecture. It will explore the current and future challenges facing the architects of \nmodern computers. These will also be used to illustrate the many different influences and \ntrade-offs involved in computer architecture. \n \nObjectives \nOn completion of this module students should: \n \nunderstand the challenges of designing and verifying modern microprocessors \nbe familiar with recent research themes and emerging challenges \nappreciate the complex trade-offs at the heart of computer architecture \nSyllabus \nEach seminar will focus on a different topic. The proposed topics are listed below but there may \nbe some minor changes to this: \n \nTrends in computer architecture \nState-of-the-art microprocessor design \nMemory system design \nHardware reliability \nSpecification, verification and test (may be be replaced with a different topic) \nHardware security (2) \nHW accelerators and accelerators for machine learning \nEach two hour seminar will include three student presentations (15mins) questions (5mins) and \na broader discussion of the topics (around 30mins). The last part of the seminar will include a \nshort scene setting lecture (around 20mins) to introduce the following week's topic. \n \nAssessment \nEach week students will compare and contrast two of the main papers and submit a written \nsummary and review in advance of each seminar (except when presenting). \n \nStudents will be expected to give a number of 15 minute presentations. \n \nEssays and presentations will be marked out of 10. After dropping the lowest mark, the \nremaining marks will be scaled to give a final score out of 100. \n \nStudents will give at least one presentation during the course. They will not be required to \nsubmit an essay during the weeks they are presenting. \n \nEach presentation will focus on a single paper from the reading list. Marks will be awarded for \nclarity and the communication of the paper's key ideas, an analysis of the work's strengths and \nweaknesses and the work\u2019s relationship to related work and broader trends and constraints. \n\n \nRecommended prerequisite reading \nPatterson, D. A., Hennessy, J. L. (2017). Computer organization and design: The \nHardware/software interface RISC-V edition Morgan Kaufmann. ISBN 978-0-12-812275-4. \n \nHennessy, J. and Patterson, D. (2012). Computer architecture: a quantitative approach. Elsevier \n(5th ed.) ISBN 9780123838728. (the 3rd and 4th editions are also relevant)\n \n\nADVANCED TOPICS IN PROGRAMMING LANGUAGES \nAims \nThis module explores various topics in programming languages beyond the scope of \nundergraduate courses. It aims to introduce students to ideas, results and techniques found in \nthe literature and prepare them for research in the field. \n \nSyllabus and structure \nThe module consists of eight two-hour seminars, each on a particular topic. Topics will vary from \nyear to year, but may include, for example, \n \nAbstract interpretation \nVerified software \nMetaprogramming \nBehavioural types \nProgram synthesis \nVerified compilation \nPartial evaluation \nGarbage collection \nDependent types \nAutomatic differentiation \nDelimited continuations \nModule systems \nThere will be three papers assigned for each topic, which students are expected to read before \nthe seminar. \nEach seminar will include three 20 minute student presentations (15 minutes + 5 minutes \nquestions), time for general discussion of the topic, and a brief overview lecture for the following \nweek\u2019s topic. \nBefore each seminar, except in weeks in which they give presentations, students will submit a \nshort essay about two of the papers. \n \n \nObjectives \nOn completion of this module, students should \n \nbe able to identify some major themes in programming language research \nbe familiar with some classic papers and recent advances \nhave an understanding of techniques used in the field \n \nAssessment \nAssessment consists of: \n \nPresentation of one of the papers from the reading list (typically once or twice in total for each \nstudent, depending on class numbers) \nOne essay per week (except on the first week and on presentation weeks) \n\nAll essays and presentations carry equal numbers of marks. \n \nEssay marks are awarded for understanding, for insight and analysis, and for writing quality. \nEssays should be around 1500 words (with a lower limit of 1450 and upper limit of 1650). \nPresentation marks are awarded for clarity, for effective communication, and for selection and \norganisation of topics. \n \nThere will be seven submissions (essays or presentations) in total and, as in other courses, the \nlowest mark for each student will be disregarded when computing the final mark. \n \nMarking, deadlines and extensions will be handled in accordance with the MPhil Assessment \nGuidelines. \n \nRecommended reading material and resources \nResearch and survey papers from programming language conferences and journals (e.g. \nPOPL, PLDI, TOPLAS, FTPL) will be assigned each week. General background material may \nbe found in: \n \n\u2022 Types and Programming Languages (Benjamin C. Pierce) \n   The MIT Press \n   ISBN 0-262-16209-1 \n \n\u2022 Advanced Topics in Types and Programming Languages (ed. Benjamin C. Pierce) \n   The MIT Press \n   ISBN 0-262-16228-8 \n \n\u2022 Practical Foundations for Programming Languages (Robert Harper) \n   Cambridge University Press \n   9781107150300\n \n\nAFFECTIVE ARTIFICIAL INTELLIGENCE \n \nSynopsis \nAffective Artificial Intelligence (Affective AI) aims to imbue machines with social and emotional \nintelligence (EQ). More specifically, Affective AI aims to create artificially intelligent systems and \nmachines that can recognize, interpret, process, and simulate human social signals and \nbehaviours, expressions, and emotions, to enhance human-AI interaction and communication. \n\\r\\n\\r\\nTo achieve this goal, Affective AI draws upon various scientific disciplines, including \nmachine learning, computer vision, speech / natural language / signal processing, psychology \nand cognitive science, and ethics and social sciences. \n \n  \nBackground & Aims \nAffective Artificial Intelligence (Affective AI) aims to imbue machines with social and emotional \nintelligence (EQ). More specifically, Affective AI aims to create artificially intelligent systems and \nmachines that can recognize, interpret, process, and simulate human social signals and \nbehaviours, expressions, and emotions, to enhance human-AI interaction and communication.  \n \nTo achieve this goal, Affective AI draws upon various scientific disciplines, including machine \nlearning, computer vision, speech / natural language / signal processing, psychology and \ncognitive science, and ethics and social sciences. \n \nAffective AI has direct applications in and implications for the design of innovative interactive \ntechnology (e.g., interaction with chat bots, virtual agents, robots), single and multi-user smart \nenvironments ( e.g., in-car/ virtual / augmented / mixed reality, serious games), public speaking \nand cognitive training, and clinical and biomedical studies (e.g., autism, depression, pain). \n \nThe aim of this module is to impart knowledge and ability needed to make informed choices of \nmodels, data, and machine learning techniques for sensing, recognition, and generation of \naffective and social behaviour (e.g., smile, frown, head nodding/shaking, \nagreement/disagreement) in order to create Affectively intelligent AI systems, with a \nconsideration for various ethical issues (e.g., privacy, bias) arising from the real-world \ndeployment of these systems. \n \n  \n \nSyllabus \nThe following list provides a representative list of topics: \n \nIntroduction, definitions, and overview \nTheories from various disciplines \nSensing from multiple modalities (e.g., vision, audio, bio signals, text) \nData acquisition and annotation \nSignal processing / feature extraction \n\nLearning / prediction / recognition and evaluation \nBehaviour synthesis / generation (e.g., for embodied agents / robots) \nAdvanced topics and ethical considerations (e.g., bias and fairness) \nCross-disciplinary applications (via seminar presentations and discussions) \nGuest lectures (diverse topics \u2013 changes each year) \nHands-on research and programming work (i.e., mini project & report)  \n \n  \n \nObjectives \nOn completion of this module, students will: \n \nunderstand and demonstrate knowledge in key characteristics of affectively intelligent AI, which \ninclude: \nRecognition: How to equip Affective AI systems with capabilities of analysing facial expressions, \nvocal intonations, gestures, and other physiological signals to infer human affective states? \nGeneration: How to enable affectively intelligent AI simulate expressions of emotions in \nmachines, allowing them to respond in a more human-like manner, such as virtual agents and \nhumanoid robots, showing empathy or sympathy? \nAdaptation and Personalization: How to enable affectively intelligent AI adapt system responses \nbased on users' affective states and/or needs, or tailor interactions and experiences to individual \nusers based on their expressivity / emotional profiles, personalities, and past interactions? \nEmpathetic Communication: How to design Affective AI systems to communicate with users in a \nway that demonstrates empathy, understanding, and sensitivity to their affective states and \nneeds? \nEthical and societal considerations: What are the various human differences, ethical guidelines, \nand societal impacts that need to be considered when designing and deploying Affective AI to \nensure that these systems respect users' privacy, autonomy, and well-being? \ncomprehend and apply (appropriate) methods for collection, analysis, representation, and \nevaluation of human affective and communicative behavioural data. \nenhance programming skills for creating and implementing (components of) Affective AI \nsystems. \ndemonstrate critical thinking, analysis and synthesis while deciding on 'when' and 'how' to \nincorporate human affect and social signals in a specific AI system context and gain practical \nexperience in proposing and justifying computational solution(s) of suitable nature and scope. \n  \n \nAssessment - Part II Students \nSeminar presentation: 20% \nParticipating in Q&A and discussions: 10% \nMid-term mini project report and presentation (as a team of 2): 10%  \nFinal mini project report & code (as a team of 2): 60% \n  \n \n\nAssessment - MPhil / Part III Students \nSeminar presentation: 20% \nParticipating in Q&A and discussions: 10% \nMid-term mini project report and presentation: 10%  \nFinal mini project report & code: 60% \nFurther Information \nCurrent Cambridge undergraduate students who are continuing onto Part III or the MPhil in \nAdvanced Computer Science may only take this module if they did NOT take it as a Unit of \nAssessment in Part II. \n \nThis module is shared with ACS. Assessment will be adjusted for the two groups of students to \nbe at an appropriate level for whichever course the student is enrolled on. More information will \nfollow at the first lecture. \n \nThis module is shared with Part II of the Computer Science Tripos. Assessment will be adjusted \nfor the two groups of students to be at an appropriate level for whichever course the student is \nenrolled on. Further information about assessment and practicals will follow at the first lecture.\n\nCATEGORY THEORY \n \nAims \nCategory theory provides a unified treatment of mathematical properties and constructions that \ncan be expressed in terms of 'morphisms' between structures. It gives a precise framework for \ncomparing one branch of mathematics (organized as a category) with another and for the \ntransfer of problems in one area to another. Since its origins in the 1940s motivated by \nconnections between algebra and geometry, category theory has been applied to diverse fields, \nincluding computer science, logic and linguistics. This course introduces the basic notions of \ncategory theory: adjunction, natural transformation, functor and category. We will use category \ntheory to organize and develop the kinds of structure that arise in models and semantics for \nlogics and programming languages. \n \nSyllabus \nIntroduction; some history. Definition of category. The category of sets and functions. \nCommutative diagrams. Examples of categories: preorders and monotone functions; monoids \nand monoid homomorphisms; a preorder as a category; a monoid as a category. Definition of \nisomorphism. Informal notion of a 'category-theoretic' property. \nTerminal objects. The opposite of a category and the duality principle. Initial objects. Free \nmonoids as initial objects. \nBinary products and coproducts. Cartesian categories. \nExponential objects: in the category of sets and in general. Cartesian closed categories: \ndefinition and examples. \nIntuitionistic Propositional Logic (IPL) in Natural Deduction style. Semantics of IPL in a cartesian \nclosed preorder. \nSimply Typed Lambda Calculus (STLC). The typing relation. Semantics of STLC types and \nterms in a cartesian closed category (ccc). The internal language of a ccc. The \nCurry-Howard-Lawvere correspondence. \nFunctors. Contravariance. Identity and composition for functors. \nSize: small categories and locally small categories. The category of small categories. Finite \nproducts of categories. \nNatural transformations. Functor categories. The category of small categories is cartesian \nclosed. \nHom functors. Natural isomorphisms. Adjunctions. Examples of adjoint functors. Theorem \ncharacterising the existence of right (respectively left) adjoints in terms of a universal property. \nDependent types. Dependent product sets and dependent function sets as adjoint functors. \nEquivalence of categories. Example: the category of I-indexed sets and functions is equivalent \nto the slice category Set/I. \nPresheaves. The Yoneda Lemma. Categories of presheaves are cartesian closed. \nMonads. Modelling notions of computation as monads. Moggi's computational lambda calculus. \nObjectives \nOn completion of this module, students should: \n \n\nbe familiar with some of the basic notions of category theory and its connections with logic and \nprogramming language semantics \nAssessment \na graded exercise sheet (25% of the final mark), and \na take-home test (75%) \nRecommended reading \nAwodey, S. (2010). Category theory. Oxford University Press (2nd ed.). \n \nCrole, R. L. (1994). Categories for types. Cambridge University Press. \n \nLambek, J. and Scott, P. J. (1986). Introduction to higher order categorical logic. Cambridge \nUniversity Press. \n \nPitts, A. M. (2000). Categorical Logic. Chapter 2 of S. Abramsky, D. M. Gabbay and T. S. E. \nMaibaum (Eds) Handbook of Logic in Computer Science, Volume 5. Oxford University Press. \n(Draft copy available here.) \n \nClass Size \nThis module can accommodate upto 15 Part II students plus 15 MPhil / Part III students. \n \nPre-registration evaluation form  \nStudents who are interested in taking this module will be asked to complete a pre-registration \nevaluation form to determine whether they have enough mathematical background to take the \nmodule.  \n \nThis will take place during the week Monday 12 August - Friday 16 August.  \n \nCoursework \nThe coursework in this module will consist of a graded exercise sheet. \n \nFurther Information \nCurrent Cambridge undergraduate students who are continuing onto Part III or the MPhil in \nAdvanced Computer Science may only take this module if they did NOT take 'Catgeory Theory' \nas a Unit of Assessment in Part II. \n \nThis module is shared with Part II of the Computer Science Tripos. Assessment will be adjusted \nfor the two groups of students to be at an appropriate level for whichever course the student is \nenrolled on. Further information about assessment and practicals will follow at the first lecture.\n\nDIGITAL MONEY AND DECENTRALISED FINANCE \n \nAims \nOur society is evolving towards digital payments and the elimination of physical cash. Digital \nalternatives to cash require trade-offs between various properties including unforgeability, \ntraceability, divisibility, transferability without intermediaries, privacy, redeemability, control over \nthe money supply and many more, often in conflict with each other. Since the 1980s, \ncryptographers have proposed a variety of clever technical solutions addressing some of these \nissues but it is only with the appearance of Bitcoin, blockchain and a plethora of copycat \ncryptocurrencies that a new asset class has now emerged, worth in excess of two trillion dollars \n(although plagued by extreme volatility). Meanwhile, most major countries have been planning \nthe introduction of Central Bank Digital Currencies in an attempt to retain control. This \nseminar-style interactive module encourages the students to understand what's happening, \nwhat the underlying problems and opportunities are (technical, social and economic) and where \nwe should go from here. We are at a historical turning point where an enterprising candidate \nmight disrupt the status quo and truly make a difference. \n \nSyllabus \nUnderstanding money: from gold standard to digital cash \nIntro to the Decentralised Finance (DeFi) ecosystem: Bitcoin, blockchain, wallets, smart \ncontracts \nStablecoins and Central Bank Digital Currencies \nDecentralised Autonomous Organisations and Automated Market Makers \nYield Farming, Perpetual Futures and Maximal Extractable Value \nWeb3, Non Fungible Tokens \nCryptocurrency crashes; crimes facilitated by cryptocurrencies \nNew areas of development. Where from here? \nThe module is structured as a reading club with a high degree of interactivity. Aside from the first \nand last session, which are treated differently, the regular two-hour sessions have the following \nstructure (not necessarily in this order). \n \nTwo half-hour debates on each of two papers \nTwo rapid-fire presentations on open questions previously assigned by the lecturers. \nHalf an hour of presentation by the lecturers. \nAbout ten minutes of buffer that can account for switchover time or be absorbed into the \nlecturers' presentation. \nThe first session is scene-setting by the lecturers, with no student presentations. The last \nsession has no paper debates, only one rapid-fire presentation on an open question from each \nof the students. \n \nObjectives \nOn completion of this module, students will gain an in-depth understanding of digital money \nalternatives as well as many modern applications and components of Decentralised Finance. By \ncomparing DeFi with current processes in traditional banking, students will be able to discuss \n\nthe merits and limitations of these new technologies as well as to identify potential new areas of \ndevelopment. \n \nThrough having to write, present and defend very brief extended abstracts, the students will \nimprove their communication skills and in particular the ability to distil and convey the most \nimportant points forcefully and concisely. \n \n \nAssessment \nThe module is an interactive reading club. Each student produces four output triplets throughout \nthe course. Each output triplet consists of three parts: a 1200-word essay, a 200-word extended \nabstract and a 7-minute presentation. \n \nThe essay and abstract must be submitted in advance, using the LaTeX templates provided. \nThe abstract must consist of full sentences, not bullet points, and must fit on a single slide. \nDuring the presentation, the slide with the abstract will be projected on screen as the only visual \naid; the essay will not be accessible to either presenter or audience. Reading out the slide \nverbatim will obviously not count as a great presentation. Presenters must be able to expand \nand defend their ideas live, and answer questions from lecturers and audience. \n \nThe essays, the abstracts and the presentations are graded separately, each out of 10, and \nassigned weights of 60%, 20%, 20%, respectively, within the triplet. \n \nIn a regular 2-hour session (all but the first and last) there will be two paper slots and two \ninvestigation slots, plus a lecturer slot, described next. \n \nEach paper slot (30 min) involves a \u201csupporter\u201d and a \u201cdissenter\u201d for the paper, each \ncontributing one output triplet, and then a panel Q&A where all the other participants quiz the \nsupporter and dissenter. \n \nEach investigation slot (10 min) involves a single student contributing an output triplet on a \ngiven theme. \n \nRoles, papers and themes are assigned by the lecturers the previous week. \n \nIn the first session there will be no student presentations, only lecturer-led exploration. In the \nlast session there will be 12 investigation slots and no paper slots. \n \nOverall, each student will be supporter once, dissenter once and investigator twice, for a total of \n4 output triplets. The output triplets of the last session will be weighed twice as much as the \nothers, i.e. 40% each, while the others 20% each. \n \nAttendance is required at all sessions. Missing a session in which the candidate was due to \npresent will result in the loss of the corresponding presentation marks, while the written work will \n\nstill have to be submitted and will be marked as usual. Missing a session in which the candidate \nwas not due to present will result in the loss of 3%. \n \nRecommended reading \nThere isn't a textbook covering all the material in this course. The following textbook, also \nadopted by R47 Distributed Ledger Technologies (also freely available online), is a thorough \nintroduction to Bitcoin and Blockchain, but we will complement it with research papers and \nindustry white papers for each lecture. \n \nNarayanan, A. , Bonneau, J., Felten, E., Miller, A. and Goldfeder, S. (2016). Bitcoin and \nCryptocurrency Technologies: A Comprehensive Introduction. Princeton University Press. \n(2016 Draft available here: \nhttps://d28rh4a8wq0iu5.cloudfront.net/bitcointech/readings/princeton_bitcoin_book.pdf)\n \n\nDIGITAL SIGNAL PROCESSING \n \nAims \nThis course teaches basic signal-processing principles necessary to understand many modern \nhigh-tech systems, with examples from audio processing, image coding, radio communication, \nradar, and software-defined radio. Students will gain practical experience from numerical \nexperiments in programming assignments (in Julia, MATLAB or NumPy). \n \nLectures \nSignals and systems. Discrete sequences and systems: types and properties. Amplitude, \nphase, frequency, modulation, decibels, root-mean square. Linear time-invariant systems, \nconvolution. Some examples from electronics, optics and acoustics. \nPhasors. Eigen functions of linear time-invariant systems. Review of complex arithmetic. \nPhasors as orthogonal base functions. \nFourier transform. Forms and properties of the Fourier transform. Convolution theorem. Rect \nand sinc. \nDirac\u2019s delta function. Fourier representation of sine waves, impulse combs in the time and \nfrequency domain. Amplitude-modulation in the frequency domain. \nDiscrete sequences and spectra. Sampling of continuous signals, periodic signals, aliasing, \ninterpolation, sampling and reconstruction, sample-rate conversion, oversampling, spectral \ninversion. \nDiscrete Fourier transform. Continuous versus discrete Fourier transform, symmetry, linearity, \nFFT, real-valued FFT, FFT-based convolution, zero padding, FFT-based resampling, \ndeconvolution exercise. \nSpectral estimation. Short-time Fourier transform, leakage and scalloping phenomena, \nwindowing, zero padding. Audio and voice examples. DTFM exercise. \nFinite impulse-response filters. Properties of filters, implementation forms, window-based FIR \ndesign, use of frequency-inversion to obtain high-pass filters, use of modulation to obtain \nband-pass filters. \nInfinite impulse-response filters. Sequences as polynomials, z-transform, zeros and poles, some \nanalog IIR design techniques (Butterworth, Chebyshev I/II, elliptic filters, second-order cascade \nform). \nBand-pass signals. Band-pass sampling and reconstruction, IQ up and down conversion, \nsuperheterodyne receivers, software-defined radio front-ends, IQ representation of AM and FM \nsignals and their demodulation. \nDigital communication. Pulse-amplitude modulation. Matched-filter detector. Pulse shapes, \ninter-symbol interference, equalization. IQ representation of ASK, BSK, PSK, QAM and FSK \nsignals. [2 hours] \nRandom sequences and noise. Random variables, stationary and ergodic processes, \nautocorrelation, cross-correlation, deterministic cross-correlation sequences, filtered random \nsequences, white noise, periodic averaging. \nCorrelation coding. Entropy, delta coding, linear prediction, dependence versus correlation, \nrandom vectors, covariance, decorrelation, matrix diagonalization, eigen decomposition, \n\nKarhunen-Lo\u00e8ve transform, principal component analysis. Relation to orthogonal transform \ncoding using fixed basis vectors, such as DCT. \nLossy versus lossless compression. What information is discarded by human senses and can \nbe eliminated by encoders? Perceptual scales, audio masking, spatial resolution, colour \ncoordinates, some demonstration experiments. \nQuantization, image coding standards. Uniform and logarithmic quantization, A/\u00b5-law coding, \ndithering, JPEG. \nObjectives \napply basic properties of time-invariant linear systems; \nunderstand sampling, aliasing, convolution, filtering, the pitfalls of spectral estimation; \nexplain the above in time and frequency domain representations; \nuse filter-design software; \nvisualize and discuss digital filters in the z-domain; \nuse the FFT for convolution, deconvolution, filtering; \nimplement, apply and evaluate simple DSP applications; \nfamiliarity with a number of signal-processing concepts used in digital communication systems \nRecommended reading \nLyons, R.G. (2010). Understanding digital signal processing. Prentice Hall (3rd ed.). \nOppenheim, A.V. and Schafer, R.W. (2007). Discrete-time digital signal processing. Prentice \nHall (3rd ed.). \nStein, J. (2000). Digital signal processing \u2013 a computer science perspective. Wiley. \n \nClass size \nThis module can accommodate a maximum of 24 students (16 Part II students and 8 MPhil \nstudents) \n \nAssessment - Part II students \nThree homework programming assignments, each comprising 20% of the mark \nWritten test, comprising 40% of the total mark. \nAssessment - Part III and MPhil students \nThree homework programming exercises, each comprising 10% of the mark. \nWritten test, comprising 20% of the total mark. \nIndividual implementation project with 8-page written report, topic agreed with lecturer, \ncomprising 50% of the mark. \nFurther Information \nCurrent Cambridge undergraduate students who are continuing onto Part III or the MPhil in \nAdvanced Computer Science may only take this module if they did NOT take it as a Unit of \nAssessment in Part II. \n \nThis module is shared with Part II of the Computer Science Tripos. Assessment will be adjusted \nfor the two groups of students to be at an appropriate level for whichever course the student is \nenrolled on. Further information about assessment and practicals will follow at the first lecture.\n\nINTRODUCTION TO COMPUTATIONAL SEMANTICS \n \nAims \nThis is a lecture-style course that introduces students to various aspects of the semantics of \nNatural Languages (mainly English): \n \nLexical Semantics, with an emphasis on theory and phenomenology  \nCompositional Semantics \nDiscourse and pragmatics-related aspects of semantics \nObjectives \nGive an operational definition of what is meant by \u201cmeaning\u201d (for instance, above and beyond \nsyntax); \nName the types of phenomena in language that require semantic consideration, in terms of \nlexical, compositional and discourse/pragmatic aspects, in other words, argue why semantics is \nimportant; \nDemonstrate an understanding of the basics of various semantic representations, including \nlogic-based and graph-based semantic representations, their properties, how they are used and \nwhy they are important, and how they are different from syntactic representations; \nKnow how such semantic representations are derived during or after parsing, and how they can \nbe analysed and mapped to surface strings; \nUnderstand applications of semantic representations e.g. reasoning, validation, and methods \nhow these are approached. \nWhen designing NL tasks that clearly require semantic processing (e.g. knowledge-based QA), \nto be aware of and reuse semantic representations and algorithms when designing the task, \nrather than reinventing the wheel. \nPractical advantages of this course for NLP students \nKnowledge of underlying semantic effects helps improve NLP evaluation, for instance by \nproviding more meaningful error analysis. You will be able to link particular errors to design \ndecisions inside your system. \nYou will learn methods for better benchmarking of your system, whatever the task may be. \nSupervised ML systems (in particular black-box systems such as Deep Learning) are only as \nclever as the datasets they are based on. In this course, you will learn to design datasets so that \nthey are harder to trick without real understanding, or critique existing datasets. \nYou will be able to design tests for ML systems that better pinpoint which aspects of language \nan end-to-end system has \u201cunderstood\u201d. \nYou will learn to detect ambiguity and ill-formed semantics in human-human communication. \nThis can serve to write more clearly and logically. \nYou will learn about decomposing complex semantics-reliant tasks sensibly so that you can \nreuse the techniques underlying semantic analyzers in a modular way. In this way, rather than \nbeing forced to treat complex tasks in an end-to-end manner, you will be able to profit from \npartial explanations and a better error analysis already built into the system. \nSyllabus \nOverview of the course \nEvents and semantic role labelling \n\nReferentiality and coreference resolution \nTruth-conditional semantics \nGraph-based meaning representation \nCompositional semantics \nContext-free Graph Rewriting \nSurface realisation \nNegation and psychological approach to semantics \nDynamic semantics \nGricean pragmatics \nVector space models \nCross-modality \nAcquisition of semantics \nDiachronic change of semantics \nSummary of the course \n  \n \nAssessment \n5 take-home exercises worth 20% each: \n \nTake-home assignment 1 is given in Lecture 4 and due is Lecture 6 \n \nStudents are given 10 English sentences and asked to provide their semantic analysis \naccording to truth-conditions. Students will be expected to return 10 logical expressions as their \nanswers. No word limit. Assessment criteria: correctness of the 10 logical expressions; 2 points \nfor each logical expression. \n \nTake-home assignment 2 is given in Lecture 7 and due is Lecture 9  \n \nStudents are given 10 English sentences and asked to provide their syntactico-semantic \nderivations according to the compositionality principle. Students will be expected to return \nderivation graphs as their answers. No word limit. Assessment criteria: correctness of the 10 \nderivation graphs; 2 points for each derivation. \n \nTake-home assignment 3 is given in Lecture 11 and due is Lecture 13  \n \nAll students are assigned with a paper on modelling common ground in dialogue system. \nStudents will receive related but different papers. Each student will write a review of their \nassigned paper, including a comprehensive summary and their own thoughts. Word limit: 1000 \nwords. Assessment criteria: 15 points on whether a student understands the paper correctly; 5 \npoints on whether a student is able to think critically. \n \nTake-home assignment 4 is given in Lecture 13 and due is Lecture 15 \n \n\nAll students are assigned with a paper on language-vision interaction. Students will receive \nrelated but different papers. Each student will write a review of their assigned paper, including a \ncomprehensive summary and their own thoughts. Word limit: 1000 words. Assessment criteria: \n15 points on whether a student understands the paper correctly; 5 points on whether a student \nis able to think critically. \n \nTake-home assignment 5 is given in Lecture 14 and due is two weeks later. \n \nContent: All students are assigned with a paper on bootstrapping language acquisition. All \nstudents will receive the same paper. Each student will write a review of the paper, including a \ncomprehensive summary and their own thoughts. Word limit: 1000 words. Assessment criteria: \n15 points on whether a student understands the paper correctly; 5 points on whether a student \nis able to think critically.\n \n\nINTRODUCTION TO NATURAL LANGUAGE SYNTAX AND PARSING \n \nAims \nThis module aims to provide a brief introduction to linguistics for computer scientists and then \ngoes on to cover some of the core tasks in natural language processing (NLP), focussing on \nstatistical parsing of sentences to yield syntactic representations. We will look at how to \nevaluate parsers and see how well state-of-the-art tools perform given current techniques. \n \nSyllabus \nLinguistics for NLP focusing on morphology and syntax \nGrammars and representations \nStatistical and neural parsing \nTreebanks and evaluation \nObjectives \nOn completion of this module, students should: \n \nunderstand the basic properties of human languages and be familiar with descriptive and \ntheoretical frameworks for handling these properties; \nunderstand the design of tools for NLP tasks such as parsing and be able to apply them to text \nand evaluate their performance; \nPractical work \nWeek 1-7: There will be non-assessed practical exercises between sessions and during \nsessions. \nWeek 8: Download and apply two parsers to a designated text. Evaluate the performance of the \ntools quantitatively and qualitatively. \nAssessment \nThere will be one presentation worth 10% of the final mark; presentation topics will be allocated \nat the start of term. \nAn assessed practical report of not more 8 pages in ACL conference format. It will contribute \n90% of the final mark. \nRecommended reading \nJurafsky, D. and Martin, J. (2008). Speech and Language Processing. Prentice-Hall (2nd ed.). \n(See also 3rd ed. available online.)\n \n\nINTRODUCTION TO NETWORKING AND SYSTEMS MEASUREMENTS \n \nAims \nSystems research refers to the study of a broad range of behaviours arising from complex \nsystem design, including: resource sharing and scheduling; interactions between hardware and \nsoftware; network topology, protocol and device design and implementation; low-level operating \nsystems; Interconnect, storage and more. This module will: \n \nTeach performance characteristics and performance measurement methodology and practice \nthrough profiling experiments; \nExpose students to real-world systems artefacts evident through different measurement tools; \nDevelop scientific writing skills through a series of laboratory reports; \nProvide research skills for characterization and modelling of systems and networks using \nmeasurements. \nPrerequisites \nIt is strongly recommended that students have previously (and successfully) completed an \nundergraduate networking course -- or have equivalent experience through project or \nopen-source work. \n \nSyllabus \nIntroduction to performance measurements, performance characteristics [1 lecture] \nPerformance measurements tools and techniques [2 lectures + 2 lab sessions] \nReproducible experiments [1 lecture + 1 lab session] \nCommon pitfalls in measurements [1 lecture] \nDevice and system characterisation [1 lecture + 2 lab sessions] \nObjectives \nOn completion of this module, students should: \n \nDescribe the objectives of measurements, and what they can achieve; \nCharacterise and model a system using measurements; \nPerform reproducible measurements experiments; \nEvaluate the performance of a system using measurements; \nOperate measurements tools and be aware of their limitations; \nDetect anomalies in the network and avoid common measurements pitfalls; \nWrite system-style performance evaluations. \nPractical work \nFive 2-hour in-classroom labs will ask students to develop and use skills in performance \nmeasurements as applied to real-world systems and networking artefacts. Results from these \nlabs (and follow-up work by students outside of the classroom) will by the primary input to lab \nreports. \n \nThe first three labs will provide an introduction and hands on experience with measurement \ntools and measurements methodologies, while the last two labs will focus on practical \nmeasurements and evaluation of specific platforms. Students may find it useful to work in pairs \n\nwithin the lab, but must prepare lab reports independently. The module lecturer will give a short \nintroductory at the start of each lab, and instructors will be on-hand throughout labs to provide \nassistance. \n \nLab participation is not directly included in the final mark, but lab work is a key input to lab \nreports that are assessed. Guided lab experiments resulting in practical write ups. \n \nAssessment \nEach student will write two lab reports. The first lab report will summarise the experiments done \nin the first three labs (20%). The second will be a lab report (5000 words) summarising the \nevaluation of a device or a system (80%). \n \nRecommended reading \nThe following list provides some background to the course materials, but is not mandatory. A \nreading list, including research papers, will be provided in the course materials. \n \nGeorge Varghese. Network algorithmics. Chapman and Hall/CRC, 2010. \nMark Crovella and Balachander Krishnamurthy. Internet measurement: infrastructure, traffic and \napplications. John Wiley and Sons, Inc., 2006. \nBrendan Gregg. Systems Performance: Enterprise and the Cloud, Prentice Hall Press, Upper \nSaddle River, NJ, USA, October 2013. \nRaj Jain, The Art of Computer Systems Performance Analysis: Techniques for Experimental \nDesign, Measurement, Simulation, and Modeling, Wiley - Interscience, New York, NY, USA, \nApril 1991.\n \n\nLARGE-SCALE DATA PROCESSING AND OPTIMISATION \n \nAims \nThis module provides an introduction to large-scale data processing, optimisation, and the \nimpact on computer system's architecture. Large-scale distributed applications with high volume \ndata processing such as training of machine learning will grow ever more in importance. \nSupporting the design and implementation of robust, secure, and heterogeneous large-scale \ndistributed systems is essential. To deal with distributed systems with a large and complex \nparameter space, tuning and optimising computer systems is becoming an important and \ncomplex task, which also deals with the characteristics of input data and algorithms used in the \napplications. Algorithm designers are often unaware of the constraints imposed by systems and \nthe best way to consider these when designing algorithms with massive volume of data. On the \nother hand, computer systems often miss advances in algorithm design that can be used to cut \ndown processing time and scale up systems in terms of the size of the problem they can \naddress. Integrating machine learning approaches (e.g. Bayesian Optimisation, Reinforcement \nLearning) for system optimisation will be explored in this course. \n \nSyllabus \nThis course provides perspectives on large-scale data processing, including data-flow \nprogramming, graph data processing, probabilistic programming and computer system \noptimisation, especially using machine learning approaches, thus providing a solid basis to work \non the next generation of distributed systems. \n \nThe module consists of 8 sessions, with 5 sessions on specific aspects of large-scale data \nprocessing research. Each session discusses 3-4 papers, led by the assigned students. One \nsession is a hands-on tutorial on on dataflow programming fundamentals. The first session \nadvises on how to read/review a paper together with a brief introduction on different \nperspectives in large-scale data \nprocessing and optimisation. The last session is dedicated to the student presentation of \nopensource project studies. \n \nIntroduction to large-scale data processing and optimisation \nData flow programming: Map/Reduce to TensorFlow \nLarge-scale graph data processing: storage, processing model and parallel processing \nDataflow programming hands-on tutorial \nProbabilistic Programming \nOptimisations in ML Compiler \nOptimisations of Computer systems using ML \nPresentation of Open Source Project Study \nObjectives \nOn completion of this module, students should: \n \nUnderstand key concepts of scalable data processing approaches in future computer systems. \n\nObtain a clear understanding of building distributed systems using data centric programming \nand large-scale data processing. \nUnderstand a large and complex parameter space in computer system's optimisation and \napplicability of Machine Learning approach. \nCoursework \nReading Club: \nThe preparation for the reading club will involve 1-3 papers every week. At each session, \naround 3-4 papers are selected under the given topic, and the students present their review \nwork. \nHands-on tutorial session of data flow programming including writing an application of \nprocessing streaming in Twitter data and/or Deep Neural Networks using Google TensorFlow \nusing cluster computing. \nReports \nThe following three reports are required, which could be extended from the assignment of the \nreading club, within the scope of data centric systems. \n \nReview report on a full length paper (max 1800 words) \nDescribe the contribution of the paper in depth with criticisms \nCrystallise the significant novelty in contrast to other related work \nSuggestions for future work \nSurvey report on sub-topic in large-scale data processing and optimisation (max 2000 words) \nPick up to 5 papers as core papers in the survey scope \nRead the above and expand reading through related work \nComprehend the view and finish an own survey paper \nProject study and exploration of a prototype (max 2500 words) \nWhat is the significance of the project in the research domain? \nCompare with similar and succeeding projects \nDemonstrate the project by exploring its prototype \nReports 1 should be handed in by the end of 5th week; Report 2 in the 8th week and Report 3 \non the first day of Lent Term. \n \nAssessment \nThe final grade for the course will be provided as a percentage, and the assessment will consist \nof two parts: \n \n25%: for reading club (participation, presentation) \n75%: for the three reports: \n15%: Intensive review report \n25%: Survey report \n35%: Project study \nRecommended reading \nM. Abadi et al. TensorFlow: A System for Large-Scale Machine Learning, OSDI, 2016. \nD. Aken et al.: Automatic Database Management System Tuning Through Large-scale Machine \nLearning, SIGMOD, 2017. \n\nJ. Ansel et al. Opentuner: an extensible framework for program autotuning. PACT, 2014. \nV. Dalibard, M. Schaarschmidt, E. Yoneki. BOAT: Building Auto-Tuners with Structured Bayesian \nOptimization, WWW, 2017. \nJ. Dean et al. Large scale distributed deep networks. NIPS, 2012. \nG. Malewicz, M. Austern, A. Bik, J. Dehnert, I. Horn, N. Leiser, G. Czajkowski. Pregel: A System \nfor Large-Scale Graph Processing, SIGMOD, 2010. \nA. Mirhoseini et al. Device Placement Optimization with Reinforcement Learning, ICML, 2017. \nD. Murray, F. McSherry, R. Isaacs, M. Isard, P. Barham, M. Abadi. Naiad: A Timely Dataflow \nSystem, SOSP, 2013. \nM. Schaarschmidt, S. Mika, K. Fricke and E. Yoneki: RLgraph: Modular Computation Graphs for \nDeep Reinforcement Learning, SysML, 2019. \nZ. Jia, O. Padon, J. Thomas, T. Warszawski, M. Zaharia,  A. Aiken: TASO: Optimizing Deep \nLearning Computation with Automated Generation of Graph Substitutions: SOSP, 2019. \nH. Mao et al.: Park: An Open Platform for Learning-Augmented Computer Systems, \nOpenReview, 2019. \nJ. Shao, et al.: Tensor Program Optimization with Probabilistic Programs, NeurIPS, 2022.  \nA complete list can be found on the course material web page. See also 2023-2024 course \nmaterial on the previous course Large-Scale Data Processing and Optimisation.\n \n\nMACHINE LEARNING AND THE PHYSICAL WORLD \nAims \nThe module \u201cMachine Learning and the Physical World\u201d is focused on machine learning \nsystems that interact directly with the real world. Building artificial systems that interact with the \nphysical world have significantly different challenges compared to the purely digital domain. In \nthe real world data is scares, often uncertain and decisions can have costly and irreversible \nconsequences. However, we also have the benefit of centuries of scientific knowledge that we \ncan draw from. This module will provide the methodological background to machine learning \napplied in this scenario. We will study how we can build models with a principled treatment of \nuncertainty, allowing us to leverage prior knowledge and provide decisions that can be \ninterrogated. \n \nThere are three principle points about machine learning in the real world that will concern us. \n \nWe often have a mechanistic understanding of the real world which we should be able to \nbootstrap to make decisions. For example, equations from physics or an understanding of \neconomics. \nReal world decisions have consequences which may have costs, and often these cost functions \nneed to be assimilated into our machine learning system. \nThe real world is surprising, it does things that you do not expect and accounting for these \nchallenges requires us to build more robust and or interpretable systems. \nDecision making in the real world hasn\u2019t begun only with the advent of machine learning \ntechnologies. There are other domains which take these areas seriously, physics, environmental \nscientists, econometricians, statisticians, operational researchers. This course identifies how \nmachine learning can contribute and become a tool within these fields. It will equip you with an \nunderstanding of methodologies based on uncertainty and decision making functions for \ndelivering on these challenges. \n \nObjectives \nYou will gain detailed knowledge of \n \nsurrogate models and uncertainty \nsurrogate-based optimization \nsensitivity analysis \nexperimental design \nYou will gain knowledge of \n \ncounterfactual analysis \nsurrogate-based quadrature \nSchedule \nWeek 1: Introduction to the unit and foundation of probabilistic modelling \n \nWeek 2: Gaussian processes and probablistic inference \n \n\nWeek 3: Simulation and Sequential decision making under uncertainty \n \nWeek 4: Emulation and Experimental Design \n \nWeek 5: Sensitivity Analysis and Multifidelity Modelling \n \nWeek 6-8: Case studies of applications and Projects \n \nPractical work \nDuring the first five weeks of the unit we will provide a weekly worksheet that will focus on \nimplementation and practical exploration of the material covered in the lectures. The worksheets \nwill allow you to build up a these methods without relying on extensive external libraries. You are \nfree to use any programming language of choice however we highly recommended the use of \n=Python=. \n \nAssessment \nTwo individual tasks (15% each) \nGroup Project (70%). Each group will work on an application of uncertainty that covers the \nmaterial of the first 5 weeks of lectures in the unit. Each group will submit a report which will \nform the basis of the assessment. In addition to the report each group will also attend a short \noral examination based on the material covered both in the report and the taught material. \nRecommended Reading \nRasmussen, C. E. and Williams, C. K. I. (2006). Gaussian Processes for Machine Learning. MIT \nPress \n \nBishop, C. (2006). Pattern recognition and machine learning. Springer. \nhttps://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-an\nd-Machine-Learning-2006.pdf \n \nLaplace, P. S. (1902). A Philosophical Essay on Probabilities. John Wiley & Sons. \nhttps://archive.org/details/philosophicaless00lapliala\n \n\nMACHINE VISUAL PERCEPTION \nAims \nThis course aims at introducing the theoretical foundations and practical techniques for machine \nperception, the capability of computers to interpret data resulting from sensor measurements. \nThe course will teach the fundamentals and modern techniques of machine perception, i.e. \nreconstructing the real world starting from sensor measurements with a focus on machine \nperception for visual data. The topics covered will be image/geometry representations for \nmachine perception, semantic segmentation, object detection and recognition, geometry \ncapture, appearance modeling and acquisition, motion detection and estimation, \nhuman-in-the-loop machine perception, select topics in applied machine perception. \n \nMachine perception/computer vision is a rapidly expanding area of research with real-world \napplications. An understanding of machine perception is also important for robotics, interactive \ngraphics (especially AR/VR), applied machine learning, and several other fields and industries. \nThis course will provide a fundamental understanding of and practical experience with the \nrelevant techniques. \n \nLearning outcomes \nStudents will understand the theoretical underpinnings of the modern machine perception \ntechniques for reconstructing models of reality starting from an incomplete and imperfect view of \nreality. \nStudents will be able to apply machine perception theory to solve practical problems, e.g. \nclassification of images, geometry capture. \nStudents will gain an understanding of which machine perception techniques are appropriate for \ndifferent tasks and scenarios. \nStudents will have hands-on experience with some of these techniques via developing a \nfunctional machine perception system in their projects. \nStudents will have practical experience with the current prominent machine perception \nframeworks. \nSyllabus \nThe fundamentals of machine learning for machine perception \nDeep neural networks and frameworks for machine perception \nSemantic segmentation of objects and humans \nObject detection and recognition \nMotion estimation, tracking and recognition \n3D geometry capture \nAppearance modeling and acquisition \nSelect topics in applied machine perception \nAssessment \nA practical exercise, worth 20% of the mark. This will cover the basics and theory of machine \nperception and some of the practical techniques the students will likely use in their projects. This \nis individual work. No GPU hours will be needed for the practical work. \nA machine perception project worth 80% of the marks: \n\nCourse projects will be selected by the students following the possible project themes proposed \nby the lecturer, and will be checked by the lecturer for appropriateness.  \nThe students will form groups of 2-3 to design, implement, report, and present a project to tackle \na given task in machine perception. \nThe final mark will be composed of an implementation/report mark (60%) and a presentation \nmark (20%). Each team member will be evaluated based on her/his contribution. \nEach project will have extensions to be completed only by the ACS students. Each student will \nwrite a different part of the report, whose author will be clearly marked. Each student will further \nsummarise her/his contributions to the project in the same report. \nRecommended Reading List \nComputer Vision: Algorithms and Applications, Richard Szeliski, Springer, 2010. \nDeep Learning, Ian Goodfellow, Yoshua Bengio, and Aaron Courville, MIT Press, 2016. \nMachine Learning and Visual Perception, Baochang Zhang, De Gruyter, 2020. \nFurther Information \nCurrent Cambridge undergraduate students who are continuing onto Part III or the MPhil in \nAdvanced Computer Science may only take this module if they did NOT take it as a Unit of \nAssessment in Part II. \n \nThis module is shared with Part II of the Computer Science Tripos. Assessment will be adjusted \nfor the two groups of students to be at an appropriate level for whichever course the student is \nenrolled on. Further information about assessment and practicals will follow at the first lecture.\n\nMOBILE, WEARABLE SYSTEMS AND MACHINE LEARNING \n \nAims \nThis module aims to introduce the latest research advancements in mobile systems and mobile \ndata machine learning, spanning a range of domains including systems, data gathering, \nanalytics and machine learning, on device machine learning and applications such as health, \ntransportation, behavior monitoring, cyber-physical systems, autonomous vehicles, drones. The \ncourse will cover current and seminal research papers in the area of research. \n \nSyllabus \nThe course will consist of one introductory lecture, seven two-hour, and one three-hour, \nsessions covering a variety of topics roughly including the following material (some variation in \nthe topics might happen from year to year): \n \nSystem, Energy and Security \nBackscatter Communication, Battery Free and Energy Harvesting Devices \nNew Sensing Modalities \nMachine Learning on Wearable Data \nOn Device Machine Learning \nMobile and Wearable Health \nMobile and Wearable Systems of Sustainability \nEach week, three class participants will be assigned to introduce assigned three papers via \n20-minute presentations, conference-style and highlighting critically its features. Each \npresentation will be followed by 10 minutes of questions. This will be followed by 10 minutes of \ngeneral discussion. Slides will be used for presentation. \n \nStudents will give one or more presentations each term. Each student will submit a paper review \neach week for one of the three papers presented except for the week they will be presenting \nslides. Each review will follow a template and be up to 1,000 words. Each review will receive a \nmaximum of 10 points. As a result, each student will produce 6-7 reviews and at least one \npresentation, probably two. All participants are expected to attend and participate in every class; \nthe instructor must be notified of any absences in advance. \n \nObjectives \nOn completion of this module students should have an understanding of the recent key research \nin mobile and sensor systems and mobile analytics as well as an improved critical thinking over \nresearch papers. \n \nAssessment \nAggregate mark for 7 assignments from 6-7 reports and 1-2 presentations. Each report or \npresentation will contribute one seventh of the course mark. \nA tick for presence and participation to each class will also be awarded. \nRecommended reading \n\nReadings from the most recent conferences such as AAAI, ACM KDD, ACM MobiCom, ACM \nMobiSys, ACM SenSys, ACM UbiComp, ICLR, ICML Neurips, and WWW pertinent to mobile \nsystems and data.\n \n\nNETWORK ARCHITECTURES \nAims \nThis module aims to provide the world with more network architects. The 2011-2012 version \nwas oriented around the evolution of IP to support new services like multicast, mobility, \nmultihoming, pub/sub and, in general, data oriented networking. The course is a paper reading \nwhich puts the onus on the student to do the work. \n \nSyllabus \nIPng [2 lectures, Jon Crowcroft] \nNew Architectures [2 lectures, Jon Crowcroft] \nMulticast [2 lectures, Jon Crowcroft] \nContent Distribution and Content Centric Networks [2 lectures, Jon Crowcroft] \nResource Pooling [2 lectures, Jon Crowcroft] \nGreen Networking [2 lectures, Jon Crowcroft] \nAlternative Router Implementions [2 lectures, Jon Crowcroft] \nData Center Networks [2 Lectures, Jon Crowcroft] \nObjectives \nOn completion of this module, students should be able to: \n \ncontribute to new network system designs; \nengineer evolutionary changes in network systems; \nidentify and repair architectural design flaws in networked systems; \nsee that there are no perfect solutions (aside from academic ones) for routing, addressing, \nnaming; \nunderstand tradeoffs in modularisation and other pressures on clean software systems \nimplementation, and see how the world is changing the proper choices in protocol layering, or \nnon layered or cross-layered. \nCoursework \nAssessment is through three graded essays (each chosen individually from a number of \nsuggested or student-chosen topics), as follows: \n \nAnalysis of two different architectures for a particular scenario in terms of cost/performance \ntradeoffs for some functionality and design dimension, for example: \nATM \u2013 e.g. for hardware versus software tradeoff \nIP \u2013 e.g. for mobility, multi-homing, multicast, multipath \n3GPP \u2013 e.g. for plain complexity versus complicatedness \nA discursive essay on a specific communications systems component, in a particular context, \nsuch as ad hoc routing, or wireless sensor networks. \nA bespoke network design for a narrow, well specified specialised target scenario, for example: \nA customer baggage tracking network for an airport. \nin-flight entertainment system. \nin-car network for monitoring and control. \ninter-car sensor/control network for automatic highways. \nPractical work \n\nThis course does not feature any implementation work due to time constraints. \n \nAssessment \nThree 1,200-word essays (worth 25% each), and \nan annotated bibliography (25%). \nRecommended reading \nPre-course reading: \n \nKeshav, S. (1997). An engineering approach to computer networking. Addison-Wesley (1st ed.). \nISBN 0201634422 \nPeterson, L.L. and Davie, B.S. (2007). Computer networks: a systems approach. Morgan \nKaufmann (4th ed.). \n \nDesign patterns: \n \nDay, John (2007). Patterns in network architecture: a return to fundamentals. Prentice Hall. \n \nExample systems: \n \nKrishnamurthy, B. and Rexford, J. (2001). Web protocols and practice: HTTP/1.1, Networking \nprotocols, caching, and traffic measurement. Addison-Wesley. \n \nEconomics and networks: \n \nFrank, Robert H. (2008). The economic naturalist: why economics explains almost everything. \n \nPapers: \n \nCertainly, a collection of papers (see ACM CCR which publishes notable network researchers' \nfavourite ten papers every 6 months or so).\n \n\nOVERVIEW OF NATURAL LANGUAGE PROCESSING \n \nAims \nThis course introduces the fundamental techniques of natural language processing. It aims to \nexplain the potential and the main limitations of these techniques. Some current research issues \nare introduced and some current and potential applications discussed and evaluated. Students \nwill also be introduced to practical experimentation in natural language processing. \n \nLectures \nOverview. Brief history of NLP research, some current applications, components of NLP \nsystems. \nMorphology and Finite State Techniques. Morphology in different languages, importance of \nmorphological analysis in NLP, finite-state techniques in NLP. \nPart-of-Speech Tagging and Log-Linear Models. Lexical categories, word tagging, corpora and \nannotations, empirical evaluation. \nPhrase Structure and Structure Prediction. Phrase structures, structured prediction, context-free \ngrammars, weights and probabilities. Some limitations of context-free grammars. \nDependency Parsing. Dependency structure, grammar-free parsing, incremental processing.  \nGradient Descent and Neural Nets. Parameter optimisation by gradient descent. Non-linear \nfunctions with neural network layers. Log-linear model as softmax layer. Current findings of \nNeural NLP. \nWord representations. Representing words with vectors, count-based and prediction-based \napproaches, similarity metrics. \nRecurrent Neural Networks. Modelling sequences, parameter sharing in recurrent neural \nnetworks, neural language models, word prediction. \nCompositional Semantics. Logical representations, compositional semantics, lambda calculus, \ninference and robust entailment. \nLexical Semantics. Semantic relations, WordNet, word senses. \nDiscourse. Discourse relations, anaphora resolution, summarization. \nNatural Language Generation. Challenges of natural language generation (NLG), tasks in NLG, \nsurface realisation. \nPractical and assignments. Students will build a natural language processing system which will \nbe trained and evaluated on supplied data. The system will be built from existing components, \nbut students will be expected to compare approaches and some programming will be required \nfor this. Several assignments will be set during the practicals for assessment. \nObjectives \nBy the end of the course students should: \n \nbe able to discuss the current and likely future performance of several NLP applications; \nbe able to describe briefly a fundamental technique for processing language for several \nsubtasks, such as morphological processing, parsing, word sense disambiguation etc.; \nunderstand how these techniques draw on and relate to other areas of computer science. \nRecommended reading \n\nJurafsky, D. & Martin, J. (2023). Speech and language processing. Prentice Hall (3rd ed. draft, \nonline). \n \nAssessment - Part II Students \nAssignment 1 - 10% of marks \nAssignment 2 - 60% of marks \nAssignment 3 - 30% of marks \nCoursework \nSubmit work for 3 assignments as part of practical sessions on information extraction. \n \nThese include an annotation exercise, a feature-based classifier along with code repository and \ndocumentation, and a 4,000-word report on results and analysis from an extended information \nextraction experiment. \n \nPractical work \nStudents will build a natural language processing system which will be trained and evaluated on \nsupplied data. The system will be built from existing components, but students will be expected \nto compare approaches and some programming will be required for this. \n \nAssessment - Part III and MPhil Students \nAssessment will be based on the practicals: \n \nFirst practical exercise (10%, ticked) \nSecond practical exercise (25%, code repository or notebook with documentation) \nFinal report (65%, 4,000 words, excluding references) \nFurther Information \nAlthough the lectures don't assume any exposure to linguistics, the course will be easier to \nfollow if students have some understanding of basic linguistic concepts. The following may be \nuseful for this: The Internet Grammar of English \n \nCurrent Cambridge undergraduate students who are continuing onto Part III or the MPhil in \nAdvanced Computer Science may only take this module if they did NOT take it as a Unit of \nAssessment in Part II. \n \nStudents from other departments may attend the lectures for this module if space allows. \nHowever students wanting to take it for credit will need to make arrangements for assessment \nwithin their own department. \n \nThis module is shared with Part II of the Computer Science Tripos. Assessment will be adjusted \nfor the two groups of students to be at an appropriate level for whichever course the student is \nenrolled on. Further information about assessment and practicals will follow at the first lecture.\n\nPRACTICAL RESEARCH IN HUMAN-CENTRED AI \n \nAims \nThis is an advanced course in human-computer interaction, with a specialist focus on intelligent \nuser interfaces and interaction with machine-learning and artificial intelligence technologies. The \nformat will be largely Practical, with students carrying out a mini-project involving empirical \nresearch investigation. These studies will investigate human interaction with some kind of \nmodel-based system for planning, decision-making, automation etc. Possible study formats \nmight include: System evaluation, Field observation, Hypothesis testing experiment, Design \nintervention or Corpus analysis, following set examples from recent research publications. \nProject work will be formally evaluated through a report and presentation. \n \nLectures \n(note that Lectures 2-7 also include one hour class discussion of practical work) \n        \u2022 Current research themes in intelligent user interfaces \n        \u2022 Program synthesis \n        \u2022 Mixed initiative interaction \n        \u2022 Interpretability / explainable AI \n        \u2022 Labelling as a fundamental problem \n        \u2022 Machine learning risks and bias \n        \u2022 Visualisation and visual analytics \n        \u2022 Student research presentations \n \nObjectives \nBy the end of the course students should: \n        \u2022 be familiar with current state of the art in intelligent interactive systems \n        \u2022 understand the human factors that are most critical in the design of such systems \n        \u2022 be able to evaluate evidence for and against the utility of novel systems \n        \u2022 have experience of conducting user studies meeting the quality criteria of this field \n        \u2022 be able to write up and present user research in a professional manner \n \nClass Size \nThis module can accommodate upto 20 Part II, Part III and MPhil students. \n \nRecommended reading \nBrad A. Myers and Richard McDaniel (2000). Demonstrational Interfaces: Sometimes You Need \na Little Intelligence, Sometimes You Need a Lot. \n \nAlan Blackwell (2024). Moral Codes: Designing alternatives to AI \n \nAssessment - Part II Students \nThe format will be largely practical, with students carrying out an individual mini-project involving \nempirical research investigation. \n \n\nAssignment 1: six incremental submissions which together contribute 20% to the final module \nmark. \n \nAssignment 2: Final report - 80% of the final module mark \n  \n \nAssessment - MPhil / Part III Students \nThere will be a minor assessment component (20%) in which students compile a reflective diary \nthroughout the term, reporting on the weekly sessions. Diary entries should include citations to \nany key references, notes of possible further reading, summary of key points, questions relevant \nto the personal project, and points of interest noted in relation to the work of other students. \n \nThe major assessment component (80%) will involve a report on research findings, in the style \nof a submission to the ACM CHI or IUI conferences. This work will be submitted incrementally \nthrough the term, in order that feedback can be provided before final assessment of the full \nreport. Phased submissions will cover the following aspects of the empirical study: \n \nResearch question \nMethod \nLiterature Review \nIntroduction \nResults \nDiscussion / Conclusion \nFeedback on each phased submission will include an indicative mark for guidance, but with the \nunderstanding that the final grade will be based on the final delivered report, and that this may \ngo up (or possibly down), depending on how well the student responds to earlier feedback. \n \nReflective diary entries will also be assessed, but graded at a relatively coarse granularity \ncorresponding to the ACS grading bands, and with minimal written feedback. Informal and \ngeneric feedback will be offered verbally in class, and also potentially supplemented with peer \nassessment. \n \nFurther Information \nCurrent Cambridge undergraduate students who are continuing onto Part III or the MPhil in \nAdvanced Computer Science may only take this module if they did NOT take it as a Unit of \nAssessment in Part II. \n \nThis module is shared with Part II of the Computer Science Tripos. Assessment will be adjusted \nfor the two groups of students to be at an appropriate level for whichever course the student is \nenrolled on. Further information about assessment and practicals will follow at the first lecture.\n\nPRINCIPLES OF MACHINE LEARNING SYSTEMS \n \nPrerequisites: It is recommended that students have successfully completed introductory \ncourses, at an undergraduate level, in: 1) operating systems, 2) computer architecture, and 3) \nmachine learning. In addition, this course will heavily focus on deep neural network \nmethodologies and assume familiarity with common neural architectures and related algorithms. \nIf these topics were not covered within the machine learning course taken, students should \nsupplement by reviewing material in a book like: \"Dive into Deep Learning\". Finally, students are \nassumed to be comfortable with programming in Python. \nMoodle, timetable \n \nAims \nThis course will examine the emerging principles and methodologies that underpin scalable and \nefficient machine learning systems. Primarily, the course will focus on an exciting cross-section \nof algorithms and system techniques that are used to support the training and inference of \nmachine learning models under a spectrum of computing systems that range from constrained \nembedded systems up to large-scale distributed systems. It will also touch upon the new \nengineering practices that are developing in support of such systems at scale. When needed to \nappreciate issues of scalability and efficiency, the course will drill down to certain aspects of \ncomputer architecture, systems software and distributed systems and explore how these \ninteract with the usage and deployment of state-of-the-art machine learning. \n \nSyllabus \nTopics covered may include the following, with confirmation a month before the course begins: \n \nSystem Performance Trade-offs \nDistributed Learning Algorithms  \nModel Compression  \nDeep Learning Compilers  \nFrameworks and Run-times  \nScalable Inference Serving  \nDevelopment Practices  \nAutomated Machine Learning  \nFederated Learning  \nPrimarily, topics are covered with conventional lectures. However, where appropriate, material \nwill be delivered through hands-on lab tutorials. Lab tutorials will make use of hardware \nincluding ARM microcontrollers and multi-GPU machines to explore forms of efficient machine \nlearning (any necessary equipment will be provided to students) \n \nAssessment \nEach student will be assessed on 3 labs which will be worth 30% of their grade. They will also \nundertake a written project report which will be worth 70% of the grade. This report will detail an \ninvestigation into a particular aspect of machine learning systems, this report will be made \navailable publicly.\n \n\nPROOF ASSISTANTS \nAims \nThis module introduces students to interactive theorem proving using Isabelle and Coq. It \nincludes techniques for specifying formal models of software and hardware systems and for \nderiving properties of these models. \n \n  \n \nSyllabus \nIntroduction to proof assistants and logic. \nReasoning in predicate logic and typed set theory. \nReasoning in dependent type theory. \nInductive definitions and recursive functions: modelling them in logic, reasoning about them. \nModelling operational semantics definitions and proving properties. \n  \n \nObjectives \nOn completion of this module, students should: \n \npossess good skills in the use of Isabelle and Coq; \nbe able to specify inductive definitions and perform proofs by induction; \nbe able to formalise and reason about a variety of specifications in a proof assistant. \nCoursework \nPractical sessions will allow students to develop skills. Some of the exercises will serve as the \nbasis for assessment. \n \nAssessment \nAssessment will be based on two marked assignments (due in weeks 5 and 8) with students \nperforming small formalisation and verification projects in a proof assistant together with a \nwrite-up explaining their work (word limit 2,500 per project for the write-up). Each of the \nassignments will be worth 100 marks, distributed as follows: \n \n50 marks for completing basic formalisation and verification tasks assessing grasp of the \nmaterial taught in the lecture. Students will submit their work as theory files for either the \nIsabelle or Coq proof assistant, and they will be assessed for correctness and completeness of \nthe specifications and proofs. \n20 marks for completing designated more challenging tasks, requiring the use of advanced \ntechniques or creative proof strategies. \n30 marks for a clear write-up explaining the design decisions made during the formalisation and \nthe strategy used for the proofs, where 10 of these marks will be reserved for write-ups of \nexceptional quality, e.g. demonstrating particular insight. \nThe main tasks in the assignments will be designed to assess the student's proficiency with the \nbasic material and techniques taught in the lectures and the practical sessions, while the more \nchallenging tasks will give exceptional students the opportunity to earn distinction marks \n\n \nRecommended reading \nNipkow, T., Klein, G. (2014). Concrete Semantics with Isabelle/HOL. (The first part of this book, \n\u201cProgramming and Proving in Isabelle/HOL\u201d, comes with the Isabelle distribution.)  \n \nNipkow, T., Paulson, L.C. and Wenzel, M. (2002). A proof assistant for higher-order logic. \nSpringer LNCS 2283.  \n \nThe Software Foundations series of books, in particular the first two: \n \nPierce, B., Azevedo de Amorim, A., Casinghino, C., Gaboardi, M., Greenberg, M., Hri\u0163cu, C., \nSj\u00f6berg, V., Yorgey, B. (2023). Logical Foundations  \nPierce, B., Azevedo de Amorim, A., Casinghino, C., Gaboardi, M., Greenberg, M., Hri\u0163cu, C., \nSj\u00f6berg, V., Tolmach, A., Yorgey, B. (2023). Programming Language Foundations  \nChlipala, A. (2022). Certified programming with dependent types: a pragmatic introduction to the \nCoq proof assistant. MIT Press.  \n \nSergey, I. (2014). Programs and Proofs: Mechanizing Mathematics with Dependent Types.  \n \nAll of these are freely available online.\n \n\nQUANTUM ALGORITHMS AND COMPLEXITY \n \nAims \nThis module is a research-focused introduction to the theory of quantum computing. The aim is \nto prepare the students to conduct research in quantum algorithms and quantum complexity \ntheory. \n \nSyllabus \nTopics will vary from year to year, based on developments in cutting edge research. \nRepresentative topics include: \n \nQuantum algorithms: \n \nQuantum learning theory \nQuantum property testing \nShadow tomography \nQuantum walks \nQuantum state/unitary synthesis. \nStructure vs randomness in quantum algorithms \nQuantum complexity theory: \n \nThe quantum PCP conjecture \nEntanglement and MIP \nState complexity, AdS/CFT, and quantum gravity \nQuantum locally testable codes \nPseudorandom states and unitaries \nQuantum zero-knowledge proofs \nObjectives \nOn completion of this module, students should: \n \nbe familiar with contemporary quantum algorithms \ndevelop an understanding of the power and limitations of quantum computation \nconduct research in quantum algorithms and complexity theory \nAssessment \nMini research project (70%) \nPresentation (20%) \nAttendance and participation (10%) \nTimelines for the assignment submissions:  \n \nsubmit questions/observations on a weekly basis (i.e., Weeks 2-8) \ndeliver talks in Weeks 5-8 \nsubmit their mini-project at the end of term \nRecommended reading material and resources \n\u201cQuantum Computing Since Democritus\u201d by Scott Aaronson \n\n \n\u201cIntroduction to Quantum Information Science\u201d by Scott Aaronson \n \n\u201cQuantum Computing: Lecture Notes\u201d by Ronald de Wolf \n \n\u201cQuantum Computation and Quantum Information\u201d by Nielsen and Chuang\n \n\nUNDERSTANDING QUANTUM ARCHITECTURE \nPrerequisites: Requires introductory linear algebra - concepts such as eigenvalues, Hermitian \nmatrices, unitary matrices. Taking the Part II Quantum Computing course (or similar) is helpful \nbut not required. Familiarity with computer architecture and compilers is helpful. \nMoodle, timetable \n \nAims \nThis course covers the architecture of a practical-scale quantum computer. We will examine the \nresource requirements of practical quantum applications, understand the different layers of the \nquantum stack, the techniques used in these layers and examine how these layers come \ntogether to enable practical quantum advantage over classical computing. \n \nSyllabus \nThe course has two parts: a series of lectures to cover important aspects of the quantum stack \nand a set of student presentations. The following are a list of representative topics: \n \n- Basics of quantum computing \n- The fault-tolerant quantum stack \n- Compilation \n- Instruction sets \n- Implementations of quantum error correction \n- Implementation of magic state distillation \n- Resource estimation \n \nStudent presentations will be based on a reading list of important papers in quantum \narchitecture.  \n  \n \nObjectives \nAt the end of the course, students will have a broad understanding of the quantum computing \nstack. They will understand how major qubit technologies work, design challenges in real \nquantum hardware, how quantum applications are mapped to a system and the importance of \nquantum error correction for scalability.  \n \nAssessment \nSeminar presentation: 20% \nRead one paper from a provided reading list \nPrepare a 20 minute presentation on it + 10 minutes of answering questions. \nThe student presentation should be similar to a conference presentation - convey the problem, \nwhat are prior solutions, what did the paper do, what are the results, what are future directions \nFor the 20% of the score, the score will be split as 15% and 5%. 15% based on how well the \nstudent understands and explains the paper to the rest of the audience. 5% based on the Q&A \nsession. \n \n\n  \n \nCourse project: 80% (split across a proposal, mid-term report, final report) \nA. Research proposal - at least 500 words (10% of total) \nB. Mid-term report - at least 1000 words (including aspects like the research problem, literature \nreview, what questions will be evaluated, any early ideas or methods (20% of total)  \nC. Final report - up to 4 pages double column in a conference paper style with 3000-4000 \nwords. In addition to the mid term report, should include aspects like results and future \ndirections. (50% of the total) \nD. Report on contributions (in case of group work by 2 students) - 1 paragraph on individual \ncontribution of the student. 1 paragraph on teammate's contribution. \nStudents may work alone in the project, in which case they need only components A-C. \nStudents may work in pairs of two, but they should in addition individually submit D. The \ninstructor may conduct a short viva in case contributions from both team members are not clear \nor imbalanced. \nRecommended reading \nNielsen M.A., Chuang I.L. (2010). Quantum Computation and Quantum Information. Cambridge \nUniversity Press. \n \nMermin N.D. (2007). Quantum Computer Science: An Introduction. Cambridge University Press. \n \nAssessing requirements to scale to practical quantum advantage (2022) Beverland et al.\n\n",
        "8th Semester \nADVANCED TOPICS IN CATEGORY THEORY \n \n \nTeaching \nThe teaching style will be lecture-based, but supported by a practical component where \nstudents will learn to use a proof assistant for higher category theory, and build a small portfolio \nof proofs. Towards the end of the course we will explore some of the exciting computer science \nresearch literature on monoidal and higher categories, and students will choose a paper and \npresent it to the class. \n \nAims \nThe module will introduce advanced topics in category theory. The aim is to train students to \nengage and start modern research on the mathematical foundations of higher categories, the \ngraphical calculus, monoids and representations, type theories, and their applications in \ntheoretical computer science, both classical and quantum. \n \nObjectives \nOn completion of this module, students should: \n \nBe familiar with the techniques of compositional category theory. \nHave a strong understanding of basic categorical semantic models. \nBegun exploring current research in monoidal categories and higher structures. \nSyllabus \nPart 1, lecture course: \nThe first part of the course introduces concepts from monoidal categories and higher categories, \nand explores their application in computer science. \n\u2010 Monoidal categories and the graphical calculus \n\u2010 The proof assistant homotopy.io \n\u2010 Coherence theorems and higher category theory \n\u2010 Linearity, superposition, duality, quantum entanglement \n\u2010 Monoids, Frobenius algebras and bialgebras \n\u2010 Type theory for higher category theory \n \nPart 2, exploring the research frontier: \nIn the second part of the course, students choose a research paper to study, and give a \npresentation to the class. \nThere is a nice varied literature related to the topics of the course, and the lecturer will supply a \nlist of suggested papers.  \n \nClasses \nThere will be three exercise sheets for homework, with accompanying classes by a teaching \nassistant to go over them. \n \n\nAssessment \nProblem sheets (50%) \nClass presentation (20%) \nPractical portfolio (30%) \nReading List \nChris Heunen and Jamie Vicary, \u201cCategory for Quantum Theory: An Introduction\u201d, Oxford \nUniversity Press\n \n\nADVANCED TOPICS IN COMPUTER SYSTEMS \n \nAims \nThis module will attempt to provide an overview of \u201csystems\u201d research. This is a very broad field \nwhich has existed for over 50 years and which has historically included areas such as operating \nsystems, database systems, file systems, distributed systems and networking, to name but a \nfew. The course will thus necessarily cover only a tiny subset of the field. \n \nMany good ideas in systems research are the result of discussing and debating previous work. \nA primary aim of this course therefore will be to educate students in the art of critical thinking: \nthe ability to argue for and/or against a particular approach or idea. This will be done by having \nstudents read and critique a set of papers each week. In addition, each week will include \npresentations from a number of participants which aim to advocate or criticise each piece of \nwork. \n \nSyllabus \nThe syllabus for this course will vary from year to year so as to cover a mixture of older and \nmore contemporary systems papers. Contemporary papers will be generally selected from the \npast 5 years, primarily drawn from high quality conferences such as SOSP, OSDI, ASPLOS, \nFAST, NSDI and EuroSys. Example topics might include: \n \nSystems Research and System Design \nOS Structure and Virtual Memory \nVirtualisation \nConsensus \nScheduling \nPrivacy \nData Intensive Computing \nBugs \nThe reading each week will involve a load equivalent to 3 full length papers. Students will be \nexpected to read these in detail and prepare a written summary and review. In addition, each \nweek will contain one or more short presentations by students for each paper. The types of \npresentation will include: \n \nOverview: a balanced presentation of the paper, covering both positive and negative aspects. \nAdvocacy: a positive spin on the paper, aiming to convince others of its value. \nCriticism: a negative take on the paper, focusing on its weak spots and omissions. \nThese presentation roles will be assigned in advance, regardless of the soi disant absolute merit \nof the paper or the preference of the student. Furthermore, all students \u2013 regardless of any \nassigned presentation role in a given week \u2013 will be expected to participate in the class by \nasking questions and generally entering into the debate. \n \nObjectives \n\nOn completion of this module students should have a broad understanding of some key papers \nand concepts in computer systems research, as well as an appreciation of how to argue for or \nagainst any particular idea. \n \nCoursework and practical work \nCoursework will be the production of the weekly paper reviews. Practical work will be presenting \npapers as appropriate, as well as ongoing participation in the class. \n \nAssessment \nAssessment consists of: \n \nOne essay per week for 7 weeks (10% each) \nPresentation (20%) \nParticipation in class over the term (10%) \nRecommended reading \nMost of the reading for this course will be in the form of the selected papers each week. \nHowever, the following may be useful background reading to refresh your knowledge from \nundergraduate courses: \n \nSilberschatz, A., Peterson, J.L. and Galvin, P.C. (2005). Operating systems concepts. \nAddison-Wesley (7th ed.). \n \nTanenbaum, A.S. (2008). Modern Operating Systems. Prentice-Hall (3rd ed.). \n \nBacon, J. and Harris, T. (2003). Operating systems. Addison-Wesley (3rd ed.). \n \nAnderson, T. and Dahlin, M. (2014). Operating Systems: Principles and Practice. Recursive \nBooks (2nd ed.). \n \nHennessy, J. and Patterson, D. (2006). Computer architecture: a quantitative approach. Elsevier \n(4th ed.). ISBN 978-0-12-370490-0. \n \nKleppmann M (2016) Designing Data-Intensive Applications, O'Reilly (1st ed.)\n \n\nADVANCED TOPICS IN MACHINE LEARNING \nAims \nThis course explores current research topics in machine learning in sufficient depth that, at the \nend of the course, participants will be in a position to contribute to research on their chosen \ntopics. Each topic will be introduced with a lecture which, building on the material covered in the \nprerequisite courses, will make the current research literature accessible. Each lecture will be \nfollowed by up to three sessions which will typically be run as a reading group with student \npresentations on recent papers from the literature followed by a discussion, or a practical, or \nsimilar. \n \nStructure \nEach student will attend 3 topics and each topic's sessions will be spread over 5 contact hours. \nStudents will be expected to undertake readings for their selected topics. There will be some \ngroup work. \n \nThere will be a briefing session in Michaelmas term. \n \nSyllabus \nStudents choose five topics in preferential order from a list to be published in Michaelmas term. \nThey will be assigned to three topics out of their list. Students are assessed on one of these \ntopics which may not necessarily be their first choice topic. \n \nThe topics to be offered in 2024-25 are yet to be decided but to give an indicative idea of the \ntypes of topics, the ones offered in 2023-24 were: \n \nImitation learning  Prof A. Vlachos \nThe Future of Large Language Models: data architectures ethics Dr M. Tomalin \nPhysics and Geometry in Machine Learning Dr C. Mishra \nDiffusion Models and SDEs Francisco Vargas, Dr C. H. Ek \nExplainable Artificial Intelligence M. Espinosa, Z. Shams, Prof M. Jamnik \nUnconventional approaches to AI Dr S. Banerjee \nNarratives in Artificial Intelligence and Machine Learning Prof N. Lawrence \nMultimodal Machine Learning K. Hemker, N. Simidjievski, Prof M. Jamnik \nDeep Reinforcement Learning S. Morad, Dr C. H. Ek \nAutomation in Proof Assistants A. Jiang, W. Li, Prof M. Jamnik \nOn completion of this module, students should: \n \nbe in a strong position to contribute to the research topics covered; \nunderstand the fundamental methods (algorithms, data analysis, specific tasks) underlying each \ntopic; \nand be familiar with recent research papers and advances in the field. \nCoursework \nStudents will typically work in groups to give a presentation on assigned papers. Alternatively, a \ntopic may include practical sessions. Each topic will typically consist of one preliminary lecture \n\nfollowed by 3 reading and discussion sessions, or several lectures followed by a practical \nsession. A typical topic can accommodate up to 9 students presenting papers. There will be at \nleast 10 minutes general discussion per session. \n \nFull coursework details will be published by October. \n \nAssessment \nCoursework will be marked by the topic leaders and second marked by the module conveners. \n \nParticipation in all assigned topics, 10% \nPresentation or practical work or similar (for one of the chosen topics), 20% \nTopic coursework (for one of the chosen topics), 70% \nIndividual topic coursework will be published late Michaelmas term. \n \nAssessment criteria for topic coursework will follow project assessment criteria here: \nhttps://www.cl.cam.ac.uk/teaching/exams/acs_project_marking.pdf \n \nPlease note that students will be assessed on one of their three chosen topics but this may not \nbe their first choice. \n \nRecommended reading \nTo be confirmed by each topic convenor.\n \n\nCOMPUTING FOR COLLECTIVE INTELLIGENCE \nPrerequisites: Familiarity with core machine learning paradigms - Basic calculus (analytical \nskills) - Basic discretre optimization (combinatorics) \nMoodle, timetable \n \nObjectives \nThere is a substantial body of academic work demonstrating that complex life is built by \ncooperation across scales: collections of genes cooperate to produce organisms, cells \ncooperate to produce multi-cellular organisms and multicellular animals cooperate to form \ncomplex social groups. Arguably, all intelligence is collective intelligence. Yet, to-date, many \nartificially intelligent agents (both embodied and virtual), are generally not conceived from the \nground up to interact with other intelligent agents (be it machines or humans). The canonical AI \nproblem is that of a monolithic and solitary machine confronting a non-social environment. This \ncourse aims to balance this trend by (i) equipping students with conceptual and practical \nknowledge on collective intelligence from a computational standpoint, and (ii) by conveying \nvarious computational paradigms by which collective intelligence can be modelled as well as \nsynthesized. \n \nAssessment \nThe assessment will be based on a reading-group paper presentation (individual or in pairs), \naccompanied by a 2-page summary, and a technical position paper (individual work) handed in \nafter the course. The position paper will be assessed based on its technical correctness, \nstrength or arguments, \nand clarity of research vision, and will be accompanied by a brief 5-minute pre-recorded talk that \nsummarizes key arguments (any slides used are also submitted as part of the project). \n \nPaper presentation and 1-page summary: 25% \nTechnical position paper: 75% (4000 word limit) \nRecommended reading \nSwarm Intelligence : From Natural to Artificial Systems, Bonabeau et al (1999) \nJoined-Up Thinking, Hannah Critchlow, (2024) \nSupercooperators, Martin Nowak (2011) \nA Course on Cooperative Game Theory, Chakravarty et al., (2015) \nGoverning the Commons: The Evolution of Institutions for Collective Action. Ostrom (1990).  \n \n\nCRYPTOGRAPHY AND PROTOCOL ENGINEERING \nAims \nWe all use cryptographic protocols every day: whenever we access an https:// website or send a \nmessage via WhatsApp or Signal, for example. In this module, students will get hands-on \nexperience of how those protocols are implemented. Students start by writing their own secure \nmessaging protocol from scratch, and then move on to more advanced examples of \ncryptographic protocols for security and privacy, such as private information retrieval (searching \nfor data without revealing what you\u2019re searching for). \n \nSyllabus \nThis is a practical course in which the first half of each of the 2-hour weekly sessions is a lecture \nthat introduces a topic, while the second half is lab time during which the students can work on \ntheir implementations, discuss the topics in small groups, and get help from the lecturers. The \nmodule is structured into three blocks as follows:  \n \nCryptographic primitives (2 weeks). Using common building blocks such as symmetric ciphers \nand hash functions. Implementing selected aspects of elliptic curve Diffie-Hellman and digital \nsignatures. Protocol security, Dolev-Yao model, side-channel attacks.  \nSecure communication (3 weeks). Authenticated key exchange (e.g. SIGMA, TLS), \npassword-authenticated key exchange (e.g. SPAKE2), forward secrecy, post-compromise \nsecurity, double ratchet, the Signal protocol.  \nPrivate database lookups (3 weeks). Lattice-based post-quantum cryptography, learning with \nerrors, homomorphic encryption, private information retrieval.  \nIn each block, students will be given a description of the algorithms and protocols covered (e.g. \na research paper or an RFC standards document), and a code template that the students \nshould use for their implementation. The implementation language will probably be Python (to \nbe confirmed after the practical materials have been developed ). Students will also be given a \nset of test cases to check their code, and will have the opportunity to test their implementation \ncommunicating with other students\u2019 implementations. Finally, for each block, students will write \nand submit a lab report explaining their approach and the findings from their implementation. \n \nObjectives \nOn completion of this module, students should: \n \nUnderstand that cryptography is not magic! The mathematics can look intimidating, but this \nmodule will show students that they needn\u2019t be afraid of cryptography. \nGain appreciation for the complexity and careful implementation of real-world cryptography \nlibraries ,and understand why one should prefer vetted implementations. \nBe familiar with the mathematical notation and concepts commonly used in descriptions of \ncryptographic protocols, and be able to understand and implement research papers and RFCs \nin this field. \nBe comfortable with cryptographic primitives commonly used in protocols, and able to correctly \nuse software libraries that implement them. \n\nHave gained hands-on experience of attacks on cryptographic protocols, and an appreciation \nfor the difficulty of making these protocols correct. \nHave been exposed to ideas and techniques from recent research, which may be useful for their \nown future research. \nHave practised technical writing through lab reports. \nAssessment \nStudents will be provided with code skeletons in one or two different programming languages \n(probably Python, maybe Rust) to avoid them struggling with setup issues. For each \nassignment, students are required to produce a working implementation of the protocols \ndiscussed in the course, using the programming language and skeleton provided. Here, \n\u201cworking\u201d means that the algorithms are functionally correct, but they are not production-quality \n(in particular, no side-channel countermeasures such as constant-time algorithms are required). \nFor some primitives (e.g. symmetric ciphers and hash functions) existing libraries should be \nused, whereas other cryptographic algorithms will be implemented from scratch. Students \nshould submit their code as a Docker container that can be run against specified test cases.  \n \nAfter each of the three blocks, students will be asked to submit a lab report of approximately \n2,000 words along with their code for that block. In the lab report, the students should explain \nhow their implementation works and why it is correct, as well as any findings from the work (e.g. \nany limitations or trade-offs they found with the protocols). The report structure and marking \nscheme will be specified in advance. The lab reports provide an opportunity for students to \nprovide critical insights and creativity. For instance, they might compare their implementation \nwith existing real-world implementations which provide additional features and guarantees.  \n \nEach of the three lab reports (along with the associated code) will be marked, forming the basis \nof assessment, and feedback on the reports will be given to the students before the next \nsubmission is due, so that students can take the feedback on board for their next submission. \nOf the three reports, the worst mark will be discarded, and the other two reports each contribute \n50% of the final mark. As such, students might decide to submit only two reports.  \n \nStudents may discuss their work with others, but the implementations and lab reports must be \nindividual work. \n \nRecommended reading \nTextbook: Jonathan Katz and Yehuda Lindell. Introduction to Modern Cryptography (3rd edition). \nCRC Press, 2020. \n \nThe textbook covers prerequisite knowledge on cryptographic primitives, such as ciphers, \nMACs, hash functions, and signatures. This book is also used in the Part II Cryptography \ncourse. For the protocols we cover in this course, we are not aware of a suitable textbook; \ninstead we will use research papers, lecture slides, and RFCs as resources, which will be \nprovided at the start of the module.\n \n\nDISTRIBUTED LEDGER TECHNOLOGIES: FOUNDATIONS AND APPLICATIONS \n \nAims \nThis reading group course examines foundations and current research into distributed ledger \n(blockchain) technologies and their applications. Students will read, review, and present seminal \nresearch papers in this area. Once completed, students should be able to integrate blockchain \ntechnologies into their own research and gain familiarity with a range of research skills. \n \nLectures \nIntroduction \nConsensus protocols \nBitcoin and its variants \nEthereum, smart contracts, and other permissionless DLTs \nHybrid and permissioned DLTs \nApplications \nLearning objectives \nThere are two broad objectives: to acquire familiarity with a body of work in the area of \ndistributed ledgers and to learn some specific research skills: \n \nHow to read a paper \nHow to review a paper \nHow to analyze a paper\u2019s strengths and weaknesses \nWritten and oral presentation skills \nAssessment \nYou are expected to read all assigned papers and submit paper reviews each week. Each \nreview must either follow the provided review form [PDF] [Latex source]. Each \u201creview\u201d is worth \n5% of your total mark, and is marked out of 100 with 60 a passing grade. Marks will be awarded \nand penalties for late submission applied according to ACS Assessment Guidelines.  \n \nOne paper review for the first week, then two paper reviews each week for 6 weeks (13 reviews, \n5% each) 65% (approx 600 words per review) \nSummative essay 25% (max 3000 words) \nPresentation 5% \n100% attendance in class 5% (2 marks deducted per missed class) \nRecommended Reading \nNarayanan, A. , Bonneau, J., Felten, E., Miller, A. and Goldfeder, S. (2016). Bitcoin and \nCryptocurrency Technologies: A Comprehensive Introduction. Princeton University Press. \n(2016 Draft available here: \nhttps://d28rh4a8wq0iu5.cloudfront.net/bitcointech/readings/princeton_bitcoin_book.pdf)\n \n\nEXPLAINABLE ARTIFICIAL INTELLIGENCE \nPrerequisites: A solid background in statistics, calculus and linear algebra. We strongly \nrecommend some experience with machine learning and deep neural networks (to the level of \nthe first chapters of Goodfellow et al.\u2019s \u201cDeep Learning\u201d). Students are expected to be \ncomfortable reading and writing Python code for the module\u2019s practical sessions. \nMoodle, timetable \n \nAims \nThe recent palpable introduction of Artificial Intelligence (AI) models to everyday \nconsumer-facing products, services, and tools brings forth several new technical challenges and \nethical considerations. Amongst these is the fact that most of these models are driven by Deep \nNeural Networks (DNNs), models that, although extremely expressive and useful, are \nnotoriously complex and opaque. This \u201cblack-box\u201d nature of DNNs limits their ability to be \nsuccessfully deployed in critical scenarios such as those in healthcare and law. Explainable \nArtificial Intelligence (XAI) is a fast-moving subfield of AI that aims to circumvent this crucial \nlimitation of DNNs by either  \n \n(i) constructing human-understandable explanations for their predictions,  \n \nor (ii) designing novel neural architectures that are interpretable by construction.  \n \nIn this module, we will introduce the key ideas behind XAI methods and discuss some of the \nimportant application areas of these methods (e.g., healthcare, scientific discovery, debugging, \nmodel auditing, etc.). We will approach this by focusing on the nature of what constitutes an \nexplanation, and discussing different ways in which explanations can be constructed or learnt to \nbe generated as a by-product of a model. The main aim of this module is to introduce students \nto several commonly used approaches in XAI, both theoretically in lectures and through \nhands-on exercises in practicals, while also bringing recent promising directions within this field \nto their attention. We hope that, by the end of this module, students will be able to directly \ncontribute to XAI research and will understand how methods discussed in this module may be \npowerful tools for their own work and research. \n \nSyllabus \nOverview and taxonomy of XAI (why is explainability needed, definition of terms, taxonomy of \nthe XAI space, etc.)  \nPerturbation-based feature attribution (e.g., LIME, Anchors, SHAP, etc.) \nPropagation-based feature attribution methods (e.g., Relevance Propagation, Saliency \nmethods, etc.) \nConcept-based explainability (Net2Vec, T-CAV, ACE, etc.) \nInterpretable architectures (CBMs and variants, Concept Whitening, SENNs, etc.) \nNeurosymbolic methods (DeepProbLog, Neural Reasoners, etc.) \nSample-based Explanations (Influence functions, ProtoPNets, etc.) \nCounterfactual explanations \nProposed Schedule \n\nThe 16 hours of lectures across 8 weeks will be divided as follows:  \nWeek 1: 1h lecture + 1h lecture  \nWeek 2: 1h lecture + 1h reading group & presentations  \nWeek 3: 1h lecture + 1h reading group & presentations  \nWeek 4: 2h practical  \nWeek 5: 1h lecture + 1h reading group & presentations  \nWeek 6: 2h practical  \nWeek 7: 1h lecture + 1h reading group & presentations  \nWeek 8: 1h reading group & presentation + 1h reading group & presentations \n \nIn weeks where lectures are planned, one-hour lecture slots will intercalate with one hour of \nstudent paper presentations. During each paper presentation session, three students will \npresent a paper related to the topic covered in the earlier lecture that week for about 10 minutes \neach + 5 minutes of questions. At the end of all paper presentations, there will be a discussion \non all the papers. The order of student presentations will be allocated randomly during the first \nweek so that students know in advance when they are expected to present. For the sake of \nfairness, we will release the paper to be presented by each student a week before their \npresentation slot. \n \nObjectives \nBy the end of this module, students should be able to: \n \nRecognise and identify key concepts in XAI together with their connection to related subfields in \nAI such as fairness, accountability, and trustworthy AI. \nUnderstand how to use, design, and deploy model-agnostic perturbation methods such as \nLIME, Anchors, and RISE. In particular, students should understand the connection between \nfeature importance and cooperative game theory, and its uses in methods such as SHAP. \nIdentify the uses and limitations of propagation-based feature importance methods such as \nSaliency, SmoothGrad, GradCAM, and Integrated Gradients. Students should be able to \nimplement each of these methods on their own and connect the theoretical ideas behind them \nto practical code, exploiting modern frameworks\u2019 auto-differentiation. \nUnderstand what concept learning is and what limitations it overcomes compared to traditional \nfeature-based methods. Specifically, students should understand how probing a DNN\u2019s latent \nspace may be exploited for learning useful concepts for explainability. \nReason about the key components of inherently interpretable architectures and neuro-symbolic \nmethods and understand how interpretable neural networks can be designed from first \nprinciples. \nElaborate on what sample-based explanations are and how influence functions and prototypical \narchitectures such as ProtoPNet can be used to construct such explanations. \nExplain what counterfactual explanations are, how they are related to causality, and under which \nconditions they may be useful. \nUpon completion of this module, students will have the technical background and tools to use \nXAI as part of their own research or partake in XAI research itself. Moreover, we hope that by \ndetailing a clear timeline of how this relatively young subfield has developed, students may be \n\nable to better understand what are some fundamental open questions in this area and what are \nsome promising directions that are currently actively being explored. \n  \n \nAssessment \n(10%) student presentation: each student will be randomly assigned a presentation slot at the \nbeginning of the course. We will then distribute a paper for them to present in their slot a week \nbefore the presentation\u2019s scheduled time. Students are expected to prepare a 10-minute \npresentation of their assigned paper where they will present the motivation of their assigned \nwork and discuss the main methodology and findings reported in that paper. We will encourage \nstudents to focus on fully communicating the intuition of the work in their assigned papers and \ntry and connect it with ideas that we have previously discussed in previous lectures. All students \nnot presenting each week will be asked to submit one question pertaining to each paper \npresented that week before the paper presentations. \n \n(20%) practical exercises: We will run two practical sessions where students will be asked to \nperform a series of exercises that require them to use concepts we have introduced in lecture \nup to that point. For each practical session, we will prepare a colab notebook to guide the \nstudent through exercises and we will ask the students to submit their solutions through this \ncolab notebook. We expect students to complete about \u2154 of the exercises in the practical class \nand complete the rest at home as homework. Each practical will be worth 10%. \n \n(70%) mini-project: At the end of week 3, we will hand out a list of papers for students to select \ntheir mini-projects from. Each mini-project will consist of a student selecting a paper from our list \nand reimplementing and expanding the key idea in the paper. We encourage students to be as \ncreative as they want with how they drive their mini-project once the paper has been selected. \nFor example, they can reimplement the technique in the paper and combine it with \nmethodologies from other works we discussed in lecture, or they can apply their paper\u2019s \nmethodology to a new domain, datasets, or setup, where the technique may offer interesting \nand potentially novel insights. We will ask all students to submit a report in a workshop format of \nup to 4,000 words. This report, due roughly a week after Lent term ends, should describe their \nmethodology, experiments, and results. A crucial aspect of this report involves explaining the \nrationale behind different choices in methodology and experiments, as well as elaborating on \nthe choices made and hypotheses tested throughout their mini-project (potentially showing a \ndeep understanding of the work they are basing their mini-project on). To aid students with \nselecting their projects and making progress on them, we will hold regular office hours when \nstudents can come to discuss their progress and questions with us. \n \nRecommended reading \nTextbooks \n \n* Christoph Molnar, \u201cInterpretable Machine Learning\u201d. (2022): \nhttps://christophm.github.io/interpretable-ml-book/ \n \n\nOnline Courses and Tutorial \n \n* Su-in Lee and Ian Cover, \u201cCSEP 590B Explainable AI\u201d University of Washington* Explaining \nMachine Learning Predictions: State-of-the-art, Challenges, and Opportunities \n \n* On Explainable AI: From Theory to Motivation, Industrial Applications, XAI Coding & \nEngineering Practices - AAAI 2022 Turorial \n \nSurvey papers \n \n* Arrieta, Alejandro Barredo, et al. \"Explainable Artificial Intelligence (XAI): Concepts, \ntaxonomies, opportunities and challenges toward responsible AI.\" Information fusion 58 (2020): \n82-115. \n \n* Rudin, Cynthia, et al. \"Interpretable machine learning: Fundamental principles and 10 grand \nchallenges.\" Statistics Surveys 16 (2022): 1-85.\n \n\nFEDERATED LEARNING: THEORY AND PRACTICE \nPrerequisites: It is strongly recommended that students have previously (and successfully) \ncompleted an undergraduate machine learning course - or have equivalent experience through \nopen-source material (e.g., lectures or similar). An example course would be: Part1B \"Artificial \nIntelligence\". Students should feel comfortable with SGD and optimization methods used to train \ncurrent popular neural networks and simple forms of neural network architectures. \nMoodle, timetable \n \nObjectives \nThis course aims to extend the machine learning knowledge available to students in Part I (or \npresent in typical undergraduate degrees in other universities), and allow them to understand \nhow these concepts can manifest in a decentralized setting. The course will consider both \ntheoretical (e.g., decentralized optimization) and practical (e.g., networking efficiency) aspects \nthat combine to define this growing area of machine learning.  \n \nAt the end of the course students should: \n \nUnderstand popular methods used in federated learning \nBe able to construct and scale a simple federated system \nHave gained an appreciation of the core limitations to existing methods, and the approaches \navailable to cope with these issues \nDeveloped an intuition for related technologies like differential privacy and secure aggregation, \nand are able to use them within typical federated settings  \nCan reason about the privacy and security issues with federated systems \nLectures \nCourse Overview. Introduction to Federated Learning. \nDecentralized Optimization. \nStatistical and Systems Heterogeneity. \nVariations of Federated Aggregation. \nSecure Aggregation. \nDifferential Privacy within Federated Systems. \nExtensions to Federated Analytics. \nApplications to Speech, Video, Images and Robotics. \nLab sessions \nFederating a Centralized ML Classifier. \nBehaviour under Heterogeneity. \nScaling a Federated Implementation. \nExploring Privacy with Federated Settings \nRecommended Reading \nReadings will be assigned for each lecture. Readings will be taken from either research papers, \ntutorials, source code or blogs that provide more comprehensive treatment of taught concepts. \n \nAssessment - Part II Students \n\nFour labs are performed during the course, and students receive 10% of their total grade for \nwork done as part of each lab. (For a total of 40% of the total grade from lab work alone). Labs \nwill primarily provide hands-on teaching opportunities, that are then utilized within the lab \nassignment which is completed outside of the lab contact time. MPhil and Part III students will \nbe given additional questions to answer within their version of the lab assignment which will \ndiffer from the assignment given to Part II CST students. \n \nThe remainder of the course grade (60%) will be given based on a hands-on project that applies \nthe concepts taught in lectures and labs. This hands-on project will be assessed based on upon \na combination of source code, related documentation and brief 8-minute pre-recorded talk that \nsummarizes key project elements (any slides used are also submitted as part of the project). \nPlease note, that in the case of Part II CST students, the talk itself is not examinable -- as such \nwill be made optional to those students. \n \nA range of possible practical projects will be described and offered to students to select from, or \nalternatively students may propose their own. MPhil and Part III students will select from a \nproject pool that is separate from those offered to Part II CST students. MPhil and Part III \nprojects will contain a greater emphasis on a research element, and the pre-recorded talks \nprovided by this student group will focus on this research contribution. The project will be \nassessed on the level of student understanding demonstrated, the degree of difficulty, \ncorrectness of implementation -- and for Part III/MPhil students the additional criteria of the \nquality and execution of the research methodology, and depth and quality of results analysis. \n \nThis project can be done individually or in groups -- although individual projects will be strongly \nencouraged. It will be required the project is performed using a code repository that also will \ncontain all documentation -- access to this repository will be shared with course staff (e.g., \nlecturer and TAs). Where needed, marks assigned to students within a group will be \ndifferentiated using this repository as an input. Furthermore if groups are formed, members must \nbe either entirely from Part III/MPhil students or Part II CST, i.e., these two student groups \nshould not mix to form a project group. \n \nProjects will be made available publicly. A maximum word count for written contributions for the \nproject will be enforced.  \n \n  \nAssessment - MPhil / Part III Students \n50% final mini-project. This hands-on project will be assessed based on upon a combination of \nsource code, related documentation and brief 8-minute pre-recorded talk that summarizes key \nproject elements (any slides used are also submitted as part of the project). \n \n30% midterm lab report (a written report of a bit more depth of thought than required in a lab, \nthe report provides the results of experiments -- with the skills to perform these experiments \ncoming from lab sessions),  \n \n\n20% for two individual hands-on labs. Labs will primarily provide hands-on teaching \nopportunities, that are then utilized within the lab assignment which is completed outside of the \nlab contact time. There will also be additional questions to answer within their version of the lab \nassignment. \n \nFor the mini-project, a range of possible practical projects will be described and offered to \nstudents to select from, or alternatively students may propose their own. The project will be \nassessed on the level of student understanding demonstrated, the degree of difficulty, \ncorrectness of implementation, the quality and execution of the research methodology, and \ndepth and quality of results analysis. \n \nThe project can be done individually or in groups -- although individual projects will be strongly \nencouraged. It will be required the project is performed using a code repository that also will \ncontain all documentation -- access to this repository will be shared with course staff (e.g., \nlecturer and TAs). Where needed, marks assigned to students within a group will be \ndifferentiated using this repository as an input. Furthermore if groups are formed, members must \nbe either entirely from Part III or entirely from MPhil students, i.e., these two student groups \nshould not mix to form a project group. \n \nProjects will be made available publicly. A maximum word count for written contributions for the \nproject will be enforced.   \n \nAdvanced Material sessions - MPhil / Part III Students \nThese sessions are mandatory for the MPhil and Part III students. Part II CST students may \nattend if they wish \n \nTopics and announced the first week of class. Selected to extend beyond topics covered in \nlectures and labs. Content is a mixture of sessions run in the lecture room the are either (1) \npresentations by guest lectures by an invited domain expert or (2) class-wide discussions \nregarding one or more related academic papers. Paper discussions will require students to read \nthe paper ahead of the lecture, and a brief discussion primer presentation will be given students \nbefore discussions begin.  \n \nThis module is shared with Part II of the Computer Science Tripos. Assessment will be adjusted \nfor the two groups of students to be at an appropriate level for whichever course the student is \nenrolled on. Further information about assessment and practicals will follow at the first lecture.\n\nGEOMETRIC DEEP LEARNING \n \nPrerequisites: Experience with machine learning and deep neural networks is recommended. \nKnowledge of concepts from graph theory and group theory is useful, although the relevant \nparts will be explicitly retaught. \nMoodle, timetable \n \nAims and Objectives \nMost of the patterns we see in nature can be elegantly reasoned about using spatial \nsymmetries\u2014transformations that leave underlying objects unchanged. This observation has \nhad direct implications for the development of modern deep learning architectures that are \nseemingly able to escape the \u201ccurse of dimensionality\u201d and fit complex high-dimensional tasks \nfrom noisy real-world data. Beyond this, one of the most generic symmetries\u2014the \npermutation\u2014will prove remarkably powerful in building models that reason over graph \nstructured-data, which is an excellent abstraction to reason about naturally-occurring, \nirregularly-structured data. Prominent examples include molecules (represented as graphs of \natoms and bonds, with three-dimensional coordinates provided), social networks and \ntransportation networks. Several already-impacted application areas include traffic forecasting, \ndrug discovery, social network analysis and recommender systems. The module will provide the \nstudents the capability to analyse irregularly- and nontrivially-structured data in an effective way, \nand position geometric deep learning in a proper context with related fields. The main aim of the \ncourse is to enable students to make direct contributions to the field, thoroughly assimilate the \nkey concepts in the area, and draw relevant connections to various other fields (such as NLP, \nFourier Analysis and Probabilistic Graphical Models). We assume only a basic background in \nmachine learning with deep neural networks. \n \nLearning outcomes \nThe framework of geometric deep learning, and its key building blocks: symmetries, \nrepresentations, invariance and equivariance \nFundamentals of processing data on graphs, as well as impactful application areas for graph \nrepresentation learning \nTheoretical principles of graph machine learning: permutation invariance and equivariance \nThe three \"flavours\" of spatial graph neural networks (GNNs) (convolutional, attentional, \nmessage passing) and their relative merits. The Transformer architecture as a special case. \nAttaching symmetries to graphs: CNNs on images, spheres and manifolds, Geometric Graphs \nand E(n)-equivariant GNNs \nRelevant connections of geometric deep learning to various other fields (such as NLP, Fourier \nAnalysis and Probabilistic Graphical Models) \nLectures \nThe lectures will cover the following topics: \n \nLearning with invariances and symmetries: geometric deep learning. Foundations of group \ntheory and representation theory. \n\nWhy study data on graphs? Success stories: drug screening, travel time estimation, \nrecommender systems. Fundamentals of graph data processing: network science, spectral \nclustering, node embeddings. \nPermutation invariance and equivariance on sets and graphs. The principal tasks of node, edge \nand graph classification. Neural networks for point clouds: Deep Sets, PointNet; universal \napproximation properties. \nThe three flavours of spatial GNNs: convolutional, attentional, message passing. Prominent \nexamples: GCN, SGC, ChebyNets, MoNet, GAT, GATv2, IN, MPNN, GraphNets. Tradeoffs of \nusing different GNN variants. \nGraph Rewiring: how to apply GNNs when there is no graph? Links to natural language \nprocessing---Transformers as a special case of attentional GNNs. Representative \nmethodologies for graph rewiring: GDC, SDRF, EGP, DGCNN. \nExpressive power of graph neural networks: the Weisfeiler-Lehman hierarchy. GINs as a \nmaximally expressive GNN. Links between GNNs and graph algorithms: neural algorithmic \nreasoning. \nCombining spatial symmetries with GNNs: E(n)-equivariant GNNs, TFNs, SE(3)-Transformer. A \ndeep dive into AlphaFold 2. \nWorked examples: Circulant matrices on grids, the discrete Fourier transform, and convolutional \nnetworks on spheres. Graph Fourier transform and the Laplacian eigenbasis. \nPracticals \nThe practical is designed to complement the knowledge learnt in lectures and teach students to \nderive additional important results and architectures not directly shown in lectures. The practical \nwill be given as a series of individual exercises (each either code implementation or \nproof/derivation). Each of these exercises can be individually assessed based on a specified \nmark budget. \n \nPossible practical topics include the study of higher-order GNNs and equivariant message \npassing. \n \nAssessment \n(60%) Group Mini-project (writeup) at the end of the course. The mini projects can either be \nself-proposed, or the students can express their preference for one of the provided topics, the \nlist of which will be announced at the start of term. The projects will consist of implementing \nand/or extending graph representation learning models in the literature, applying them to \npublicly available datasets. Students will undertake the project in pairs and submit a joint \nwriteup limited to 4,000 words (in line with other modules); appendix of work logs to be included \nbut ungraded; \n \n(10%) Short presentation and viva: students will give a short presentation to explain their \nindividual contribution to the mini-project and there will be a short viva following. \n \n(30%) Practical work completion. Completing the exercises specified in the practical to a \nsatisfactory standard. The practical assessor should be satisfied that the student derived their \nanswers using insight gained from the course; coupled with original thought, not by simple \n\ncopy-pasting of relevant related work. The students would submit code and a short report, which \nwould then be marked in line with the predetermined mark budget for each practical item. \nThe students will learn how to run advanced architectures on GPU but no specific need for \ndedicated GPU resources. Practicals will be made possible to do on CPU; if required, students \ncan use GPUs on publicly available free services (such as Colab) for their mini-project work. \n \nReferences \nThe course will be based on the following literature: \n \n\"Geometric Deep Learning: Grids, Graphs, Groups, Geodesics, and Gauges\", by Michael \nBronstein, Joan Bruna, Taco Cohen and Petar Veli\u010dkovi\u0107 \n\"Graph Representation Learning\", by Will Hamilton \n\"Deep Learning\", by Ian Goodfellow, Yoshua Bengio and Aaron Courville.\n \n\nMOBILE HEALTH \nAims \nThe course aims to explore how wearable and mobile systems sensors can be used to gather \ndata relevant to understand health, how the data can be analysed with advanced signal \nprocessing and machine learning and the performance of these systems in terms of diagnostics \nand disease progression detection. \n \nSyllabus \nCourse Overview. Introduction to Mobile Health. Evaluation metrics and methodology. Basics of \nSignal Processing. \nInertial Measurement Units, Human Activity Recognition (HAR) and Gait Analysis and Machine \nLearning for IMU data. \nRadios, Bluetooth, GPS and Cellular. Epidemiology and contact tracing, Social interaction \nsensing and applications. Location tracking in health monitoring and public health. \nAudio Signal Processing. Voice and Speech Analysis: concepts and data analysis. Body \nSounds analysis. \nPhotoplethysmogram and Light sensing for health (heart and sleep) \nContactless and wireless behaviour and physiological monitoring \nGenerative Models for Wearable Health Data \nTopical Guest Lectures \nObjectives \nThe course aims to explore how wearable and mobile systems sensors can be used to gather \ndata relevant to understand health, how the data can be analysed with advanced signal \nprocessing and machine learning and the performance of these systems in terms of diagnostics \nand disease progression detection. \n \nRoughly, each lecture contains a theory part about the working of \u201csensor signals\u201d or \u201cdata \nanalysis methods\u201d and an application part which contextualises the concepts. \n \nAt the end of the course students should: Understand how mobile/wearable sensors capture \ndata and their working. Understand different approaches to acquiring and analysing sensor data \nfrom different types of sensors. Understand the concept of signal processing applied to time \nseries data and their practical application in health. Be able to extract sensor data and analyse it \nwith basic signal processing and machine learning techniques. Be aware of the different health \napplications of the various sensor techniques. The course will also touch on privacy and ethics \nimplications of the approaches developed in an orthogonal fashion. \n \nRecommended Reading \nPlease see Course Materials for recommended reading for each session. \n \nAssessment - Part II students \nTwo assignments will be based on two datasets which will be provided to the students: \n \n\nAssignment 1 (shared for Part II and Part III/MPhil): this will be based on a dataset and will be \nworth 40% of the final mark. The task of the assessment will be to perform pre-processing and \nbasic data analysis in a \"colab\" and an answer sheet of no more than 1000 words. \n \nAssignment 2 (Part II): This assignment (worth 60% of the final mark) will be a fuller analysis of \na dataset focusing on machine learning algorithms and metrics. Discussion and interpretation of \nthe findings will be reported in a colab and a report of no more than 1200 words. \n \nAssessment  - Part III and MPhil students \nTwo assignments will be based on datasets which will be provided to the students: \n \nAssignment 1: this will be based on a dataset and will be worth 40% of the final mark. The task \nof the assessment will be to perform pre-processing and basic data analysis in a \"colab\" and an \nanswer sheet of no more than 1000 words. \n \nAssignment 2: The second assignment (worth 60% of the final mark) will be based on multiple \ndatasets. The students will be asked to compare and contract ML algorithms/solutions (trained \nand tested on the different data) and discuss the findings and interpretation in terms of health \ncontext. This will be in the form of a colab and a report of 1800 words. \n \nFurther Information \nCurrent Cambridge undergraduate students who are continuing onto Part III or the MPhil in \nAdvanced Computer Science may only take this module if they did NOT take it as a Unit of \nAssessment in Part II. \n \nThis module is shared with Part II of the Computer Science Tripos. Assessment will be adjusted \nfor the two groups of students to be at an appropriate level for whichever course the student is \nenrolled on. Further information about assessment and practicals will follow at the first lecture.\n\nMULTICORE SEMANTICS AND PROGRAMMING \nPrerequisites: Some familiarity with discrete mathematics (sets, partial orders, etc.) and with \nsequential Java programming will be assumed. Experience with operational semantics and with \nsome concurrent programming would be helpful. \nMoodle, timetable \n \nAims \nIn recent years multiprocessors have become ubiquitous, but building reliable concurrent \nsystems with good performance remains very challenging. The aim of this module is to \nintroduce some of the theory and the practice of concurrent programming, from hardware \nmemory models and the design of high-level programming languages to the correctness and \nperformance properties of concurrent algorithms. \n \nLectures \nPart 1: Introduction and relaxed-memory concurrency [Professor P. Sewell] \n \nIntroduction. Sequential consistency, atomicity, basic concurrent problems. [1 block] \nConcurrency on real multiprocessors: the relaxed memory model(s) for x86, ARM, and IBM \nPower, and theoretical tools for reasoning about x86-TSO programs. [2 blocks] \nHigh-level languages. An introduction to C/C++11 and Java shared-memory concurrency. [1 \nblock] \nPart 2: Concurrent algorithms [Dr T. Harris] \n \nConcurrent programming. Simple algorithms (readers/writers, stacks, queues) and correctness \ncriteria (linearisability and progress properties). Advanced synchronisation patterns (e.g. some \nof the following: optimistic and lazy list algorithms, hash tables, double-checked locking, RCU, \nhazard pointers), with discussion of performance and on the interaction between algorithm \ndesign and the underlying relaxed memory models. [3 blocks] \nResearch topics, likely to include one hour on transactional memory and one guest lecture. [1 \nblock] \nObjectives \nBy the end of the course students should: \n \nhave a good understanding of the semantics of concurrent programs, both at the multprocessor \nlevel and the C/Java programming language level; \nhave a good understanding of some key concurrent algorithms, with practical experience. \nAssessment - Part II Students \nTwo assignments each worth 50% \n \nRecommended reading \nHerlihy, M. and Shavit, N. (2008). The art of multiprocessor programming. Morgan Kaufmann. \n \nCoursework \nCoursework will consist of assessed exercises. \n\n \nPractical work \nPart 2 of the course will include a practical exercise sheet.  The practical exercises involve \nbuilding concurrent data structures and measuring their performance.  The work can be \ncompleted in C, Java, or similar languages. \n \nAssessment - Part III and MPhil Students \nAssignment 1, for 50% of the overall grade. Written coursework comprising a series of questions \non hardware and software relaxed-memory concurrency semantics. \nAssignment 2, for 50% of the overall grade. Written coursework comprising a series of questions \non the design of mutual exclusion locks and shared-memory data structures. \nFurther Information \nCurrent Cambridge undergraduate students who are continuing onto Part III or the MPhil in \nAdvanced Computer Science may only take this module if they did NOT take it as a Unit of \nAssessment in Part II. \n \nThis module is shared with Part II of the Computer Science Tripos. Assessment will be adjusted \nfor the two groups of students to be at an appropriate level for whichever course the student is \nenrolled on. Further information about assessment and practicals will follow at the first lecture.\n\nREINFORCEMENT LEARNING \n \nAims \nThe aim of this module is to introduce students to the foundations of the field of reinforcement \nlearning (RL), discuss state-of-the-art RL methods, incentivise students to produce critical \nanalysis of recent methods, and prompt students to propose novel solutions that address \nshortcomings of existing methods. If judged by the headlines, RL has seen an unprecedented \nsuccess in recent years. However, the vast majority of RL methods still have shortcomings that \nmight not be apparent at first glance. The aim of the course is to inspire students by \ncommunicating the promising aspects of RL, but ensure that students develop the ability to \nproduce a critical analysis of current RL limitations. \n \nObjectives \nStudents will learn the following technical concepts: \n \nfundamental RL terminology and mathematical formalism; a brief history of RL and its \nconnection to neuroscience and biological systems \nRL methods for discrete action spaces, e.g. deep Q-learning and large-scale Monte Carlo Tree \nSearch \nmethods for exploration, modelling uncertainty, and partial observability for RL \nmodern policy gradient and actor-critic methods \nconcepts needed to construct model-based RL and Model Predictive Control methods \napproaches to make RL data-efficient and ways to enable simulation-to-reality transfer \nexamples of fine-tuning foundation models and large language models (LLMs) with human \nfeedback; safe RL concepts; examples of using RL for safety validation \nexamples of using RL for scientific discovery \nStudents will also gain experience with analysing RL methods to uncover their strengths and \nshortcomings, as well as proposing extensions to improve performance. \n \nFinally, students will gain skills needed to create and deliver a successful presentation in a \nformat similar to that of conference presentations. \n \nWith all of the above, students who take part in this module will be well-prepared to start \nconducting research in the field of reinforcement learning. \n  \n \nSyllabus \nTopic 1: Introduction and Fundamentals \n \nOverview of RL: foundational ideas, history, and books; connection to neuroscience and \nbiological systems, recent industrial applications and research demonstrations \nMathematical fundamentals: Markov decision processes, Bellman equations, policy and value \niteration, temporal difference learning \nTopic 2: RL in Discrete Action Spaces \n\n \nQ-learning, function approximation and deep Q-learning; nonstationarity in RL and its \nimplications for deep learning; example applications (video games; initial example: Atari) \nMonte Carlo Tree Search; example applications (AlphaGo) \nTopic 3: Exploration, Uncertainty, Partial Observability \n \nMulti-armed bandits, Bayesian optimisation, regret analysis \nPartially observable Markov decision process; belief, memory, and sequence modelling \n(probabilistic methods, recurrent networks, transformers) \nTopic 4: Policy Gradient and Actor-critic Methods for Continuous Action Spaces \n \nImportance sampling, policy gradient theorem, actor-critic methods (SPG, DDPG) \nProximal policy optimisation; example applications \nTopic 5: Model-based RL and Model Predictive Control \n \nLearning dynamics models (graph networks, stochastic processes, diffusion models, \nphysics-based models, ensembles); planning with learned models \nModel predictive control; example applications (real-time control) \nTopic 6: Data-efficient RL and Simulation-to-reality Transfer \n \nData-efficient learning with probabilistic methods from real data (e.g. policy search in robotics), \nreal-to-sim inference and differentiable simulation, data-efficient simulation-to-reality transfer \nRL for physical systems (successful examples in locomotion, open problems in contact-rich \nmanipulation, applications to logistics, energy, and transport systems); examples of RL for \nhealthcare. \nTopic 7: RL with Human Feedback ; Safe RL and RL for Validation \n \nFine-tuning large language models (LLMs) and other foundation models with human feedback \n(TRLX,RL4LMs, a light-weight overview of RLHF) \nA review of SafeRL, example: optimising commercial HVAC systems using policy improvement \nwith constraints; improving safety using RL for validation: examples in autonomous driving and \nautonomous flying and aircraft collision avoidance \nTopic 8: RL for Scientific Discovery; Student Presentations \n \nExamples of RL for molecular design and drug discovery, active learning for synthesising new \nmaterials, RL for nuclear fusion experiments \nStudent presentations (based on essays and mini-projects) for other topics in RL, e.g. \nmulti-agent RL, hierarchical RL, RL for hyperparameter optimisation and NN architecture \nsearch, RL for multi-task transfer, lifelong RL, RL in biological systems, etc. \nAssessment \nThe assessment for this module consists of: \n \nEssay and mini-project (60%)  \n\nStudents will choose a concrete RL method from the literature, then complete a 2-part essay \ndescribed below: \nEssay Part 1 (1500 words, 20%): Introduce a formal description of the method and explain how \nthe method extended the state-of-the-art at the time of its publication \nEssay Part 2 or a mini-project (2500 words, 40%): Provide a critical analysis of the chosen RL \nmethod. \nPresentation of the essay and mini-project results (15%)  \nstudents will be expected to create and record a 15-minute video presentation of the analysis \ndescribed in their essay and mini-project. \nShort test of RL theory fundamentals (15%)  \nto test the understanding of RL fundamentals (~15 minutes, in-class, closed book) \nParticipation in seminar discussions (10%)  \ntake an active part in the seminars by asking clarifying questions and mentioning related works. \nRecommended Reading \nBooks \n \n[S&B] Reinforcement Learning: An Introduction (second print edition). Richard S. Sutton, \nAndrew G. Barto. [Available from the book\u2019s website as a free PDF updated in 2022] \n \n[CZ] Algorithms for Reinforcement Learning. Csaba Szepesvari. [Available from the book\u2019s \nwebsite as a free PDF updated in 2019] \n \n[MK] Algorithms for Decision Making. Mykel J. Kochenderfer Tim A. WheelerKyle H. Wray. \n[Available from the book\u2019s website as a free PDF updated in 2023] \n \n[DB] Reinforcement Learning and Optimal Control. Dimitri Bertsekas. [Available from the book\u2019s \nwebsite as a free PDF updated in 2023] \n \nPresentation guidelines \n \n[KN] Ten simple rules for effective presentation slides. Kristen M.Naegle. PLoS computational \nbiology 17, no. 12 (2021).\n \n\nTHEORIES OF SOCIO-DIGITAL DESIGN FOR HUMAN-CENTRED AI \n \nPrerequisites: This course is appropriate to any ACS student, and will assist in the development \nof critical thinking, argument and long-form writing skills. Prior experience of essay-based \nhumanities subjects at university or secondary school will be beneficial, but not required. \nMoodle, timetable \n \nAims \nThis module is a theoretically-oriented advanced introduction to the broad field of \nhuman-computer interaction, extending to consider topics such as intelligent user interfaces, \ncritical and speculative design, participatory design, cognitive models of users, human-centred, \nas well as more-than-human-centred design, and others. The course will not address purely \nengineering approaches to the development of user interfaces (unless there is a clear \ntheoretical question being addressed), but allow students to effectively link the political, social, \nand ethical considerations in HCI to specific technical and conceptual design challenges. The \nmodule builds on the Practical Research in Human-Centred AI course offered in Michaelmas \n(developed with researchers at Microsoft Research Cambridge) and a collaboration with the \nLeverhulme Centre for the Future of Intelligence at Cambridge. Participants may include visitors \nfrom these groups and/or interdisciplinary research students and academic guests from other \nUniversity departments, including Cambridge Digital Humanities. \n \nSyllabus \nThe syllabus will remain broadly within the area of human-computer interaction, including \ntheories of design practice and the social contexts of technology use. Individual seminar topics \nwill be selected in response to contemporary and recent research developments, in consultation \nwith members of the class and visiting contributors. \n \nRepresentative topics in the next year are likely to include: \n \nResponsible AI and HCI \nIntersectional design for social justice \nParticipatory design and co-design \nSustainable AI and more-than-human-centred design \nUsefulness and usability of ethical design toolkits \nThe EU AI Act in design practice \nAI and interface design for transparency \nDesigning otherwise: on anarchist HCI \n  \nObjectives \nOn completion of this module, students should have developed facility in discussing and \ncritiquing the aims of their research, especially for an audience drawn from other academic \ndisciplines, including the following skills: \n \nTheoretical motivation and defence of a research question. \n\nConsideration of a research proposal from one or more alternative theoretical perspectives. \nPotential critique of the theoretical basis for a programme of research. \nCoursework \nIn advance of each seminar, all participants must read the essential reading - generally one \nlong, plus one or two short papers. Further reading is suggested for some seminars. Some \nweeks, instead of shorter papers, students will be asked to explore specific design tools or \nresources. \n \nBefore seminars 2-8, each student will submit a written commentary on the paper, either \ngenerated using a large language model (LLM), or written in the style of an LLM, supplemented \nby a short original observation. \n \nEach member of the class must select one of the 8 seminar themes as the subject for a more \nextended critical review essay. The critical review should be written as an academic research \nessay, not in LLM style. \n \nEach seminar will include an informal design exercise to be carried out in small groups. The \npurpose of these exercises is to reflect on a variety of practical design resources, by applying \nthose resources to concrete case studies. \n \nThroughout the course, students will be expected to keep a \"reflective diary\", briefly reporting \nthemes that arise in discussion, reflections on the design exercises, and ways in which the \nreadings relate to their own research interests. \n  \n \nPractical work \nThere is no practical work element in this course. \n \nAssessment \nWritten critical review of a single discussion text (20%) \nReflective diary submitted at the end of the module (60%) \nIn advance of the session, each student will submit a written commentary on the paper, either \ngenerated using a large language model (LLM), or written in the style of an LLM. A short original \nobservation should be added (20%) \nRecommended reading \nTo be assigned during the module, as discussed above. Most set readings will be available \neither from the ACM Digital Library, or from institutional repositories of the authors. \n \nFurther Information \nStudents from the MPhil in the Ethics of AI, Data and Algorithms, MSt in AI, and MPhil in Digital \nHumanities courses also attend the lectures for this module.\n \n\nTHEORY OF DEEP LEARNING \nPrerequisites: A strong background in calculus, probability theory and linear algebra, familiarity \nwith differential equations, optimization and information theory is required. Students need to \nhave studied Deep Neural Networks, familiarity with deep learning terminology (e.g. \narchitectures, benchmark problems) as well as deep learning frameworks (pytorch, jax or \nsimilar) is assumed \nMoodle, timetable \n \nAims \nThe objectives of this course is to expose you to one of the most active contemporary research \ndirections within machine learning: the theory of deep learning (DL). While the first wave of \nmodern DL has focussed on empirical breakthroughs and ever more complex techniques, the \nattention is now shifting to building a solid mathematical understanding of why these techniques \nwork so well in the first place. The purpose of this course is to review this recent progress \nthrough a mixture of reading group sessions and invited talks by leading researchers in the \ntopic, and prepare you to embark on a PhD in modern deep learning research. Compared to \ntypical, non-mathematical courses on deep learning, this advanced module will appeal to those \nwho have strong foundations in mathematics and theoretical computer science. In a way, this \ncourse is our answer to the question \u201cWhat should the world\u2019s best computer science students \nknow about deep learning in 2023?\u201d \n \nLearning Outcomes \nThis course should prepare the best students to start a PhD in the theory and mathematics of \ndeep learning, and to start formulating their own hypotheses in this space. You will be \nintroduced to a range of empirical and mathematical tools developed in recent years for the \nstudy of deep learning behaviour, and you will build an awareness of the main open questions \nand current lines of attack. At the end of the course you will: \n \nbe able to explain why classical learning theory is insufficient to describe the phenomenon of \ngeneralization in DL \nbe able to design and interpret empirical studies aimed at understanding generalization \nbe able to explain the role of overparameterization: be able to use deep linear models as a \nmodel to study implicit regularisation of gradient-based learning \nbe able to state PAC-Bayes and Information-theoretic bounds, and apply them to DL \nbe able to explain the connection between Gaussian processes and neural networks, and will \nbe able to study learning dynamics in the neural tangent kernel (NTK) regime. \nbe able to formulate your own hypotheses about DL and choose tools to prove/test them \nleverage your deeper theoretical understanding to produce more robust, rigorous and \nreproducible solutions to practical machine learning problems. \nSyllabus \nEach week we'll have two to four student-lead presentations about a research paper chosen \nfrom a reading list. The reading list loosely follows the weekly breakdown below (but we adapt it \neach year based as this is an active research area): \n \n\nWeek 1: Introduction to the topic \nWeek 2: Empirical Studies of Deep Learning Phenomena \nWeek 3: Interpolation Regime and \u201cDouble Descent\u201d Phenomena \nWeek 4: Implicit Regularization in Deep Linear Models \nWeek 5: Approximation Theory \nWeek 6: Networks in the Infinite Width Limit \nWeek 7: PAC-Bayes and Information Theoretic Bounds for SGD \nWeek 8: Discussion and Coursework Spotlight Session \n \nAssessment \nStudents will be assessed on the following basis: \n \n20% for presentation/content contributed to the module: Each student will have an opportunity \nto present one of the recommended papers during Weeks 1-7 (30 minute slot including Q&A). \nFor the presentation, students should aim to communicate the core ideas behind the paper, and \nclearly present the results, conclusions, and future directions. Where possible, students are \nencouraged to comment on how the work itself fits into broader research goals. \n10% for active participation (regular attendance and contribution to discussions during the Q&A \nsessions). \n70% for a group project report, with a word limit of 4000. Either (1) an original research \nproposal/report with a hypothesis, review of related literature, and ideally preliminary findings, \nor, (2) reproduction and ideally extension of an existing relevant paper. \nCoursework reports are marked in line with general ACS guidelines, reports receiving top marks \nwill have have demonstrable research value (contain an original research idea, extension of \nexisting work, or a thorough reproduction effort which is valuable to the research community). \nAdditionally, some projects will be suggested during the first weeks of the course, although \nstudents are encouraged to come up with their own ideas. Students may be required to \nparticipate in group projects, with groups of size 2-3 (the class groups will be separated). For \nany given project, individual contributions would be noted for assessment though a viva \ncomponent. \nRelationship with related modules \nThis course can be considered as an advanced follow-up to the Part IIB course on Deep Neural \nNetworks. That course introduces some high level concepts that this course significantly \nexpands on. \n \nThis module complements L48: Machine Learning in the Physical World and L46: Principles of \nMachine Learning Systems, which focus on applications and hardware/systems aspects of ML \nrespectively. \n \nRecommended reading \nPNAS Colloquium on the Science of Deep Learning \nMathematics of Machine Learning book by Marc Deisenroth, Aldo Faisal and Cheng Soon Ong. \nProbabilistic Machine Learning: An Introduction book by Kevin Murphy \nMatus Telgarsky's lecture notes on deep learning theory  \n\nThese are in addition to the papers which will be discussed in the lectures.\n \n\nUNDERSTANDING NETWORKED-SYSTEMS PERFORMANCE \nPrerequisites: A student must possess knowledge comparable to the undergraduate subjects on \nUnix Tools, C/C++ programming and Computer Networks. Optionally; a student will be \nadvantaged by doing an introduction to Computer Systems Modelling; such as the Part 2 \nComputer Systems Modelling subject as well as having a background in measurement \nmethodologies such as that of L50: Introduction to networking and systems measurements \nprovided in the ACS/Part III curriculum. \nMoodle, timetable \n \nAims \nThis is a practical course, actively building and extending software tools to observe the detailed \nbehaviour of transaction-oriented datacenter-like software. Students will observe and \nunderstand sources of user-facing tail latency, including that stemming \nfrom resource contention, cross-program interference, bad software locking, and simple design \nerrors. \nA 2-hour weekly hybrid, lecture/lab format permits students continuous monitored progress with \ncomplexity of tasks building naturally upon the previous weeks learning. \n \nObjectives \nUpon successful completion of the course, students will be able to: \n \nMake order-of-magnitude estimates of software, hardware, and I/O speeds \nMake valid measurements of actual software, hardware, and I/O speeds \nCreate observation facilities, including logs and dashboards, as part of a software system \ndesign \nCreate tracing facilities to fully observe the execution of complex software \nTime-align traces across multiple computers \nDisplay dense tracing information in meaningful ways \nReason about the sources of real-time and transaction delays, including cross-program \nresource interference, remote procedure call delays, and software locking surprises \nFix programs based on the above reasoning, making their response times faster and more \nrobust \n \nSyllabus \nMeasurement \n   Week 1 Intro, Measuring CPU time, rdtsc, Measuring memory access times, Measuring disks, \n   Week 2 gettimeofday, logs Measuring networks, Remote Procedure Calls, Multi-threads, locks \nObservation \n   Week 3 RPC, logs, displaying traces, interference \n   Week 4 Antagonist programs, Logging, dashboards, profiling. \nKUtrace \n   Week 5 Kernel patches, hello world, post-processing \n   Week 6 KUtrace multi-CPU time display \n   Week 7 Client-server KUtrace, with antagonists and interference \n\n   Week 8 Other trace mysteries \n \n \nAssessment \nThis is a Lab-based module; assessment is based upon reports covering guided laboratory work \nperformed each week. Assessment will be via two submissions: \n \n20% assignment deadline week 4 based upon Lab work from Weeks 1-4; word target 1000, limit \n2000 \n80% assignment deadline week 8 based upon lab work from weeks 5-8; word target 2000, limit \n4000 \nThe intent of the first assessment point is to provide rich feedback to students based upon a \n20% assignment permitting focussed and improved work to be executedfor the final assignment. \n \nAll work in this module is expected to be the effort of the student. Enough equipment is \nprovisioned to allow each student an independent set of apparatus. While classmembers may \nfind sharing operational experience valuable all assessment is based upon a students sole \nsubmission based upon. their own experiments and findings. \n \nReading Material \nCore text \n \nRichard L. Sites, Understanding Software Dynamics, Addison-Wesley Professional Computing \nSeries, 2022 \nFurther reading: papers and presentations that you might find interesting. None are required \nreading \u2013 they are well-written and/or informative. \n \nJohn K. Ousterhout et al., A trace-driven analysis of the UNIX 4.2 BSD file system ACM \nSIGOPS Operating Systems Review, Vol. 19, No. 5, Dec, 1985 \nhttps://dl.acm.org/doi/pdf/10.1145/323627.323631 \nLuiz Andr\u00b4e Barroso, Jimmy Clidaras, Urs H\u00a8olzle, The Datacenter as a Computer: An \nIntroduction to the Design of Warehouse-Scale Machines, Second Edition \nJohn L. Hennessy, David A. Patterson, Computer Architecture, A Quantitative Approach 5th \nEdition \nGeorge Varghese, Network Algorithmics. Morgan Kaufmann \nActually keeping datacenter software up and running \u2013 how Google runs production systems \nSite Reliability Engineering Edited by Betsy Beyer, Chris Jones, Jennifer Petoff and Niall \nRichard Murphy free PDF: https://landing.google.com/sre/book.html \nHow NOT to run datacenters (27 minute video, funny/sad) dotScale 2014 - Robert Kennedy - \nLife in the Trenches of healthcare.gov https://www.youtube.com/watch?v=GLQyj-kBRdo \nAn extended (optional) reading list will also be provided. \n \n"
    ],
    "semesters_-5825188247759939644": [
        "1st Semester \n \nDATABASES \n \nAims \nThis course introduces basic concepts for database systems as seen from the perspective of \napplication designers. That is, the focus is on the abstractions supported by database \nmanagement systems and not on how those abstractions are implemented. \n \nThe database world is currently undergoing swift and dramatic transformations largely driven by \nInternet-oriented applications and services. Today many more options are available to database \napplication developers than in the past and so it is becoming increasingly difficult to sort fact \nfrom fiction. The course attempts to cut through the fog with a practical approach that \nemphasises engineering tradeoffs that underpin these recent developments and also guide our \nselection of \u201cthe right tool for the job.\u201d \n \nThis course covers three approaches. First, the traditional mainstay of the database industry -- \nthe relational approach -- is described with emphasis on eliminating logical redundancy in data. \nThen two representatives of recent trends are presented -- graph-oriented and \ndocument-oriented databases. The lectures are supported with two Help and Tick sessions, \nwhere students gain hands-on experience and guidance with the Assessed Exercises (ticks). \n \nLectures \nL1 Introduction. What is a database system? What is a data model? Typical DBMS \nconfigurations and variations (fast queries or reliable update). In-core, in secondary store or \ndistributed. \nL2 Conceptual modelling. Entities, relations, E/R diagrams and implementation-independent \nmodelling, weak entities, cardinality. \nL3+4 The relational database model. Implementing E/R models with relational tables. Relational \nalgebra and SQL. Basic query primitives. Update anomalies caused by redundancy. Minimising \nredundancy with normalised schemas. \nL5 Transactions. On-Line Transaction Processing. On-line Analytical Processing. Reliability, \nthroughput, normal forms, ACID, BASE, eventual consistency. \nL6 Documents and semi-structured data. The NoSQL, schema-free movement. XML/JSON. \nKey/value stores. Embracing data redundancy: representing data for fast, application-specific \naccess. Path queries (if time permits). \nL7 Further SQL. Multisets, NULL values, aggregates, transitive closure, expressibility (nested \nqueries and recursive SQL). \nL8 Graph databases. Optimised for processing enormous numbers of nodes and edges. \nImplementing E/R models in a graph-oriented database. Comparison of the presented models. \nObjectives \nAt the end of the course students should \n\n \nbe able to design entity-relationship diagrams to represent simple database application \nscenarios \nknow how to convert entity-relationship diagrams to relational- and graph-oriented \nimplementations \nunderstand the fundamental tradeoff between the ease of updating data and the response time \nof complex queries \nunderstand that no single data architecture can be used to meet all data management \nrequirements \nbe familiar with recent trends in the database area. \nRecommended reading \nLemahieu, W., Broucke, S. van den and Baesens, B. (2018) Principles of database \nmanagement. Cambridge University Press.\n \n\nDIGITAL ELECTRONICS \n \nAims \nThe aims of this course are to present the principles of combinational and sequential digital logic \ndesign and optimisation at a gate level. The use of n and p channel MOSFETs for building logic \ngates is also introduced. \n \nTopics \nIntroduction. Semiconductors to computers. Logic variables. Examples of simple logic. Logic \ngates. Boolean algebra. De Morgan\u2019s theorem. \nLogic minimisation. Truth tables and normal forms. Karnaugh maps. Quine-McCluskey method. \nBinary adders. Half adder, full adder, ripple carry adder, fast carry generation. \nCombinational logic design: further considerations. Multilevel logic. Gate propagation delay. An \nintroduction to timing diagrams. Hazards and hazard elimination. Other ways to implement \ncombinational logic. \nIntroduction to practical classes. Prototyping box. Breadboard and Dual in line (DIL) packages. \nWiring. \nSequential logic. Memory elements. RS latch. Transparent D latch. Master-slave D flip-flop. T \nand JK flip-flops. Setup and hold times. \nSequential logic. Counters: Ripple and synchronous. Shift registers. System timing - setup time \nconstraint, clock skew, metastability. \nSynchronous State Machines. Moore and Mealy finite state machines (FSMs). Reset and self \nstarting. State transition diagrams. Elimination of redundant states - row matching and state \nequivalence/implication table. \nFurther state machines. State assignment: sequential, sliding, shift register, one hot. \nImplementation of FSMs. \nIntroduction to microprocessors. Microarchitecture, fetch, register access, memory access, \nbranching, execution time. \nElectronics, Devices and Circuits. Current and voltage, conductors/insulators/semiconductors, \nresistance, basic circuit theory, the potential divider. Solving non-linear circuits. P-N junction \n(forward and reverse bias), N and p channel MOSFETs (operation and characteristics) and \nn-MOSFET logic, e.g., n-MOSFET inverter. Power consumption and switching time problems \nproblems in n-MOSFET logic. CMOS logic (NOT, NAND and NOR gates), logic families, noise \nmargin. \nObjectives \nAt the end of the course students should \n \nunderstand the relationships between combination logic and boolean algebra, and between \nsequential logic and finite state machines; \nbe able to design and minimise combinational logic; \nappreciate tradeoffs in complexity and speed of combinational designs; \nunderstand how state can be stored in a digital logic circuit; \nknow how to design a simple finite state machine from a specification and be able to implement \nthis in gates and edge triggered flip-flops; \n\nunderstand how to use MOSFETs to build digital logic circuits. \nRecommended reading \n* Harris, D.M. and Harris, S.L. (2013). Digital design and computer architecture. Morgan \nKaufmann (2nd ed.). The first edition is still relevant. \nKatz, R.H. (2004). Contemporary logic design. Benjamin/Cummings. The 1994 edition is more \nthan sufficient. \nHayes, J.P. (1993). Introduction to digital logic design. Addison-Wesley. \n \nBooks for reference: \n \nHorowitz, P. and Hill, W. (1989). The art of electronics. Cambridge University Press (2nd ed.) \n(more analog). \nWeste, N.H.E. and Harris, D. (2005). CMOS VLSI Design - a circuits and systems perspective. \nAddison-Wesley (3rd ed.). \nMead, C. and Conway, L. (1980). Introduction to VLSI systems. Addison-Wesley. \nCrowe, J. and Hayes-Gill, B. (1998). Introduction to digital electronics. Butterworth-Heinemann. \nGibson, J.R. (1992). Electronic logic circuits. Butterworth-Heinemann.\n \n\nDISCRETE MATHEMATICS \n \nAims \nThe course aims to introduce the mathematics of discrete structures, showing it as an essential \ntool for computer science that can be clever and beautiful. \n \nLectures \nProof [5 lectures]. \nProofs in practice and mathematical jargon. Mathematical statements: implication, bi-implication, \nuniversal quantification, conjunction, existential quantification, disjunction, negation. Logical \ndeduction: proof strategies and patterns, scratch work, logical equivalences. Proof by \ncontradiction. Divisibility and congruences. Fermat\u2019s Little Theorem. \n \nNumbers [5 lectures]. \nNumber systems: natural numbers, integers, rationals, modular integers. The Division Theorem \nand Algorithm. Modular arithmetic. Sets: membership and comprehension. The greatest \ncommon divisor, and Euclid\u2019s Algorithm and Theorem. The Extended Euclid\u2019s Algorithm and \nmultiplicative inverses in modular arithmetic. The Diffie-Hellman cryptographic method. \nMathematical induction: Binomial Theorem, Pascal\u2019s Triangle, Fundamental Theorem of \nArithmetic, Euclid\u2019s infinity of primes. \n \nSets [9 lectures]. \nExtensionality Axiom: subsets and supersets. Separation Principle: Russell\u2019s Paradox, the \nempty set. Powerset Axiom: the powerset Boolean algebra, Venn and Hasse diagrams. Pairing \nAxiom: singletons, ordered pairs, products. Union axiom: big unions, big intersections, disjoint \nunions. Relations: composition, matrices, directed graphs, preorders and partial orders. Partial \nand (total) functions. Bijections: sections and retractions. Equivalence relations and set \npartitions. Calculus of bijections: characteristic (or indicator) functions. Finite cardinality and \ncounting. Infinity axiom. Surjections. Enumerable and countable sets. Axiom of choice. \nInjections. Images: direct and inverse images. Replacement Axiom: set-indexed constructions. \nSet cardinality: Cantor-Schoeder-Bernstein Theorem, unbounded cardinality, diagonalisation, \nfixed-points. Foundation Axiom. \n \nFormal languages and automata [5 lectures]. \nIntroduction to inductive definitions using rules and proof by rule induction. Abstract syntax \ntrees. Regular expressions and their algebra. Finite automata and regular languages: Kleene\u2019s \ntheorem and the Pumping Lemma. \n \nObjectives \nOn completing the course, students should be able to \n \nprove and disprove mathematical statements using a variety of techniques; \napply the mathematical principle of induction; \nknow the basics of modular arithmetic and appreciate its role in cryptography; \n\nunderstand and use the language of set theory in applications to computer science; \ndefine sets inductively using rules and prove properties about them; \nconvert between regular expressions and finite automata; \nuse the Pumping Lemma to prove that a language is not regular. \nRecommended reading \nBiggs, N.L. (2002). Discrete mathematics. Oxford University Press (Second Edition). \nDavenport, H. (2008). The higher arithmetic: an introduction to the theory of numbers. \nCambridge University Press. \nHammack, R. (2013). Book of proof. Privately published (Second edition). Available at: \nhttp://www.people.vcu.edu/ rhammack/BookOfProof/index.html \nHouston, K. (2009). How to think like a mathematician: a companion to undergraduate \nmathematics. Cambridge University Press. \nKozen, D.C. (1997). Automata and computability. Springer. \nLehman, E.; Leighton, F.T.; Meyer, A.R. (2014). Mathematics for computer science. Available \non-line. \nVelleman, D.J. (2006). How to prove it: a structured approach. Cambridge University Press \n(Second Edition).\n \n\nFOUNDATIONS OF COMPUTER SCIENCE \nAims \nThe main aim of this course is to present the basic principles of programming. As the \nintroductory course of the Computer Science Tripos, it caters for students from all backgrounds. \nTo those who have had no programming experience, it will be comprehensible; to those \nexperienced in languages such as C, it will attempt to correct any bad habits that they have \nlearnt. \n \nA further aim is to introduce the principles of data structures and algorithms. The course will \nemphasise the algorithmic side of programming, focusing on problem-solving rather than on \nhardware-level bits and bytes. Accordingly it will present basic algorithms for sorting, searching, \netc., and discuss their efficiency using O-notation. Worked examples (such as polynomial \narithmetic) will demonstrate how algorithmic ideas can be used to build efficient applications. \n \nThe course will use a functional language (OCaml). OCaml is particularly appropriate for \ninexperienced programmers, since a faulty program cannot crash and OCaml\u2019s unobtrusive type \nsystem captures many program faults before execution. The course will present the elements of \nfunctional programming, such as curried and higher-order functions. But it will also introduce \ntraditional (procedural) programming, such as assignments, arrays and references. \n \nLectures \nIntroduction to Programming. The role of abstraction and representation. Introduction to integer \nand floating-point arithmetic. Declaring functions. Decisions and booleans. Example: integer \nexponentiation. \nRecursion and Efficiency. Examples: Exponentiation and summing integers. Iteration versus \nrecursion. Examples of growth rates. Dominance and O-Notation. The costs of some \nrepresentative functions. Cost estimation. \nLists. Basic list operations. Append. Na\u00efve versus efficient functions for length and reverse. \nStrings. \nMore on lists. The utilities take and drop. Pattern-matching: zip, unzip. A word on polymorphism. \nThe \u201cmaking change\u201d example. \nSorting. A random number generator. Insertion sort, mergesort, quicksort. Their efficiency. \nDatatypes and trees. Pattern-matching and case expressions. Exceptions. Binary tree traversal \n(conversion to lists): preorder, inorder, postorder. \nDictionaries and functional arrays. Functional arrays. Dictionaries: association lists (slow) versus \nbinary search trees. Problems with unbalanced trees. \nFunctions as values. Nameless functions. Currying. The \u201capply to all\u201d functional, map. \nExamples: The predicate functionals filter and exists. \nSequences, or lazy lists. Non-strict functions such as IF. Call-by-need versus call-by-name. Lazy \nlists. Their implementation in OCaml. Applications, for example Newton-Raphson square roots. \nQueues and search strategies. Depth-first search and its limitations. Breadth-first search (BFS). \nImplementing BFS using lists. An efficient representation of queues. Importance of efficient data \nrepresentation. \n\nElements of procedural programming. Address versus contents. Assignment versus binding. \nOwn variables. Arrays, mutable or not. Introduction to linked lists. \nObjectives \nAt the end of the course, students should \n \nbe able to write simple OCaml programs; \nunderstand the fundamentals of using a data structure to represent some mathematical \nabstraction; \nbe able to estimate the efficiency of simple algorithms, using the notions of average-case, \nworse-case and amortised costs; \nknow the comparative advantages of insertion sort, quick sort and merge sort; \nunderstand binary search and binary search trees; \nknow how to use currying and higher-order functions; \nunderstand how OCaml combines imperative and functional programming in a single language. \nRecommended reading \nJohn Whitington. OCaml from the Very Beginning. (http://ocaml-book.com). \n \nOkasaki, C. (1998). Purely functional data structures. Cambridge University Press.\n \n\nHARDWARE PRACTICAL CLASSES \nThe Hardware Practical Classes accompany the Digital Electronics series of lectures. The aim \nof the Practical Classes is to enable students to get hands-on experience of designing, building, \nand testing and debugging of digital electronic circuits. The labs will take place in the Intel Lab. \nlocated in the William Gates Building (WGB). \n \nThe Digital Electronics lecture series occupies 12 lectures in the first 4 weeks of the Michaelmas \nTerm, while the Practical Classes occupy the latter 6 weeks of the Michaelmas Term and the \nfirst 6 weeks of the Lent Term. If required, extra sessions will be available for any students \nneeding to 'catch-up' on missed sessions or to complete any remaining practical work. \n \nThe Practical Classes take the form of 4 workshops, specifically: \n \nWorkshop 1 \u2013 Electronic Die; \nWorkshop 2 \u2013 Shaft Position Encoder; \nWorkshop 3 \u2013 Debouncing a Switch; \nWorkshop 4 \u2013 Framestore for an LED Array. \nIn general, the workshops require some preparatory work to be done prior to the practical \nsession. These tasks are highlighted at the beginning of each Worksheet. Typically this involves \npreparing a design that you will then build, test and modify during the practical class. Note that \ninsufficient preparation prior to the practical classes may compromise effective use of your time \nin the Intel lab. \n \nIn the Practical Classes you will usually work on your own, and you are expected to complete \none Workshop in each of your scheduled sessions. Demonstrators are available during the \nsessions to assist you with any queries or problems you may have. \n \nImportant: Remember to get your work ticked.\n \n\nINTRODUCTION TO GRAPHICS \nAims \nTo introduce the necessary background, the basic algorithms, and the applications of computer \ngraphics and graphics hardware. \n \nLectures \nBackground. What is an image? Resolution and quantisation. Storage of images in memory. [1 \nlecture] \nRendering. Perspective. Reflection of light from surfaces and shading. Geometric models. Ray \ntracing. [2 lectures] \nGraphics pipeline. Polygonal mesh models. Transformations using matrices in 2D and 3D. \nHomogeneous coordinates. Projection: orthographic and perspective. [1 lecture] \nGraphics hardware and modern OpenGL. GPU rendering. GPU frameworks and APIs. Vertex \nprocessing. Rasterisation. Fragment processing. Working with meshes and textures. Z-buffer. \nDouble-buffering and frame synchronization. [2 lectures] \n  \nHuman vision, colour and tone mapping. Perception of colour. Tone mapping. Colour spaces. [2 \nlectures] \nObjectives \nBy the end of the course students should be able to: \n \nunderstand and apply in practice basic concepts of ray-tracing: ray-object intersection, \nreflections, refraction, shadow rays, distributed ray-tracing, direct and indirect illumination; \ndescribe and explain the following algorithms: z-buffer, texture mapping, double buffering, \nmip-map, and normal-mapping; \nuse matrices and homogeneous coordinates to represent and perform 2D and 3D \ntransformations; understand and use 3D to 2D projection, the viewing volume, and 3D clipping; \nimplement OpenGL code for rendering of polygonal objects, control camera and lighting, work \nwith vertex and fragment shaders; \ndescribe a number of colour spaces and their relative merits. \nexplain the need for tone mapping and colour processing in rendering pipeline. \nRecommended reading \n* Shirley, P. and Marschner, S. (2009). Fundamentals of Computer Graphics. CRC Press (3rd \ned.). \n \nFoley, J.D., van Dam, A., Feiner, S.K. and Hughes, J.F. (1990). Computer graphics: principles \nand practice. Addison-Wesley (2nd ed.). \n \nKessenich, J.M., Sellers, G. and Shreiner, D (2016). OpenGL Programming Guide: The Official \nGuide to Learning OpenGL, Version 4.5 with SPIR-V, [seventh edition and later]\n \n\nOBJECT-ORIENTED PROGRAMMING \n \nAims \nThe goal of this course is to provide students with an understanding of Object-Oriented \nProgramming. Concepts are demonstrated in multiple languages, but the primary language is \nJava. \n \nLecture syllabus \nTypes, Objects and Classes Moving from functional to imperative. Functions, methods. Control \nflow. values, variables and types. Primitive Types. Classes as custom types. Objects vs \nClasses. Class definition, constructors. Static data and methods. \nDesigning Classes Identifying classes. UML class diagrams. Modularity. Encapsulation/data \nhiding. Immutability. Access modifiers. Parameterised types (Generics). \nPointers, References and Memory Pointers and references. Reference types in Java. The call \nstack. The heap. Iteration and recursion. Pass-by-value and pass-by-reference. \nInheritance Inheritance. Casting. Shadowing. Overloading. Overriding. Abstract Methods and \nClasses. \nPolymorphism and Multiple Inheritance Polymorphism in ML and Java. Multiple inheritance. \nInterfaces in Java. \nLifecycle of an Object Constructors and chaining. Destructors. Finalizers. Garbage Collection: \nreference counting, tracing. \nJava Collections and Object Comparison Java Collection interface. Key classes. Collections \nclass. Iteration options and the use of Iterator. Comparing primitives and objects. Operator \noverloading. \nError Handling Types of errors. Limitations of return values. Deferred error handling. Exceptions. \nCustom exceptions. Checked vs unchecked. Inappropriate use of exceptions. Assertions. \nDesignLanguage evolution Need for languages to evolve. Generics in Java. Type erasure. \nIntroduction to Java 8: Lambda functions, functions as values, method references, streams. \nDesign Patterns Introduction to design patterns. Open-closed principle. Examples of Singleton, \nDecorator, State, Composite, Strategy, Observer. [2 lectures] \nObjectives \nAt the end of the course students should \n \nProvide an overview of key OOP concepts that are transferable in mainstream programming \nlanguages; \nGain an understanding of software development approaches adopted in the industry including \nmaintainability, testing, and software design patterns; \nIncrease programming familiarity with Java; \nRecommended reading \n1. Real-World Software Development: A Project-Driven Guide to Fundamentals in Java [Urma et \nal.] \n2. Modern Java in Action (2nd edition) [Urma et al.]. \n3. Java How to Program: early objects [Deitel et al.]\n \n\nOCAML PRACTICAL CLASSES \n \nThe Role of Tickers \nA ticking session is a short (5 minute) one-to-one session with a ticker conducted on Thursday \nafternoons. The role of the ticker is twofold: \n \nto check you understand your solution (and didn\u2019t just have some lucky guesses or copy code \nfrom elsewhere) \nto give you general feedback on ways to improve your code. \nTo those ends a Ticker will typically ask you questions related to the core material of the tick and \ndiscuss your code directly. \n \nDeadline Extensions \nDeadline extensions can be granted for illness or similar reasons. To obtain an extension please \nemail Jon Ludlam in the first instance, clearly stating your justification for an extension and \nCCing your Director of Studies, who will need to support your request.\n \n\nSCIENTIFIC COMPUTING \n \nAims \nThis course is a hands-on introduction to using computers to investigate scientific models and \ndata. \n \nSyllabus \nPython notebooks. Overview of the Python programming language. Use of notebooks for \nscientific computing. \nNumerical computation. Writing fast vectorized code in numpy. Optimization and fitting. \nSimulation. \nWorking with data. Data import. Common ways to summarize and plot data, for univariate and \nmultivariate analysis. \nObjectives \nAt the end of the course students should \n \nbe able to import data, plot it, and summarize it appropriately \nbe able to write fast vectorized code for scientific / data work\n \n\n",
        "2nd Semester \nALGORITHMS 1 \nAims \nThe aim of this course is to provide an introduction to computer algorithms and data structures, \nwith an emphasis on foundational material. \n \nLectures \nSorting. Review of complexity and O-notation. Trivial sorting algorithms of quadratic complexity. \nReview of merge sort and quicksort, understanding their memory behaviour on statically \nallocated arrays. Heapsort. Stability. Other sorting methods including sorting in linear time. \nMedian and order statistics. [Ref: CLRS3 chapters 1, 2, 3, 6, 7, 8, 9] [about 4 lectures] \nStrategies for algorithm design. Dynamic programming, divide and conquer, greedy algorithms \nand other useful paradigms. [Ref: CLRS3 chapters 4, 15, 16] [about 3 lectures] \nData structures. Elementary data structures: pointers, objects, stacks, queues, lists, trees. \nBinary search trees. Red-black trees. B-trees. Hash tables. Priority queues and heaps. [Ref: \nCLRS3 chapters 6, 10, 11, 12, 13, 18] [about 5 lectures]. \nObjectives \nAt the end of the course students should: \n \nhave a thorough understanding of several classical algorithms and data structures; \nbe able to analyse the space and time efficiency of most algorithms; \nhave a good understanding of how a smart choice of data structures may be used to increase \nthe efficiency of particular algorithms; \nbe able to design new algorithms or modify existing ones for new applications and reason about \nthe efficiency of the result. \nRecommended reading \n* Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest and Clifford Stein Introduction to \nAlgorithms, Fourth Edition ISBN 9780262046305 Published: April 5, 2022. \n \nSedgewick, R., Wayne, K. (2011). Algorithms. Addison-Wesley. ISBN 978-0-321-57351-3. \n \nKleinberg, J. and Tardos, \u00c9. (2006). Algorithm design. Addison-Wesley. ISBN \n978-0-321-29535-4. \n \nKnuth, D.A. (2011). The Art of Computer Programming. Addison-Wesley. ISBN \n978-0-321-75104-1.\n \n\nALGORITHMS 2 \n \nAims \nThe aim of this course is to provide an introduction to computer algorithms and data structures, \nwith an emphasis on foundational material. \n \nLectures \nGraphs and path-finding algorithms. Graph representations. Breadth-first and depth-first search. \nSingle-source shortest paths: Bellman-Ford and Dijkstra algorithms. All-pairs shortest paths: \ndynamic programming and Johnson\u2019s algorithms. [About 4 lectures] \nGraphs and subgraphs. Maximum flow: Ford-Fulkerson method, Max-flow min-cut theorem. \nMatchings in bipartite graphs. Minimum spanning tree: Kruskal and Prim algorithms. Topological \nsort. [About 4 lectures] \nAdvanced data structures. Binomial heap. Amortized analysis: aggregate analysis, potential \nmethod. Fibonacci heaps. Disjoint sets. [About 4 lectures] \nObjectives \nAt the end of the course students should: \n \nhave a thorough understanding of several classical algorithms and data structures; \nbe able to analyse the space and time efficiency of most algorithms; \nhave a good understanding of how a smart choice of data structures may be used to increase \nthe efficiency of particular algorithms; \nbe able to design new algorithms or modify existing ones for new applications and reason about \nthe efficiency of the result. \nRecommended reading \n* Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest and Clifford Stein Introduction to \nAlgorithms, Fourth Edition ISBN 9780262046305 Published: April 5, 2022. \n \nSedgewick, R., Wayne, K. (2011). Algorithms. Addison-Wesley. ISBN 978-0-321-57351-3. \n \nKleinberg, J. and Tardos, \u00c9. (2006). Algorithm design. Addison-Wesley. ISBN \n978-0-321-29535-4. \n \nKnuth, D.A. (2011). The Art of Computer Programming. Addison-Wesley. ISBN \n978-0-321-75104-1.\n \n\nMACHINE LEARNING AND REAL-WORLD DATA \n \nAims \nThis course introduces students to machine learning algorithms as used in real-world \napplications, and to the experimental methodology necessary to perform statistical analysis of \nlarge-scale data from unpredictable processes. Students will perform 3 extended practicals, as \nfollows: \n \nStatistical classification: Determining movie review sentiment using Naive Bayes; \nSequence Analysis: Hidden Markov Modelling and its application to a task from biology \n(predicting protein interactions with a cell membrane); \nAnalysis of social networks, including detection of cliques and central nodes. \nSyllabus \nTopic One: Statistical Classification \nIntroduction to sentiment classification. \nNaive Bayes parameter estimation. \nStatistical laws of language. \nStatistical tests for classification tasks. \nCross-validation and test sets. \nUncertainty and human agreement. \nTopic Two: Sequence Analysis  \nHidden Markov Models (HMM) and HMM training. \nThe Viterbi algorithm. \nUsing an HMM in a biological application. \nTopic Three: Social Networks  \nProperties of networks: Degree, Diameter. \nBetweenness Centrality. \nClustering using betweenness centrality. \nObjectives \nBy the end of the course students should be able to: \n \nunderstand and program two simple supervised machine learning algorithms; \nuse these algorithms in statistically valid experiments, including the design of baselines, \nevaluation metrics, statistical testing of results, and provision against overtraining; \nvisualise the connectivity and centrality in large networks; \nuse clustering (i.e., a type of unsupervised machine learning) for detection of cliques in \nunstructured networks. \nRecommended reading \nJurafsky, D. and Martin, J. (2008). Speech and language processing. Prentice Hall. \n \nEasley, D. and Kleinberg, J. (2010). Networks, crowds, and markets: reasoning about a highly \nconnected world. Cambridge University Press.\n \n\nOPERATING SYSTEMS \n \nAims \nThe overall aim of this course is to provide a general understanding of the structure and key \nfunctions of the operating system. Case studies will be used to illustrate and reinforce \nfundamental concepts. \n \nLectures \nIntroduction to operating systems. Elementary computer organisaton. Abstract view of an \noperating system. Key concepts: layering, multiplexing, performance, caching, buffering, policies \nversus mechanisms. OS evolution: multi-programming, time-sharing. Booting. [1 lecture] \nProtection. Dual-mode operation. Protecting CPU, memory, I/O, storage. Kernels, micro-kernels, \nand modules. System calls. Virtual machines and containers. Subjects and objects. \nAuthentication. Access matrix: ACLs and capabilities. Covert channels. [1 lecture] \nProcesses. Job/process concepts. Process lifecycle. Process management. Context switching. \nInter-process communication. [1 lecture] \nScheduling. CPU-I/O burst cycle. Scheduling basics: preemption and non-preemption. \nScheduling algorithms: FCFS, SJF, SRTF, Priority, and Round Robin. Multilevel scheduling. [2 \nlectures] \nMemory management. Processes in memory. Logical versus physical addresses. Partitions: \nstatic versus dynamic, free space management, external fragmentation. Segmented memory. \nPaged memory: concepts, internal fragmentation, page tables. Demand paging/segmentation. \nPage replacement strategies: FIFO, OPT, and LRU with approximations including NRU, LFU, \nMFU, MRU. Working set schemes. Thrashing. [3 lectures] \nI/O subsystem. General structure. Polled mode versus interrupt-driven I/O. Programmed I/O \n(PIO) versus Direct Memory Access (DMA). Application I/O interface: block and character \ndevices, buffering, blocking, non-blocking, asynchronous, and vectored I/O. Other issues: \ncaching, scheduling, spooling, performance. [1 lecture] \nFile management. File concept. Directory and storage services. File names and metadata. \nDirectory name-space: hierarchies, DAGs, hard and soft links. File operations. Access control. \nExistence and concurrency control. [1 lecture] \nUnix case study. History. General structure. Unix file system: file abstraction, directories, mount \npoints, implementation details. Processes: memory image, lifecycle, start of day. The shell: \nbasic operation, commands, standard I/O, redirection, pipes, signals. Character and block I/O. \nProcess scheduling. [2 lectures] \nObjectives \nAt the end of the course students should be able to: \n \ndescribe the general structure and purpose of an operating system; \nexplain the concepts of process, address space, and file; \ncompare and contrast various CPU scheduling algorithms; \ncompare and contrast mechanisms involved in memory and storage management; \ncompare and contrast polled, interrupt-driven, and DMA-based access to I/O devices. \nRecommended reading \n\n* Silberschatz, A., Galvin, P.C., and Gagne, G. (2018). Operating systems concepts. Wiley (10th \ned.). Prior editions, at least back to 8th ed., should also be acceptable. \nAnderson, T. and Dahlin, M. (2014). Operating Systems: Principles and Practice. Recursive \nBooks (2nd ed.). \nBacon, J. and Harris, T. (2003). Operating systems. Addison-Wesley (3rd ed.). Currently \nappears out-of-print. \nLeffler, S. (1989). The design and implementation of the 4.3BSD Unix operating system. \nAddison-Wesley. \nMcKusick, M.K., Neville-Neil, G.N. and Watson, R.N.M. (2014) The Design and Implementation \nof the FreeBSD Operating System. Pearson Education. (2nd ed.).\n \n\nINTERACTION DESIGN \nAims \nThe aim of this course is to provide an introduction to interaction design, with an emphasis on \nunderstanding and experiencing the user-centred design process, from conducting user \nresearch and requirements development to implementation and evaluation, while understanding \nthe background to human-computer interaction. \n \nLectures \nCourse overview and user research methods. Introduction to the course and practicals. \nUser-Centred Design. User research methods. \nKeeping usera in mind. User research data analysis. Identifying users and stakeholders. \nRepresenting user goals and activities. Identifying and establishing requirements. \nDesign and prototyping. Methods for exploring the design space. Prototyping and different kinds \nof prototypes. \nVisual and interaction design. Memory, perception, attention, and their implications for \ninteraction design. Modalities of interaction, interaction design patterns, information architecture, \nand their implications for interaction design. \nEvaluation. Practical methods for evaluating designs. Evaluation methods without users. \nEvaluation methods with users. \nCase studies from industry and research. Guest lectures (the topics of these lectures are \nsubject to change). \nObjectives \nBy the end of the course students should \n \nhave a thorough understanding of the user-centred design process and be able to apply it to \ninteraction design; \nbe able to design new user interfaces that are informed by principles of good design, and the \nprinciples of human visual perception, cognition and communication; \nbe able to prototype and implement interactive user interfaces with a strong emphasis on users, \nusability and appearance; \nbe able to evaluate existing or new user interfaces using multiple techniques; \nbe able to compare and contrast different design techniques and to critique their applicability to \nnew domains. \nRecommended reading \n* Preece, J., Rogers, Y. and Sharp, H. (2015). Interaction design. Wiley (4th ed.).\n \n\nINTRODUCTION TO PROBABILITY \n \nAims \nThis course provides an elementary introduction to probability and statistics with applications. \nProbability theory and the related field of statistical inference provide the foundations for \nanalysing and making sense of data. The focus of this course is to introduce the language and \ncore concepts of probability theory. The course will also cover some applications of statistical \ninference and algorithms in order to equip students with the ability to represent problems using \nthese concepts and analyse the data within these principles. \n \nLectures \nPart 1 - Introduction to Probability \n \nIntroduction. Counting/Combinatorics (revision), Probability Space, Axioms, Union Bound. \nConditional probability. Conditional Probabilities and Independence, Bayes\u2019Theorem, Partition \nTheorem \nPart 2 - Discrete Random Variables \n \nRandom variables. Definition of a Random Variable, Probability Mass Function, Cumulative \nDistribution, Expectation. \nProbability distributions. Definition and Properties of Expectation, Variance, different ways of \ncomputing them, Examples of important Distributions (Bernoulli, Binomial, Geometric, Poisson), \nPrimer on Continuous Distributions including Normal and Exponential Distributions. \nMultivariate distributions. Multiple Random Variables, Joint and Marginal Distributions, \nIndependence of Random Variables, Covariance. \nPart 3 - Moments and Limit Theorems \n \nIntroduction. Law of Average, Useful inequalities (Markov and Chebyshef), Weak Law of Large \nNumbers (including Proof using Chebyshef\u2019s inequality), Examples. \nMoments and Central Limit Theorem. Introduction to Moments of Random Variables, Central \nLimit Theorem (Proof using Moment Generating functions), Example. \nPart 4 - Applications/Statistics \n \nStatistics. Classical Parameter Estimation (Maximum-Likelihood-Estimation), bias, sample \nmean, sample variance), Examples (Collision-Sampling, Estimating Population Size). \nAlgorithms. Online Algorithms (Secretary Problem, Odd\u2019s Algorithm). \nObjectives \nAt the end of the course students should \n \nunderstand the basic principles of probability spaces and random variables \nbe able to formulate problems using concepts from probability theory and compute or estimate \nprobabilities \nbe familiar with more advanced concepts such as moments, limit theorems and applications \nsuch as parameter estimation \n\nRecommended reading \n* Ross, S.M. (2014). A First course in probability. Pearson (9th ed.). \n \nBertsekas, D.P. and Tsitsiklis, J.N. (2008). Introduction to probability. Athena Scientific. \n \nGrimmett, G. and Welsh, D. (2014). Probability: an Introduction. Oxford University Press (2nd \ned.). \n \nDekking, F.M., et. al. (2005) A modern introduction to probability and statistics. Springer.\n\nSOFTWARE AND SECURITY ENGINEERING \n \nAims \nThis course aims to introduce students to software and security engineering, and in particular to \nthe problems of building large systems, safety-critical systems and systems that must withstand \nattack by capable opponents. Case histories of failure are used to illustrate what can go wrong, \nwhile current software and security engineering practice is studied as a guide to how failures \ncan be avoided. \n \nLectures \n1. What is a security policy or a safety case? Definitions and examples; one-way flows for both \nconfidentiality and safety properties; separation of duties. Top-down and bottom-up analysis \nmethods. What architecture can do, versus benefits of decoupling policy from mechanism. \n \n2. Examples of safety and security policies. Safety and security usability; the pyramid of harms. \nPredicting and mitigating user errors. The prevention of fraud and error in accounting systems; \nthe safety usability of medical devices. \n \n3. Attitudes to risk: expected  utility, prospect theory, framing, status quo bias. Authority, \nconformity and gender; mental models, affordances and defaults. The characteristics of human \nmemory; forgetting passwords versus guessing them. \n \n4. Security protocols; how to enforce policy using  structured human interaction, cryptography or \nboth. Middleperson attacks.The role of verification and its limitations. \n \n5. Attacks on TLS, from rogue CAs through side channels to Heartbleed. Other types of \nsoftware bugs: syntactic, timing, concurrency, code injection, buffer overflows. Defensive \nprogramming: secure coding, contracts. Fuzzing. \n \n6. The software crisis. Examples of large-scale project failure, such as the London Ambulance \nService system and the NHS National Programme for IT. Intrinsic difficulties with complex \nsoftware. \n \n7. Software engineering as the management of complexity. The software life cycle; requirements \nanalysis methods; modular design; the role of prototyping; the waterfall, spiral and agile models. \n \n8. The economics of software as a Service (SaaS); the impact SaaS has on software \nengineering. Continuous integration, release engineering, behavioural analytics and experiment \nframeworks, rearchitecting systems while in operation. \n \n9. Critical systems: safety as an emergent system property. Examples of catastrophic failure: \nfrom Therac-25 to the Boeing 737Max. The problems of managing redundancy. The overall \nprocess of safety engineering. \n \n\n10. Managing the development of critical systems: tools and methods, individual versus group \nproductivity, economics of testing and agile development, measuring outcomes versus process, \nthe technical and human aspects of management, post-market surveillance and coordinated \ndisclosure. The sustainability of products with software components. \n \nAt the end of the course students should know how writing programs with tough assurance \ntargets, in large teams, or both, differs from the programming exercises they have engaged in \nso far. They should understand the different models of software development described in the \ncourse as well as the value of various development and management tools. They should \nunderstand the development life cycle and its basic economics. They should understand the \nvarious types of bugs, vulnerabilities and hazards, how to find them, and how to avoid \nintroducing them. Finally, they should be prepared for the organizational aspects of their Part IB \ngroup project. \n \nRecommended reading \nAnderson, R. (Third Edition 2020). Security engineering (Part 1 and Chapters 27-28). Wiley. \nAvailable at: http://www.cl.cam.ac.uk/users/rja14/book.html\n \n\n",
        "3rd Semester \nCONCURRENT AND DISTRIBUTED SYSTEMS \n \nAims \nThis course considers two closely related topics, Concurrent Systems and Distributed Systems, \nover 16 lectures. The aim of the first half of the course is to introduce concurrency control \nconcepts and their implications for system design and implementation. The aims of the latter \nhalf of the course are to study the fundamental characteristics of distributed systems, including \ntheir models and architectures; the implications for software design; some of the techniques that \nhave been used to build them; and the resulting details of good distributed algorithms and \napplications. \n \nLectures: Concurrency \nIntroduction to concurrency, threads, and mutual exclusion. Introduction to concurrent systems; \nthreads; interleaving; preemption; parallelism; execution orderings; processes and threads; \nkernel vs. user threads; M:N threads; atomicity; mutual exclusion; and mutual exclusion locks \n(mutexes). \nAutomata Composition. Synchronous and asynchronous parallelism; sequential consistency; \nrendezvous. Safety, liveness and deadlock; the Dining Philosophers; Hardware foundations for \natomicity: test-and-set, load-linked/store-conditional and fence instructions. Lamport bakery \nalgorithm. \nCommon design patterns: semaphores, producer-consumer, and MRSW. Locks and invariants; \nsemaphores; condition synchronisation; N-resource allocation; two-party and generalised \nproducer-consumer; Multi-Reader, Single-Write (MRSW) locks. \nCCR, monitors, and concurrency in practice. Conditional critical regions (CCR); monitors; \ncondition variables; signal-wait vs. signal-continue semantics; concurrency in practice (kernels, \npthreads, Java, Cilk, OpenMP). \nDeadlock and liveness guarantees Offline vs. online; model checking; resource allocation \ngraphs; lock order checking; deadlock prevention, avoidance, detection, and recovery; livelock; \npriority inversion; auto parallelisation. \nConcurrency without shared data; transactions. Active objects; message passing; tuple spaces; \nCSP; and actor models. Composite operations; transactions; ACID; isolation; and serialisability. \nFurther transactions History graphs; good and bad schedules; isolation vs. strict isolation; \n2-phase locking; rollback; timestamp ordering (TSO); and optimistic concurrency control (OCC). \nCrash recovery, lock-free programming, and transactional memory. Write-ahead logging, \ncheckpoints, and recovery. Lock-free programming. Hardware and software transactional \nmemories. \n  \n \nLectures: Distributed Systems \nIntroduction to distributed systems; RPC. Avantages and challenges of distributed systems; \nunbounded delay and partial failure; network protocols; transparency; client-server systems; \nremote procedure call (RPC); marshalling; interface definition languages (IDLs). \n\nSystem models and faults. Synchronous, partially synchronous, and asynchronous network \nmodels; crash-stop, crash-recovery, and Byzantine faults; failures, faults, and fault tolerance; \ntwo generals problem. \nTime, clocks, and ordering of events. Physical clocks; leap seconds; UTC; clock synchronisation \nand drift; Network Time Protocol (NTP). Causality; happens-before relation. \nLogical time; Lamport clocks; vector clocks. Broadcast (FIFO, causal, total order); gossip \nprotocols. \nReplication. Quorums; idempotence; replica consistency; read-after-write consistency. State \nmachine replication; leader-based replication. \nConsensus and total order broadcast. FLP result; leader election; the Raft consensus algorithm. \nReplica consistency. Two-phase commit; relationship between 2PC and consensus; \nlinearizability; ABD algorithm; eventual consistency; CAP theorem. \nCase studies. Conflict-free Replicated Data Types (CRDTs); collaborative text editing. Google's \nSpanner; TrueTime. \nObjectives \nAt the end of Concurrent Systems portion of the course, students should: \n \nunderstand the need for concurrency control in operating systems and applications, both mutual \nexclusion and condition synchronisation; \nunderstand how multi-threading can be supported and the implications of different approaches; \nbe familiar with the support offered by various programming languages for concurrency control \nand be able to judge the scope, performance implications and possible applications of the \nvarious approaches; \nbe aware that dynamic resource allocation can lead to deadlock; \nunderstand the concept of transaction; the properties of transactions, how they can be \nimplemented, and how their performance can be optimised based on optimistic assumptions; \nunderstand how the persistence properties of transactions are addressed through logging; and \nhave a high-level understanding of the evolution of software use of concurrency in the \noperating-system kernel case study. \nAt the end of the Distributed Systems portion of the course, students should: \n \nunderstand the difference between shared-memory concurrency and distributed systems; \nunderstand the fundamental properties of distributed systems and their implications for system \ndesign; \nunderstand notions of time, including logical clocks, vector clocks, and physical time \nsynchronisation; \nbe familiar with various approaches to data and service replication, as well as the concept of \nreplica consistency; \nunderstand the effects of large scale on the provision of fundamental services and the tradeoffs \narising from scale; \nappreciate the implications of individual node and network communications failures on \ndistributed computation; \nbe aware of a variety of programming models and abstractions for distributed systems, such as \nRPC, middleware, and total order broadcast; \n\nbe familiar with a range of distributed algorithms, such as consensus, causal broadcast, and \ntwo-phase commit. \nRecommended reading \nModern Operating Systems (free PDF available online) by Andrew S Tanenbaum, Herbert Bos \nJava Concurrency in Practice' (2006 but still highly relevant) by Brian Goetz \nKleppmann, M. (2017). Designing data-intensive applications. O\u2019Reilly. \nTanenbaum, A.S. and van Steen, M. (2017). Distributed systems, 3rd edition. available online. \nCachin, C., Guerraoui, R. and Rodrigues, L. (2011) Introduction to Reliable and Secure \nDistributed Programming. Springer (2nd edition).\n \n\nDATA SCIENCE \n \nAims \nThis course introduces fundamental tools for describing and reasoning about data. There are \ntwo themes: designing probability models to describe systems; and drawing conclusions based \non data generated by such systems. \n \nLectures \nSpecifying and fitting probability models. Random variables. Maximum likelihood estimation. \nGenerative and supervised models. Goodness of fit. \nFeature spaces. Vector spaces, bases, inner products, projection. Linear models. Model fitting \nas projection. Design of features. \nHandling probability models. Handling pdf and cdf. Bayes\u2019s rule. Monte Carlo estimation. \nEmpirical distribution. \nInference. Bayesianism. Frequentist confidence intervals, hypothesis testing. Bootstrap \nresampling. \nRandom processes. Markov chains. Stationarity, and drift analysis. Processes with memory. \nLearning a random process. \nObjectives \nAt the end of the course students should \n \nbe able to formulate basic probabilistic models, including discrete time Markov chains and linear \nmodels \nbe familiar with common random variables and their uses, and with the use of empirical \ndistributions rather than formulae \nunderstand different types of inference about noisy data, including model fitting, hypothesis \ntesting, and making predictions \nunderstand the fundamental properties of inner product spaces and orthonormal systems, and \ntheir application to modelling \nRecommended reading \n* F.M. Dekking, C. Kraaikamp, H.P. Lopuha\u00e4, L.E. Meester (2005). A modern introduction to \nprobability and statistics: understanding why and how. Springer. \n \nS.M. Ross (2002). Probability models for computer science. Harcourt / Academic Press. \n \nM. Mitzenmacher and E. Upfal (2005). Probability and computing: randomized algorithms and \nprobabilistic analysis. Cambridge University Press.\n \n\nECAD AND ARCHITECTURE PRACTICAL CLASSES \nAims \nThe aims of this course are to enable students to apply the concepts learned in the Computer \nDesign course. In particular a web based tutor is used to introduce the SystemVerilog hardware \ndescription language, while the remaining practical classes will then allow students to implement \nthe design of components in this language. \n \nPractical Classes \nWeb tutor The first class uses a web based tutor to rapidly teach the SystemVerilog language. \nFPGA design flow Test driven hardware development for FPGA including an embedded \nprocessor and peripherals [3 classes] \nEmbedded system implementation Embedded system implementation on FPGA [3-4 classes] \nObjectives \nGain experience in electronic computer aided design (ECAD) through learning a design-flow for \nfield programmable gate arrays (FPGAs). \nLearn how to interface to peripherals like a touch screen. \nLearn how to debug hardware and software systems in simulation. \nUnderstand how to construct and program a heterogeneous embedded system. \nRecommended reading \n* Harris, D.M. and Harris, S.L. (2007). Digital design and computer architecture: from gates to \nprocessors. Morgan Kaufmann. \n \nPointers to sources of more specialist information are included on the associated course web \npage. \n \nIntroduction \nThe ECAD and Architecture Laboratory sessions are a companion to the Introduction to \nComputer Architecture course. The objective is to provide experience of hardware design for \nFPGA including use of a small embedded processor. It covers hardware design in \nSystemVerilog, embedded software design in RISC-V assembly and C, and use of FPGA tools. \n \nPrerequisites \nThis course assumes familiarity with the material from Part IA Digital Electronics, although we \nwill use automated tools to perform many of the steps you might have done there by hand (for \nexample, logic minimisation). \n \nWe will be using a Linux command-line environment. We provide scripts and guidance on what \nyou need, however you may find it useful to review Unix Tools, in particular basic navigation \nsuch as ls, cd, mkdir, cp, mv. We'll be using the GCC compiler and Makefiles, described in parts \n31-32. We will also be using git for revision control and submission of work for ticking - you \nshould review at least parts 23-25 and 29. \n \nPracticalities \n\nThe course runs over the 8 weeks of Michaelmas term. The course has been designed so you \ncan do most of the work at home or at any time you wish. You should be able to run all the \nnecessary tools on your own machine, through the use of virtual machines and Docker \ncontainers. We have a number of routes to do this in case some of them aren't suitable for the \nmachine you have. \n \nWe're running the course in a hybrid mode this year. We will provide online help sessions via \nMicrosoft Teams as well as in-person lab sessions on Fridays and Tuesdays 2-5pm during term. \n \nThe core components of the course, being the RISC-V architecture and software and ticked \nexercise, can be done entirely online, without needing access to hardware. They can be \nundertaken with any of the platforms we support, including MCS Linux (on the Intel Lab \nworkstations and remotely) if your own machine isn't suitable. We expect everyone to complete \nthese parts. \n \nThe rest of the course is optional. The hardware simulation and FPGA compilation can be done \non your machine without access to hardware, if you are able to run the virtual machine. \n \nThe parts that require access to an FPGA board will need the ability to run the virtual machine \nand you be able to collect and return an FPGA board from the Department. That process will be \ndescribed when you come to those parts. There is an optional starred tick available for \ncompletion of this part, which will need assessment by a demonstrator in person. \n \nWe'll do our best to support all the different platforms and variations, but please bear with us - \nit's quite possible we'll come across a problem we haven't seen before! \n \nWe have provided four routes you can use different tools to complete the course. Some routes \nnecessarily exclude some material but this material is optional. \n \nAssessment \nIn a similar way to other practical courses, this course carries one 'tick'. Everyone should expect \nto receive this tick, and those who do not should expect to lose marks on their final exams. \n \nThis year we have adopted the 'chime' autoticker used by Further Java. You will check out the \ninitial files as a git repository from the chime server, commit your work, and push the repository \nback to submit it. You can then press a button to run tests against your code and the autoticker \nwill tell you whether you have passed. Additionally you may be selected for a (real/video) \ninterview with a ticker to discuss your code and understanding of the material. \n \nTicking procedures will only be available during weeks 1-8 of Michaelmas term. The deadline for \nsubmitting Tick 1 is 5pm on the Tuesday of week 6 (19 November 2024). After passing the \nautoticker you may be asked for a tick interview with a demonstrator (online or in person). You \ncan gain Tick 1* during the timetabled lab sessions, assuming demonstrators are available. \n \n\nIf you do not submit a successful Tick 1 to the autoticker by 5pm on the Tuesday of week 6 you \ncan self-certify an extension. If you need more time than this you will need to ask your Directory \nof Studies to organise a further extension. The hard deadline by which all ticks must be \ncompleted is noon on Friday 31 January 2025 (the Head of Department Notices is the definitive \ndocument). Extensions beyond this date due to exceptional circumstances require a formal \napplication from your college to the Examiners. \n \nScheduling \nWhile the full course contains 8 exercises, there is not a tight binding to doing one exercise per \nweek. Students should expect to spend about three hours per week on the course, however it is \nrecommended that you make a start on the next exercise if you have time in hand. \n \nDemonstrator help will be focused on the Tuesday and Friday 2-5pm slots during term the lab \nwould normally operate in, so you may find it helpful to work in these times as that will provide \nthe quickest response to questions. \n \nCollaboration \nWhen we have done these labs in person, we have often found they worked well when doing \nthem collaboratively. While your work must be your own, students can work together and help \neach other, and demonstrators contribute advice and experience with the tools. \n \nFor the timetabled sessions, which are Tuesdays and Fridays 2-5pm UK time during term, there \nwill be demonstrators available in the lab and at the same time we'll use the ECAD group on \nMicrosoft Teams. With this we can provide online audio/video help, screensharing and (for the \nWindows and possibly Mac Teams apps) optional remote control of your development \nenvironment. You will be added to the Team in week 1 - if you believe you have been missed \nplease post in the Moodle forum. In Teams there are a number of Helpdesk Channels (A-C) - if \nyou need help during lab times, post in Help Centre and a demonstrator can ask you to go to a \nspecific channel. There are also Student Breakout Channels (D-F) which are available for \nstudents to chat amongst themselves. \n \nFor help outside of timetabled sessions we've set up a Moodle forum where you can post \nquestions - we'll keep an eye on this at other times, but students are encouraged to use it to \nsupport each other. \n \nStudents are of course free to use other platforms to help each other. If you find anything useful \nthat we might use in future, please let us know! \n \nIf you are having difficulty accessing both Moodle and Teams, please email theo.markettos at \ncl.cam.ac.uk. \n \nFeedback \nWe revise the course each year, which may cause new bugs in the code or the notes. The \ncreators (Theo Markettos and Simon Moore) appreciate constructive feedback. \n\n \nCredits \nThis course was written by Theo Markettos and Simon Moore, with portions developed by \nRobert Eady, Jonas Fiala, Paul Fox, Vlad Gavrila, Brian Jones and Roy Spliet. We would like to \nthank Altera, Intel and Terasic for funding and other contributions.\n \n\nECONOMICS, LAW AND ETHICS \n \nAims \nThis course aims to give students an introduction to some basic concepts in economics, law and \nethics. \n \nLectures \nClassical economics and consumer theory. Prices and markets; Pareto efficiency; preferences; \nutility; supply and demand; the marginalist revolution; elasticity; the welfare theorems; \ntransaction costs. \nInformation economics. The discriminating monopolist; marginal costs; effects of technology on \nsupply and demand; competition and information; lock in; real and virtual networks; Metcalfe\u2019s \nlaw; the dominant firm model; price discrimination; bundling; income distribution. \nMarket failure and behavioural economics. Market failure: the business cycle; recession and \ntechnology; tragedy of the commons; externalities; monopoly rents; asymmetric information: the \nmarket for lemons; adverse selection; moral hazard; signalling. Behavioural economics: \nbounded rationality, heuristics and biases; nudge theory; the power of defaults; agency effects. \nAuction theory and game theory. Auction theory: types of auctions; strategic equivalence; the \nrevenue equivalence theorem; the winner\u2019s curse; problems with real auctions; mechanism \ndesign and the combinatorial auction; applicability of auction mechanisms in computer science; \nadvertising auctions. Game theory: the choice between cooperation and conflict; strategic \nforms; dominant strategy equilibrium; Nash equilibrium; the prisoners\u2019 dilemma; evolution of \nstrategies; stag hunt; volunteer\u2019s dilemma; chicken; iterated games; hawk-dove; application to \ncomputer science. \nPrinciples of law. Criminal and civil law; contract law; choice of law and jurisdiction; arbitration; \ntort; negligence; defamation; intellectual property rights. \nLaw and the Internet. Computer evidence; the General Data Protection Regulation; UK laws that \nspecifically affect the Internet; e-commerce regulations; privacy and electronic communications. \nPhilosophies of ethics. Authority, intuitionist, egoist and deontological theories; utilitarian and \nRawlsian models; morality; insights from evolutionary psychology, neurology, and experimental \nethics; professional codes of ethics; research ethics. \nContemporary ethical issues. The Internet and social policy; current debates on privacy, \nsurveillance, and censorship; responsible vulnerability disclosure; algorithmic bias; predictive \npolicing; gamification and engagement; targeted political advertising; environmental impacts. \nObjectives \nOn completion of this course, students should be able to: \n \nReflect on and discuss professional, economic, social, environmental, moral and ethical issues \nrelating to computer science \nDefine and explain economic and legal terminology and arguments \nApply the philosophies and theories covered to computer science problems and scenarios \nReflect on the main constraints that market, legislation and ethics place on firms dealing in \ninformation goods and services \nRecommended reading \n\n* Shapiro, C. & Varian, H. (1998). Information rules. Harvard Business School Press. \nHare, S. (2022). Technology is not neutral: A short guide to technology ethics. London \nPublishing Partnership. \n \nFurther reading: \nSmith, A. (1776). An inquiry into the nature and causes of the wealth of nations, available at    \nhttp://www.econlib.org/library/Smith/smWN.html \nThaler, R.H. (2016). Misbehaving. Penguin. \nGalbraith, J.K. (1991). A history of economics. Penguin. \nPoundstone, W. (1992). Prisoner\u2019s dilemma. Anchor Books. \nPinker, S (2011). The Better Angels of our Nature. Penguin. \nAnderson, R. (2008). Security engineering (Chapter 7). Wiley. \nVarian, H. (1999). Intermediate microeconomics - a modern approach. Norton. \nNuffield Council on Bioethics (2015) The collection, linking and use of data in biomedical \nresearch and health care.\n \n\nFURTHER GRAPHICS \n \nAims \nThe course introduces fundamental concepts of modern graphics pipelines. \n \nLectures \nThe order and content of lectures is provisional and subject to change. \n \nGeometry Representations. Parametric surfaces, implicit surfaces, meshes, point-set surfaces, \ngeometry processing pipeline. [1 lecture] \nDiscrete Differential Geometry. Surface normal and curvature, Laplace-Beltrami operator, heat \ndiffusion. [1 lecture] \nGeometry Processing.  Parametrization, filtering, 3D capture. [1 lecture] \nAnimation I. Animation types, animation pipeline, rigging/skinning, character animation. [1 \nlecture] \nAnimation II. Blending transformations, pose-space animation, controllers. [1 lecture] \nThe Rendering Equation. Radiosity, reflection models, BRDFs, local vs. global illumination. [1 \nlecture] \nDistributed Ray Tracing. Quadrature, importance sampling, recursive ray tracing. [1 lecture] \nInverse Rendering. Differentiable rendering, inverse problems. [1 lecture] \nObjectives \nOn completing the course, students should be able to \n \nunderstand and use fundamental 3D geometry/scene representations and operations; \nlearn animation techniques and controls; \nlearn how light transport is modeled and simulated in rendering; \nunderstand how graphics and rendering can be used to solve computer perception problems. \nRecommended reading \nStudents should expect to refer to one or more of these books, but should not find it necessary \nto purchase any of them. \n \nShirley, P. and Marschner, S. (2009). Fundamentals of Computer Graphics. CRC Press (3rd \ned.). \nWatt, A. (2000). 3D Computer Graphics. Addison-Wesley (3rd ed). \nHughes, van Dam, McGuire, Skalar, Foley, Feiner and Akeley (2013). Computer Graphics: \nPrinciples and Practice. Addison-Wesley (3rd edition) \nAkenine-M\u00f6ller, et. al. (2018). Real-time rendering. CRC Press (4th ed.).\n \n\nFURTHER JAVA \n \nAims \nThe goal of this course is to provide students with the ability to understand the advanced \nprogramming features available in the Java programming language, completing the coverage of \nthe language started in the Programming in Java course. The course is designed to \naccommodate students with diverse programming backgrounds; consequently Java is taught \nfrom first principles in a practical class setting where students can work at their own pace from a \ncourse handbook. \n \nObjectives \n \nAt the end of the course students should \n \nunderstand different mechanisms for communication between distributed applications and be \nable to evaluate their trade-offs; \nbe able to use Java generics and annotations to improve software usability, readability and \nsafety; \nunderstand and be able to exploit the Java class-loading mechansim; \nunderstand and be able to use concurrency control correctly; \nbe able to implement a vector clock algorithm and the happens-before relation. \nRecommended reading \n* Goetz, B. (2006). Java concurrency in practice. Addison-Wesley. \nGosling, J., Joy, B., Steele, G., Bracha, G. and Buckley, A. (2014). The Java language \nspecification, Java SE 8 Edition. Addison-Wesley. \nhttp://docs.oracle.com/javase/specs/jls/se8/html/\n \n\nGROUP PROJECTS \nExample Risk Report \nThe risk report should focus on the risks to your project succeeding on time (and not, say, on \ngeneral health and safety points). It is only 50 words. Example: \n \n\"Identified risks: \n * Training data requires access authorisation that has not been approved, and we don\u2019t know if \nclient can do this \n * Two group members have unreliable network connections and may miss meetings \n * One group member is in timezone UTC+8, and will have to attend meetings late at night \n * Bug reports on Github for a library we plan to use suggest maintenance updates will be \nnecessary for recent Android release\"\n \n\nINTRODUCTION TO COMPUTER ARCHITECTURE \n \nAims \nThe aims of this course are to introduce a hardware description language (SystemVerilog) and \ncomputer architecture concepts in order to design computer systems. The parallel ECAD+Arch \npractical classes will allow students to apply the concepts taught in lectures. \n \nLectures \nPart 1 - Gates to processors  \n \nTechnology trends and design challenges. Current technology, technology trends, ECAD trends, \nchallenges. [1 lecture] \nDigital system design. Practicalities of mapping SystemVerilog descriptions of hardware \n(including a processor) onto an FPGA board. Tips and pitfalls when generating larger modular \ndesigns. [1 lecture] \nEight great ideas in computer architecture. [1 lecture] \nReduced instruction set computers and RISC-V. Introduction to the RISC-V processor design. [1 \nlecture] \nExecutable and synthesisable models. [1 lecture] \nPipelining. [2 lectures] \nMemory hierarchy and caching. Caching, etc. [1 lecture] \nSupport for operating systems. Memory protection, exceptions, interrupts, etc. [1 lecture] \nOther instruction set architectures. CISC, stack, accumulator. [1 lecture] \nPart 2  \n \nOverview of Systems-on-Chip (SoCs) and DRAM. [1 lecture] High-level SoCs, DRAM storage \nand accessing. \nMulticore Processors. [2 lectures] Communication, cache coherence, barriers and \nsynchronisation primitives. \nGraphics processing units (GPUs) [2 lectures] Basic GPU architecture and programming. \nFuture Directions [1 lecture] Where is computer architecture heading? \nObjectives \nAt the end of the course students should \n \nbe able to read assembler given a guide to the instruction set and be able to write short pieces \nof assembler if given an instruction set or asked to invent an instruction set; \nunderstand the differences between RISC and CISC assembler; \nunderstand what facilities a processor provides to support operating systems, from memory \nmanagement to software interrupts; \nunderstand memory hierarchy including different cache structures and coherency needed for \nmulticore systems; \nunderstand how to implement a processor in SystemVerilog; \nappreciate the use of pipelining in processor design; \nhave an appreciation of control structures used in processor design; \n\nhave an appreciation of system-on-chips and their components; \nunderstand how DRAM stores data; \nunderstand how multicore processors communicate; \nunderstand how GPUs work and have an appreciation of how to program them. \nRecommended reading \n* Patterson, D.A. and Hennessy, J.L. (2017). Computer organization and design: The \nhardware/software interface RISC-V edition. Morgan Kaufmann. ISBN 978-0-12-812275-4. \n \nRecommended further reading: \n \nHarris, D.M. and Harris, S.L. (2012). Digital design and computer architecture. Morgan \nKaufmann. ISBN 978-0-12-394424-5. \nHennessy, J. and Patterson, D. (2006). Computer architecture: a quantitative approach. Elsevier \n(4th ed.). ISBN 978-0-12-370490-0. (Older versions of the book are also still generally relevant.) \n \nPointers to sources of more specialist information are included in the lecture notes and on the \nassociated course web page\n \n\nPROGRAMMING IN C AND C++ \n \nAims \nThis course aims \n \nto provide a solid introduction to programming in C and to provide an overview of the principles \nand constraints that affect the way in which the C programming language has been designed \nand is used; \nto introduce the key additional features of C++. \n  \n \nLectures \nIntroduction to the C language. Background and goals of C. Types, expressions, control flow \nand strings. \n(continued) Functions. Multiple compilation units. Scope. Segments. Incremental compilation. \nPreprocessor. \n(continued) Pointers and pointer arithmetic. Function pointers. Structures and Unions. \n(continued) Const and volatile qualifiers. Typedefs. Standard input/output. Heap allocation. \nMiscellaneous Features, Hints and Tips. \nC semantics and tools. Undefined vs implementation-defined behaviour. Buffer and integer \noverflows. ASan, MSan, UBsan, Valgrind checkers. \nMemory allocation, data structures and aliasing. Malloc/free, tree and DAG examples and their \ndeallocation using a memory arena. \nFurther memory management. Reference Counting and Garbage Collection \nMemory hierarchy and cache optimization. Data structure layouts. Intrusive lists. Array-of-structs \nvs struct-of-arrays representations. Loop blocking. \nDebugging. Using print statements, assertions and an interactive debugger. Unit testing and \nregression. \nIntroduction to C++. Goals of C++. Differences between C and C++. References versus \npointers. Overloaded functions. \nObjects in C++. Classes and structs. Destructors. Resource Acquisition is Initialisation. Operator \noverloading. Virtual functions. Casts. Multiple inheritance. Virtual base classes. \nOther C++ concepts. Templates and meta-programming. \nObjectives \nAt the end of the course students should \n \nbe able to read and write C programs; \nunderstand the interaction between C programs and the host operating system; \nbe familiar with the structure of C program execution in machine memory; \nunderstand the potential dangers of writing programs in C; \nunderstand the object model and main additional features of C++. \nRecommended reading \n* Kernighan, B.W. and Ritchie, D.M. (1988). The C programming language. Prentice Hall (2nd \ned.).\n \n\nSEMANTICS OF PROGRAMMING LANGUAGES \n \nAims \nThe aim of this course is to introduce the structural, operational approach to programming \nlanguage semantics. It will show how to specify the meaning of typical programming language \nconstructs, in the context of language design, and how to reason formally about semantic \nproperties of programs. \n \nLectures \nIntroduction. Transition systems. The idea of structural operational semantics. Transition \nsemantics of a simple imperative language. Language design options. [2 lectures] \nTypes. Introduction to formal type systems. Typing for the simple imperative language. \nStatements of desirable properties. [2 lectures] \nInduction. Review of mathematical induction. Abstract syntax trees and structural induction. \nRule-based inductive definitions and proofs. Proofs of type safety properties. [2 lectures] \nFunctions. Call-by-name and call-by-value function application, semantics and typing. Local \nrecursive definitions. [2 lectures] \nData. Semantics and typing for products, sums, records, references. [1 lecture] \nSubtyping. Record subtyping and simple object encoding. [1 lecture] \nSemantic equivalence. Semantic equivalence of phrases in a simple imperative language, \nincluding the congruence property. Examples of equivalence and non-equivalence. [1 lecture] \nConcurrency. Shared variable interleaving. Semantics for simple mutexes; a serializability \nproperty. [1 lecture] \nObjectives \nAt the end of the course students should \n \nbe familiar with rule-based presentations of the operational semantics and type systems for \nsome simple imperative, functional and interactive program constructs; \nbe able to prove properties of an operational semantics using various forms of induction \n(mathematical, structural, and rule-based); \nbe familiar with some operationally-based notions of semantic equivalence of program phrases \nand their basic properties. \nRecommended reading \n* Pierce, B.C. (2002). Types and programming languages. MIT Press. \nHennessy, M. (1990). The semantics of programming languages. Wiley. Out of print, but \navailable on the web at \nhttp://www.cs.tcd.ie/matthew.hennessy/splexternal2015/resources/sembookWiley.pdf \nWinskel, G. (1993). The formal semantics of programming languages. MIT Press.\n \n\nUNIX TOOLS \n \nAims \nThis video-lecture course gives students who have already basic Unix/Linux experience some \nadditional practical software-engineering knowledge: how to use the shell and related tools as \nan efficient working environment, how to automate routine tasks, and how version-control and \nautomated-build tools can help to avoid confusion and accidents, especially when working in \nteams. These are essential skills, both in industrial software development and student projects. \n \nLectures \nUnix concepts. Brief review of Unix history and design philosophy, documentation, terminals, \ninter-process communication mechanisms and conventions, shell, command-line arguments, \nenvironment variables, file descriptors. \nShell concepts. Program invocation, redirection, pipes, file-system navigation, argument \nexpansion, quoting, job control, signals, process groups, variables, locale, history and alias \nfunctions, security considerations. \nScripting. Plain-text formats, executables, #!, shell control structures and functions. Startup \nscripts. \nText, file and networking tools. sed, grep, chmod, find, ssh, rsync, tar, zip, etc. \nVersion control. diff, patch, RCS, Subversion, git. \nSoftware development tools. C compiler, linker, debugger, make. \nPerl. Introduction to a powerful scripting and text-manipulation language. [2 lectures] \nObjectives \nAt the end of the course students should \n \nbe confident in performing routine user tasks on a POSIX system, understand command-line \nuser-interface conventions and know how to find more detailed documentation; \nappreciate how simple tools can be combined to perform a large variety of tasks; \nbe familiar with the most common tools, file formats and configuration practices; \nbe able to understand, write, and maintain shell scripts and makefiles; \nappreciate how using a version-control system and fully automated build processes help to \nmaintain reproducibility and audit trails during software development; \nknow enough about basic development tools to be able to install, modify and debug C source \ncode; \nhave understood the main concepts of, and gained initial experience in, writing Perl scripts \n(excluding the facilities for object-oriented programming). \nRecommended reading \nRobbins, A. (2005). Unix in a nutshell. O\u2019Reilly (4th ed.). \nSchwartz, R.L., Foy, B.D. and Phoenix, T. (2011). Learning Perl. O\u2019Reilly (6th ed.).\n \n\n",
        "4th Semester \n \nCOMPILER CONSTRUCTION \n \nAims \nThis course aims to cover the main concepts associated with implementing compilers for \nprogramming languages. We use a running example called SLANG (a Small LANGuage) \ninspired by the languages described in 1B Semantics. A toy compiler (written in ML) is provided, \nand students are encouraged to extend it in various ways. \n \nLectures \nOverview of compiler structure The spectrum of interpreters and compilers; compile-time and \nrun-time. Compilation as a sequence of translations from higher-level to lower-level intermediate \nlanguages, where each translation preserves semantics. The structure of a simple compiler: \nlexical analysis and syntax analysis, type checking, intermediate representations, optimisations, \ncode generation. Overview of run-time data structures: stack and heap. Virtual machines. [1 \nlecture] \nLexical analysis and syntax analysis. Lexical analysis based on regular expressions and finite \nstate automata. Using LEX-tools. How does LEX work? Parsing based on context-free \ngrammars and push-down automata. Grammar ambiguity, left- and right-associativity and \noperator precedence. Using YACC-like tools. How does YACC work? LL(k) and LR(k) parsing \ntheory. [3 lectures] \nCompiler Correctness Recursive functions can be transformed into iterative functions using the \nContinuation-Passing Style (CPS) transformation. CPS applied to a (recursive) SLANG \ninterpreter to derive, in a step-by-step manner, a correct stack-based compiler. [3 lectures] \nData structures, procedures/functions Representing tuples, arrays, references. Procedures and \nfunctions: calling conventions, nested structure, non-local variables. Functions as first-class \nvalues represented as closures. Simple optimisations: inline expansion, constant folding, \nelimination of tail recursion, peephole optimisation. [5 lectures] \nAdvanced topics Run-time memory management (garbage collection). Static and dynamic \nlinking. Objects and inheritance; implementation of method dispatch. Try-catch exception \nmechanisms. Compiling a compiler via bootstrapping. [4 lectures] \nObjectives \nAt the end of the course students should understand the overall structure of a compiler, and will \nknow significant details of a number of important techniques commonly used. They will be \naware of the way in which language features raise challenges for compiler builders. \n \nRecommended reading \n* Aho, A.V., Sethi, R. and Ullman, J.D. (2007). Compilers: principles, techniques and tools. \nAddison-Wesley (2nd ed.). \nMogensen, T. \u00c6. (2011). Introduction to compiler design. Springer. http://www.diku.dk/ \ntorbenm/Basics.\n \n\nCOMPUTATION THEORY \n \nAims \nThe aim of this course is to introduce several apparently different formalisations of the informal \nnotion of algorithm; to show that they are equivalent; and to use them to demonstrate that there \nare uncomputable functions and algorithmically undecidable problems. \n \nLectures \nIntroduction: algorithmically undecidable problems. Decision problems. The informal notion of \nalgorithm, or effective procedure. Examples of algorithmically undecidable problems. [1 lecture] \nRegister machines. Definition and examples; graphical notation. Register machine computable \nfunctions. Doing arithmetic with register machines. [1 lecture] \nUniversal register machine. Natural number encoding of pairs and lists. Coding register machine \nprograms as numbers. Specification and implementation of a universal register machine. [2 \nlectures] \nUndecidability of the halting problem. Statement and proof. Example of an uncomputable partial \nfunction. Decidable sets of numbers; examples of undecidable sets of numbers. [1 lecture] \nTuring machines. Informal description. Definition and examples. Turing computable functions. \nEquivalence of register machine computability and Turing computability. The Church-Turing \nThesis. [2 lectures] \nPrimitive and partial recursive functions. Definition and examples. Existence of a recursive, but \nnot primitive recursive function. A partial function is partial recursive if and only if it is \ncomputable. [2 lectures] \nLambda-Calculus. Alpha and beta conversion. Normalization. Encoding data. Writing recursive \nfunctions in the lambda-calculus. The relationship between computable functions and \nlambda-definable functions. [3 lectures] \nObjectives \nAt the end of the course students should \n \nbe familiar with the register machine, Turing machine and lambda-calculus models of \ncomputability; \nunderstand the notion of coding programs as data, and of a universal machine; \nbe able to use diagonalisation to prove the undecidability of the Halting Problem; \nunderstand the mathematical notion of partial recursive function and its relationship to \ncomputability. \nRecommended reading \n* Hopcroft, J.E., Motwani, R. and Ullman, J.D. (2001). Introduction to automata theory, \nlanguages, and computation. Addison-Wesley (2nd ed.). \n* Hindley, J.R. and Seldin, J.P. (2008). Lambda-calculus and combinators, an introduction. \nCambridge University Press (2nd ed.). \nCutland, N.J. (1980). Computability: an introduction to recursive function theory. Cambridge \nUniversity Press. \nDavis, M.D., Sigal, R. and Weyuker, E.J. (1994). Computability, complexity and languages. \nAcademic Press (2nd ed.). \n\nSudkamp, T.A. (2005). Languages and machines. Addison-Wesley (3rd ed.).\n \n\nCOMPUTER NETWORKING \n \nAims \nThe aim of this course is to introduce key concepts and principles of computer networks. The \ncourse will use a top-down approach to study the Internet and its protocol stack. Instances of \narchitecture, protocol, application-examples will include email, web and media-streaming. We \nwill cover communications services (e.g., TCP/IP) required to support such network \napplications. The implementation and deployment of communications services in practical \nnetworks: including wired and wireless LAN environments, will be followed by a discussion of \nissues of network-management. Throughout the course, the Internet\u2019s architecture and \nprotocols will be used as the primary examples to illustrate the fundamental principles of \ncomputer networking. \n \nLectures \nIntroduction. Overview of networking using the Internet as an example. LANs and WANs. OSI \nreference model, Internet TCP/IP Protocol Stack. Circuit-switching, packet-switching, Internet \nstructure, networking delays and packet loss. [3 lectures] \nLink layer and local area networks. Link layer services, error detection and correction, Multiple \nAccess Protocols, link layer addressing, Ethernet, hubs and switches, Point-to-Point Protocol. [2 \nlectures] \nWireless and mobile networks. Wireless links and network characteristics, Wi-Fi: IEEE 802.11 \nwireless LANs. [1 lecture] \nNetwork layer addressing. Network layer services, IP, IP addressing, IPv4, DHCP, NAT, ICMP, \nIPv6. [3 lectures] \nNetwork layer routing. Routing and forwarding, routing algorithms, routing in the Internet, \nmulticast. [3 lectures] \nTransport layer. Service models, multiplexing/demultiplexing, connection-less transport (UDP), \nprinciples of reliable data transfer, connection-oriented transport (TCP), TCP congestion control, \nTCP variants. [6 lectures] \nApplication layer. Client/server paradigm, WWW, HTTP, Domain Name System, P2P. [1.5 \nlectures] \nMultimedia networking. Networked multimedia applications, multimedia delivery requirements, \nmultimedia protocols (SIP), content distribution networks. [0.5 lecture] \nObjectives \nAt the end of the course students should \n \nbe able to analyse a communication system by separating out the different functions provided \nby the network; \nunderstand that there are fundamental limits to any communications system; \nunderstand the general principles behind multiplexing, addressing, routing, reliable transmission \nand other stateful protocols as well as specific examples of each; \nunderstand what FEC is; \nbe able to compare communications systems in how they solve similar problems; \n\nhave an informed view of both the internal workings of the Internet and of a number of common \nInternet applications and protocols. \nRecommended reading \n* Peterson, L.L. and Davie, B.S. (2011). Computer networks: a systems approach. Morgan \nKaufmann (5th ed.). ISBN 9780123850591 \nKurose, J.F. and Ross, K.W. (2009). Computer networking: a top-down approach. \nAddison-Wesley (5th ed.). \nComer, D. and Stevens, D. (2005). Internetworking with TCP-IP, vol. 1 and 2. Prentice Hall (5th \ned.). \nStevens, W.R., Fenner, B. and Rudoff, A.M. (2003). UNIX network programming, Vol.I: The \nsockets networking API. Prentice Hall (3rd ed.).\n \n\nFURTHER HUMAN\u2013COMPUTER INTERACTION \n \nAims \nThis aim of this course is to provide an introduction to the theoretical foundations of Human \nComputer Interaction, and an understanding of how these can be applied to the design of \ncomplex technologies. \n \nLectures \nTheory driven approaches to HCI. What is a theory in HCI? Why take a theory driven approach \nto HCI? \nDesign of visual displays. Segmentation and variables of the display plane. Modes of \ncorrespondence. \nGoal-oriented interaction. Using cognitive theories of planning, learning and understanding to \nunderstand user behaviour, and what they find hard. \nDesigning smart systems. Using statistical methods to anticipate user needs and actions with \nBayesian strategies. \nDesigning efficient systems. Measuring and optimising human performance through quantitative \nexperimental methods. \nDesigning meaningful systems. Qualitative research methods to understand social context and \nrequirements of user experience. \nEvaluating interactive system designs. Approaches to evaluation in systems research and \nengineering, including Part II Projects. \nDesigning complex systems. Worked case studies of applying the theories to a hard HCI \nproblem. Research directions in HCI. \nObjectives \nAt the end of the course students should be able to apply theories of human performance and \ncognition to system design, including selection of appropriate techniques to analyse, observe \nand improve the usability of a wide range of technologies. \n \nRecommended reading \n* Preece, J., Sharp, H. and Rogers, Y. (2015). Interaction design: beyond human-computer \ninteraction. Wiley (Currently in 4th edition, but earlier editions will suffice). \n \nFurther reading: \n \nCarroll, J.M. (ed.) (2003). HCI models, theories and frameworks: toward a multi-disciplinary \nscience. Morgan Kaufmann.\n \n\nLOGIC AND PROOF \n \nAims \nThis course will teach logic, especially the predicate calculus. It will present the basic principles \nand definitions, then describe a variety of different formalisms and algorithms that can be used \nto solve problems in logic. Putting logic into the context of Computer Science, the course will \nshow how the programming language Prolog arises from the automatic proof method known as \nresolution. It will introduce topics that are important in mechanical verification, such as binary \ndecision diagrams (BDDs), SAT solvers and modal logic. \n \nLectures \nIntroduction to logic. Schematic statements. Interpretations and validity. Logical consequence. \nInference. \nPropositional logic. Basic syntax and semantics. Equivalences. Normal forms. Tautology \nchecking using CNF. \nThe sequent calculus. A simple (Hilbert-style) proof system. Natural deduction systems. \nSequent calculus rules. Sample proofs. \nFirst order logic. Basic syntax. Quantifiers. Semantics (truth definition). \nFormal reasoning in FOL. Free versus bound variables. Substitution. Equivalences for \nquantifiers. Sequent calculus rules. Examples. \nClausal proof methods. Clause form. A SAT-solving procedure. The resolution rule. Examples. \nRefinements. \nSkolem functions, Unification and Herbrand\u2019s theorem. Prenex normal form. Skolemisation. \nMost general unifiers. A unification algorithm. Herbrand models and their properties. \nResolution theorem-proving and Prolog. Binary resolution. Factorisation. Example of Prolog \nexecution. Proof by model elimination. \nSatisfiability Modulo Theories. Decision problems and procedures. How SMT solvers work. \nBinary decision diagrams. General concepts. Fast canonical form algorithm. Optimisations. \nApplications. \nModal logics. Possible worlds semantics. Truth and validity. A Hilbert-style proof system. \nSequent calculus rules. \nTableaux methods. Simplifying the sequent calculus. Examples. Adding unification. \nSkolemisation. The world\u2019s smallest theorem prover? \nObjectives \nAt the end of the course students should \n \nbe able to manipulate logical formulas accurately; \nbe able to perform proofs using the presented formal calculi; \nbe able to construct a small BDD; \nunderstand the relationships among the various calculi, e.g. SAT solving, resolution and Prolog; \nunderstand the concept of a decision procedure and the basic principles of \u201csatisfiability modulo \ntheories\u201d. \nbe able to apply the unification algorithm and to describe its uses. \nRecommended reading \n\n* Huth, M. and Ryan, M. (2004). Logic in computer science: modelling and reasoning about \nsystems. Cambridge University Press (2nd ed.). \nBen-Ari, M. (2001). Mathematical logic for computer science. Springer (2nd ed.).\n \n\nPROLOG \n \nAims \nThe aim of this course is to introduce programming in the Prolog language. Prolog encourages \na different programming style to Java or ML and particular focus is placed on programming to \nsolve real problems that are suited to this style. Practical experimentation with the language is \nstrongly encouraged. \n \nLectures \nIntroduction to Prolog. The structure of a Prolog program and how to use the Prolog interpreter. \nUnification. Some simple programs. \nArithmetic and lists. Prolog\u2019s support for evaluating arithmetic expressions and lists. The space \ncomplexity of program evaluation discussed with reference to last-call optimisation. \nBacktracking, cut, and negation. The cut operator for controlling backtracking. Negation as \nfailure and its uses. \nSearch and cut. Prolog\u2019s search method for solving problems. Graph searching exploiting \nProlog\u2019s built-in search mechanisms. \nDifference structures. Difference lists: introduction and application to example programs. \nBuilding on Prolog. How particular limitations of Prolog programs can be addressed by \ntechniques such as Constraint Logic Programming (CLP) and tabled resolution. \nObjectives \nAt the end of the course students should \n \nbe able to write programs in Prolog using techniques such as accumulators and difference \nstructures; \nknow how to model the backtracking behaviour of program execution; \nappreciate the unique perspective Prolog gives to problem solving and algorithm design; \nunderstand how larger programs can be created using the basic programming techniques used \nin this course. \nRecommended reading \n* Bratko, I. (2001). PROLOG programming for artificial intelligence. Addison-Wesley (3rd or 4th \ned.). \nSterling, L. and Shapiro, E. (1994). The art of Prolog. MIT Press (2nd ed.). \n \nFurther reading: \n \nO\u2019Keefe, R. (1990). The craft of Prolog. MIT Press. [This book is beyond the scope of this \ncourse, but it is very instructive. If you understand its contents, you\u2019re more than prepared for \nthe examination.]\n \n\nARTIFICIAL INTELLIGENCE \n \nPrerequisites: Algorithms 1, Algorithms 2. In addition the course requires some mathematics, in \nparticular some use of vectors and some calculus. Part IA Natural Sciences Mathematics or \nequivalent and Discrete Mathematics are likely to be helpful although not essential. Similarly, \nelements of Machine Learning and Real World Data, Foundations of Data Science, Logic and \nProof, Prolog and Complexity Theory are likely to be useful. This course is a prerequisite for the \nPart II courses Machine Learning and Bayesian Inference and Natural Language Processing. \nThis course is a prerequisite for: Natural Language Processing \nExam: Paper 7 Question 1, 2 \nPast exam questions, Moodle, timetable \n \nAims \nThe aim of this course is to provide an introduction to some fundamental issues and algorithms \nin artificial intelligence (AI). The course approaches AI from an algorithmic, computer \nscience-centric perspective; relatively little reference is made to the complementary \nperspectives developed within psychology, neuroscience or elsewhere. The course aims to \nprovide some fundamental tools and algorithms required to produce AI systems able to exhibit \nlimited human-like abilities, particularly in the form of problem solving by search, game-playing, \nrepresenting and reasoning with knowledge, planning, and learning. \n \nLectures \nIntroduction. Alternate ways of thinking about AI. Agents as a unifying view of AI systems. [1 \nlecture] \nSearch I. Search as a fundamental paradigm for intelligent problem-solving. Simple, uninformed \nsearch algorithms. Tree search and graph search. [1 lecture] \nSearch II. More sophisticated heuristic search algorithms. The A* algorithm and its properties. \nImproving memory efficiency: the IDA* and recursive best first search algorithms. Local search \nand gradient descent. [1 lecture] \nGame-playing. Search in an adversarial environment. The minimax algorithm and its \nshortcomings. Improving minimax using alpha-beta pruning. [1 lecture] \nConstraint satisfaction problems (CSPs). Standardising search problems to a common format. \nThe backtracking algorithm for CSPs. Heuristics for improving the search for a solution. Forward \nchecking, constraint propagation and arc consistency. [1 lecture] \nBackjumping in CSPs. Backtracking, backjumping using Gaschnig\u2019s algorithm, graph-based \nbackjumping. [1 lecture] \nKnowledge representation and reasoning I. How can we represent and deal with commonsense \nknowledge and other forms of knowledge? Semantic networks, frames and rules. How can we \nuse inference in conjunction with a knowledge representation scheme to perform reasoning \nabout the world and thereby to solve problems? Inheritance, forward and backward chaining. [1 \nlecture] \nKnowledge representation and reasoning II. Knowledge representation and reasoning using first \norder logic. The frame, qualification and ramification problems. The situation calculus. [1 lecture] \n\nPlanning I. Methods for planning in advance how to solve a problem. The STRIPS language. \nAchieving preconditions, backtracking and fixing threats by promotion or demotion: the \npartial-order planning algorithm. [1 lecture] \nPlanning II. Incorporating heuristics into partial-order planning. Planning graphs. The \nGRAPHPLAN algorithm. Planning using propositional logic. Planning as a constraint satisfaction \nproblem. [1 lecture] \nNeural Networks I. A brief introduction to supervised learning from examples. Learning as fitting \na curve to data. The perceptron. Learning by gradient descent. [1 lecture] \nNeural Networks II. Multilayer perceptrons and the backpropagation algorithm. [1 lecture] \nObjectives \nAt the end of the course students should: \n \nappreciate the distinction between the popular view of the field and the actual research results; \nappreciate the fact that the computational complexity of most AI problems requires us regularly \nto deal with approximate techniques; \nbe able to design basic problem solving methods based on AI-based search, knowledge \nrepresentation, reasoning, planning, and learning algorithms. \nRecommended reading \nThe recommended text is: \n \n* Russell, S. and Norvig, P. (2010). Artificial intelligence: a modern approach. Prentice Hall (3rd \ned.). \n \nThere are many good books available on artificial intelligence; one alternative is: \n \nPoole, D. L. and Mackworth, A. K. (2017). Artificial intelligence: foundations of computational \nagents. Cambridge University Press (2nd ed.). \n \nFor some of the material you might find it useful to consult more specialised texts, in particular: \n \nDechter, R. (2003). Constraint processing. Morgan Kaufmann. \n \nCawsey, A. (1998). The essence of artificial intelligence. Prentice Hall. \n \nGhallab, M., Nau, D. and Traverso, P. (2004). Automated planning: theory and practice. Morgan \nKaufmann. \n \nBishop, C.M. (2006). Pattern recognition and machine learning. Springer. \n \nBrachman, R.J and Levesque, H.J. (2004). Knowledge Representation and Reasoning. Morgan \nKaufmann.\n \n\nCOMPLEXITY THEORY \n \nAims \nThe aim of the course is to introduce the theory of computational complexity. The course will \nexplain measures of the complexity of problems and of algorithms, based on time and space \nused on abstract models. Important complexity classes will be defined, and the notion of \ncompleteness established through a thorough study of NP-completeness. Applications to \ncryptography will be considered. \n \nSyllabus \nIntroduction to Complexity Theory. Decidability and complexity; lower and upper bounds; the \nChurch-Turing hypothesis. \nTime complexity. Time complexity classes; polynomial time computation and the class P. \nNon-determinism. Non-deterministic machines; the class NP; the problem 3SAT. \nNP-completeness. Reductions and completeness; the Cook-Levin theorem; graph-theoretic \nproblems. \nMore NP-complete problems. 3-colourability; scheduling, matching, set covering, and knapsack.  \ncoNP. Complement classes. NP \u2229 coNP; primality and factorisation. \nCryptography. One-way functions; the class UP. \nSpace complexity. Space complexity classes; Savitch\u2019s theorem. \nHierarchy theorems. Time and space hierarchy theorems. \nRandomised algorithms. The class BPP; derandomisation; error-correcting codes, \ncommunication complexity. \nQuantum Complexity. The classes BQP and QMA. \nInteractive Proofs. IP=PSPACE; Zero Knowledge Proofs; Probabilistically Checkable Proofs \n(PCPs).  \n  \nObjectives \nAt the end of the course students should \n \nbe able to analyse practical problems and classify them according to their complexity; \nbe familiar with the phenomenon of NP-completeness, and be able to identify problems that are \nNP-complete; \nbe aware of a variety of complexity classes and their interrelationships; \nunderstand the role of complexity analysis in cryptography. \nRecommended reading \nA Survey of P vs. NP by Scott Aaronson. \nComputational Complexity: A Modern Approach by Arora and Barak \nComputational Complexity: A Conceptual Perspective by Oded Goldreich \nMathematics and Computation by Avi Wigderson\n \n\nCYBERSECURITY \n \nAims \nIn today\u2019s digital society, computer security is vital for commercial competitiveness, national \nsecurity and privacy of individuals. Protection against both criminal and state-sponsored attacks \nwill need a large cohort of skilled individuals with an understanding of the principles of security \nand with practical experience of the application of these principles. We want you to become one \nof them. In this adversarial game, to defeat the bad guys, the good guys have to be at least as \nskilled at them. Therefore this course has a strong foundation of becoming proficient at actual \nattacks. A theoretical understanding is of course necessary, but without some practice the bad \nguys will run circles around the good guys. In 12 hours I can\u2019t bring you from zero to the stage \nwhere you can invent new attacks and countermeasures, but at least by practicing and \ndissecting common attacks (akin to \u201cstudying the classics\u201d) I\u2019ll give you a feeling for the kind of \nadversarial thinking that is required in this field. Large portions of this course are hands-on: you \nwill need to acquire the relevant skills rather than just reading about it. The recommended \ncourse textbook will be especially helpful to those with no prior low-level hacking experience. \n \nLectures \nIntroduction. \nThe adversarial nature of security; thinking like the attacker; confidentiality, integrity and \navailability; systems-level thinking; Trusted Computing Base; role of cryptography. \n \nFundamentals of access control (chapter 1). \nDiscretionary vs mandatory access control; discretionary access control in POSIX; file \npermissions, file ownership, groups, permission bits. \n \nSoftware security (spanning 4 book chapters) \nSetuid (chapter 2): privileged programs, attack surfaces, exploitable vulnerabilities, privilege \nescalation from regular user to root. \nBuffer overflow (chapter 4): stack memory layout, exploiting a buffer overflow vulnerability, \nguessing unknown addresses, payload, countermeasures and counter-countermeasures. \nReturn to libc and return-oriented programming (chapter 5): exploiting a non-executable stack, \nchaining function calls, chaining ROP gadgets. \nSQL injection (chapter 14): vulnerability and its exploitation, countermeasures, input filtering, \nprepared statements. \n \nAuthentication. \nSomething you know, have, are; passwords, their dominance, their shortcomings and the many \nattempts at replacing them; password cracking; tokens; biometrics; taxonomy of Single Sign-On \nsystems; password managers. \n \nWeb and internet security (spanning 3 book chapters). \nCross-Site Request Forgery (chapter 12): why cross-site requests, CSRF attacks on HTTP GET \nand HTTP POST, countermeasures. \n\nCross-Site Scripting (chapter 13): non-persistent vs persistent XSS, javascript, self-propagating \nXSS worm, countermeasures; \n \nHuman factors. \nUsers are not the enemy; Compliance budget; Prospect theory; 7 principles for systems \nsecurity. \n \nAdditional topics. \nViruses, worms and quines; lockpicking; privilege escalation in physical locks; conclusions. \n \nTo complete the course successfully, students must acquire the practical ability to carry out (as \nopposed to just describing) the exploits mentioned in the syllabus, given a vulnerable system. \nThe low-level hands-on portions of the course are supported by the very helpful course \ntextbook. Those who choose not to study on the recommended textbook will be severely \ndisadvantaged. \n \nRecommended textbook: Wenliang Du. Computer Security: A Hands-on Approach. 3rd Edition. \nISBN: 978-17330039-5-7. Published: 1 May 2022. \n \nhttps://www.handsonsecurity.net. Some chapters freely available online. \n \n \nOptional additional books (not substitutes): \n \nRoss Anderson, Security Engineering 3rd Ed, Wiley, 2020. ISBN: 978-1-119-64278-7. \nhttps://www.cl.cam.ac.uk/~rja14/book.html. Some chapters freely available online. \n \nPaul van Oorschot, Computer Security and the Internet, Springer, 2020. ISBN: \n978-3-030-33648-6 (hardcopy), 978-3-030-33649-3 (eBook). \nhttps://people.scs.carleton.ca/~paulv/toolsjewels.html. All chapters freely available online.\n\nFORMAL MODELS OF LANGUAGE \n \nAims \nThis course studies formal models of language and considers how they might be relevant to the \nprocessing and acquisition of natural languages. The course will extend knowledge of formal \nlanguage theory; introduce several new grammars; and use concepts from information theory to \ndescribe natural language. \n \nLectures \nNatural language and the Chomsky hierarchy 1. Recap classes of language. Closure properties \nof language classes. Recap pumping lemma for regular languages. Discussion of relevance (or \nnot) to natural languages (example embedded clauses in English). \nNatural language and the Chomsky hierarchy 2. Pumping lemma for context free languages. \nDiscussion of relevance (or not) to natural languages (example Swiss-German cross serial \ndependancies). Properties of minimally context sensitive languages. Introduction to tree \nadjoining grammars. \nLanguage processing and context free grammar parsing 1. Recap of context free grammar \nparsing. Language processing predictions based on top down parsing models (example Yngve\u2019s \nlanguage processing predictions). Language processing predictions based on probabilistic \nparsing (example Halle\u2019s language processing predictions). \nLanguage processing and context free grammar parsing 2. Introduction to context free grammar \nequivalent dependancy grammars. Language processing predictions based on Shift-Reduce \nparsing (examples prosodic look-ahead parsers, Parsey McParseface). \nGrammar induction of language classes. Introduction to grammar induction. Discussion of \nrelevance (or not) to natural language acquisition. Gold\u2019s theorem. Introduction to context free \ngrammar equivalent categorial grammars and their learnable classes. \nNatural language and information theory 1. Entropy and natural language typology. Uniform \ninformation density as a predictor for language processing. \nNatural language and information theory 2. Noisy channel encoding as a model for spelling \nerror, translation and language processing. \nVector space models and word vectors. Introduction to word vectors (example Word2Vec). Word \nvectors as predictors for semantic language processing. \nObjectives \nAt the end of the course students should \n \nunderstand how known natural languages relate to formal languages in the Chomsky hierarchy; \nhave knowledge of several context free grammars equivalents; \nunderstand how we might make predictions about language processing and language \nacquisition from formal models; \nknow how to use information theoretic concepts to describe aspects of natural language. \nRecommended reading \n* Jurafsky, D. and Martin, J. (2008). Speech and language processing. Prentice Hall. \nManning, C.D. and Schutze, H. (1999) Foundations of statistical natural language processing. \nMIT Press. \n\nRuslan, M. (2003) The Oxford handbook of computational linguistics. Oxford University Press. \nClark, A., Fox, C. and Lappin, S. (2010) The handbook of computational linguistics and natural \nlanguage processing. Wiley-Blackwell. \nKozen, D. (1997) Automata and computibility. Springer.\n \n\n",
        "5th Semester \nBIOINFORMATICS \nAims \nThis course focuses on algorithms used in Bioinformatics and System Biology. Most of the \nalgorithms are general and can be applied in other fields on multidimensional and noisy data. All \nthe necessary biological terms and concepts useful for the course and the examination will be \ngiven in the lectures. The most important software implementing the described algorithms will be \ndemonstrated. \n \nLectures \nIntroduction to biological data: Bioinformatics as an interesting field in computer science. \nComputing and storing information with DNA (including Adleman\u2019s experiment). \nDynamic programming. Longest common subsequence, DNA global and local alignment, linear \nspace alignment, Nussinov algorithm for RNA, heuristics for multiple alignment. (Vol. 1, chapter \n5) \nSequence database search. Blast. (see notes and textbooks) \nGenome sequencing. De Bruijn graph. (Vol. 1, chapter 3) \nPhylogeny. Distance based algorithms (UPGMA, Neighbour-Joining). Parsimony-based \nalgorithms. Examples in Computer Science. (Vol. 2, chapter 7) \nClustering. Hard and soft K-means clustering, use of Expectation Maximization in clustering, \nHierarchical clustering, Markov clustering algorithm. (Vol. 2, chapter 8) \nGenomics Pattern Matching. Suffix Tree String Compression and the Burrows-Wheeler \nTransform. (Vol. 2, chapter 9) \nHidden Markov Models. The Viterbi algorithm, profile HMMs for sequence alignment, classifying \nproteins with profile HMMs, soft decoding problem, Baum-Welch learning. (Vol. 2, chapter 10) \nObjectives \nAt the end of this course students should \n \nunderstand Bioinformatics terminology; \nhave mastered the most important algorithms in the field; \nbe able to work with bioinformaticians and biologists; \nbe able to find data and literature in repositories. \nRecommended reading \n* Compeau, P. and Pevzner, P.A. (2015). Bioinformatics algorithms: an active learning approach. \nActive Learning Publishers. \nDurbin, R., Eddy, S., Krough, A. and Mitchison, G. (1998). Biological sequence analysis: \nprobabilistic models of proteins and nucleic acids. Cambridge University Press. \nJones, N.C. and Pevzner, P.A. (2004). An introduction to bioinformatics algorithms. MIT Press. \nFelsenstein, J. (2003). Inferring phylogenies. Sinauer Associates.\n \n\nBUSINESS STUDIES \n \nAims \nHow to start and run a computer company; the aims of this course are to introduce students to \nall the things that go to making a successful project or product other than just the programming. \nThe course will survey some of the issues that students are likely to encounter in the world of \ncommerce and that need to be considered when setting up a new computer company. \n \nSee also Business Seminars in the Easter Term. \n \nLectures \nSo you\u2019ve got an idea? Introduction. Why are you doing it and what is it? Types of company. \nMarket analysis. The business plan. \nMoney and tools for its management. Introduction to accounting: profit and loss, cash flow, \nbalance sheet, budgets. Sources of finance. Stocks and shares. Options and futures. \nSetting up: legal aspects. Company formation. Brief introduction to business law; duties of \ndirectors. Shares, stock options, profit share schemes and the like. Intellectual Property Rights, \npatents, trademarks and copyright. Company culture and management theory. \nPeople. Motivating factors. Groups and teams. Ego. Hiring and firing: employment law. \nInterviews. Meeting techniques. \nProject planning and management. Role of a manager. PERT and GANTT charts, and critical \npath analysis. Estimation techniques. Monitoring. \nQuality, maintenance and documentation. Development cycle. Productization. Plan for quality. \nPlan for maintenance. Plan for documentation. \nMarketing and selling. Sales and marketing are different. Marketing; channels; marketing \ncommunications. Stages in selling. Control and commissions. \nGrowth and exit routes. New markets: horizontal and vertical expansion. Problems of growth; \nsecond system effects. Management structures. Communication. Exit routes: acquisition, \nfloatation, MBO or liquidation. Futures: some emerging ideas for new computer businesses. \nSummary. Conclusion: now you do it! \nObjectives \nAt the end of the course students should \n \nbe able to write and analyse a business plan; \nknow how to construct PERT and GANTT diagrams and perform critical path analysis; \nappreciate the differences between profitability and cash flow, and have some notion of budget \nestimation; \nhave an outline view of company formation, share structure, capital raising, growth and exit \nroutes; \nhave been introduced to concepts of team formation and management; \nknow about quality documentation and productization processes; \nunderstand the rudiments of marketing and the sales process. \nRecommended reading \n\nLang, J. (2001). The high-tech entrepreneur\u2019s handbook: how to start and run a high-tech \ncompany. FT.COM/Prentice Hall. \n \nStudents will be expected to be able to use Microsoft Excel and Microsoft Project. \n \nFor additional reading on a lecture-by-lecture basis, please see the course website. \n \nStudents are strongly recommended to enter the CU Entrepreneurs Business Ideas Competition \nhttp://www.cue.org.uk/\n \n\nDENOTATIONAL SEMANTICS \n \nAims \nThe aims of this course are to introduce domain theory and denotational semantics, and to \nshow how they provide a mathematical basis for reasoning about the behaviour of programming \nlanguages. \n \nLectures \nIntroduction.The denotational approach to the semantics of programming \nlanguages.Recursively defined objects as limits of successive approximations. \nLeast fixed points.Complete partial orders (cpos) and least elements.Continuous functions and \nleast fixed points. \nConstructions on domains.Flat domains.Product domains. Function domains. \nScott induction.Chain-closed and admissible subsets of cpos and domains.Scott\u2019s fixed-point \ninduction principle. \nPCF.The Scott-Plotkin language PCF.Evaluation. Contextual equivalence. \nDenotational semantics of PCF.Denotation of types and terms. Compositionality. Soundness \nwith respect to evaluation. [2 lectures]. \nRelating denotational and operational semantics.Formal approximation relation and its \nfundamental property.Computational adequacy of the PCF denotational semantics with respect \nto evaluation. Extensionality properties of contextual equivalence. [2 lectures]. \nFull abstraction.Failure of full abstraction for the domain model. PCF with parallel or. \nObjectives \nAt the end of the course students should \n \nbe familiar with basic domain theory: cpos, continuous functions, admissible subsets, least fixed \npoints, basic constructions on domains; \nbe able to give denotational semantics to simple programming languages with simple types; \nbe able to apply denotational semantics; in particular, to understand the use of least fixed points \nto model recursive programs and be able to reason about least fixed points and simple \nrecursive programs using fixed point induction; \nunderstand the issues concerning the relation between denotational and operational semantics, \nadequacy and full abstraction, especially with respect to the language PCF. \nRecommended reading \nWinskel, G. (1993). The formal semantics of programming languages: an introduction. MIT \nPress. \nGunter, C. (1992). Semantics of programming languages: structures and techniques. MIT Press. \nTennent, R. (1991). Semantics of programming languages. Prentice Hall.\n \n\nINFORMATION THEORY \n \nAims \nThis course introduces the principles and applications of information theory: how information is \nmeasured in terms of probability and various entropies, how these are used to calculate the \ncapacity of communication channels, with or without noise, and to measure how much random \nvariables reveal about each other. Coding schemes including error correcting codes are studied \nalong with data compression, spectral analysis, transforms, and wavelet coding. Applications of \ninformation theory are reviewed, from astrophysics to pattern recognition. \n \nLectures \nInformation, probability, uncertainty, and surprise. How concepts of randomness and uncertainty \nare related to information. How the metrics of information are grounded in the rules of \nprobability. Shannon Information. Weighing problems and other examples. \nEntropy of discrete variables. Definition and link to Shannon information. Joint entropy, Mutual \ninformation. Visual depictions of the relationships between entropy types. Why entropy gives \nfundamental measures of information content. \nSource coding theorem and data compression; prefix, variable-, and fixed-length codes. \nInformation rates; Asymptotic equipartition principle; Symbol codes; Huffman codes and the \nprefix property. Binary symmetric channels. Capacity of a noiseless discrete channel. Stream \ncodes. \nNoisy discrete channel coding. Joint distributions; mutual information; Conditional Entropy; \nError-correcting codes; Capacity of a discrete channel. Noisy channel coding theorem. \nEntropy of continuous variables. Differential entropy; Mutual information; Channel Capacity; \nGaussian channels. \nEntropy for comparing probability distributions and for machine learning. Relative entropy/KL \ndivergence; cross-entropy; use as loss function. \nApplications of information theory in other sciences.  \nObjectives \nAt the end of the course students should be able to \n \ncalculate the information content of a random variable from its probability distribution; \nrelate the joint, conditional, and marginal entropies of variables in terms of their coupled \nprobabilities; \ndefine channel capacities and properties using Shannon\u2019s Theorems; \nconstruct efficient codes for data on imperfect communication channels; \ngeneralize the discrete concepts to continuous signals on continuous channels; \nunderstand encoding and communication schemes in terms of the spectral properties of signals \nand channels; \ndescribe compression schemes, and efficient coding using wavelets and other representations \nfor data. \nRecommended reading \n  Mackay's Information Theory, inference and Learning Algorithms \n \n\n Stone's Information Theory: A Tutorial Introduction.\n \n\nLATEX AND JULIA \n \nAims \nIntroduction to two widely-used languages for typesetting dissertations and scientific \npublications, for prototyping numerical algorithms and to visualize results. \n \nLectures \nLATEX. Workflow example, syntax, typesetting conventions, non-ASCII characters, document \nstructure, packages, mathematical typesetting, graphics and figures, cross references, build \ntools. \nJulia. Tools for technical computing and visualization. The Array type and its operators, 2D/3D \nplotting, functions and methods, notebooks, packages, vectorized audio demonstration. \nObjectives \nStudents should be able to avoid the most common LATEX mistakes, to prototype simple image \nand signal-processing algorithms in Julia, and to visualize the results. \n \nRecommended reading \n* Lamport, L. (1994). LATEX \u2013 a documentation preparation system user\u2019s guide and reference \nmanual. Addison-Wesley (2nd ed.). \nMittelbach, F., et al. (2023). The LATEX companion. Addison-Wesley (3rd ed.).\n \n\nPRINCIPLES OF COMMUNICATIONS \n \nAims \nThis course aims to provide a detailed understanding of the underlying principles for how \ncommunications systems operate. Practical examples (from wired and wireless \ncommunications, the Internet, and other communications systems) are used to illustrate the \nprinciples. \n \nLectures \nIntroduction. Course overview. Abstraction, layering. Review of structure of real networks, links, \nend systems and switching systems. [1 lecture] \nRouting. Central versus Distributed Routing Policy Routing. Multicast Routing Circuit Routing [6 \nlectures] \nFlow control and resource optimisation. 1[Control theory] is a branch of engineering familiar to \npeople building dynamic machines. It can be applied to network traffic. Stemming the flood, at \nsource, sink, or in between? Optimisation as a model of networkand user. TCP in the wild. [3 \nlectures] \nPacket Scheduling. Design choices for scheduling and queue management algorithms for \npacket forwarding, and fairness. [2 lectures] \nThe big picture for managing traffic. Economics and policy are relevant to networks in many \nways. Optimisation and game theory are both relevant topics discussed here. [2 lectures] \nSystem Structures and Summary. Abstraction, layering. The structure of real networks, links, \nend systems and switching. [2 lectures] \n1 Control theory was not taught and will not be the subject of any exam question. \n \nObjectives \nAt the end of the course students should be able to explain the underlying design and behaviour \nof protocols and networks, including capacity, topology, control and use. Several specific \nmathematical approaches are covered (control theory, optimisation). \n \nRecommended reading \n* Keshav, S. (2012). Mathematical Foundations of Computer Networking. Addison Wesley. ISBN \n9780321792105 \nBackground reading: \nKeshav, S. (1997). An engineering approach to computer networking. Addison-Wesley (1st ed.). \nISBN 0201634422 \nStevens, W.R. (1994). TCP/IP illustrated, vol. 1: the protocols. Addison-Wesley (1st ed.). ISBN \n0201633469\n \n\nTYPES \n \nAims \nThe aim of this course is to show by example how type systems for programming languages can \nbe defined and their properties developed, using techniques that were introduced in the Part IB \ncourse on Semantics of Programming Languages. The emphasis is on type systems for \nfunctional languages and their connection to constructive logic. \n \nLectures \nIntroduction. The role of type systems in programming languages. Review of rule-based \nformalisation of type systems. [1 lecture] \nPropositions as types. The Curry-Howard correspondence between intuitionistic propositional \ncalculus and simply-typed lambda calculus. Inductive types and iteration. Consistency and \ntermination. [2 lectures] \nPolymorphic lambda calculus (PLC). PLC syntax and reduction semantics. Examples of \ndatatypes definable in the polymorphic lambda calculus. Type inference. [3 lectures] \nMonads and effects. Explicit versus implicit effects. Using monadic types to control effects. \nReferences and polymorphism. Recursion and looping. [2 lectures] \nContinuations and classical logic. First-class continuations and control operators. Continuations \nas Curry-Howard for classical logic. Continuation-passing style. [2 lectures] \nDependent types. Dependent function types. Indexed datatypes. Equality types and combining \nproofs with programming. [2 lectures] \nObjectives \nAt the end of the course students should \n \nbe able to use a rule-based specification of a type system to carry out type checking and type \ninference; \nunderstand by example the Curry-Howard correspondence between type systems and logics; \nunderstand how types can be used to control side-effects in programming; \nappreciate the expressive power of parametric polymorphism and dependent types. \nRecommended reading \n* Pierce, B.C. (2002). Types and programming languages. MIT Press. \nPierce, B. C. (Ed.) (2005). Advanced Topics in Types and Programming Languages. MIT Press. \nGirard, J-Y. (tr. Taylor, P. and Lafont, Y.) (1989). Proofs and types. Cambridge University Press.\n\nADVANCED DATA SCIENCE \n \nPracticals \nThe module will have a practical session for each of the three stages of the pipeline. Each of the \nparts will be tied into an aspect of the final project. \n \nObjectives \nAt the end of the course students will be familiar with the purpose of data science, how it differs \nfrom the closely related fields of machine learning, statistics and artificial intelligence and what a \ntypical data analysis pipeline looks like in practice. As well as emphasising the importance of \nanalysis methods we will introduce a formalism for organising how data science is done in \npractice and what the different aspects the data scientist faces when giving data-driven answers \nto questions of interest. \n \nRecommended reading \nSimon Rogers et al. (2016). A First Course in Machine Learning, Second Edition. [] Chapman \nand Hall/CRC, nil \nShai Shalev-Shwartz et al. (2014). Understanding Machine Learning: From Theory to \nAlgorithms. New York, NY, USA: Cambridge  University Press \nChristopher M. Bishop (2006). Pattern Recognition and Machine Learning (Information Science \nand Statistics). Secaucus, NJ, USA: Springer-Verlag New York, Inc. \nAssessment \nPractical There will be four practicals contributing 20% to the final module mark. Data science is \na topic where there is rarely a single \ncorrect answer therefore the important thing is that an informed attempt has been made to \naddress the questions that can be supported and motivated. \nReport The course will be concluded by an individual report covering the material in the course \nthrough \npractical exercise working with real data. This report will make up 80% of the mark for the \ncourse. \n \n \n\nAFFECTIVE ARTIFICIAL INTELLIGENCE \n \nSynopsis \nAffective Artificial Intelligence (Affective AI) aims to imbue machines with social and emotional \nintelligence (EQ). More specifically, Affective AI aims to create artificially intelligent systems and \nmachines that can recognize, interpret, process, and simulate human social signals and \nbehaviours, expressions, and emotions, to enhance human-AI interaction and communication. \n\\r\\n\\r\\nTo achieve this goal, Affective AI draws upon various scientific disciplines, including \nmachine learning, computer vision, speech / natural language / signal processing, psychology \nand cognitive science, and ethics and social sciences. \n \n  \nBackground & Aims \nAffective Artificial Intelligence (Affective AI) aims to imbue machines with social and emotional \nintelligence (EQ). More specifically, Affective AI aims to create artificially intelligent systems and \nmachines that can recognize, interpret, process, and simulate human social signals and \nbehaviours, expressions, and emotions, to enhance human-AI interaction and communication.  \n \nTo achieve this goal, Affective AI draws upon various scientific disciplines, including machine \nlearning, computer vision, speech / natural language / signal processing, psychology and \ncognitive science, and ethics and social sciences. \n \nAffective AI has direct applications in and implications for the design of innovative interactive \ntechnology (e.g., interaction with chat bots, virtual agents, robots), single and multi-user smart \nenvironments ( e.g., in-car/ virtual / augmented / mixed reality, serious games), public speaking \nand cognitive training, and clinical and biomedical studies (e.g., autism, depression, pain). \n \nThe aim of this module is to impart knowledge and ability needed to make informed choices of \nmodels, data, and machine learning techniques for sensing, recognition, and generation of \naffective and social behaviour (e.g., smile, frown, head nodding/shaking, \nagreement/disagreement) in order to create Affectively intelligent AI systems, with a \nconsideration for various ethical issues (e.g., privacy, bias) arising from the real-world \ndeployment of these systems. \n \n  \n \nSyllabus \nThe following list provides a representative list of topics: \n \nIntroduction, definitions, and overview \nTheories from various disciplines \nSensing from multiple modalities (e.g., vision, audio, bio signals, text) \nData acquisition and annotation \nSignal processing / feature extraction \n\nLearning / prediction / recognition and evaluation \nBehaviour synthesis / generation (e.g., for embodied agents / robots) \nAdvanced topics and ethical considerations (e.g., bias and fairness) \nCross-disciplinary applications (via seminar presentations and discussions) \nGuest lectures (diverse topics \u2013 changes each year) \nHands-on research and programming work (i.e., mini project & report)  \n \n  \n \nObjectives \nOn completion of this module, students will: \n \nunderstand and demonstrate knowledge in key characteristics of affectively intelligent AI, which \ninclude: \nRecognition: How to equip Affective AI systems with capabilities of analysing facial expressions, \nvocal intonations, gestures, and other physiological signals to infer human affective states? \nGeneration: How to enable affectively intelligent AI simulate expressions of emotions in \nmachines, allowing them to respond in a more human-like manner, such as virtual agents and \nhumanoid robots, showing empathy or sympathy? \nAdaptation and Personalization: How to enable affectively intelligent AI adapt system responses \nbased on users' affective states and/or needs, or tailor interactions and experiences to individual \nusers based on their expressivity / emotional profiles, personalities, and past interactions? \nEmpathetic Communication: How to design Affective AI systems to communicate with users in a \nway that demonstrates empathy, understanding, and sensitivity to their affective states and \nneeds? \nEthical and societal considerations: What are the various human differences, ethical guidelines, \nand societal impacts that need to be considered when designing and deploying Affective AI to \nensure that these systems respect users' privacy, autonomy, and well-being? \ncomprehend and apply (appropriate) methods for collection, analysis, representation, and \nevaluation of human affective and communicative behavioural data. \nenhance programming skills for creating and implementing (components of) Affective AI \nsystems. \ndemonstrate critical thinking, analysis and synthesis while deciding on 'when' and 'how' to \nincorporate human affect and social signals in a specific AI system context and gain practical \nexperience in proposing and justifying computational solution(s) of suitable nature and scope. \n  \n \nAssessment - Part II Students \nSeminar presentation: 20% \nParticipating in Q&A and discussions: 10% \nMid-term mini project report and presentation (as a team of 2): 10%  \nFinal mini project report & code (as a team of 2): 60% \n \n\nCATEGORY THEORY \n \nAims \nCategory theory provides a unified treatment of mathematical properties and constructions that \ncan be expressed in terms of 'morphisms' between structures. It gives a precise framework for \ncomparing one branch of mathematics (organized as a category) with another and for the \ntransfer of problems in one area to another. Since its origins in the 1940s motivated by \nconnections between algebra and geometry, category theory has been applied to diverse fields, \nincluding computer science, logic and linguistics. This course introduces the basic notions of \ncategory theory: adjunction, natural transformation, functor and category. We will use category \ntheory to organize and develop the kinds of structure that arise in models and semantics for \nlogics and programming languages. \n \nSyllabus \nIntroduction; some history. Definition of category. The category of sets and functions. \nCommutative diagrams. Examples of categories: preorders and monotone functions; monoids \nand monoid homomorphisms; a preorder as a category; a monoid as a category. Definition of \nisomorphism. Informal notion of a 'category-theoretic' property. \nTerminal objects. The opposite of a category and the duality principle. Initial objects. Free \nmonoids as initial objects. \nBinary products and coproducts. Cartesian categories. \nExponential objects: in the category of sets and in general. Cartesian closed categories: \ndefinition and examples. \nIntuitionistic Propositional Logic (IPL) in Natural Deduction style. Semantics of IPL in a cartesian \nclosed preorder. \nSimply Typed Lambda Calculus (STLC). The typing relation. Semantics of STLC types and \nterms in a cartesian closed category (ccc). The internal language of a ccc. The \nCurry-Howard-Lawvere correspondence. \nFunctors. Contravariance. Identity and composition for functors. \nSize: small categories and locally small categories. The category of small categories. Finite \nproducts of categories. \nNatural transformations. Functor categories. The category of small categories is cartesian \nclosed. \nHom functors. Natural isomorphisms. Adjunctions. Examples of adjoint functors. Theorem \ncharacterising the existence of right (respectively left) adjoints in terms of a universal property. \nDependent types. Dependent product sets and dependent function sets as adjoint functors. \nEquivalence of categories. Example: the category of I-indexed sets and functions is equivalent \nto the slice category Set/I. \nPresheaves. The Yoneda Lemma. Categories of presheaves are cartesian closed. \nMonads. Modelling notions of computation as monads. Moggi's computational lambda calculus. \nObjectives \nOn completion of this module, students should: \n \n\nbe familiar with some of the basic notions of category theory and its connections with logic and \nprogramming language semantics \nAssessment \na graded exercise sheet (25% of the final mark), and \na take-home test (75%) \nRecommended reading \nAwodey, S. (2010). Category theory. Oxford University Press (2nd ed.). \n \nCrole, R. L. (1994). Categories for types. Cambridge University Press. \n \nLambek, J. and Scott, P. J. (1986). Introduction to higher order categorical logic. Cambridge \nUniversity Press. \n \nPitts, A. M. (2000). Categorical Logic. Chapter 2 of S. Abramsky, D. M. Gabbay and T. S. E. \nMaibaum (Eds) Handbook of Logic in Computer Science, Volume 5. Oxford University Press. \n(Draft copy available here.) \n \nClass Size \nThis module can accommodate upto 15 Part II students plus 15 MPhil / Part III students.\n\nDIGITAL SIGNAL PROCESSING \n \nAims \nThis course teaches basic signal-processing principles necessary to understand many modern \nhigh-tech systems, with examples from audio processing, image coding, radio communication, \nradar, and software-defined radio. Students will gain practical experience from numerical \nexperiments in programming assignments (in Julia, MATLAB or NumPy). \n \nLectures \nSignals and systems. Discrete sequences and systems: types and properties. Amplitude, \nphase, frequency, modulation, decibels, root-mean square. Linear time-invariant systems, \nconvolution. Some examples from electronics, optics and acoustics. \nPhasors. Eigen functions of linear time-invariant systems. Review of complex arithmetic. \nPhasors as orthogonal base functions. \nFourier transform. Forms and properties of the Fourier transform. Convolution theorem. Rect \nand sinc. \nDirac\u2019s delta function. Fourier representation of sine waves, impulse combs in the time and \nfrequency domain. Amplitude-modulation in the frequency domain. \nDiscrete sequences and spectra. Sampling of continuous signals, periodic signals, aliasing, \ninterpolation, sampling and reconstruction, sample-rate conversion, oversampling, spectral \ninversion. \nDiscrete Fourier transform. Continuous versus discrete Fourier transform, symmetry, linearity, \nFFT, real-valued FFT, FFT-based convolution, zero padding, FFT-based resampling, \ndeconvolution exercise. \nSpectral estimation. Short-time Fourier transform, leakage and scalloping phenomena, \nwindowing, zero padding. Audio and voice examples. DTFM exercise. \nFinite impulse-response filters. Properties of filters, implementation forms, window-based FIR \ndesign, use of frequency-inversion to obtain high-pass filters, use of modulation to obtain \nband-pass filters. \nInfinite impulse-response filters. Sequences as polynomials, z-transform, zeros and poles, some \nanalog IIR design techniques (Butterworth, Chebyshev I/II, elliptic filters, second-order cascade \nform). \nBand-pass signals. Band-pass sampling and reconstruction, IQ up and down conversion, \nsuperheterodyne receivers, software-defined radio front-ends, IQ representation of AM and FM \nsignals and their demodulation. \nDigital communication. Pulse-amplitude modulation. Matched-filter detector. Pulse shapes, \ninter-symbol interference, equalization. IQ representation of ASK, BSK, PSK, QAM and FSK \nsignals. [2 hours] \nRandom sequences and noise. Random variables, stationary and ergodic processes, \nautocorrelation, cross-correlation, deterministic cross-correlation sequences, filtered random \nsequences, white noise, periodic averaging. \nCorrelation coding. Entropy, delta coding, linear prediction, dependence versus correlation, \nrandom vectors, covariance, decorrelation, matrix diagonalization, eigen decomposition, \n\nKarhunen-Lo\u00e8ve transform, principal component analysis. Relation to orthogonal transform \ncoding using fixed basis vectors, such as DCT. \nLossy versus lossless compression. What information is discarded by human senses and can \nbe eliminated by encoders? Perceptual scales, audio masking, spatial resolution, colour \ncoordinates, some demonstration experiments. \nQuantization, image coding standards. Uniform and logarithmic quantization, A/\u00b5-law coding, \ndithering, JPEG. \nObjectives \napply basic properties of time-invariant linear systems; \nunderstand sampling, aliasing, convolution, filtering, the pitfalls of spectral estimation; \nexplain the above in time and frequency domain representations; \nuse filter-design software; \nvisualize and discuss digital filters in the z-domain; \nuse the FFT for convolution, deconvolution, filtering; \nimplement, apply and evaluate simple DSP applications; \nfamiliarity with a number of signal-processing concepts used in digital communication systems \nRecommended reading \nLyons, R.G. (2010). Understanding digital signal processing. Prentice Hall (3rd ed.). \nOppenheim, A.V. and Schafer, R.W. (2007). Discrete-time digital signal processing. Prentice \nHall (3rd ed.). \nStein, J. (2000). Digital signal processing \u2013 a computer science perspective. Wiley. \n \nClass size \nThis module can accommodate a maximum of 24 students (16 Part II students and 8 MPhil \nstudents) \n \nAssessment - Part II students \nThree homework programming assignments, each comprising 20% of the mark \nWritten test, comprising 40% of the total mark.\n \n\nMACHINE VISUAL PERCEPTION \n \nAims \nThis course aims at introducing the theoretical foundations and practical techniques for machine \nperception, the capability of computers to interpret data resulting from sensor measurements. \nThe course will teach the fundamentals and modern techniques of machine perception, i.e. \nreconstructing the real world starting from sensor measurements with a focus on machine \nperception for visual data. The topics covered will be image/geometry representations for \nmachine perception, semantic segmentation, object detection and recognition, geometry \ncapture, appearance modeling and acquisition, motion detection and estimation, \nhuman-in-the-loop machine perception, select topics in applied machine perception. \n \nMachine perception/computer vision is a rapidly expanding area of research with real-world \napplications. An understanding of machine perception is also important for robotics, interactive \ngraphics (especially AR/VR), applied machine learning, and several other fields and industries. \nThis course will provide a fundamental understanding of and practical experience with the \nrelevant techniques. \n \nLearning outcomes \nStudents will understand the theoretical underpinnings of the modern machine perception \ntechniques for reconstructing models of reality starting from an incomplete and imperfect view of \nreality. \nStudents will be able to apply machine perception theory to solve practical problems, e.g. \nclassification of images, geometry capture. \nStudents will gain an understanding of which machine perception techniques are appropriate for \ndifferent tasks and scenarios. \nStudents will have hands-on experience with some of these techniques via developing a \nfunctional machine perception system in their projects. \nStudents will have practical experience with the current prominent machine perception \nframeworks. \nSyllabus \nThe fundamentals of machine learning for machine perception \nDeep neural networks and frameworks for machine perception \nSemantic segmentation of objects and humans \nObject detection and recognition \nMotion estimation, tracking and recognition \n3D geometry capture \nAppearance modeling and acquisition \nSelect topics in applied machine perception \nAssessment \nA practical exercise, worth 20% of the mark. This will cover the basics and theory of machine \nperception and some of the practical techniques the students will likely use in their projects. This \nis individual work. No GPU hours will be needed for the practical work. \nA machine perception project worth 80% of the marks: \n\nCourse projects will be selected by the students following the possible project themes proposed \nby the lecturer, and will be checked by the lecturer for appropriateness.  \nThe students will form groups of 2-3 to design, implement, report, and present a project to tackle \na given task in machine perception. \nThe final mark will be composed of an implementation/report mark (60%) and a presentation \nmark (20%). Each team member will be evaluated based on her/his contribution. \nEach project will have extensions to be completed only by the ACS students. Each student will \nwrite a different part of the report, whose author will be clearly marked. Each student will further \nsummarise her/his contributions to the project in the same report. \nRecommended Reading List \nComputer Vision: Algorithms and Applications, Richard Szeliski, Springer, 2010. \nDeep Learning, Ian Goodfellow, Yoshua Bengio, and Aaron Courville, MIT Press, 2016. \nMachine Learning and Visual Perception, Baochang Zhang, De Gruyter, 2020.\n \n\nNATURAL LANGUAGE PROCESSING \n \nAims \nThis course introduces the fundamental techniques of natural language processing. It aims to \nexplain the potential and the main limitations of these techniques. Some current research issues \nare introduced and some current and potential applications discussed and evaluated. Students \nwill also be introduced to practical experimentation in natural language processing. \n \nLectures \nOverview. Brief history of NLP research, some current applications, components of NLP \nsystems. \nMorphology and Finite State Techniques. Morphology in different languages, importance of \nmorphological analysis in NLP, finite-state techniques in NLP. \nPart-of-Speech Tagging and Log-Linear Models. Lexical categories, word tagging, corpora and \nannotations, empirical evaluation. \nPhrase Structure and Structure Prediction. Phrase structures, structured prediction, context-free \ngrammars, weights and probabilities. Some limitations of context-free grammars. \nDependency Parsing. Dependency structure, grammar-free parsing, incremental processing.  \nGradient Descent and Neural Nets. Parameter optimisation by gradient descent. Non-linear \nfunctions with neural network layers. Log-linear model as softmax layer. Current findings of \nNeural NLP. \nWord representations. Representing words with vectors, count-based and prediction-based \napproaches, similarity metrics. \nRecurrent Neural Networks. Modelling sequences, parameter sharing in recurrent neural \nnetworks, neural language models, word prediction. \nCompositional Semantics. Logical representations, compositional semantics, lambda calculus, \ninference and robust entailment. \nLexical Semantics. Semantic relations, WordNet, word senses. \nDiscourse. Discourse relations, anaphora resolution, summarization. \nNatural Language Generation. Challenges of natural language generation (NLG), tasks in NLG, \nsurface realisation. \nPractical and assignments. Students will build a natural language processing system which will \nbe trained and evaluated on supplied data. The system will be built from existing components, \nbut students will be expected to compare approaches and some programming will be required \nfor this. Several assignments will be set during the practicals for assessment. \nObjectives \nBy the end of the course students should: \n \nbe able to discuss the current and likely future performance of several NLP applications; \nbe able to describe briefly a fundamental technique for processing language for several \nsubtasks, such as morphological processing, parsing, word sense disambiguation etc.; \nunderstand how these techniques draw on and relate to other areas of computer science. \nRecommended reading \n\nJurafsky, D. & Martin, J. (2023). Speech and language processing. Prentice Hall (3rd ed. draft, \nonline). \n \nAssessment - Part II Students \nAssignment 1 - 10% of marks \nAssignment 2 - 60% of marks \nAssignment 3 - 30% of marks\n \n\nPRACTICAL RESEARCH IN HUMAN-CENTRED AI \n \nAims \nThis is an advanced course in human-computer interaction, with a specialist focus on intelligent \nuser interfaces and interaction with machine-learning and artificial intelligence technologies. The \nformat will be largely Practical, with students carrying out a mini-project involving empirical \nresearch investigation. These studies will investigate human interaction with some kind of \nmodel-based system for planning, decision-making, automation etc. Possible study formats \nmight include: System evaluation, Field observation, Hypothesis testing experiment, Design \nintervention or Corpus analysis, following set examples from recent research publications. \nProject work will be formally evaluated through a report and presentation. \n \nLectures \n(note that Lectures 2-7 also include one hour class discussion of practical work) \n        \u2022 Current research themes in intelligent user interfaces \n        \u2022 Program synthesis \n        \u2022 Mixed initiative interaction \n        \u2022 Interpretability / explainable AI \n        \u2022 Labelling as a fundamental problem \n        \u2022 Machine learning risks and bias \n        \u2022 Visualisation and visual analytics \n        \u2022 Student research presentations \n \nObjectives \nBy the end of the course students should: \n        \u2022 be familiar with current state of the art in intelligent interactive systems \n        \u2022 understand the human factors that are most critical in the design of such systems \n        \u2022 be able to evaluate evidence for and against the utility of novel systems \n        \u2022 have experience of conducting user studies meeting the quality criteria of this field \n        \u2022 be able to write up and present user research in a professional manner \n \nClass Size \nThis module can accommodate upto 20 Part II, Part III and MPhil students. \n \nRecommended reading \nBrad A. Myers and Richard McDaniel (2000). Demonstrational Interfaces: Sometimes You Need \na Little Intelligence, Sometimes You Need a Lot. \n \nAlan Blackwell (2024). Moral Codes: Designing alternatives to AI \n \nAssessment - Part II Students \nThe format will be largely practical, with students carrying out an individual mini-project involving \nempirical research investigation. \n \n\nAssignment 1: six incremental submissions which together contribute 20% to the final module \nmark. \n \nAssignment 2: Final report - 80% of the final module mark \n \n\n",
        "6th Semester \nADVANCED COMPUTER ARCHITECTURE \nAims \nThis course examines the techniques and underlying principles that are used to design \nhigh-performance computers and processors. Particular emphasis is placed on understanding \nthe trade-offs involved when making design decisions at the architectural level. A range of \nprocessor architectures are explored and contrasted. In each case we examine their merits and \nlimitations and how ultimately the ability to scale performance is restricted. \n \nLectures \nIntroduction. The impact of technology scaling and market trends. \nFundamentals of Computer Design. Amdahl\u2019s law, energy/performance trade-offs, ISA design. \nAdvanced pipelining. Pipeline hazards; exceptions; optimal pipeline depth; branch prediction; \nthe branch target buffer [2 lectures] \nSuperscalar techniques. Instruction-Level Parallelism (ILP); superscalar processor architecture \n[2 lectures] \nSoftware approaches to exploiting ILP. VLIW architectures; local and global instruction \nscheduling techniques; predicated instructions and support for speculative compiler \noptimisations. \nMultithreaded processors. Coarse-grained, fine-grained, simultaneous multithreading \nThe memory hierarchy. Caches; programming for caches; prefetching [2 lectures] \nVector processors. Vector machines; short vector/SIMD instruction set extensions; stream \nprocessing \nChip multiprocessors. The communication model; memory consistency models; false sharing; \nmultiprocessor memory hierarchies; cache coherence protocols; synchronization [2 lectures] \nOn-chip interconnection networks. Bus-based interconnects; on-chip packet switched networks \nSpecial-purpose architectures. Converging approaches to computer design \nObjectives \nAt the end of the course students should \n \nunderstand what determines processor design goals; \nappreciate what constrains the design process and how architectural trade-offs are made within \nthese constraints; \nbe able to describe the architecture and operation of pipelined and superscalar processors, \nincluding techniques such as branch prediction, register renaming and out-of-order execution; \nhave an understanding of vector, multithreaded and multi-core processor architectures; \nfor the architectures discussed, understand what ultimately limits their performance and \napplication domain. \nRecommended reading \n* Hennessy, J. and Patterson, D. (2012). Computer architecture: a quantitative approach. \nElsevier (5th ed.) ISBN 9780123838728. (the 3rd and 4th editions are also good)\n \n\nCRYPTOGRAPHY \n \nAims \nThis course provides an overview of basic modern cryptographic techniques and covers \nessential concepts that users of cryptographic standards need to understand to achieve their \nintended security goals. \n \nLectures \nCryptography. Overview, private vs. public-key ciphers, MACs vs. signatures, certificates, \ncapabilities of adversary, Kerckhoffs\u2019 principle. \nClassic ciphers. Attacks on substitution and transposition ciphers, Vigen\u00e9re. Perfect secrecy: \none-time pads. \nPrivate-key encryption. Stream ciphers, pseudo-random generators, attacking \nlinear-congruential RNGs and LFSRs. Semantic security definitions, oracle queries, advantage, \ncomputational security, concrete-security proofs. \nBlock ciphers. Pseudo-random functions and permutations. Birthday problem, random \nmappings. Feistel/Luby-Rackoff structure, DES, TDES, AES. \nChosen-plaintext attack security. Security with multiple encryptions, randomized encryption. \nModes of operation: ECB, CBC, OFB, CNT. \nMessage authenticity. Malleability, MACs, existential unforgeability, CBC-MAC, ECBC-MAC, \nCMAC, birthday attacks, Carter-Wegman one-time MAC. \nAuthenticated encryption. Chosen-ciphertext attack security, ciphertext integrity, \nencrypt-and-authenticate, authenticate-then-encrypt, encrypt-then-authenticate, padding oracle \nexample, GCM. \nSecure hash functions. One-way functions, collision resistance, padding, Merkle-Damg\u00e5rd \nconstruction, sponge function, duplex construct, entropy pool, SHA standards. \nApplications of secure hash functions. HMAC, stream authentication, Merkle tree, commitment \nprotocols, block chains, Bitcoin. \nKey distribution problem. Needham-Schroeder protocol, Kerberos, hardware-security modules, \npublic-key encryption schemes, CPA and CCA security for asymmetric encryption. \nNumber theory, finite groups and fields. Modular arithmetic, Euclid\u2019s algorithm, inversion, \ngroups, rings, fields, GF(2n), subgroup order, cyclic groups, Euler\u2019s theorem, Chinese \nremainder theorem, modular roots, quadratic residues, modular exponentiation, easy and \ndifficult problems. [2 lectures] \nDiscrete logarithm problem. Baby-step-giant-step algorithm, computational and decision \nDiffie-Hellman problem, DH key exchange, ElGamal encryption, hybrid cryptography, Schnorr \ngroups, elliptic-curve systems, key sizes. [2 lectures] \nTrapdoor permutations. Security definition, turning one into a public-key encryption scheme, \nRSA, attacks on \u201ctextbook\u201d RSA, RSA as a trapdoor permutation, optimal asymmetric \nencryption padding, common factor attacks. \nDigital signatures. One-time signatures, RSA signatures, Schnorr identification scheme, \nElGamal signatures, DSA, PS3 hack, certificates, PKI. \nObjectives \nBy the end of the course students should \n\n \nbe familiar with commonly used standardized cryptographic building blocks; \nbe able to match application requirements with concrete security definitions and identify their \nabsence in naive schemes; \nunderstand various adversarial capabilities and basic attack algorithms and how they affect key \nsizes; \nunderstand and compare the finite groups most commonly used with discrete-logarithm \nschemes; \nunderstand the basic number theory underlying the most common public-key schemes, and \nsome efficient implementation techniques. \nRecommended reading \nKatz, J., Lindell, Y. (2015). Introduction to modern cryptography. Chapman and Hall/CRC (2nd \ned.).\n \n\nE-COMMERCE \n \nAims \nThis course aims to give students an outline of the issues involved in setting up an e-commerce \nsite. \n \nLectures \nThe history of electronic commerce. Mail order; EDI; web-based businesses, credit card \nprocessing, PKI, identity and other hot topics. \nNetwork economics. Real and virtual networks, supply-side versus demand-side scale \neconomies, Metcalfe\u2019s law, the dominant firm model, the differentiated pricing model Data \nProtection Act, Distance Selling regulations, business models. \nWeb site design. Stock and price control; domain names, common mistakes, dynamic pages, \ntransition diagrams, content management systems, multiple targets. \nWeb site implementation. Merchant systems, system design and sizing, enterprise integration, \npayment mechanisms, CRM and help desks. Personalisation and internationalisation. \nThe law and electronic commerce. Contract and tort; copyright; binding actions; liabilities and \nremedies. Legislation: RIP; Data Protection; EU Directives on Distance Selling and Electronic \nSignatures. \nPutting it into practice. Search engine interaction, driving and analysing traffic; dynamic pricing \nmodels. Integration with traditional media. Logs and audit, data mining modelling the user. \ncollaborative filtering and affinity marketing brand value, building communities, typical behaviour. \nFinance. How business plans are put together. Funding Internet ventures; the recent hysteria; \nmaximising shareholder value. Future trends. \nUK and International Internet Regulation. Data Protection Act and US Privacy laws; HIPAA, \nSarbanes-Oxley, Security Breach Disclosure, RIP Act 2000, Electronic Communications Act \n2000, Patriot Act, Privacy Directives, data retention; specific issues: deep linking, Inlining, brand \nmisuse, phishing. \nObjectives \nAt the end of the course students should know how to apply their computer science skills to the \nconduct of e-commerce with some understanding of the legal, security, commercial, economic, \nmarketing and infrastructure issues involved. \n \nRecommended reading \nShapiro, C. and Varian, H. (1998). Information rules. Harvard Business School Press. \n \nAdditional reading: \n \nStandage, T. (1999). The Victorian Internet. Phoenix Press. Klemperer, P. (2004). Auctions: \ntheory and practice. Princeton Paperback ISBN 0-691-11925-2.\n \n\nMACHINE LEARNING AND BAYESIAN INFERENCE \nAims \nThe Part 1B course Artificial Intelligence introduced simple neural networks for supervised \nlearning, and logic-based methods for knowledge representation and reasoning. This course \nhas two aims. First, to provide a rigorous introduction to machine learning, moving beyond the \nsupervised case and ultimately presenting state-of-the-art methods. Second, to provide an \nintroduction to the wider area of probabilistic methods for representing and reasoning with \nknowledge. \n \nLectures \nIntroduction to learning and inference. Supervised, unsupervised, semi-supervised and \nreinforcement learning. Bayesian inference in general. What the naive Bayes method actually \ndoes. Review of backpropagation. Other kinds of learning and inference. [1 lecture] \nHow to classify optimally. Treating learning probabilistically. Bayesian decision theory and Bayes \noptimal classification. Likelihood functions and priors. Bayes theorem as applied to supervised \nlearning. The maximum likelihood and maximum a posteriori hypotheses. What does this teach \nus about the backpropagation algorithm? [2 lectures] \nLinear classifiers I. Supervised learning via error minimization. Iterative reweighted least \nsquares. The maximum margin classifier. [2 lectures] \nGaussian processes. Learning and inference for regression using Gaussian process models. [2 \nlectures] \nSupport vector machines (SVMs). The kernel trick. Problem formulation. Constrained \noptimization and the dual problem. SVM algorithm. [2 lectures] \nPractical issues. Hyperparameters. Measuring performance. Cross-validation. Experimental \nmethods. [1 lecture] \nLinear classifiers II. The Bayesian approach to neural networks. [1 lecture] \nUnsupervised learning I. The k-means algorithm. Clustering as a maximum likelihood problem. \n[1 lecture] \nUnsupervised learning II. The EM algorithm and its application to clustering. [1 lecture] \nBayesian networks I. Representing uncertain knowledge using Bayesian networks. Conditional \nindependence. Exact inference in Bayesian networks. [2 lectures] \nBayesian networks II. Markov random fields. Approximate inference. Markov chain Monte Carlo \nmethods. [1 lecture] \nObjectives \nAt the end of this course students should: \n \nUnderstand how learning and inference can be captured within a probabilistic framework, and \nknow how probability theory can be applied in practice as a means of handling uncertainty in AI \nsystems. \nUnderstand several algorithms for machine learning and apply those methods in practice with \nproper regard for good experimental practice. \nRecommended reading \nIf you are going to buy a single book for this course I recommend: \n \n\n* Bishop, C.M. (2006). Pattern recognition and machine learning. Springer. \n \nThe course text for Artificial Intelligence I: \n \nRussell, S. and Norvig, P. (2010). Artificial intelligence: a modern approach. Prentice Hall (3rd \ned.). \n \ncovers some relevant material but often in insufficient detail. Similarly: \n \nMitchell, T.M. (1997). Machine Learning. McGraw-Hill. \n \ngives a gentle introduction to some of the course material, but only an introduction. \n \nRecently a few new books have appeared that cover a lot of relevant ground well. For example: \n \nBarber, D. (2012). Bayesian Reasoning and Machine Learning. Cambridge University Press. \nFlach, P. (2012). Machine Learning: The Art and Science of Algorithms that Make Sense of \nData. Cambridge University Press. \nMurphy, K.P. (2012). Machine Learning: A Probabilistic Perspective. MIT Press.\n \n\nOPTIMISING COMPILERS \n \nAims \nThe aims of this course are to introduce the principles of program optimisation and related \nissues in decompilation. The course will cover optimisations of programs at the abstract syntax, \nflowgraph and target-code level. It will also examine how related techniques can be used in the \nprocess of decompilation. \n \nLectures \nIntroduction and motivation. Outline of an optimising compiler. Optimisation partitioned: analysis \nshows a property holds which enables a transformation. The flow graph; representation of \nprogramming concepts including argument and result passing. The phase-order problem. \nKinds of optimisation. Local optimisation: peephole optimisation, instruction scheduling. Global \noptimisation: common sub-expressions, code motion. Interprocedural optimisation. The call \ngraph. \nClassical dataflow analysis. Graph algorithms, live and avail sets. Register allocation by register \ncolouring. Common sub-expression elimination. Spilling to memory; treatment of \nCSE-introduced temporaries. Data flow anomalies. Static Single Assignment (SSA) form. \nHigher-level optimisations. Abstract interpretation, Strictness analysis. Constraint-based \nanalysis, Control flow analysis for lambda-calculus. Rule-based inference of program properties, \nTypes and effect systems. Points-to and alias analysis. \nTarget-dependent optimisations. Instruction selection. Instruction scheduling and its phase-order \nproblem. \nDecompilation. Legal/ethical issues. Some basic ideas, control flow and type reconstruction. \nObjectives \nAt the end of the course students should \n \nbe able to explain program analyses as dataflow equations on a flowgraph; \nknow various techniques for high-level optimisation of programs at the abstract syntax level; \nunderstand how code may be re-scheduled to improve execution speed; \nknow the basic ideas of decompilation. \nRecommended reading \n* Nielson, F., Nielson, H.R. and Hankin, C.L. (1999). Principles of program analysis. Springer. \nGood on part A and part B. \nAppel, A. (1997). Modern compiler implementation in Java/C/ML (3 editions). \nMuchnick, S. (1997). Advanced compiler design and implementation. Morgan Kaufmann. \nWilhelm, R. (1995). Compiler design. Addison-Wesley. \nAho, A.V., Sethi, R. and Ullman, J.D. (2007). Compilers: principles, techniques and tools. \nAddison-Wesley (2nd ed.).\n \n\nQUANTUM COMPUTING \n \nAims \nThe principal aim of the course is to introduce students to the basics of the quantum model of \ncomputation. The model will be used to study algorithms for searching, factorisation and \nquantum chemistry as well as other important topics in quantum information such as \ncryptography and super-dense coding. Issues in the complexity of computation will also be \nexplored. A second aim of the course is to introduce student to near-term quantum computing. \nTo this end, error-correction and adiabatic quantum computing are studied. \n \nLectures \nBits and qubits. Introduction to quantum states and measurements with motivating examples. \nComparison with discrete classical states. \nLinear algebra. Review of linear algebra: vector spaces, linear operators, Dirac notation, the \ntensor product. \nThe postulates of quantum mechanics. Postulates of quantum mechanics, incl. evolution and \nmeasurement. \nImportant concepts in quantum mechanics. Entanglement, distinguishing orthogonal and \nnon-orthogonal quantum states, no-cloning and no signalling. \nThe quantum circuit model. The circuit model of quantum computation. Quantum gates and \ncircuits. Universality of the quantum circuit model, and efficient simulation of arbitrary two-qubit \ngates with a standard universal set of gates. \nSome applications of quantum information. Applications of quantum information (other than \nquantum computation): quantum key distribution, superdense coding and quantum teleportation. \nDeutsch-Jozsa algorithm. Introducing Deutsch\u2019s problem and Deutsch\u2019s algorithm leading onto \nits generalisation, the Deutsch-Jozsa algorithm. \nQuantum search. Grover\u2019s search algorithm: analysis and lower bounds. \nQuantum Fourier Transform and Quantum Phase Estimation. Definition of the Quantum Fourier \nTransform (QFT), and efficient representation thereof as a quantum circuit. Application of the \nQFT to enable Quantum Phase Estimation (QPE). \nApplication 1 of QFT / QPE: Factoring. Shor\u2019s algorithm: reduction of factoring to period finding \nand then using the QFT for period finding. \nApplication 2 of QFT / QPE: Quantum Chemistry. Efficient simulation of quantum systems, and \napplications to real-world problems in quantum chemistry. \nQuantum complexity. Quantum complexity classes and their relationship to classical complexity. \nComparison with probabilistic computation. \nQuantum error correction. Introducing the concept of quantum error correction required for the \nfollowing lecture on fault-tolerance. \nFault tolerant quantum computing. Elements of fault tolerant computing; the threshold theorem \nfor efficient suppression of errors. \nAdiabatic quantum computing and quantum optimisation. The quantum adiabatic theorem, and \nadiabatic optimisation. Quantum annealing and D-Wave. \nCase studies in near-term quantum computation. Examples of state-of-the-art quantum \nalgorithms and computers, including superconducting and networked quantum computers. \n\nObjectives \nAt the end of the course students should: \n \nunderstand the quantum model of computation and the basic principles of quantum mechanics; \nbe familiar with basic quantum algorithms and their analysis; \nbe familiar with basic quantum protocols such as teleportation and superdense coding; \nsee how the quantum model relates to classical models of deterministic and probabilistic \ncomputation. \nappreciate the importance of efficient error-suppression if quantum computation is to yield an \nadvantage over classical computation. \ngain a general understanding of the important topics in near-term quantum computing, including \nadiabatic quantum computing. \nRecommended reading \nBooks: \nNielsen M.A., Chuang I.L. (2010). Quantum Computation and Quantum Information. Cambridge \nUniversity Press. \nMermin N.D. (2007). Quantum Computer Science: An Introduction. Cambridge University Press. \nMcGeoch, C. (2014). Adiabatic Quantum Computation and Quantum Annealing Theory and \nPractice. Morgan and Claypool. https://ieeexplore.ieee.org/document/7055969 \n \nPapers: \n \nBraunstein S.L. (2003). Quantum computation tutorial. Available at: \nhttps://www-users.cs.york.ac.uk/~schmuel/comp/comp_best.pdf \nAharonov D., Quantum computation [arXiv:quant-ph/9812037] \nAlbash T., Adiabatic Quantum Computing https://arxiv.org/pdf/1611.04471.pdf \nMcCardle S. et al, Quantum computational chemistry https://arxiv.org/abs/1808.10402 \n \nOther lecture notes: \n \nAndrew Childs (University of Maryland): http://cs.umd.edu/~amchilds/qa/ \n \n\nRANDOMISED ALGORITHMS \n \nAims: \nThe aim of this course is to introduce advanced techniques in the design and analysis \nalgorithms, with a strong focus on randomised algorithms. It covers essential tools and ideas \nfrom probability, optimisation and graph theory, and develops this knowledge through a variety \nof examples of algorithms and processes. This course will demonstrate that randomness is not \nonly an important design technique which often leads to simpler and more elegant algorithms, \nbut may also be essential in settings where time and space are restricted.   \n \nLectures: \nIntroduction. Course Overview. Why Randomised Algorithms? A first Randomised Algorithm for \nthe MAX-CUT problem. [1 Lecture]  \n \nConcentration Inequalities. Moment-Generating Functions and Chernoff Bounds. Extension: \nMethod of Bounded Independent Differences. Applications: Balls-into-Bins, Quick-Sort and Load \nBalancing. [approx. 2 Lectures] \n \nMarkov Chains and Mixing Times. Random Walks on Graphs. Application: Randomised \nAlgorithm for the 2-SAT problem. Mixing Times of Markov Chains. Application: Load Balancing \non Networks. [approx. 2 Lectures] \n \nLinear Programming and Applications. Definitions and Applications. Formulating Linear \nPrograms. The Simplex Algorithm. Finding Initial Solutions. How to use Linear Programs and \nBranch & Bound to Solve a Classical TSP instance. [approx. 3 Lectures] \n \nRandomised Approximation Algorithms. Randomised Approximation Schemes. Linearity of \nExpectations, Derandomisation. Deterministic and Randomised Rounding of Linear Programs. \nApplications: MAX3-SAT problem, Weighted Vertex Cover, Weighted Set Cover, MAX-SAT. \n[approx. 2 Lectures] \n \nSpectral Graph Theory and Clustering. Eigenvalues of Graphs and Matrices: Relations between \nEigenvalues and Graph Properties, Spectral Graph Drawing. Spectral Clustering: Conductance, \nCheeger's Inequality. Spectral Partitioning Algorithm [approx. 2 Lectures] \n \nObjectives: \nBy the end of the course students should be able to: \n \nlearn how to use randomness in the design of algorithms, in particular, approximation \nalgorithms; \nlearn the basics of linear programming, integer programming and randomised rounding; \napply randomisation to various problems coming from optimisation, machine learning and data \nscience; \nuse results from probability theory to analyse the performance of randomised algorithms. \n\nRecommended reading \n* Michael Mitzenmacher and Eli Upfal. Probability and Computing: Randomized Algorithms and \nProbabilistic Analysis., Cambridge University Press, 2nd edition. \n \n* David P. Williamson and David B. Shmoys. The Design of Approximation Algorithms, \nCambridge University Press, 2011 \n \n* Cormen, T.H., Leiserson, C.D., Rivest, R.L. and Stein, C. (2009). Introduction to Algorithms. \nMIT Press (3rd ed.). ISBN 978-0-262-53305-8 \n \n \n\nCLOUD COMPUTING \nAims \nThis module aims to teach students the fundamentals of Cloud Computing covering topics such \nas virtualization, data centres, cloud resource management, cloud storage and popular cloud \napplications including batch and data stream processing. Emphasis is given on the different \nbackend technologies to build and run efficient clouds and the way clouds are used by \napplications to realise computing on demand. The course will include practical tutorials on cloud \ninfrastructure technologies. Students will be assessed via a Cloud-based coursework project. \n \nLectures \nIntroduction to Cloud Computing \nData centres \nVirtualization I \nVirtualization II \nMapReduce \nMapReduce advanced \nResource management for virtualized data centres \nCloud storage \nCloud-based data stream processing \nObjectives \nBy the end of the course students should: \n \nunderstand how modern clouds operate and provide computing on demand; \nunderstand about cloud availability, performance, scalability and cost; \nknow about cloud infrastructure technologies including virtualization, data centres, resource \nmanagement and storage; \nknow how popular applications such as batch and data stream processing run efficiently on \nclouds; \nAssessment \nAssignment 1, worth 55% of the final mark. Develop batch processing applications running on a \ncluster assessed through automatic testing, code inspection and a report. \nAssignment 2, worth 30% of the final mark: Design and develop a framework for running the \nbatch processing applications from Assignment 1 on a cluster according to certain resource \nallocation policies. This assigment will be assessed by an expert, testing, code inspection and a \nreport. \nAssignment 3, worth 15% of the final mark: Write two individual project reports describing your \nwork for assignments 1 and 2. Write two sets of scripts to manage and run your code. Reports \nand scripts will be submitted with each assignment. \nRecommended reading \nMarinescu, D.C. Cloud Computing, Theory and Practice. Morgan Kaufmann. \nBarham, P., et. al. (2003). \u201cXen and the Art of Virtualization\u201d. In Proceedings of SOSP 2003. \nCharkasova, L., Gupta, D. and Vahdat, A. (2007). \u201cComparison of the Three CPU Schedulers in \nXen\u201d. In SIGMETRICS 2007. \n\nDean, J. and Ghemawat, S. (2004). \u201cMapReduce: Simplified Data Processing on Large \nClusters\u201d. In Proceedings of OSDI 2004. \nZaharia, M, et al. (2008). \u201cImproving MapReduce Performance in Heterogeneous \nEnvironments\u201d. In Proceedings of OSDI 2008. \nHindman, A., et al. (2011). \u201cMesos: A Platform for Fine-Grained Resource Sharing in Data \nCenter\u201d. In Proceedings of NSDI 2011. \nSchwarzkopf, M., et al. (2013). \u201cOmega: Flexible, Scalable Schedulers for Large Compute \nClusters\u201d. In EuroSys 2013. \nGhemawat, S. (2003). \u201cThe Google File System\u201d. In Proceedings of SOSP 2003. \nChang, F. (2006). \u201cBigtable: A Distributed Storage System for Structured Data\u201d. In Proceedings \nof OSDI 2006. \nFernandez, R.C., et al. (2013). \u201cIntegrating Scale Out and Fault Tolerance in Stream Processing \nusing Operator State Management\u201d. In SIGMOD 2013.\n \n\nCOMPUTER SYSTEMS MODELLING \n \nAims \n \nThe aims of this course are to introduce the concepts and principles of mathematical modelling \n \nand simulation, with particular emphasis on using queuing theory and control theory for \nunderstanding the behaviour of computer and communications systems. \n \nSyllabus \n \n1.     Overview of computer systems modeling using both analytic techniques and simulation. \n \n2.     Introduction to discrete event simulation. \n \na.     Simulation techniques \n \nb.     Random number generation methods \n \nc.     Statistical aspects: confidence intervals, stopping criteria \n \nd.     Variance reduction techniques. \n \n3.     Stochastic processes (builds on starred material in Foundations of Data Science) \n \na.     Discrete and continuous stochastic processes. \n \nb.     Markov processes and Chapman-Kolmogorov equations. \n \nc.     Discrete time Markov chains. \n \nd.     Ergodicity and the stationary distribution. \n \ne.     Continuous time Markov chains. \n \nf.      Birth-death processes, flow balance equations. \n \ng.     The Poisson process. \n \n4.     Queueing theory \n \na.     The M/M/1 queue in detail. \n \n\nb.     The equilibrium distribution with conditions for existence and common performance \nmetrics. \n \nc.     Extensions of the M/M/1 queue: the M/M/k queue, the M/M/infinity queue. \n \nd.     Queueing networks. Jacksonian networks. \n \ne.     The M/G/1 queue. \n \n5.     Signals, systems, and transforms \n \na.     Discrete- and continuous-time convolution. \n \nb.     Signals. The complex exponential signal. \n \nc.     Linear Time-Invariant Systems. Modeling practical systems as an LTI system. \n \nd.     Fourier and Laplace transforms. \n \n6.     Control theory. \n \na.     Controlled systems. Modeling controlled systems. \n \nb.     State variables. The transfer function model. \n \nc.     First-order and second-order systems. \n \nd.     Feedback control. PID control. \n \ne.     Stability. BIBO stability. Lyapunov stability. \n \nf.      Introduction to Model Predictive Control. \n \nObjectives \n \nAt the end of the course students should \n \n\u00b7       Be aware of different approaches to modeling a computer system; their pros and cons \n \n\u00b7       Be aware of the issues in building a simulation of a computer system and analysing the \nresults obtained \n \n\u00b7       Understand the concept of a stochastic process and how they arise in practice \n \n\n\u00b7       Be able to build simple Markov models and understand the critical modelling assumptions \n \n\u00b7       Be able to solve simple birth-death processes \n \n\u00b7       Understand and use M/M/1 queues to model computer systems \n \n\u00b7       Be able to model a computer system as a linear time-invariant system \n \n\u00b7       Understand the dynamics of a second-order controlled system \n \n\u00b7       Design a PID control for an LTI system \n \n\u00b7       Understand what is meant by BIBO and Lyapunov stability \n \nAssessment \n \nThere will be one programming assignment to build a discrete event simulator and using it to \nsimulate an M/M/1 and an M/D/1 queue (worth 15% of the final marks). There will also be two \none-hour in-person assessments for the course on: Stochastic processes and Queueing theory \n(40%) and Signals, Systems, Transforms, and Control Theory (45%). \n \nReference books \n \nKeshav, S. (2012)*. Mathematical Foundations of Computer Networking. Addison-Wesley. A \ncustomized PDF will be made available to class participants. \n \nKleinrock, L. (1975). Queueing systems, vol. 1. Theory. Wiley. \n \nKraniauskas, Peter. Transforms in signals and systems. Addison-Wesley Longman, 1992. \n \nJain, R. (1991). The art of computer systems performance analysis. Wiley. \n \n \n\nCOMPUTING EDUCATION \n \nAims \nThis module considers various aspects of the teaching and learning of computer science in \nschool, including practical experience, and provides an introduction to the field of computing \neducation research. It is offered by the Raspberry Pi Computing Education Research Centre, \npart of the Department of Computer Science and Technology, in conjunction with local schools \nin the Cambridge area. Students will have the opportunity to observe, plan and deliver a lesson, \nand explore the teaching of computing in a secondary education setting. The lesson taught will \nbe on a topic within the computing/computer science curriculum in England but will be \nnegotiated with the teacher mentoring the school placement. In the sessions in the Department, \nstudents will be introduced to elements of pedagogy and assessment alongside current issues \nin computing education. The course can also serve as a useful introduction to research in \ncomputing education for those interested in this area. Assessment will include lesson planning, \npresentations, an in-person viva and a 3000-word report which will include evidence of \nengagement with relevant research underpinning the lessons and activities undertaken. An \ninformation session relating to the module will be available in the preceding Easter Term. The \narrangement of school placements and DBS checks will be carried out in Michaelmas Term, so \nstudents should be prepared to engage in these preparatory activities. \n \nLectures \nThis is not a traditional lecture course and the term will be structured as follows: \n \nWeek 1: Introduction to computing education & visit to school placement \nWeek 2: In school (observation, supporting teacher) \nWeek 3: In school (observation, supporting teacher) \nWeek 4: In school (co-teach / small group teaching) \nWeek 5: Computing pedagogy and assessment (school half-term) \nWeek 6: In school (co-teach / small group teaching) \nWeek 7: In school (teach part/whole lesson) \nWeek 8: Presentations \nObjectives \nBy the end of the course students should: \n \ngain experience of computing education in practice \ndemonstrate an ability to communicate an aspect of computer science clearly and effectively \nbe able to plan and design a teaching activity/lesson with consideration of the audience \nbe able to thoroughly evaluate the design and implementation of a teaching/activity lesson \nbe able to demonstrate an understanding of relevant theories of learning and pedagogy from the \nliterature and how they apply to the learning/teaching of the topic under consideration \ngain an understanding of the breadth and depth of the field of computing education research \nClass Size \nThis module can accommodate up to 10 Part II students. \n \n\nRecommended reading \nBrown, N. C., Sentance, S., Crick, T., & Humphreys, S. (2014). Restart: The resurgence of \ncomputer science in UK schools. ACM Transactions on Computing Education (TOCE), 14(2), \n1-22. https://doi.org/10.1145/2602484 \n \nSentance, S., Barendsen, E., Howard, N. R., & Schulte, C. (Eds.). (2023). Computer science \neducation: Perspectives on teaching and learning in school. Bloomsbury Publishing. \n \nGrover, S. (Ed). 2020. Computer Science in K-12: An A-to-Z Handbook on Teaching \nProgramming. Edfinity. https://www.shuchigrover.com/atozk12cs/ \n \nRaspberry Pi Foundation (2022). Hello World Big Book of Pedagogy. \nhttps://www.raspberrypi.org/hello-world/issues/the-big-book-of-computing-pedagogy \n \nAssessment \nWeek 2: Reflections on an observed lesson (10%, graded out of 20) \nWeek 4: Submission of a lesson plan outline (10% graded out of 20) \nWeek 8: Delivery of an oral presentation relating to the lesson delivery (10% graded out of 20) \nAfter course completion: 3000 word report (60%), graded out of 20 \nViva with assessor (10%) (pass/fail) \nFurther Information \nWe will find placements for students in secondary schools within cycling or bus distance from \nthe Department/centre of Cambridge. There will be a timetabled slot for the course that students \ncan use to go to their school placement, but students may need to be flexible and liaise with the \nschool to find the best time for working with a specific class. \n \n \n\nDEEP NEURAL NETWORKS \n \nObjectives \nYou will gain detailed knowledge of \n \nCurrent understanding of generalization in neural networks vs classical statistical models. \nOptimization procedures for neural network models such as stochastic gradient descent and \nADAM. \nAutomatic differentiation and at least one software framework (PyTorch, TensorFolow) as well as \nan overview of other software approaches. \nArchitectures that are deployed to deal with different data types such as images or sequences \nincluding a. convolutional networks b. recurrent networks and c. transformers. \nIn addition you will gain knowledge of more advanced topics reflecting recent research in \nmachine learning. These change from year to year, examples include: \n \nApproaches to unsupervised learning including autoencoders and self-supervised learning.. \nTechniques for deploying models in low data regimes such as transfer learning and \nmeta-learning. \nTechniques for propagating uncertainty such as Bayesian neural networks. \nReinforcement learning and its applications to robotics or games. \nGraph Neural Networks and Geometric Deep Learning. \nTeaching Style \nThe start of the course will focus on the latest undertanding of current theory of neural networks, \ncontrasting with previous classical understandings of generalization performance. Then we will \nmove to practical examples of network architectures and deployment. We will end with more \nadvanced topics reflecting current research, mostly delivered by guest lecturers from the \nDepartment and beyond. \n \nSchedule \nWeek 1 \nTwo lectures: Generalization and Approximation with Neural Networks \n \nWeek 2 \nTwo lectures: Optimization: Stochastic Gradient Descent and ADAM \n \nWeek 3 \nTwo lectures: Hardware Acceleration and Convolutional Neural Networks \n \nWeek 4 \nTwo lectures: Vanishing Gradients, Recurrent and Residual Networks, Sequence Modeling. \n \nWeek 5 \nTwo lectures: Attention, The Transformer Architecture, Large Language Models \n \n\nWeek 6-8 \nLectures and guest lectures from the special topics below. \n \nSpecial topics \nSelf-Supervised Learning, AutoEncoders, Generative Modeling of Images \nHardware Implementations \nReinforcement learning \nTransfer learning and meta-learning \nUncertainty and Bayesian Neural Networks \nGraph Neural Networks \nAssessment \nAssessment involves solving a number of exercises in a jupyter notebook environment. \nFamiliarity with the python programming language is assumed. The exercises rely on the \npytorch deep learning framework, the syntax of which we don\u2019t cover in detail in the lectures, \nstudents are referred to documentation and tutorials to learn this component of the assessment. \n \nAssignment 1 \n \n30% of the total marks, with guided exercises. \n \nAssignment 2 \n \n70% of the total marks, a combination of guided exercises and a more exploratory mini-project.\n\nEXTENDED REALITY \n \nSyllabus & Schedule \nWeek 1 \nIntroduction \nCourse schedule and concept \nThe history of AR/VR/XR \nApplications of AR/VR/XR \nOverview of the XR pipeline \u2013 3 main components of XR \nDisplay technologies and rendering \nMachine perception and input interfaces \nContent generation \u2013 geometry, appearance, motion \nOverview of the AR/VR/XR frameworks \nPractical 1 \nProject structure \nProject themes \nIntroduction to the XR programming platform \nWeek 2 \n3D scene representations and rendering \nObject representations with polygonal meshes \nEfficient rendering via rasterization \nPractical XR rendering of textured geometries \nTracking and pose estimation for XR \nRigid transforms \nCamera pose estimation \nBody/ hand tracking \nPractical 2 \nProject draft proposal due \nProject feedback \nRendering and displaying a first object \nWeek 3 \n3D scene capture and understanding \nDepth estimation \n3D geometry capture \nAppearance acquisition \nLighting estimation and relighting \nLighting models for 3D scenes \nEstimating lighting in a scene \nRelighting rendered models \nPractical 3 \nAcquiring and utilizing depth from sensors \nRelighting a displayed object \nWeek 4 \nPhysically-based simulation of rigid objects \n\nRigid-body dynamics \nCollision detection \nTime integration \nPhysically-based simulation of non-rigid volumetric phenomena \nParticle systems \nBasic fluid dynamics \nPractical 4 \nPractical due \nProject prototype due \nProject check-in \nSimulating and rendering a simple object \nWeek 5 \nAdvanced topics in XR I: AR/VR displays technologies \nMicro LED-based displays \nWaveguide-based displays \nNext generation displays \nWeek 6 \nAdvanced topics in XR II \u2013 Mobile AR \nComputer vision on limited hardware \nUX/UI challenges \nContent challenges \nProject due \n  \n \nAssessment \nA practical exercise, worth 20% of the mark. This will cover the basics of AR/VR development \nand some of the practical techniques the students will use in their projects. This is individual \nwork. No mobile device or GPU hours are needed. \nAn AR/VR project worth 80% of the marks: \nCourse projects will be designed by the students following the possible project themes proposed \nby the lecturer and will be checked by the lecturer for appropriateness. \nThe students will form groups of 2-3 to design, implement, report, and present the course \nproject. \nThe final mark will be an implementation mark (40%), a report mark (20%), and a \npresentation/demo mark (20%). Each team member will be evaluated based on their \ncontribution. \nEach project will have extensions to be completed only by the ACS students. Each student will \nwrite a different part of the report, whose author will be clearly marked. Each student will further \nsummarise her/his contributions to the project in the same report. \nNote: Submission is by one submission per group but work by each individual must be clearly \nlabelled.\n \n\nFEDERATED LEARNING: THEORY AND PRACTICE \n \nObjectives \nThis course aims to extend the machine learning knowledge available to students in Part I (or \npresent in typical undergraduate degrees in other universities), and allow them to understand \nhow these concepts can manifest in a decentralized setting. The course will consider both \ntheoretical (e.g., decentralized optimization) and practical (e.g., networking efficiency) aspects \nthat combine to define this growing area of machine learning.  \n \nAt the end of the course students should: \n \nUnderstand popular methods used in federated learning \nBe able to construct and scale a simple federated system \nHave gained an appreciation of the core limitations to existing methods, and the approaches \navailable to cope with these issues \nDeveloped an intuition for related technologies like differential privacy and secure aggregation, \nand are able to use them within typical federated settings  \nCan reason about the privacy and security issues with federated systems \nLectures \nCourse Overview. Introduction to Federated Learning. \nDecentralized Optimization. \nStatistical and Systems Heterogeneity. \nVariations of Federated Aggregation. \nSecure Aggregation. \nDifferential Privacy within Federated Systems. \nExtensions to Federated Analytics. \nApplications to Speech, Video, Images and Robotics. \nLab sessions \nFederating a Centralized ML Classifier. \nBehaviour under Heterogeneity. \nScaling a Federated Implementation. \nExploring Privacy with Federated Settings \nRecommended Reading \nReadings will be assigned for each lecture. Readings will be taken from either research papers, \ntutorials, source code or blogs that provide more comprehensive treatment of taught concepts. \n \nAssessment - Part II Students \nFour labs are performed during the course, and students receive 10% of their total grade for \nwork done as part of each lab. (For a total of 40% of the total grade from lab work alone). Labs \nwill primarily provide hands-on teaching opportunities, that are then utilized within the lab \nassignment which is completed outside of the lab contact time. MPhil and Part III students will \nbe given additional questions to answer within their version of the lab assignment which will \ndiffer from the assignment given to Part II CST students. \n \n\nThe remainder of the course grade (60%) will be given based on a hands-on project that applies \nthe concepts taught in lectures and labs. This hands-on project will be assessed based on upon \na combination of source code, related documentation and brief 8-minute pre-recorded talk that \nsummarizes key project elements (any slides used are also submitted as part of the project). \nPlease note, that in the case of Part II CST students, the talk itself is not examinable -- as such \nwill be made optional to those students. \n \nA range of possible practical projects will be described and offered to students to select from, or \nalternatively students may propose their own. MPhil and Part III students will select from a \nproject pool that is separate from those offered to Part II CST students. MPhil and Part III \nprojects will contain a greater emphasis on a research element, and the pre-recorded talks \nprovided by this student group will focus on this research contribution. The project will be \nassessed on the level of student understanding demonstrated, the degree of difficulty, \ncorrectness of implementation -- and for Part III/MPhil students the additional criteria of the \nquality and execution of the research methodology, and depth and quality of results analysis. \n \nThis project can be done individually or in groups -- although individual projects will be strongly \nencouraged. It will be required the project is performed using a code repository that also will \ncontain all documentation -- access to this repository will be shared with course staff (e.g., \nlecturer and TAs). Where needed, marks assigned to students within a group will be \ndifferentiated using this repository as an input. Furthermore if groups are formed, members must \nbe either entirely from Part III/MPhil students or Part II CST, i.e., these two student groups \nshould not mix to form a project group. \n \nProjects will be made available publicly. A maximum word count for written contributions for the \nproject will be enforced.\n \n\nMOBILE HEALTH \n \nAims \nThe course aims to explore how wearable and mobile systems sensors can be used to gather \ndata relevant to understand health, how the data can be analysed with advanced signal \nprocessing and machine learning and the performance of these systems in terms of diagnostics \nand disease progression detection. \n \nSyllabus \nCourse Overview. Introduction to Mobile Health. Evaluation metrics and methodology. Basics of \nSignal Processing. \nInertial Measurement Units, Human Activity Recognition (HAR) and Gait Analysis and Machine \nLearning for IMU data. \nRadios, Bluetooth, GPS and Cellular. Epidemiology and contact tracing, Social interaction \nsensing and applications. Location tracking in health monitoring and public health. \nAudio Signal Processing. Voice and Speech Analysis: concepts and data analysis. Body \nSounds analysis. \nPhotoplethysmogram and Light sensing for health (heart and sleep) \nContactless and wireless behaviour and physiological monitoring \nGenerative Models for Wearable Health Data \nTopical Guest Lectures \nObjectives \nThe course aims to explore how wearable and mobile systems sensors can be used to gather \ndata relevant to understand health, how the data can be analysed with advanced signal \nprocessing and machine learning and the performance of these systems in terms of diagnostics \nand disease progression detection. \n \nRoughly, each lecture contains a theory part about the working of \u201csensor signals\u201d or \u201cdata \nanalysis methods\u201d and an application part which contextualises the concepts. \n \nAt the end of the course students should: Understand how mobile/wearable sensors capture \ndata and their working. Understand different approaches to acquiring and analysing sensor data \nfrom different types of sensors. Understand the concept of signal processing applied to time \nseries data and their practical application in health. Be able to extract sensor data and analyse it \nwith basic signal processing and machine learning techniques. Be aware of the different health \napplications of the various sensor techniques. The course will also touch on privacy and ethics \nimplications of the approaches developed in an orthogonal fashion. \n \nRecommended Reading \nPlease see Course Materials for recommended reading for each session. \n \nAssessment - Part II students \nTwo assignments will be based on two datasets which will be provided to the students: \n \n\nAssignment 1 (shared for Part II and Part III/MPhil): this will be based on a dataset and will be \nworth 40% of the final mark. The task of the assessment will be to perform pre-processing and \nbasic data analysis in a \"colab\" and an answer sheet of no more than 1000 words. \n \nAssignment 2 (Part II): This assignment (worth 60% of the final mark) will be a fuller analysis of \na dataset focusing on machine learning algorithms and metrics. Discussion and interpretation of \nthe findings will be reported in a colab and a report of no more than 1200 words.\n \n\nMULTICORE SEMANTICS AND PROGRAMMING \n \nAims \nIn recent years multiprocessors have become ubiquitous, but building reliable concurrent \nsystems with good performance remains very challenging. The aim of this module is to \nintroduce some of the theory and the practice of concurrent programming, from hardware \nmemory models and the design of high-level programming languages to the correctness and \nperformance properties of concurrent algorithms. \n \nLectures \nPart 1: Introduction and relaxed-memory concurrency [Professor P. Sewell] \n \nIntroduction. Sequential consistency, atomicity, basic concurrent problems. [1 block] \nConcurrency on real multiprocessors: the relaxed memory model(s) for x86, ARM, and IBM \nPower, and theoretical tools for reasoning about x86-TSO programs. [2 blocks] \nHigh-level languages. An introduction to C/C++11 and Java shared-memory concurrency. [1 \nblock] \nPart 2: Concurrent algorithms [Dr T. Harris] \n \nConcurrent programming. Simple algorithms (readers/writers, stacks, queues) and correctness \ncriteria (linearisability and progress properties). Advanced synchronisation patterns (e.g. some \nof the following: optimistic and lazy list algorithms, hash tables, double-checked locking, RCU, \nhazard pointers), with discussion of performance and on the interaction between algorithm \ndesign and the underlying relaxed memory models. [3 blocks] \nResearch topics, likely to include one hour on transactional memory and one guest lecture. [1 \nblock] \nObjectives \nBy the end of the course students should: \n \nhave a good understanding of the semantics of concurrent programs, both at the multprocessor \nlevel and the C/Java programming language level; \nhave a good understanding of some key concurrent algorithms, with practical experience. \nAssessment - Part II Students \nTwo assignments each worth 50% \n \nRecommended reading \nHerlihy, M. and Shavit, N. (2008). The art of multiprocessor programming. Morgan Kaufmann.\n\nBUSINESS STUDIES SEMINARS \nAims \nThis course is a series of seminars by former members and friends of the Laboratory about their \nreal-world experiences of starting and running high technology companies. It is a follow on to \nthe Business Studies course in the Michaelmas Term. It provides practical examples and case \nstudies, and the opportunity to network with and learn from actual entrepreneurs. \n \nLectures \nEight lectures by eight different entrepreneurs. \n \nObjectives \nAt the end of the course students should have a better knowledge of the pleasures and pitfalls \nof starting a high tech company. \n \nRecommended reading \nLang, J. (2001). The high-tech entrepreneur\u2019s handbook: how to start and run a high-tech \ncompany. FT.COM/Prentice Hall. \nMaurya, A. (2012). Running Lean: Iterate from Plan A to a Plan That Works. O\u2019Reilly. \nOsterwalder, A. and Pigneur, Y. (2010). Business Model Generation: A Handbook for \nVisionaires, Game Changers, and Challengers. Wiley. \nKim, W. and Mauborgne, R. (2005). Blue Ocean Strategy. Harvard Business School Press. \n \nSee also the additional reading list on the Business Studies web page.\n \n\nHOARE LOGIC AND MODEL CHECKING \n \nAims \nThe course introduces two verification methods, Hoare Logic and Temporal Logic, and uses \nthem to formally specify and verify imperative programs and systems. \n \nThe first aim is to introduce Hoare logic for a simple imperative language and then to show how \nit can be used to formally specify programs (along with discussion of soundness and \ncompleteness), and also how to use it in a mechanised program verifier. \n \nThe second aim is to introduce model checking: to show how temporal models can be used to \nrepresent systems, how temporal logic can describe the behaviour of systems, and finally to \nintroduce model-checking algorithms to determine whether properties hold, and to find \ncounter-examples. \n \nCurrent research trends also will be outlined. \n \nLectures \nPart 1: Hoare logic. Formal versus informal methods. Specification using preconditions and \npostconditions. \nAxioms and rules of inference. Hoare logic for a simple language with assignments, sequences, \nconditionals and while-loops. Syntax-directedness. \nLoops and invariants. Various examples illustrating loop invariants and how they can be found. \nPartial and total correctness. Hoare logic for proving termination. Variants. \nSemantics and metatheory Mathematical interpretation of Hoare logic. Semantics and \nsoundness of Hoare logic. \nSeparation logic Separation logic as a resource-aware reinterpretation of Hoare logic to deal \nwith aliasing in programs with pointers. \nPart 2: Model checking. Models. Representation of state spaces. Reachable states. \nTemporal logic. Linear and branching time. Temporal operators. Path quantifiers. CTL, LTL, and \nCTL*. \nModel checking. Simple algorithms for verifying that temporal properties hold. \nApplications and more recent developments Simple software and hardware examples. CEGAR \n(counter-example guided abstraction refinement). \nObjectives \nAt the end of the course students should \n \nbe able to prove simple programs correct by hand and implement a simple program verifier; \nbe familiar with the theory and use of separation logic; \nbe able to write simple models and specify them using temporal logic; \nbe familiar with the core ideas of model checking, and be able to implement a simple model \nchecker. \nRecommended reading \n\nHuth, M. and Ryan M. (2004). Logic in Computer Science: Modelling and Reasoning about \nSystems. Cambridge University Press (2nd ed.).\n \n\n",
        "7th Semester \nADVANCED TOPICS IN COMPUTER ARCHITECTURE \nAims \nThis course aims to provide students with an introduction to a range of advanced topics in \ncomputer architecture. It will explore the current and future challenges facing the architects of \nmodern computers. These will also be used to illustrate the many different influences and \ntrade-offs involved in computer architecture. \n \nObjectives \nOn completion of this module students should: \n \nunderstand the challenges of designing and verifying modern microprocessors \nbe familiar with recent research themes and emerging challenges \nappreciate the complex trade-offs at the heart of computer architecture \nSyllabus \nEach seminar will focus on a different topic. The proposed topics are listed below but there may \nbe some minor changes to this: \n \nTrends in computer architecture \nState-of-the-art microprocessor design \nMemory system design \nHardware reliability \nSpecification, verification and test (may be be replaced with a different topic) \nHardware security (2) \nHW accelerators and accelerators for machine learning \nEach two hour seminar will include three student presentations (15mins) questions (5mins) and \na broader discussion of the topics (around 30mins). The last part of the seminar will include a \nshort scene setting lecture (around 20mins) to introduce the following week's topic. \n \nAssessment \nEach week students will compare and contrast two of the main papers and submit a written \nsummary and review in advance of each seminar (except when presenting). \n \nStudents will be expected to give a number of 15 minute presentations. \n \nEssays and presentations will be marked out of 10. After dropping the lowest mark, the \nremaining marks will be scaled to give a final score out of 100. \n \nStudents will give at least one presentation during the course. They will not be required to \nsubmit an essay during the weeks they are presenting. \n \nEach presentation will focus on a single paper from the reading list. Marks will be awarded for \nclarity and the communication of the paper's key ideas, an analysis of the work's strengths and \nweaknesses and the work\u2019s relationship to related work and broader trends and constraints. \n\n \nRecommended prerequisite reading \nPatterson, D. A., Hennessy, J. L. (2017). Computer organization and design: The \nHardware/software interface RISC-V edition Morgan Kaufmann. ISBN 978-0-12-812275-4. \n \nHennessy, J. and Patterson, D. (2012). Computer architecture: a quantitative approach. Elsevier \n(5th ed.) ISBN 9780123838728. (the 3rd and 4th editions are also relevant)\n \n\nADVANCED TOPICS IN PROGRAMMING LANGUAGES \nAims \nThis module explores various topics in programming languages beyond the scope of \nundergraduate courses. It aims to introduce students to ideas, results and techniques found in \nthe literature and prepare them for research in the field. \n \nSyllabus and structure \nThe module consists of eight two-hour seminars, each on a particular topic. Topics will vary from \nyear to year, but may include, for example, \n \nAbstract interpretation \nVerified software \nMetaprogramming \nBehavioural types \nProgram synthesis \nVerified compilation \nPartial evaluation \nGarbage collection \nDependent types \nAutomatic differentiation \nDelimited continuations \nModule systems \nThere will be three papers assigned for each topic, which students are expected to read before \nthe seminar. \nEach seminar will include three 20 minute student presentations (15 minutes + 5 minutes \nquestions), time for general discussion of the topic, and a brief overview lecture for the following \nweek\u2019s topic. \nBefore each seminar, except in weeks in which they give presentations, students will submit a \nshort essay about two of the papers. \n \n \nObjectives \nOn completion of this module, students should \n \nbe able to identify some major themes in programming language research \nbe familiar with some classic papers and recent advances \nhave an understanding of techniques used in the field \n \nAssessment \nAssessment consists of: \n \nPresentation of one of the papers from the reading list (typically once or twice in total for each \nstudent, depending on class numbers) \nOne essay per week (except on the first week and on presentation weeks) \n\nAll essays and presentations carry equal numbers of marks. \n \nEssay marks are awarded for understanding, for insight and analysis, and for writing quality. \nEssays should be around 1500 words (with a lower limit of 1450 and upper limit of 1650). \nPresentation marks are awarded for clarity, for effective communication, and for selection and \norganisation of topics. \n \nThere will be seven submissions (essays or presentations) in total and, as in other courses, the \nlowest mark for each student will be disregarded when computing the final mark. \n \nMarking, deadlines and extensions will be handled in accordance with the MPhil Assessment \nGuidelines. \n \nRecommended reading material and resources \nResearch and survey papers from programming language conferences and journals (e.g. \nPOPL, PLDI, TOPLAS, FTPL) will be assigned each week. General background material may \nbe found in: \n \n\u2022 Types and Programming Languages (Benjamin C. Pierce) \n   The MIT Press \n   ISBN 0-262-16209-1 \n \n\u2022 Advanced Topics in Types and Programming Languages (ed. Benjamin C. Pierce) \n   The MIT Press \n   ISBN 0-262-16228-8 \n \n\u2022 Practical Foundations for Programming Languages (Robert Harper) \n   Cambridge University Press \n   9781107150300\n \n\nAFFECTIVE ARTIFICIAL INTELLIGENCE \n \nSynopsis \nAffective Artificial Intelligence (Affective AI) aims to imbue machines with social and emotional \nintelligence (EQ). More specifically, Affective AI aims to create artificially intelligent systems and \nmachines that can recognize, interpret, process, and simulate human social signals and \nbehaviours, expressions, and emotions, to enhance human-AI interaction and communication. \n\\r\\n\\r\\nTo achieve this goal, Affective AI draws upon various scientific disciplines, including \nmachine learning, computer vision, speech / natural language / signal processing, psychology \nand cognitive science, and ethics and social sciences. \n \n  \nBackground & Aims \nAffective Artificial Intelligence (Affective AI) aims to imbue machines with social and emotional \nintelligence (EQ). More specifically, Affective AI aims to create artificially intelligent systems and \nmachines that can recognize, interpret, process, and simulate human social signals and \nbehaviours, expressions, and emotions, to enhance human-AI interaction and communication.  \n \nTo achieve this goal, Affective AI draws upon various scientific disciplines, including machine \nlearning, computer vision, speech / natural language / signal processing, psychology and \ncognitive science, and ethics and social sciences. \n \nAffective AI has direct applications in and implications for the design of innovative interactive \ntechnology (e.g., interaction with chat bots, virtual agents, robots), single and multi-user smart \nenvironments ( e.g., in-car/ virtual / augmented / mixed reality, serious games), public speaking \nand cognitive training, and clinical and biomedical studies (e.g., autism, depression, pain). \n \nThe aim of this module is to impart knowledge and ability needed to make informed choices of \nmodels, data, and machine learning techniques for sensing, recognition, and generation of \naffective and social behaviour (e.g., smile, frown, head nodding/shaking, \nagreement/disagreement) in order to create Affectively intelligent AI systems, with a \nconsideration for various ethical issues (e.g., privacy, bias) arising from the real-world \ndeployment of these systems. \n \n  \n \nSyllabus \nThe following list provides a representative list of topics: \n \nIntroduction, definitions, and overview \nTheories from various disciplines \nSensing from multiple modalities (e.g., vision, audio, bio signals, text) \nData acquisition and annotation \nSignal processing / feature extraction \n\nLearning / prediction / recognition and evaluation \nBehaviour synthesis / generation (e.g., for embodied agents / robots) \nAdvanced topics and ethical considerations (e.g., bias and fairness) \nCross-disciplinary applications (via seminar presentations and discussions) \nGuest lectures (diverse topics \u2013 changes each year) \nHands-on research and programming work (i.e., mini project & report)  \n \n  \n \nObjectives \nOn completion of this module, students will: \n \nunderstand and demonstrate knowledge in key characteristics of affectively intelligent AI, which \ninclude: \nRecognition: How to equip Affective AI systems with capabilities of analysing facial expressions, \nvocal intonations, gestures, and other physiological signals to infer human affective states? \nGeneration: How to enable affectively intelligent AI simulate expressions of emotions in \nmachines, allowing them to respond in a more human-like manner, such as virtual agents and \nhumanoid robots, showing empathy or sympathy? \nAdaptation and Personalization: How to enable affectively intelligent AI adapt system responses \nbased on users' affective states and/or needs, or tailor interactions and experiences to individual \nusers based on their expressivity / emotional profiles, personalities, and past interactions? \nEmpathetic Communication: How to design Affective AI systems to communicate with users in a \nway that demonstrates empathy, understanding, and sensitivity to their affective states and \nneeds? \nEthical and societal considerations: What are the various human differences, ethical guidelines, \nand societal impacts that need to be considered when designing and deploying Affective AI to \nensure that these systems respect users' privacy, autonomy, and well-being? \ncomprehend and apply (appropriate) methods for collection, analysis, representation, and \nevaluation of human affective and communicative behavioural data. \nenhance programming skills for creating and implementing (components of) Affective AI \nsystems. \ndemonstrate critical thinking, analysis and synthesis while deciding on 'when' and 'how' to \nincorporate human affect and social signals in a specific AI system context and gain practical \nexperience in proposing and justifying computational solution(s) of suitable nature and scope. \n  \n \nAssessment - Part II Students \nSeminar presentation: 20% \nParticipating in Q&A and discussions: 10% \nMid-term mini project report and presentation (as a team of 2): 10%  \nFinal mini project report & code (as a team of 2): 60% \n  \n \n\nAssessment - MPhil / Part III Students \nSeminar presentation: 20% \nParticipating in Q&A and discussions: 10% \nMid-term mini project report and presentation: 10%  \nFinal mini project report & code: 60% \nFurther Information \nCurrent Cambridge undergraduate students who are continuing onto Part III or the MPhil in \nAdvanced Computer Science may only take this module if they did NOT take it as a Unit of \nAssessment in Part II. \n \nThis module is shared with ACS. Assessment will be adjusted for the two groups of students to \nbe at an appropriate level for whichever course the student is enrolled on. More information will \nfollow at the first lecture. \n \nThis module is shared with Part II of the Computer Science Tripos. Assessment will be adjusted \nfor the two groups of students to be at an appropriate level for whichever course the student is \nenrolled on. Further information about assessment and practicals will follow at the first lecture.\n\nCATEGORY THEORY \n \nAims \nCategory theory provides a unified treatment of mathematical properties and constructions that \ncan be expressed in terms of 'morphisms' between structures. It gives a precise framework for \ncomparing one branch of mathematics (organized as a category) with another and for the \ntransfer of problems in one area to another. Since its origins in the 1940s motivated by \nconnections between algebra and geometry, category theory has been applied to diverse fields, \nincluding computer science, logic and linguistics. This course introduces the basic notions of \ncategory theory: adjunction, natural transformation, functor and category. We will use category \ntheory to organize and develop the kinds of structure that arise in models and semantics for \nlogics and programming languages. \n \nSyllabus \nIntroduction; some history. Definition of category. The category of sets and functions. \nCommutative diagrams. Examples of categories: preorders and monotone functions; monoids \nand monoid homomorphisms; a preorder as a category; a monoid as a category. Definition of \nisomorphism. Informal notion of a 'category-theoretic' property. \nTerminal objects. The opposite of a category and the duality principle. Initial objects. Free \nmonoids as initial objects. \nBinary products and coproducts. Cartesian categories. \nExponential objects: in the category of sets and in general. Cartesian closed categories: \ndefinition and examples. \nIntuitionistic Propositional Logic (IPL) in Natural Deduction style. Semantics of IPL in a cartesian \nclosed preorder. \nSimply Typed Lambda Calculus (STLC). The typing relation. Semantics of STLC types and \nterms in a cartesian closed category (ccc). The internal language of a ccc. The \nCurry-Howard-Lawvere correspondence. \nFunctors. Contravariance. Identity and composition for functors. \nSize: small categories and locally small categories. The category of small categories. Finite \nproducts of categories. \nNatural transformations. Functor categories. The category of small categories is cartesian \nclosed. \nHom functors. Natural isomorphisms. Adjunctions. Examples of adjoint functors. Theorem \ncharacterising the existence of right (respectively left) adjoints in terms of a universal property. \nDependent types. Dependent product sets and dependent function sets as adjoint functors. \nEquivalence of categories. Example: the category of I-indexed sets and functions is equivalent \nto the slice category Set/I. \nPresheaves. The Yoneda Lemma. Categories of presheaves are cartesian closed. \nMonads. Modelling notions of computation as monads. Moggi's computational lambda calculus. \nObjectives \nOn completion of this module, students should: \n \n\nbe familiar with some of the basic notions of category theory and its connections with logic and \nprogramming language semantics \nAssessment \na graded exercise sheet (25% of the final mark), and \na take-home test (75%) \nRecommended reading \nAwodey, S. (2010). Category theory. Oxford University Press (2nd ed.). \n \nCrole, R. L. (1994). Categories for types. Cambridge University Press. \n \nLambek, J. and Scott, P. J. (1986). Introduction to higher order categorical logic. Cambridge \nUniversity Press. \n \nPitts, A. M. (2000). Categorical Logic. Chapter 2 of S. Abramsky, D. M. Gabbay and T. S. E. \nMaibaum (Eds) Handbook of Logic in Computer Science, Volume 5. Oxford University Press. \n(Draft copy available here.) \n \nClass Size \nThis module can accommodate upto 15 Part II students plus 15 MPhil / Part III students. \n \nPre-registration evaluation form  \nStudents who are interested in taking this module will be asked to complete a pre-registration \nevaluation form to determine whether they have enough mathematical background to take the \nmodule.  \n \nThis will take place during the week Monday 12 August - Friday 16 August.  \n \nCoursework \nThe coursework in this module will consist of a graded exercise sheet. \n \nFurther Information \nCurrent Cambridge undergraduate students who are continuing onto Part III or the MPhil in \nAdvanced Computer Science may only take this module if they did NOT take 'Catgeory Theory' \nas a Unit of Assessment in Part II. \n \nThis module is shared with Part II of the Computer Science Tripos. Assessment will be adjusted \nfor the two groups of students to be at an appropriate level for whichever course the student is \nenrolled on. Further information about assessment and practicals will follow at the first lecture.\n\nDIGITAL MONEY AND DECENTRALISED FINANCE \n \nAims \nOur society is evolving towards digital payments and the elimination of physical cash. Digital \nalternatives to cash require trade-offs between various properties including unforgeability, \ntraceability, divisibility, transferability without intermediaries, privacy, redeemability, control over \nthe money supply and many more, often in conflict with each other. Since the 1980s, \ncryptographers have proposed a variety of clever technical solutions addressing some of these \nissues but it is only with the appearance of Bitcoin, blockchain and a plethora of copycat \ncryptocurrencies that a new asset class has now emerged, worth in excess of two trillion dollars \n(although plagued by extreme volatility). Meanwhile, most major countries have been planning \nthe introduction of Central Bank Digital Currencies in an attempt to retain control. This \nseminar-style interactive module encourages the students to understand what's happening, \nwhat the underlying problems and opportunities are (technical, social and economic) and where \nwe should go from here. We are at a historical turning point where an enterprising candidate \nmight disrupt the status quo and truly make a difference. \n \nSyllabus \nUnderstanding money: from gold standard to digital cash \nIntro to the Decentralised Finance (DeFi) ecosystem: Bitcoin, blockchain, wallets, smart \ncontracts \nStablecoins and Central Bank Digital Currencies \nDecentralised Autonomous Organisations and Automated Market Makers \nYield Farming, Perpetual Futures and Maximal Extractable Value \nWeb3, Non Fungible Tokens \nCryptocurrency crashes; crimes facilitated by cryptocurrencies \nNew areas of development. Where from here? \nThe module is structured as a reading club with a high degree of interactivity. Aside from the first \nand last session, which are treated differently, the regular two-hour sessions have the following \nstructure (not necessarily in this order). \n \nTwo half-hour debates on each of two papers \nTwo rapid-fire presentations on open questions previously assigned by the lecturers. \nHalf an hour of presentation by the lecturers. \nAbout ten minutes of buffer that can account for switchover time or be absorbed into the \nlecturers' presentation. \nThe first session is scene-setting by the lecturers, with no student presentations. The last \nsession has no paper debates, only one rapid-fire presentation on an open question from each \nof the students. \n \nObjectives \nOn completion of this module, students will gain an in-depth understanding of digital money \nalternatives as well as many modern applications and components of Decentralised Finance. By \ncomparing DeFi with current processes in traditional banking, students will be able to discuss \n\nthe merits and limitations of these new technologies as well as to identify potential new areas of \ndevelopment. \n \nThrough having to write, present and defend very brief extended abstracts, the students will \nimprove their communication skills and in particular the ability to distil and convey the most \nimportant points forcefully and concisely. \n \n \nAssessment \nThe module is an interactive reading club. Each student produces four output triplets throughout \nthe course. Each output triplet consists of three parts: a 1200-word essay, a 200-word extended \nabstract and a 7-minute presentation. \n \nThe essay and abstract must be submitted in advance, using the LaTeX templates provided. \nThe abstract must consist of full sentences, not bullet points, and must fit on a single slide. \nDuring the presentation, the slide with the abstract will be projected on screen as the only visual \naid; the essay will not be accessible to either presenter or audience. Reading out the slide \nverbatim will obviously not count as a great presentation. Presenters must be able to expand \nand defend their ideas live, and answer questions from lecturers and audience. \n \nThe essays, the abstracts and the presentations are graded separately, each out of 10, and \nassigned weights of 60%, 20%, 20%, respectively, within the triplet. \n \nIn a regular 2-hour session (all but the first and last) there will be two paper slots and two \ninvestigation slots, plus a lecturer slot, described next. \n \nEach paper slot (30 min) involves a \u201csupporter\u201d and a \u201cdissenter\u201d for the paper, each \ncontributing one output triplet, and then a panel Q&A where all the other participants quiz the \nsupporter and dissenter. \n \nEach investigation slot (10 min) involves a single student contributing an output triplet on a \ngiven theme. \n \nRoles, papers and themes are assigned by the lecturers the previous week. \n \nIn the first session there will be no student presentations, only lecturer-led exploration. In the \nlast session there will be 12 investigation slots and no paper slots. \n \nOverall, each student will be supporter once, dissenter once and investigator twice, for a total of \n4 output triplets. The output triplets of the last session will be weighed twice as much as the \nothers, i.e. 40% each, while the others 20% each. \n \nAttendance is required at all sessions. Missing a session in which the candidate was due to \npresent will result in the loss of the corresponding presentation marks, while the written work will \n\nstill have to be submitted and will be marked as usual. Missing a session in which the candidate \nwas not due to present will result in the loss of 3%. \n \nRecommended reading \nThere isn't a textbook covering all the material in this course. The following textbook, also \nadopted by R47 Distributed Ledger Technologies (also freely available online), is a thorough \nintroduction to Bitcoin and Blockchain, but we will complement it with research papers and \nindustry white papers for each lecture. \n \nNarayanan, A. , Bonneau, J., Felten, E., Miller, A. and Goldfeder, S. (2016). Bitcoin and \nCryptocurrency Technologies: A Comprehensive Introduction. Princeton University Press. \n(2016 Draft available here: \nhttps://d28rh4a8wq0iu5.cloudfront.net/bitcointech/readings/princeton_bitcoin_book.pdf)\n \n\nDIGITAL SIGNAL PROCESSING \n \nAims \nThis course teaches basic signal-processing principles necessary to understand many modern \nhigh-tech systems, with examples from audio processing, image coding, radio communication, \nradar, and software-defined radio. Students will gain practical experience from numerical \nexperiments in programming assignments (in Julia, MATLAB or NumPy). \n \nLectures \nSignals and systems. Discrete sequences and systems: types and properties. Amplitude, \nphase, frequency, modulation, decibels, root-mean square. Linear time-invariant systems, \nconvolution. Some examples from electronics, optics and acoustics. \nPhasors. Eigen functions of linear time-invariant systems. Review of complex arithmetic. \nPhasors as orthogonal base functions. \nFourier transform. Forms and properties of the Fourier transform. Convolution theorem. Rect \nand sinc. \nDirac\u2019s delta function. Fourier representation of sine waves, impulse combs in the time and \nfrequency domain. Amplitude-modulation in the frequency domain. \nDiscrete sequences and spectra. Sampling of continuous signals, periodic signals, aliasing, \ninterpolation, sampling and reconstruction, sample-rate conversion, oversampling, spectral \ninversion. \nDiscrete Fourier transform. Continuous versus discrete Fourier transform, symmetry, linearity, \nFFT, real-valued FFT, FFT-based convolution, zero padding, FFT-based resampling, \ndeconvolution exercise. \nSpectral estimation. Short-time Fourier transform, leakage and scalloping phenomena, \nwindowing, zero padding. Audio and voice examples. DTFM exercise. \nFinite impulse-response filters. Properties of filters, implementation forms, window-based FIR \ndesign, use of frequency-inversion to obtain high-pass filters, use of modulation to obtain \nband-pass filters. \nInfinite impulse-response filters. Sequences as polynomials, z-transform, zeros and poles, some \nanalog IIR design techniques (Butterworth, Chebyshev I/II, elliptic filters, second-order cascade \nform). \nBand-pass signals. Band-pass sampling and reconstruction, IQ up and down conversion, \nsuperheterodyne receivers, software-defined radio front-ends, IQ representation of AM and FM \nsignals and their demodulation. \nDigital communication. Pulse-amplitude modulation. Matched-filter detector. Pulse shapes, \ninter-symbol interference, equalization. IQ representation of ASK, BSK, PSK, QAM and FSK \nsignals. [2 hours] \nRandom sequences and noise. Random variables, stationary and ergodic processes, \nautocorrelation, cross-correlation, deterministic cross-correlation sequences, filtered random \nsequences, white noise, periodic averaging. \nCorrelation coding. Entropy, delta coding, linear prediction, dependence versus correlation, \nrandom vectors, covariance, decorrelation, matrix diagonalization, eigen decomposition, \n\nKarhunen-Lo\u00e8ve transform, principal component analysis. Relation to orthogonal transform \ncoding using fixed basis vectors, such as DCT. \nLossy versus lossless compression. What information is discarded by human senses and can \nbe eliminated by encoders? Perceptual scales, audio masking, spatial resolution, colour \ncoordinates, some demonstration experiments. \nQuantization, image coding standards. Uniform and logarithmic quantization, A/\u00b5-law coding, \ndithering, JPEG. \nObjectives \napply basic properties of time-invariant linear systems; \nunderstand sampling, aliasing, convolution, filtering, the pitfalls of spectral estimation; \nexplain the above in time and frequency domain representations; \nuse filter-design software; \nvisualize and discuss digital filters in the z-domain; \nuse the FFT for convolution, deconvolution, filtering; \nimplement, apply and evaluate simple DSP applications; \nfamiliarity with a number of signal-processing concepts used in digital communication systems \nRecommended reading \nLyons, R.G. (2010). Understanding digital signal processing. Prentice Hall (3rd ed.). \nOppenheim, A.V. and Schafer, R.W. (2007). Discrete-time digital signal processing. Prentice \nHall (3rd ed.). \nStein, J. (2000). Digital signal processing \u2013 a computer science perspective. Wiley. \n \nClass size \nThis module can accommodate a maximum of 24 students (16 Part II students and 8 MPhil \nstudents) \n \nAssessment - Part II students \nThree homework programming assignments, each comprising 20% of the mark \nWritten test, comprising 40% of the total mark. \nAssessment - Part III and MPhil students \nThree homework programming exercises, each comprising 10% of the mark. \nWritten test, comprising 20% of the total mark. \nIndividual implementation project with 8-page written report, topic agreed with lecturer, \ncomprising 50% of the mark. \nFurther Information \nCurrent Cambridge undergraduate students who are continuing onto Part III or the MPhil in \nAdvanced Computer Science may only take this module if they did NOT take it as a Unit of \nAssessment in Part II. \n \nThis module is shared with Part II of the Computer Science Tripos. Assessment will be adjusted \nfor the two groups of students to be at an appropriate level for whichever course the student is \nenrolled on. Further information about assessment and practicals will follow at the first lecture.\n\nINTRODUCTION TO COMPUTATIONAL SEMANTICS \n \nAims \nThis is a lecture-style course that introduces students to various aspects of the semantics of \nNatural Languages (mainly English): \n \nLexical Semantics, with an emphasis on theory and phenomenology  \nCompositional Semantics \nDiscourse and pragmatics-related aspects of semantics \nObjectives \nGive an operational definition of what is meant by \u201cmeaning\u201d (for instance, above and beyond \nsyntax); \nName the types of phenomena in language that require semantic consideration, in terms of \nlexical, compositional and discourse/pragmatic aspects, in other words, argue why semantics is \nimportant; \nDemonstrate an understanding of the basics of various semantic representations, including \nlogic-based and graph-based semantic representations, their properties, how they are used and \nwhy they are important, and how they are different from syntactic representations; \nKnow how such semantic representations are derived during or after parsing, and how they can \nbe analysed and mapped to surface strings; \nUnderstand applications of semantic representations e.g. reasoning, validation, and methods \nhow these are approached. \nWhen designing NL tasks that clearly require semantic processing (e.g. knowledge-based QA), \nto be aware of and reuse semantic representations and algorithms when designing the task, \nrather than reinventing the wheel. \nPractical advantages of this course for NLP students \nKnowledge of underlying semantic effects helps improve NLP evaluation, for instance by \nproviding more meaningful error analysis. You will be able to link particular errors to design \ndecisions inside your system. \nYou will learn methods for better benchmarking of your system, whatever the task may be. \nSupervised ML systems (in particular black-box systems such as Deep Learning) are only as \nclever as the datasets they are based on. In this course, you will learn to design datasets so that \nthey are harder to trick without real understanding, or critique existing datasets. \nYou will be able to design tests for ML systems that better pinpoint which aspects of language \nan end-to-end system has \u201cunderstood\u201d. \nYou will learn to detect ambiguity and ill-formed semantics in human-human communication. \nThis can serve to write more clearly and logically. \nYou will learn about decomposing complex semantics-reliant tasks sensibly so that you can \nreuse the techniques underlying semantic analyzers in a modular way. In this way, rather than \nbeing forced to treat complex tasks in an end-to-end manner, you will be able to profit from \npartial explanations and a better error analysis already built into the system. \nSyllabus \nOverview of the course \nEvents and semantic role labelling \n\nReferentiality and coreference resolution \nTruth-conditional semantics \nGraph-based meaning representation \nCompositional semantics \nContext-free Graph Rewriting \nSurface realisation \nNegation and psychological approach to semantics \nDynamic semantics \nGricean pragmatics \nVector space models \nCross-modality \nAcquisition of semantics \nDiachronic change of semantics \nSummary of the course \n  \n \nAssessment \n5 take-home exercises worth 20% each: \n \nTake-home assignment 1 is given in Lecture 4 and due is Lecture 6 \n \nStudents are given 10 English sentences and asked to provide their semantic analysis \naccording to truth-conditions. Students will be expected to return 10 logical expressions as their \nanswers. No word limit. Assessment criteria: correctness of the 10 logical expressions; 2 points \nfor each logical expression. \n \nTake-home assignment 2 is given in Lecture 7 and due is Lecture 9  \n \nStudents are given 10 English sentences and asked to provide their syntactico-semantic \nderivations according to the compositionality principle. Students will be expected to return \nderivation graphs as their answers. No word limit. Assessment criteria: correctness of the 10 \nderivation graphs; 2 points for each derivation. \n \nTake-home assignment 3 is given in Lecture 11 and due is Lecture 13  \n \nAll students are assigned with a paper on modelling common ground in dialogue system. \nStudents will receive related but different papers. Each student will write a review of their \nassigned paper, including a comprehensive summary and their own thoughts. Word limit: 1000 \nwords. Assessment criteria: 15 points on whether a student understands the paper correctly; 5 \npoints on whether a student is able to think critically. \n \nTake-home assignment 4 is given in Lecture 13 and due is Lecture 15 \n \n\nAll students are assigned with a paper on language-vision interaction. Students will receive \nrelated but different papers. Each student will write a review of their assigned paper, including a \ncomprehensive summary and their own thoughts. Word limit: 1000 words. Assessment criteria: \n15 points on whether a student understands the paper correctly; 5 points on whether a student \nis able to think critically. \n \nTake-home assignment 5 is given in Lecture 14 and due is two weeks later. \n \nContent: All students are assigned with a paper on bootstrapping language acquisition. All \nstudents will receive the same paper. Each student will write a review of the paper, including a \ncomprehensive summary and their own thoughts. Word limit: 1000 words. Assessment criteria: \n15 points on whether a student understands the paper correctly; 5 points on whether a student \nis able to think critically.\n \n\nINTRODUCTION TO NATURAL LANGUAGE SYNTAX AND PARSING \n \nAims \nThis module aims to provide a brief introduction to linguistics for computer scientists and then \ngoes on to cover some of the core tasks in natural language processing (NLP), focussing on \nstatistical parsing of sentences to yield syntactic representations. We will look at how to \nevaluate parsers and see how well state-of-the-art tools perform given current techniques. \n \nSyllabus \nLinguistics for NLP focusing on morphology and syntax \nGrammars and representations \nStatistical and neural parsing \nTreebanks and evaluation \nObjectives \nOn completion of this module, students should: \n \nunderstand the basic properties of human languages and be familiar with descriptive and \ntheoretical frameworks for handling these properties; \nunderstand the design of tools for NLP tasks such as parsing and be able to apply them to text \nand evaluate their performance; \nPractical work \nWeek 1-7: There will be non-assessed practical exercises between sessions and during \nsessions. \nWeek 8: Download and apply two parsers to a designated text. Evaluate the performance of the \ntools quantitatively and qualitatively. \nAssessment \nThere will be one presentation worth 10% of the final mark; presentation topics will be allocated \nat the start of term. \nAn assessed practical report of not more 8 pages in ACL conference format. It will contribute \n90% of the final mark. \nRecommended reading \nJurafsky, D. and Martin, J. (2008). Speech and Language Processing. Prentice-Hall (2nd ed.). \n(See also 3rd ed. available online.)\n \n\nINTRODUCTION TO NETWORKING AND SYSTEMS MEASUREMENTS \n \nAims \nSystems research refers to the study of a broad range of behaviours arising from complex \nsystem design, including: resource sharing and scheduling; interactions between hardware and \nsoftware; network topology, protocol and device design and implementation; low-level operating \nsystems; Interconnect, storage and more. This module will: \n \nTeach performance characteristics and performance measurement methodology and practice \nthrough profiling experiments; \nExpose students to real-world systems artefacts evident through different measurement tools; \nDevelop scientific writing skills through a series of laboratory reports; \nProvide research skills for characterization and modelling of systems and networks using \nmeasurements. \nPrerequisites \nIt is strongly recommended that students have previously (and successfully) completed an \nundergraduate networking course -- or have equivalent experience through project or \nopen-source work. \n \nSyllabus \nIntroduction to performance measurements, performance characteristics [1 lecture] \nPerformance measurements tools and techniques [2 lectures + 2 lab sessions] \nReproducible experiments [1 lecture + 1 lab session] \nCommon pitfalls in measurements [1 lecture] \nDevice and system characterisation [1 lecture + 2 lab sessions] \nObjectives \nOn completion of this module, students should: \n \nDescribe the objectives of measurements, and what they can achieve; \nCharacterise and model a system using measurements; \nPerform reproducible measurements experiments; \nEvaluate the performance of a system using measurements; \nOperate measurements tools and be aware of their limitations; \nDetect anomalies in the network and avoid common measurements pitfalls; \nWrite system-style performance evaluations. \nPractical work \nFive 2-hour in-classroom labs will ask students to develop and use skills in performance \nmeasurements as applied to real-world systems and networking artefacts. Results from these \nlabs (and follow-up work by students outside of the classroom) will by the primary input to lab \nreports. \n \nThe first three labs will provide an introduction and hands on experience with measurement \ntools and measurements methodologies, while the last two labs will focus on practical \nmeasurements and evaluation of specific platforms. Students may find it useful to work in pairs \n\nwithin the lab, but must prepare lab reports independently. The module lecturer will give a short \nintroductory at the start of each lab, and instructors will be on-hand throughout labs to provide \nassistance. \n \nLab participation is not directly included in the final mark, but lab work is a key input to lab \nreports that are assessed. Guided lab experiments resulting in practical write ups. \n \nAssessment \nEach student will write two lab reports. The first lab report will summarise the experiments done \nin the first three labs (20%). The second will be a lab report (5000 words) summarising the \nevaluation of a device or a system (80%). \n \nRecommended reading \nThe following list provides some background to the course materials, but is not mandatory. A \nreading list, including research papers, will be provided in the course materials. \n \nGeorge Varghese. Network algorithmics. Chapman and Hall/CRC, 2010. \nMark Crovella and Balachander Krishnamurthy. Internet measurement: infrastructure, traffic and \napplications. John Wiley and Sons, Inc., 2006. \nBrendan Gregg. Systems Performance: Enterprise and the Cloud, Prentice Hall Press, Upper \nSaddle River, NJ, USA, October 2013. \nRaj Jain, The Art of Computer Systems Performance Analysis: Techniques for Experimental \nDesign, Measurement, Simulation, and Modeling, Wiley - Interscience, New York, NY, USA, \nApril 1991.\n \n\nLARGE-SCALE DATA PROCESSING AND OPTIMISATION \n \nAims \nThis module provides an introduction to large-scale data processing, optimisation, and the \nimpact on computer system's architecture. Large-scale distributed applications with high volume \ndata processing such as training of machine learning will grow ever more in importance. \nSupporting the design and implementation of robust, secure, and heterogeneous large-scale \ndistributed systems is essential. To deal with distributed systems with a large and complex \nparameter space, tuning and optimising computer systems is becoming an important and \ncomplex task, which also deals with the characteristics of input data and algorithms used in the \napplications. Algorithm designers are often unaware of the constraints imposed by systems and \nthe best way to consider these when designing algorithms with massive volume of data. On the \nother hand, computer systems often miss advances in algorithm design that can be used to cut \ndown processing time and scale up systems in terms of the size of the problem they can \naddress. Integrating machine learning approaches (e.g. Bayesian Optimisation, Reinforcement \nLearning) for system optimisation will be explored in this course. \n \nSyllabus \nThis course provides perspectives on large-scale data processing, including data-flow \nprogramming, graph data processing, probabilistic programming and computer system \noptimisation, especially using machine learning approaches, thus providing a solid basis to work \non the next generation of distributed systems. \n \nThe module consists of 8 sessions, with 5 sessions on specific aspects of large-scale data \nprocessing research. Each session discusses 3-4 papers, led by the assigned students. One \nsession is a hands-on tutorial on on dataflow programming fundamentals. The first session \nadvises on how to read/review a paper together with a brief introduction on different \nperspectives in large-scale data \nprocessing and optimisation. The last session is dedicated to the student presentation of \nopensource project studies. \n \nIntroduction to large-scale data processing and optimisation \nData flow programming: Map/Reduce to TensorFlow \nLarge-scale graph data processing: storage, processing model and parallel processing \nDataflow programming hands-on tutorial \nProbabilistic Programming \nOptimisations in ML Compiler \nOptimisations of Computer systems using ML \nPresentation of Open Source Project Study \nObjectives \nOn completion of this module, students should: \n \nUnderstand key concepts of scalable data processing approaches in future computer systems. \n\nObtain a clear understanding of building distributed systems using data centric programming \nand large-scale data processing. \nUnderstand a large and complex parameter space in computer system's optimisation and \napplicability of Machine Learning approach. \nCoursework \nReading Club: \nThe preparation for the reading club will involve 1-3 papers every week. At each session, \naround 3-4 papers are selected under the given topic, and the students present their review \nwork. \nHands-on tutorial session of data flow programming including writing an application of \nprocessing streaming in Twitter data and/or Deep Neural Networks using Google TensorFlow \nusing cluster computing. \nReports \nThe following three reports are required, which could be extended from the assignment of the \nreading club, within the scope of data centric systems. \n \nReview report on a full length paper (max 1800 words) \nDescribe the contribution of the paper in depth with criticisms \nCrystallise the significant novelty in contrast to other related work \nSuggestions for future work \nSurvey report on sub-topic in large-scale data processing and optimisation (max 2000 words) \nPick up to 5 papers as core papers in the survey scope \nRead the above and expand reading through related work \nComprehend the view and finish an own survey paper \nProject study and exploration of a prototype (max 2500 words) \nWhat is the significance of the project in the research domain? \nCompare with similar and succeeding projects \nDemonstrate the project by exploring its prototype \nReports 1 should be handed in by the end of 5th week; Report 2 in the 8th week and Report 3 \non the first day of Lent Term. \n \nAssessment \nThe final grade for the course will be provided as a percentage, and the assessment will consist \nof two parts: \n \n25%: for reading club (participation, presentation) \n75%: for the three reports: \n15%: Intensive review report \n25%: Survey report \n35%: Project study \nRecommended reading \nM. Abadi et al. TensorFlow: A System for Large-Scale Machine Learning, OSDI, 2016. \nD. Aken et al.: Automatic Database Management System Tuning Through Large-scale Machine \nLearning, SIGMOD, 2017. \n\nJ. Ansel et al. Opentuner: an extensible framework for program autotuning. PACT, 2014. \nV. Dalibard, M. Schaarschmidt, E. Yoneki. BOAT: Building Auto-Tuners with Structured Bayesian \nOptimization, WWW, 2017. \nJ. Dean et al. Large scale distributed deep networks. NIPS, 2012. \nG. Malewicz, M. Austern, A. Bik, J. Dehnert, I. Horn, N. Leiser, G. Czajkowski. Pregel: A System \nfor Large-Scale Graph Processing, SIGMOD, 2010. \nA. Mirhoseini et al. Device Placement Optimization with Reinforcement Learning, ICML, 2017. \nD. Murray, F. McSherry, R. Isaacs, M. Isard, P. Barham, M. Abadi. Naiad: A Timely Dataflow \nSystem, SOSP, 2013. \nM. Schaarschmidt, S. Mika, K. Fricke and E. Yoneki: RLgraph: Modular Computation Graphs for \nDeep Reinforcement Learning, SysML, 2019. \nZ. Jia, O. Padon, J. Thomas, T. Warszawski, M. Zaharia,  A. Aiken: TASO: Optimizing Deep \nLearning Computation with Automated Generation of Graph Substitutions: SOSP, 2019. \nH. Mao et al.: Park: An Open Platform for Learning-Augmented Computer Systems, \nOpenReview, 2019. \nJ. Shao, et al.: Tensor Program Optimization with Probabilistic Programs, NeurIPS, 2022.  \nA complete list can be found on the course material web page. See also 2023-2024 course \nmaterial on the previous course Large-Scale Data Processing and Optimisation.\n \n\nMACHINE LEARNING AND THE PHYSICAL WORLD \nAims \nThe module \u201cMachine Learning and the Physical World\u201d is focused on machine learning \nsystems that interact directly with the real world. Building artificial systems that interact with the \nphysical world have significantly different challenges compared to the purely digital domain. In \nthe real world data is scares, often uncertain and decisions can have costly and irreversible \nconsequences. However, we also have the benefit of centuries of scientific knowledge that we \ncan draw from. This module will provide the methodological background to machine learning \napplied in this scenario. We will study how we can build models with a principled treatment of \nuncertainty, allowing us to leverage prior knowledge and provide decisions that can be \ninterrogated. \n \nThere are three principle points about machine learning in the real world that will concern us. \n \nWe often have a mechanistic understanding of the real world which we should be able to \nbootstrap to make decisions. For example, equations from physics or an understanding of \neconomics. \nReal world decisions have consequences which may have costs, and often these cost functions \nneed to be assimilated into our machine learning system. \nThe real world is surprising, it does things that you do not expect and accounting for these \nchallenges requires us to build more robust and or interpretable systems. \nDecision making in the real world hasn\u2019t begun only with the advent of machine learning \ntechnologies. There are other domains which take these areas seriously, physics, environmental \nscientists, econometricians, statisticians, operational researchers. This course identifies how \nmachine learning can contribute and become a tool within these fields. It will equip you with an \nunderstanding of methodologies based on uncertainty and decision making functions for \ndelivering on these challenges. \n \nObjectives \nYou will gain detailed knowledge of \n \nsurrogate models and uncertainty \nsurrogate-based optimization \nsensitivity analysis \nexperimental design \nYou will gain knowledge of \n \ncounterfactual analysis \nsurrogate-based quadrature \nSchedule \nWeek 1: Introduction to the unit and foundation of probabilistic modelling \n \nWeek 2: Gaussian processes and probablistic inference \n \n\nWeek 3: Simulation and Sequential decision making under uncertainty \n \nWeek 4: Emulation and Experimental Design \n \nWeek 5: Sensitivity Analysis and Multifidelity Modelling \n \nWeek 6-8: Case studies of applications and Projects \n \nPractical work \nDuring the first five weeks of the unit we will provide a weekly worksheet that will focus on \nimplementation and practical exploration of the material covered in the lectures. The worksheets \nwill allow you to build up a these methods without relying on extensive external libraries. You are \nfree to use any programming language of choice however we highly recommended the use of \n=Python=. \n \nAssessment \nTwo individual tasks (15% each) \nGroup Project (70%). Each group will work on an application of uncertainty that covers the \nmaterial of the first 5 weeks of lectures in the unit. Each group will submit a report which will \nform the basis of the assessment. In addition to the report each group will also attend a short \noral examination based on the material covered both in the report and the taught material. \nRecommended Reading \nRasmussen, C. E. and Williams, C. K. I. (2006). Gaussian Processes for Machine Learning. MIT \nPress \n \nBishop, C. (2006). Pattern recognition and machine learning. Springer. \nhttps://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-an\nd-Machine-Learning-2006.pdf \n \nLaplace, P. S. (1902). A Philosophical Essay on Probabilities. John Wiley & Sons. \nhttps://archive.org/details/philosophicaless00lapliala\n \n\nMACHINE VISUAL PERCEPTION \nAims \nThis course aims at introducing the theoretical foundations and practical techniques for machine \nperception, the capability of computers to interpret data resulting from sensor measurements. \nThe course will teach the fundamentals and modern techniques of machine perception, i.e. \nreconstructing the real world starting from sensor measurements with a focus on machine \nperception for visual data. The topics covered will be image/geometry representations for \nmachine perception, semantic segmentation, object detection and recognition, geometry \ncapture, appearance modeling and acquisition, motion detection and estimation, \nhuman-in-the-loop machine perception, select topics in applied machine perception. \n \nMachine perception/computer vision is a rapidly expanding area of research with real-world \napplications. An understanding of machine perception is also important for robotics, interactive \ngraphics (especially AR/VR), applied machine learning, and several other fields and industries. \nThis course will provide a fundamental understanding of and practical experience with the \nrelevant techniques. \n \nLearning outcomes \nStudents will understand the theoretical underpinnings of the modern machine perception \ntechniques for reconstructing models of reality starting from an incomplete and imperfect view of \nreality. \nStudents will be able to apply machine perception theory to solve practical problems, e.g. \nclassification of images, geometry capture. \nStudents will gain an understanding of which machine perception techniques are appropriate for \ndifferent tasks and scenarios. \nStudents will have hands-on experience with some of these techniques via developing a \nfunctional machine perception system in their projects. \nStudents will have practical experience with the current prominent machine perception \nframeworks. \nSyllabus \nThe fundamentals of machine learning for machine perception \nDeep neural networks and frameworks for machine perception \nSemantic segmentation of objects and humans \nObject detection and recognition \nMotion estimation, tracking and recognition \n3D geometry capture \nAppearance modeling and acquisition \nSelect topics in applied machine perception \nAssessment \nA practical exercise, worth 20% of the mark. This will cover the basics and theory of machine \nperception and some of the practical techniques the students will likely use in their projects. This \nis individual work. No GPU hours will be needed for the practical work. \nA machine perception project worth 80% of the marks: \n\nCourse projects will be selected by the students following the possible project themes proposed \nby the lecturer, and will be checked by the lecturer for appropriateness.  \nThe students will form groups of 2-3 to design, implement, report, and present a project to tackle \na given task in machine perception. \nThe final mark will be composed of an implementation/report mark (60%) and a presentation \nmark (20%). Each team member will be evaluated based on her/his contribution. \nEach project will have extensions to be completed only by the ACS students. Each student will \nwrite a different part of the report, whose author will be clearly marked. Each student will further \nsummarise her/his contributions to the project in the same report. \nRecommended Reading List \nComputer Vision: Algorithms and Applications, Richard Szeliski, Springer, 2010. \nDeep Learning, Ian Goodfellow, Yoshua Bengio, and Aaron Courville, MIT Press, 2016. \nMachine Learning and Visual Perception, Baochang Zhang, De Gruyter, 2020. \nFurther Information \nCurrent Cambridge undergraduate students who are continuing onto Part III or the MPhil in \nAdvanced Computer Science may only take this module if they did NOT take it as a Unit of \nAssessment in Part II. \n \nThis module is shared with Part II of the Computer Science Tripos. Assessment will be adjusted \nfor the two groups of students to be at an appropriate level for whichever course the student is \nenrolled on. Further information about assessment and practicals will follow at the first lecture.\n\nMOBILE, WEARABLE SYSTEMS AND MACHINE LEARNING \n \nAims \nThis module aims to introduce the latest research advancements in mobile systems and mobile \ndata machine learning, spanning a range of domains including systems, data gathering, \nanalytics and machine learning, on device machine learning and applications such as health, \ntransportation, behavior monitoring, cyber-physical systems, autonomous vehicles, drones. The \ncourse will cover current and seminal research papers in the area of research. \n \nSyllabus \nThe course will consist of one introductory lecture, seven two-hour, and one three-hour, \nsessions covering a variety of topics roughly including the following material (some variation in \nthe topics might happen from year to year): \n \nSystem, Energy and Security \nBackscatter Communication, Battery Free and Energy Harvesting Devices \nNew Sensing Modalities \nMachine Learning on Wearable Data \nOn Device Machine Learning \nMobile and Wearable Health \nMobile and Wearable Systems of Sustainability \nEach week, three class participants will be assigned to introduce assigned three papers via \n20-minute presentations, conference-style and highlighting critically its features. Each \npresentation will be followed by 10 minutes of questions. This will be followed by 10 minutes of \ngeneral discussion. Slides will be used for presentation. \n \nStudents will give one or more presentations each term. Each student will submit a paper review \neach week for one of the three papers presented except for the week they will be presenting \nslides. Each review will follow a template and be up to 1,000 words. Each review will receive a \nmaximum of 10 points. As a result, each student will produce 6-7 reviews and at least one \npresentation, probably two. All participants are expected to attend and participate in every class; \nthe instructor must be notified of any absences in advance. \n \nObjectives \nOn completion of this module students should have an understanding of the recent key research \nin mobile and sensor systems and mobile analytics as well as an improved critical thinking over \nresearch papers. \n \nAssessment \nAggregate mark for 7 assignments from 6-7 reports and 1-2 presentations. Each report or \npresentation will contribute one seventh of the course mark. \nA tick for presence and participation to each class will also be awarded. \nRecommended reading \n\nReadings from the most recent conferences such as AAAI, ACM KDD, ACM MobiCom, ACM \nMobiSys, ACM SenSys, ACM UbiComp, ICLR, ICML Neurips, and WWW pertinent to mobile \nsystems and data.\n \n\nNETWORK ARCHITECTURES \nAims \nThis module aims to provide the world with more network architects. The 2011-2012 version \nwas oriented around the evolution of IP to support new services like multicast, mobility, \nmultihoming, pub/sub and, in general, data oriented networking. The course is a paper reading \nwhich puts the onus on the student to do the work. \n \nSyllabus \nIPng [2 lectures, Jon Crowcroft] \nNew Architectures [2 lectures, Jon Crowcroft] \nMulticast [2 lectures, Jon Crowcroft] \nContent Distribution and Content Centric Networks [2 lectures, Jon Crowcroft] \nResource Pooling [2 lectures, Jon Crowcroft] \nGreen Networking [2 lectures, Jon Crowcroft] \nAlternative Router Implementions [2 lectures, Jon Crowcroft] \nData Center Networks [2 Lectures, Jon Crowcroft] \nObjectives \nOn completion of this module, students should be able to: \n \ncontribute to new network system designs; \nengineer evolutionary changes in network systems; \nidentify and repair architectural design flaws in networked systems; \nsee that there are no perfect solutions (aside from academic ones) for routing, addressing, \nnaming; \nunderstand tradeoffs in modularisation and other pressures on clean software systems \nimplementation, and see how the world is changing the proper choices in protocol layering, or \nnon layered or cross-layered. \nCoursework \nAssessment is through three graded essays (each chosen individually from a number of \nsuggested or student-chosen topics), as follows: \n \nAnalysis of two different architectures for a particular scenario in terms of cost/performance \ntradeoffs for some functionality and design dimension, for example: \nATM \u2013 e.g. for hardware versus software tradeoff \nIP \u2013 e.g. for mobility, multi-homing, multicast, multipath \n3GPP \u2013 e.g. for plain complexity versus complicatedness \nA discursive essay on a specific communications systems component, in a particular context, \nsuch as ad hoc routing, or wireless sensor networks. \nA bespoke network design for a narrow, well specified specialised target scenario, for example: \nA customer baggage tracking network for an airport. \nin-flight entertainment system. \nin-car network for monitoring and control. \ninter-car sensor/control network for automatic highways. \nPractical work \n\nThis course does not feature any implementation work due to time constraints. \n \nAssessment \nThree 1,200-word essays (worth 25% each), and \nan annotated bibliography (25%). \nRecommended reading \nPre-course reading: \n \nKeshav, S. (1997). An engineering approach to computer networking. Addison-Wesley (1st ed.). \nISBN 0201634422 \nPeterson, L.L. and Davie, B.S. (2007). Computer networks: a systems approach. Morgan \nKaufmann (4th ed.). \n \nDesign patterns: \n \nDay, John (2007). Patterns in network architecture: a return to fundamentals. Prentice Hall. \n \nExample systems: \n \nKrishnamurthy, B. and Rexford, J. (2001). Web protocols and practice: HTTP/1.1, Networking \nprotocols, caching, and traffic measurement. Addison-Wesley. \n \nEconomics and networks: \n \nFrank, Robert H. (2008). The economic naturalist: why economics explains almost everything. \n \nPapers: \n \nCertainly, a collection of papers (see ACM CCR which publishes notable network researchers' \nfavourite ten papers every 6 months or so).\n \n\nOVERVIEW OF NATURAL LANGUAGE PROCESSING \n \nAims \nThis course introduces the fundamental techniques of natural language processing. It aims to \nexplain the potential and the main limitations of these techniques. Some current research issues \nare introduced and some current and potential applications discussed and evaluated. Students \nwill also be introduced to practical experimentation in natural language processing. \n \nLectures \nOverview. Brief history of NLP research, some current applications, components of NLP \nsystems. \nMorphology and Finite State Techniques. Morphology in different languages, importance of \nmorphological analysis in NLP, finite-state techniques in NLP. \nPart-of-Speech Tagging and Log-Linear Models. Lexical categories, word tagging, corpora and \nannotations, empirical evaluation. \nPhrase Structure and Structure Prediction. Phrase structures, structured prediction, context-free \ngrammars, weights and probabilities. Some limitations of context-free grammars. \nDependency Parsing. Dependency structure, grammar-free parsing, incremental processing.  \nGradient Descent and Neural Nets. Parameter optimisation by gradient descent. Non-linear \nfunctions with neural network layers. Log-linear model as softmax layer. Current findings of \nNeural NLP. \nWord representations. Representing words with vectors, count-based and prediction-based \napproaches, similarity metrics. \nRecurrent Neural Networks. Modelling sequences, parameter sharing in recurrent neural \nnetworks, neural language models, word prediction. \nCompositional Semantics. Logical representations, compositional semantics, lambda calculus, \ninference and robust entailment. \nLexical Semantics. Semantic relations, WordNet, word senses. \nDiscourse. Discourse relations, anaphora resolution, summarization. \nNatural Language Generation. Challenges of natural language generation (NLG), tasks in NLG, \nsurface realisation. \nPractical and assignments. Students will build a natural language processing system which will \nbe trained and evaluated on supplied data. The system will be built from existing components, \nbut students will be expected to compare approaches and some programming will be required \nfor this. Several assignments will be set during the practicals for assessment. \nObjectives \nBy the end of the course students should: \n \nbe able to discuss the current and likely future performance of several NLP applications; \nbe able to describe briefly a fundamental technique for processing language for several \nsubtasks, such as morphological processing, parsing, word sense disambiguation etc.; \nunderstand how these techniques draw on and relate to other areas of computer science. \nRecommended reading \n\nJurafsky, D. & Martin, J. (2023). Speech and language processing. Prentice Hall (3rd ed. draft, \nonline). \n \nAssessment - Part II Students \nAssignment 1 - 10% of marks \nAssignment 2 - 60% of marks \nAssignment 3 - 30% of marks \nCoursework \nSubmit work for 3 assignments as part of practical sessions on information extraction. \n \nThese include an annotation exercise, a feature-based classifier along with code repository and \ndocumentation, and a 4,000-word report on results and analysis from an extended information \nextraction experiment. \n \nPractical work \nStudents will build a natural language processing system which will be trained and evaluated on \nsupplied data. The system will be built from existing components, but students will be expected \nto compare approaches and some programming will be required for this. \n \nAssessment - Part III and MPhil Students \nAssessment will be based on the practicals: \n \nFirst practical exercise (10%, ticked) \nSecond practical exercise (25%, code repository or notebook with documentation) \nFinal report (65%, 4,000 words, excluding references) \nFurther Information \nAlthough the lectures don't assume any exposure to linguistics, the course will be easier to \nfollow if students have some understanding of basic linguistic concepts. The following may be \nuseful for this: The Internet Grammar of English \n \nCurrent Cambridge undergraduate students who are continuing onto Part III or the MPhil in \nAdvanced Computer Science may only take this module if they did NOT take it as a Unit of \nAssessment in Part II. \n \nStudents from other departments may attend the lectures for this module if space allows. \nHowever students wanting to take it for credit will need to make arrangements for assessment \nwithin their own department. \n \nThis module is shared with Part II of the Computer Science Tripos. Assessment will be adjusted \nfor the two groups of students to be at an appropriate level for whichever course the student is \nenrolled on. Further information about assessment and practicals will follow at the first lecture.\n\nPRACTICAL RESEARCH IN HUMAN-CENTRED AI \n \nAims \nThis is an advanced course in human-computer interaction, with a specialist focus on intelligent \nuser interfaces and interaction with machine-learning and artificial intelligence technologies. The \nformat will be largely Practical, with students carrying out a mini-project involving empirical \nresearch investigation. These studies will investigate human interaction with some kind of \nmodel-based system for planning, decision-making, automation etc. Possible study formats \nmight include: System evaluation, Field observation, Hypothesis testing experiment, Design \nintervention or Corpus analysis, following set examples from recent research publications. \nProject work will be formally evaluated through a report and presentation. \n \nLectures \n(note that Lectures 2-7 also include one hour class discussion of practical work) \n        \u2022 Current research themes in intelligent user interfaces \n        \u2022 Program synthesis \n        \u2022 Mixed initiative interaction \n        \u2022 Interpretability / explainable AI \n        \u2022 Labelling as a fundamental problem \n        \u2022 Machine learning risks and bias \n        \u2022 Visualisation and visual analytics \n        \u2022 Student research presentations \n \nObjectives \nBy the end of the course students should: \n        \u2022 be familiar with current state of the art in intelligent interactive systems \n        \u2022 understand the human factors that are most critical in the design of such systems \n        \u2022 be able to evaluate evidence for and against the utility of novel systems \n        \u2022 have experience of conducting user studies meeting the quality criteria of this field \n        \u2022 be able to write up and present user research in a professional manner \n \nClass Size \nThis module can accommodate upto 20 Part II, Part III and MPhil students. \n \nRecommended reading \nBrad A. Myers and Richard McDaniel (2000). Demonstrational Interfaces: Sometimes You Need \na Little Intelligence, Sometimes You Need a Lot. \n \nAlan Blackwell (2024). Moral Codes: Designing alternatives to AI \n \nAssessment - Part II Students \nThe format will be largely practical, with students carrying out an individual mini-project involving \nempirical research investigation. \n \n\nAssignment 1: six incremental submissions which together contribute 20% to the final module \nmark. \n \nAssignment 2: Final report - 80% of the final module mark \n  \n \nAssessment - MPhil / Part III Students \nThere will be a minor assessment component (20%) in which students compile a reflective diary \nthroughout the term, reporting on the weekly sessions. Diary entries should include citations to \nany key references, notes of possible further reading, summary of key points, questions relevant \nto the personal project, and points of interest noted in relation to the work of other students. \n \nThe major assessment component (80%) will involve a report on research findings, in the style \nof a submission to the ACM CHI or IUI conferences. This work will be submitted incrementally \nthrough the term, in order that feedback can be provided before final assessment of the full \nreport. Phased submissions will cover the following aspects of the empirical study: \n \nResearch question \nMethod \nLiterature Review \nIntroduction \nResults \nDiscussion / Conclusion \nFeedback on each phased submission will include an indicative mark for guidance, but with the \nunderstanding that the final grade will be based on the final delivered report, and that this may \ngo up (or possibly down), depending on how well the student responds to earlier feedback. \n \nReflective diary entries will also be assessed, but graded at a relatively coarse granularity \ncorresponding to the ACS grading bands, and with minimal written feedback. Informal and \ngeneric feedback will be offered verbally in class, and also potentially supplemented with peer \nassessment. \n \nFurther Information \nCurrent Cambridge undergraduate students who are continuing onto Part III or the MPhil in \nAdvanced Computer Science may only take this module if they did NOT take it as a Unit of \nAssessment in Part II. \n \nThis module is shared with Part II of the Computer Science Tripos. Assessment will be adjusted \nfor the two groups of students to be at an appropriate level for whichever course the student is \nenrolled on. Further information about assessment and practicals will follow at the first lecture.\n\nPRINCIPLES OF MACHINE LEARNING SYSTEMS \n \nPrerequisites: It is recommended that students have successfully completed introductory \ncourses, at an undergraduate level, in: 1) operating systems, 2) computer architecture, and 3) \nmachine learning. In addition, this course will heavily focus on deep neural network \nmethodologies and assume familiarity with common neural architectures and related algorithms. \nIf these topics were not covered within the machine learning course taken, students should \nsupplement by reviewing material in a book like: \"Dive into Deep Learning\". Finally, students are \nassumed to be comfortable with programming in Python. \nMoodle, timetable \n \nAims \nThis course will examine the emerging principles and methodologies that underpin scalable and \nefficient machine learning systems. Primarily, the course will focus on an exciting cross-section \nof algorithms and system techniques that are used to support the training and inference of \nmachine learning models under a spectrum of computing systems that range from constrained \nembedded systems up to large-scale distributed systems. It will also touch upon the new \nengineering practices that are developing in support of such systems at scale. When needed to \nappreciate issues of scalability and efficiency, the course will drill down to certain aspects of \ncomputer architecture, systems software and distributed systems and explore how these \ninteract with the usage and deployment of state-of-the-art machine learning. \n \nSyllabus \nTopics covered may include the following, with confirmation a month before the course begins: \n \nSystem Performance Trade-offs \nDistributed Learning Algorithms  \nModel Compression  \nDeep Learning Compilers  \nFrameworks and Run-times  \nScalable Inference Serving  \nDevelopment Practices  \nAutomated Machine Learning  \nFederated Learning  \nPrimarily, topics are covered with conventional lectures. However, where appropriate, material \nwill be delivered through hands-on lab tutorials. Lab tutorials will make use of hardware \nincluding ARM microcontrollers and multi-GPU machines to explore forms of efficient machine \nlearning (any necessary equipment will be provided to students) \n \nAssessment \nEach student will be assessed on 3 labs which will be worth 30% of their grade. They will also \nundertake a written project report which will be worth 70% of the grade. This report will detail an \ninvestigation into a particular aspect of machine learning systems, this report will be made \navailable publicly.\n \n\nPROOF ASSISTANTS \nAims \nThis module introduces students to interactive theorem proving using Isabelle and Coq. It \nincludes techniques for specifying formal models of software and hardware systems and for \nderiving properties of these models. \n \n  \n \nSyllabus \nIntroduction to proof assistants and logic. \nReasoning in predicate logic and typed set theory. \nReasoning in dependent type theory. \nInductive definitions and recursive functions: modelling them in logic, reasoning about them. \nModelling operational semantics definitions and proving properties. \n  \n \nObjectives \nOn completion of this module, students should: \n \npossess good skills in the use of Isabelle and Coq; \nbe able to specify inductive definitions and perform proofs by induction; \nbe able to formalise and reason about a variety of specifications in a proof assistant. \nCoursework \nPractical sessions will allow students to develop skills. Some of the exercises will serve as the \nbasis for assessment. \n \nAssessment \nAssessment will be based on two marked assignments (due in weeks 5 and 8) with students \nperforming small formalisation and verification projects in a proof assistant together with a \nwrite-up explaining their work (word limit 2,500 per project for the write-up). Each of the \nassignments will be worth 100 marks, distributed as follows: \n \n50 marks for completing basic formalisation and verification tasks assessing grasp of the \nmaterial taught in the lecture. Students will submit their work as theory files for either the \nIsabelle or Coq proof assistant, and they will be assessed for correctness and completeness of \nthe specifications and proofs. \n20 marks for completing designated more challenging tasks, requiring the use of advanced \ntechniques or creative proof strategies. \n30 marks for a clear write-up explaining the design decisions made during the formalisation and \nthe strategy used for the proofs, where 10 of these marks will be reserved for write-ups of \nexceptional quality, e.g. demonstrating particular insight. \nThe main tasks in the assignments will be designed to assess the student's proficiency with the \nbasic material and techniques taught in the lectures and the practical sessions, while the more \nchallenging tasks will give exceptional students the opportunity to earn distinction marks \n\n \nRecommended reading \nNipkow, T., Klein, G. (2014). Concrete Semantics with Isabelle/HOL. (The first part of this book, \n\u201cProgramming and Proving in Isabelle/HOL\u201d, comes with the Isabelle distribution.)  \n \nNipkow, T., Paulson, L.C. and Wenzel, M. (2002). A proof assistant for higher-order logic. \nSpringer LNCS 2283.  \n \nThe Software Foundations series of books, in particular the first two: \n \nPierce, B., Azevedo de Amorim, A., Casinghino, C., Gaboardi, M., Greenberg, M., Hri\u0163cu, C., \nSj\u00f6berg, V., Yorgey, B. (2023). Logical Foundations  \nPierce, B., Azevedo de Amorim, A., Casinghino, C., Gaboardi, M., Greenberg, M., Hri\u0163cu, C., \nSj\u00f6berg, V., Tolmach, A., Yorgey, B. (2023). Programming Language Foundations  \nChlipala, A. (2022). Certified programming with dependent types: a pragmatic introduction to the \nCoq proof assistant. MIT Press.  \n \nSergey, I. (2014). Programs and Proofs: Mechanizing Mathematics with Dependent Types.  \n \nAll of these are freely available online.\n \n\nQUANTUM ALGORITHMS AND COMPLEXITY \n \nAims \nThis module is a research-focused introduction to the theory of quantum computing. The aim is \nto prepare the students to conduct research in quantum algorithms and quantum complexity \ntheory. \n \nSyllabus \nTopics will vary from year to year, based on developments in cutting edge research. \nRepresentative topics include: \n \nQuantum algorithms: \n \nQuantum learning theory \nQuantum property testing \nShadow tomography \nQuantum walks \nQuantum state/unitary synthesis. \nStructure vs randomness in quantum algorithms \nQuantum complexity theory: \n \nThe quantum PCP conjecture \nEntanglement and MIP \nState complexity, AdS/CFT, and quantum gravity \nQuantum locally testable codes \nPseudorandom states and unitaries \nQuantum zero-knowledge proofs \nObjectives \nOn completion of this module, students should: \n \nbe familiar with contemporary quantum algorithms \ndevelop an understanding of the power and limitations of quantum computation \nconduct research in quantum algorithms and complexity theory \nAssessment \nMini research project (70%) \nPresentation (20%) \nAttendance and participation (10%) \nTimelines for the assignment submissions:  \n \nsubmit questions/observations on a weekly basis (i.e., Weeks 2-8) \ndeliver talks in Weeks 5-8 \nsubmit their mini-project at the end of term \nRecommended reading material and resources \n\u201cQuantum Computing Since Democritus\u201d by Scott Aaronson \n\n \n\u201cIntroduction to Quantum Information Science\u201d by Scott Aaronson \n \n\u201cQuantum Computing: Lecture Notes\u201d by Ronald de Wolf \n \n\u201cQuantum Computation and Quantum Information\u201d by Nielsen and Chuang\n \n\nUNDERSTANDING QUANTUM ARCHITECTURE \nPrerequisites: Requires introductory linear algebra - concepts such as eigenvalues, Hermitian \nmatrices, unitary matrices. Taking the Part II Quantum Computing course (or similar) is helpful \nbut not required. Familiarity with computer architecture and compilers is helpful. \nMoodle, timetable \n \nAims \nThis course covers the architecture of a practical-scale quantum computer. We will examine the \nresource requirements of practical quantum applications, understand the different layers of the \nquantum stack, the techniques used in these layers and examine how these layers come \ntogether to enable practical quantum advantage over classical computing. \n \nSyllabus \nThe course has two parts: a series of lectures to cover important aspects of the quantum stack \nand a set of student presentations. The following are a list of representative topics: \n \n- Basics of quantum computing \n- The fault-tolerant quantum stack \n- Compilation \n- Instruction sets \n- Implementations of quantum error correction \n- Implementation of magic state distillation \n- Resource estimation \n \nStudent presentations will be based on a reading list of important papers in quantum \narchitecture.  \n  \n \nObjectives \nAt the end of the course, students will have a broad understanding of the quantum computing \nstack. They will understand how major qubit technologies work, design challenges in real \nquantum hardware, how quantum applications are mapped to a system and the importance of \nquantum error correction for scalability.  \n \nAssessment \nSeminar presentation: 20% \nRead one paper from a provided reading list \nPrepare a 20 minute presentation on it + 10 minutes of answering questions. \nThe student presentation should be similar to a conference presentation - convey the problem, \nwhat are prior solutions, what did the paper do, what are the results, what are future directions \nFor the 20% of the score, the score will be split as 15% and 5%. 15% based on how well the \nstudent understands and explains the paper to the rest of the audience. 5% based on the Q&A \nsession. \n \n\n  \n \nCourse project: 80% (split across a proposal, mid-term report, final report) \nA. Research proposal - at least 500 words (10% of total) \nB. Mid-term report - at least 1000 words (including aspects like the research problem, literature \nreview, what questions will be evaluated, any early ideas or methods (20% of total)  \nC. Final report - up to 4 pages double column in a conference paper style with 3000-4000 \nwords. In addition to the mid term report, should include aspects like results and future \ndirections. (50% of the total) \nD. Report on contributions (in case of group work by 2 students) - 1 paragraph on individual \ncontribution of the student. 1 paragraph on teammate's contribution. \nStudents may work alone in the project, in which case they need only components A-C. \nStudents may work in pairs of two, but they should in addition individually submit D. The \ninstructor may conduct a short viva in case contributions from both team members are not clear \nor imbalanced. \nRecommended reading \nNielsen M.A., Chuang I.L. (2010). Quantum Computation and Quantum Information. Cambridge \nUniversity Press. \n \nMermin N.D. (2007). Quantum Computer Science: An Introduction. Cambridge University Press. \n \nAssessing requirements to scale to practical quantum advantage (2022) Beverland et al.\n\n",
        "8th Semester \nADVANCED TOPICS IN CATEGORY THEORY \n \n \nTeaching \nThe teaching style will be lecture-based, but supported by a practical component where \nstudents will learn to use a proof assistant for higher category theory, and build a small portfolio \nof proofs. Towards the end of the course we will explore some of the exciting computer science \nresearch literature on monoidal and higher categories, and students will choose a paper and \npresent it to the class. \n \nAims \nThe module will introduce advanced topics in category theory. The aim is to train students to \nengage and start modern research on the mathematical foundations of higher categories, the \ngraphical calculus, monoids and representations, type theories, and their applications in \ntheoretical computer science, both classical and quantum. \n \nObjectives \nOn completion of this module, students should: \n \nBe familiar with the techniques of compositional category theory. \nHave a strong understanding of basic categorical semantic models. \nBegun exploring current research in monoidal categories and higher structures. \nSyllabus \nPart 1, lecture course: \nThe first part of the course introduces concepts from monoidal categories and higher categories, \nand explores their application in computer science. \n\u2010 Monoidal categories and the graphical calculus \n\u2010 The proof assistant homotopy.io \n\u2010 Coherence theorems and higher category theory \n\u2010 Linearity, superposition, duality, quantum entanglement \n\u2010 Monoids, Frobenius algebras and bialgebras \n\u2010 Type theory for higher category theory \n \nPart 2, exploring the research frontier: \nIn the second part of the course, students choose a research paper to study, and give a \npresentation to the class. \nThere is a nice varied literature related to the topics of the course, and the lecturer will supply a \nlist of suggested papers.  \n \nClasses \nThere will be three exercise sheets for homework, with accompanying classes by a teaching \nassistant to go over them. \n \n\nAssessment \nProblem sheets (50%) \nClass presentation (20%) \nPractical portfolio (30%) \nReading List \nChris Heunen and Jamie Vicary, \u201cCategory for Quantum Theory: An Introduction\u201d, Oxford \nUniversity Press\n \n\nADVANCED TOPICS IN COMPUTER SYSTEMS \n \nAims \nThis module will attempt to provide an overview of \u201csystems\u201d research. This is a very broad field \nwhich has existed for over 50 years and which has historically included areas such as operating \nsystems, database systems, file systems, distributed systems and networking, to name but a \nfew. The course will thus necessarily cover only a tiny subset of the field. \n \nMany good ideas in systems research are the result of discussing and debating previous work. \nA primary aim of this course therefore will be to educate students in the art of critical thinking: \nthe ability to argue for and/or against a particular approach or idea. This will be done by having \nstudents read and critique a set of papers each week. In addition, each week will include \npresentations from a number of participants which aim to advocate or criticise each piece of \nwork. \n \nSyllabus \nThe syllabus for this course will vary from year to year so as to cover a mixture of older and \nmore contemporary systems papers. Contemporary papers will be generally selected from the \npast 5 years, primarily drawn from high quality conferences such as SOSP, OSDI, ASPLOS, \nFAST, NSDI and EuroSys. Example topics might include: \n \nSystems Research and System Design \nOS Structure and Virtual Memory \nVirtualisation \nConsensus \nScheduling \nPrivacy \nData Intensive Computing \nBugs \nThe reading each week will involve a load equivalent to 3 full length papers. Students will be \nexpected to read these in detail and prepare a written summary and review. In addition, each \nweek will contain one or more short presentations by students for each paper. The types of \npresentation will include: \n \nOverview: a balanced presentation of the paper, covering both positive and negative aspects. \nAdvocacy: a positive spin on the paper, aiming to convince others of its value. \nCriticism: a negative take on the paper, focusing on its weak spots and omissions. \nThese presentation roles will be assigned in advance, regardless of the soi disant absolute merit \nof the paper or the preference of the student. Furthermore, all students \u2013 regardless of any \nassigned presentation role in a given week \u2013 will be expected to participate in the class by \nasking questions and generally entering into the debate. \n \nObjectives \n\nOn completion of this module students should have a broad understanding of some key papers \nand concepts in computer systems research, as well as an appreciation of how to argue for or \nagainst any particular idea. \n \nCoursework and practical work \nCoursework will be the production of the weekly paper reviews. Practical work will be presenting \npapers as appropriate, as well as ongoing participation in the class. \n \nAssessment \nAssessment consists of: \n \nOne essay per week for 7 weeks (10% each) \nPresentation (20%) \nParticipation in class over the term (10%) \nRecommended reading \nMost of the reading for this course will be in the form of the selected papers each week. \nHowever, the following may be useful background reading to refresh your knowledge from \nundergraduate courses: \n \nSilberschatz, A., Peterson, J.L. and Galvin, P.C. (2005). Operating systems concepts. \nAddison-Wesley (7th ed.). \n \nTanenbaum, A.S. (2008). Modern Operating Systems. Prentice-Hall (3rd ed.). \n \nBacon, J. and Harris, T. (2003). Operating systems. Addison-Wesley (3rd ed.). \n \nAnderson, T. and Dahlin, M. (2014). Operating Systems: Principles and Practice. Recursive \nBooks (2nd ed.). \n \nHennessy, J. and Patterson, D. (2006). Computer architecture: a quantitative approach. Elsevier \n(4th ed.). ISBN 978-0-12-370490-0. \n \nKleppmann M (2016) Designing Data-Intensive Applications, O'Reilly (1st ed.)\n \n\nADVANCED TOPICS IN MACHINE LEARNING \nAims \nThis course explores current research topics in machine learning in sufficient depth that, at the \nend of the course, participants will be in a position to contribute to research on their chosen \ntopics. Each topic will be introduced with a lecture which, building on the material covered in the \nprerequisite courses, will make the current research literature accessible. Each lecture will be \nfollowed by up to three sessions which will typically be run as a reading group with student \npresentations on recent papers from the literature followed by a discussion, or a practical, or \nsimilar. \n \nStructure \nEach student will attend 3 topics and each topic's sessions will be spread over 5 contact hours. \nStudents will be expected to undertake readings for their selected topics. There will be some \ngroup work. \n \nThere will be a briefing session in Michaelmas term. \n \nSyllabus \nStudents choose five topics in preferential order from a list to be published in Michaelmas term. \nThey will be assigned to three topics out of their list. Students are assessed on one of these \ntopics which may not necessarily be their first choice topic. \n \nThe topics to be offered in 2024-25 are yet to be decided but to give an indicative idea of the \ntypes of topics, the ones offered in 2023-24 were: \n \nImitation learning  Prof A. Vlachos \nThe Future of Large Language Models: data architectures ethics Dr M. Tomalin \nPhysics and Geometry in Machine Learning Dr C. Mishra \nDiffusion Models and SDEs Francisco Vargas, Dr C. H. Ek \nExplainable Artificial Intelligence M. Espinosa, Z. Shams, Prof M. Jamnik \nUnconventional approaches to AI Dr S. Banerjee \nNarratives in Artificial Intelligence and Machine Learning Prof N. Lawrence \nMultimodal Machine Learning K. Hemker, N. Simidjievski, Prof M. Jamnik \nDeep Reinforcement Learning S. Morad, Dr C. H. Ek \nAutomation in Proof Assistants A. Jiang, W. Li, Prof M. Jamnik \nOn completion of this module, students should: \n \nbe in a strong position to contribute to the research topics covered; \nunderstand the fundamental methods (algorithms, data analysis, specific tasks) underlying each \ntopic; \nand be familiar with recent research papers and advances in the field. \nCoursework \nStudents will typically work in groups to give a presentation on assigned papers. Alternatively, a \ntopic may include practical sessions. Each topic will typically consist of one preliminary lecture \n\nfollowed by 3 reading and discussion sessions, or several lectures followed by a practical \nsession. A typical topic can accommodate up to 9 students presenting papers. There will be at \nleast 10 minutes general discussion per session. \n \nFull coursework details will be published by October. \n \nAssessment \nCoursework will be marked by the topic leaders and second marked by the module conveners. \n \nParticipation in all assigned topics, 10% \nPresentation or practical work or similar (for one of the chosen topics), 20% \nTopic coursework (for one of the chosen topics), 70% \nIndividual topic coursework will be published late Michaelmas term. \n \nAssessment criteria for topic coursework will follow project assessment criteria here: \nhttps://www.cl.cam.ac.uk/teaching/exams/acs_project_marking.pdf \n \nPlease note that students will be assessed on one of their three chosen topics but this may not \nbe their first choice. \n \nRecommended reading \nTo be confirmed by each topic convenor.\n \n\nCOMPUTING FOR COLLECTIVE INTELLIGENCE \nPrerequisites: Familiarity with core machine learning paradigms - Basic calculus (analytical \nskills) - Basic discretre optimization (combinatorics) \nMoodle, timetable \n \nObjectives \nThere is a substantial body of academic work demonstrating that complex life is built by \ncooperation across scales: collections of genes cooperate to produce organisms, cells \ncooperate to produce multi-cellular organisms and multicellular animals cooperate to form \ncomplex social groups. Arguably, all intelligence is collective intelligence. Yet, to-date, many \nartificially intelligent agents (both embodied and virtual), are generally not conceived from the \nground up to interact with other intelligent agents (be it machines or humans). The canonical AI \nproblem is that of a monolithic and solitary machine confronting a non-social environment. This \ncourse aims to balance this trend by (i) equipping students with conceptual and practical \nknowledge on collective intelligence from a computational standpoint, and (ii) by conveying \nvarious computational paradigms by which collective intelligence can be modelled as well as \nsynthesized. \n \nAssessment \nThe assessment will be based on a reading-group paper presentation (individual or in pairs), \naccompanied by a 2-page summary, and a technical position paper (individual work) handed in \nafter the course. The position paper will be assessed based on its technical correctness, \nstrength or arguments, \nand clarity of research vision, and will be accompanied by a brief 5-minute pre-recorded talk that \nsummarizes key arguments (any slides used are also submitted as part of the project). \n \nPaper presentation and 1-page summary: 25% \nTechnical position paper: 75% (4000 word limit) \nRecommended reading \nSwarm Intelligence : From Natural to Artificial Systems, Bonabeau et al (1999) \nJoined-Up Thinking, Hannah Critchlow, (2024) \nSupercooperators, Martin Nowak (2011) \nA Course on Cooperative Game Theory, Chakravarty et al., (2015) \nGoverning the Commons: The Evolution of Institutions for Collective Action. Ostrom (1990).  \n \n\nCRYPTOGRAPHY AND PROTOCOL ENGINEERING \nAims \nWe all use cryptographic protocols every day: whenever we access an https:// website or send a \nmessage via WhatsApp or Signal, for example. In this module, students will get hands-on \nexperience of how those protocols are implemented. Students start by writing their own secure \nmessaging protocol from scratch, and then move on to more advanced examples of \ncryptographic protocols for security and privacy, such as private information retrieval (searching \nfor data without revealing what you\u2019re searching for). \n \nSyllabus \nThis is a practical course in which the first half of each of the 2-hour weekly sessions is a lecture \nthat introduces a topic, while the second half is lab time during which the students can work on \ntheir implementations, discuss the topics in small groups, and get help from the lecturers. The \nmodule is structured into three blocks as follows:  \n \nCryptographic primitives (2 weeks). Using common building blocks such as symmetric ciphers \nand hash functions. Implementing selected aspects of elliptic curve Diffie-Hellman and digital \nsignatures. Protocol security, Dolev-Yao model, side-channel attacks.  \nSecure communication (3 weeks). Authenticated key exchange (e.g. SIGMA, TLS), \npassword-authenticated key exchange (e.g. SPAKE2), forward secrecy, post-compromise \nsecurity, double ratchet, the Signal protocol.  \nPrivate database lookups (3 weeks). Lattice-based post-quantum cryptography, learning with \nerrors, homomorphic encryption, private information retrieval.  \nIn each block, students will be given a description of the algorithms and protocols covered (e.g. \na research paper or an RFC standards document), and a code template that the students \nshould use for their implementation. The implementation language will probably be Python (to \nbe confirmed after the practical materials have been developed ). Students will also be given a \nset of test cases to check their code, and will have the opportunity to test their implementation \ncommunicating with other students\u2019 implementations. Finally, for each block, students will write \nand submit a lab report explaining their approach and the findings from their implementation. \n \nObjectives \nOn completion of this module, students should: \n \nUnderstand that cryptography is not magic! The mathematics can look intimidating, but this \nmodule will show students that they needn\u2019t be afraid of cryptography. \nGain appreciation for the complexity and careful implementation of real-world cryptography \nlibraries ,and understand why one should prefer vetted implementations. \nBe familiar with the mathematical notation and concepts commonly used in descriptions of \ncryptographic protocols, and be able to understand and implement research papers and RFCs \nin this field. \nBe comfortable with cryptographic primitives commonly used in protocols, and able to correctly \nuse software libraries that implement them. \n\nHave gained hands-on experience of attacks on cryptographic protocols, and an appreciation \nfor the difficulty of making these protocols correct. \nHave been exposed to ideas and techniques from recent research, which may be useful for their \nown future research. \nHave practised technical writing through lab reports. \nAssessment \nStudents will be provided with code skeletons in one or two different programming languages \n(probably Python, maybe Rust) to avoid them struggling with setup issues. For each \nassignment, students are required to produce a working implementation of the protocols \ndiscussed in the course, using the programming language and skeleton provided. Here, \n\u201cworking\u201d means that the algorithms are functionally correct, but they are not production-quality \n(in particular, no side-channel countermeasures such as constant-time algorithms are required). \nFor some primitives (e.g. symmetric ciphers and hash functions) existing libraries should be \nused, whereas other cryptographic algorithms will be implemented from scratch. Students \nshould submit their code as a Docker container that can be run against specified test cases.  \n \nAfter each of the three blocks, students will be asked to submit a lab report of approximately \n2,000 words along with their code for that block. In the lab report, the students should explain \nhow their implementation works and why it is correct, as well as any findings from the work (e.g. \nany limitations or trade-offs they found with the protocols). The report structure and marking \nscheme will be specified in advance. The lab reports provide an opportunity for students to \nprovide critical insights and creativity. For instance, they might compare their implementation \nwith existing real-world implementations which provide additional features and guarantees.  \n \nEach of the three lab reports (along with the associated code) will be marked, forming the basis \nof assessment, and feedback on the reports will be given to the students before the next \nsubmission is due, so that students can take the feedback on board for their next submission. \nOf the three reports, the worst mark will be discarded, and the other two reports each contribute \n50% of the final mark. As such, students might decide to submit only two reports.  \n \nStudents may discuss their work with others, but the implementations and lab reports must be \nindividual work. \n \nRecommended reading \nTextbook: Jonathan Katz and Yehuda Lindell. Introduction to Modern Cryptography (3rd edition). \nCRC Press, 2020. \n \nThe textbook covers prerequisite knowledge on cryptographic primitives, such as ciphers, \nMACs, hash functions, and signatures. This book is also used in the Part II Cryptography \ncourse. For the protocols we cover in this course, we are not aware of a suitable textbook; \ninstead we will use research papers, lecture slides, and RFCs as resources, which will be \nprovided at the start of the module.\n \n\nDISTRIBUTED LEDGER TECHNOLOGIES: FOUNDATIONS AND APPLICATIONS \n \nAims \nThis reading group course examines foundations and current research into distributed ledger \n(blockchain) technologies and their applications. Students will read, review, and present seminal \nresearch papers in this area. Once completed, students should be able to integrate blockchain \ntechnologies into their own research and gain familiarity with a range of research skills. \n \nLectures \nIntroduction \nConsensus protocols \nBitcoin and its variants \nEthereum, smart contracts, and other permissionless DLTs \nHybrid and permissioned DLTs \nApplications \nLearning objectives \nThere are two broad objectives: to acquire familiarity with a body of work in the area of \ndistributed ledgers and to learn some specific research skills: \n \nHow to read a paper \nHow to review a paper \nHow to analyze a paper\u2019s strengths and weaknesses \nWritten and oral presentation skills \nAssessment \nYou are expected to read all assigned papers and submit paper reviews each week. Each \nreview must either follow the provided review form [PDF] [Latex source]. Each \u201creview\u201d is worth \n5% of your total mark, and is marked out of 100 with 60 a passing grade. Marks will be awarded \nand penalties for late submission applied according to ACS Assessment Guidelines.  \n \nOne paper review for the first week, then two paper reviews each week for 6 weeks (13 reviews, \n5% each) 65% (approx 600 words per review) \nSummative essay 25% (max 3000 words) \nPresentation 5% \n100% attendance in class 5% (2 marks deducted per missed class) \nRecommended Reading \nNarayanan, A. , Bonneau, J., Felten, E., Miller, A. and Goldfeder, S. (2016). Bitcoin and \nCryptocurrency Technologies: A Comprehensive Introduction. Princeton University Press. \n(2016 Draft available here: \nhttps://d28rh4a8wq0iu5.cloudfront.net/bitcointech/readings/princeton_bitcoin_book.pdf)\n \n\nEXPLAINABLE ARTIFICIAL INTELLIGENCE \nPrerequisites: A solid background in statistics, calculus and linear algebra. We strongly \nrecommend some experience with machine learning and deep neural networks (to the level of \nthe first chapters of Goodfellow et al.\u2019s \u201cDeep Learning\u201d). Students are expected to be \ncomfortable reading and writing Python code for the module\u2019s practical sessions. \nMoodle, timetable \n \nAims \nThe recent palpable introduction of Artificial Intelligence (AI) models to everyday \nconsumer-facing products, services, and tools brings forth several new technical challenges and \nethical considerations. Amongst these is the fact that most of these models are driven by Deep \nNeural Networks (DNNs), models that, although extremely expressive and useful, are \nnotoriously complex and opaque. This \u201cblack-box\u201d nature of DNNs limits their ability to be \nsuccessfully deployed in critical scenarios such as those in healthcare and law. Explainable \nArtificial Intelligence (XAI) is a fast-moving subfield of AI that aims to circumvent this crucial \nlimitation of DNNs by either  \n \n(i) constructing human-understandable explanations for their predictions,  \n \nor (ii) designing novel neural architectures that are interpretable by construction.  \n \nIn this module, we will introduce the key ideas behind XAI methods and discuss some of the \nimportant application areas of these methods (e.g., healthcare, scientific discovery, debugging, \nmodel auditing, etc.). We will approach this by focusing on the nature of what constitutes an \nexplanation, and discussing different ways in which explanations can be constructed or learnt to \nbe generated as a by-product of a model. The main aim of this module is to introduce students \nto several commonly used approaches in XAI, both theoretically in lectures and through \nhands-on exercises in practicals, while also bringing recent promising directions within this field \nto their attention. We hope that, by the end of this module, students will be able to directly \ncontribute to XAI research and will understand how methods discussed in this module may be \npowerful tools for their own work and research. \n \nSyllabus \nOverview and taxonomy of XAI (why is explainability needed, definition of terms, taxonomy of \nthe XAI space, etc.)  \nPerturbation-based feature attribution (e.g., LIME, Anchors, SHAP, etc.) \nPropagation-based feature attribution methods (e.g., Relevance Propagation, Saliency \nmethods, etc.) \nConcept-based explainability (Net2Vec, T-CAV, ACE, etc.) \nInterpretable architectures (CBMs and variants, Concept Whitening, SENNs, etc.) \nNeurosymbolic methods (DeepProbLog, Neural Reasoners, etc.) \nSample-based Explanations (Influence functions, ProtoPNets, etc.) \nCounterfactual explanations \nProposed Schedule \n\nThe 16 hours of lectures across 8 weeks will be divided as follows:  \nWeek 1: 1h lecture + 1h lecture  \nWeek 2: 1h lecture + 1h reading group & presentations  \nWeek 3: 1h lecture + 1h reading group & presentations  \nWeek 4: 2h practical  \nWeek 5: 1h lecture + 1h reading group & presentations  \nWeek 6: 2h practical  \nWeek 7: 1h lecture + 1h reading group & presentations  \nWeek 8: 1h reading group & presentation + 1h reading group & presentations \n \nIn weeks where lectures are planned, one-hour lecture slots will intercalate with one hour of \nstudent paper presentations. During each paper presentation session, three students will \npresent a paper related to the topic covered in the earlier lecture that week for about 10 minutes \neach + 5 minutes of questions. At the end of all paper presentations, there will be a discussion \non all the papers. The order of student presentations will be allocated randomly during the first \nweek so that students know in advance when they are expected to present. For the sake of \nfairness, we will release the paper to be presented by each student a week before their \npresentation slot. \n \nObjectives \nBy the end of this module, students should be able to: \n \nRecognise and identify key concepts in XAI together with their connection to related subfields in \nAI such as fairness, accountability, and trustworthy AI. \nUnderstand how to use, design, and deploy model-agnostic perturbation methods such as \nLIME, Anchors, and RISE. In particular, students should understand the connection between \nfeature importance and cooperative game theory, and its uses in methods such as SHAP. \nIdentify the uses and limitations of propagation-based feature importance methods such as \nSaliency, SmoothGrad, GradCAM, and Integrated Gradients. Students should be able to \nimplement each of these methods on their own and connect the theoretical ideas behind them \nto practical code, exploiting modern frameworks\u2019 auto-differentiation. \nUnderstand what concept learning is and what limitations it overcomes compared to traditional \nfeature-based methods. Specifically, students should understand how probing a DNN\u2019s latent \nspace may be exploited for learning useful concepts for explainability. \nReason about the key components of inherently interpretable architectures and neuro-symbolic \nmethods and understand how interpretable neural networks can be designed from first \nprinciples. \nElaborate on what sample-based explanations are and how influence functions and prototypical \narchitectures such as ProtoPNet can be used to construct such explanations. \nExplain what counterfactual explanations are, how they are related to causality, and under which \nconditions they may be useful. \nUpon completion of this module, students will have the technical background and tools to use \nXAI as part of their own research or partake in XAI research itself. Moreover, we hope that by \ndetailing a clear timeline of how this relatively young subfield has developed, students may be \n\nable to better understand what are some fundamental open questions in this area and what are \nsome promising directions that are currently actively being explored. \n  \n \nAssessment \n(10%) student presentation: each student will be randomly assigned a presentation slot at the \nbeginning of the course. We will then distribute a paper for them to present in their slot a week \nbefore the presentation\u2019s scheduled time. Students are expected to prepare a 10-minute \npresentation of their assigned paper where they will present the motivation of their assigned \nwork and discuss the main methodology and findings reported in that paper. We will encourage \nstudents to focus on fully communicating the intuition of the work in their assigned papers and \ntry and connect it with ideas that we have previously discussed in previous lectures. All students \nnot presenting each week will be asked to submit one question pertaining to each paper \npresented that week before the paper presentations. \n \n(20%) practical exercises: We will run two practical sessions where students will be asked to \nperform a series of exercises that require them to use concepts we have introduced in lecture \nup to that point. For each practical session, we will prepare a colab notebook to guide the \nstudent through exercises and we will ask the students to submit their solutions through this \ncolab notebook. We expect students to complete about \u2154 of the exercises in the practical class \nand complete the rest at home as homework. Each practical will be worth 10%. \n \n(70%) mini-project: At the end of week 3, we will hand out a list of papers for students to select \ntheir mini-projects from. Each mini-project will consist of a student selecting a paper from our list \nand reimplementing and expanding the key idea in the paper. We encourage students to be as \ncreative as they want with how they drive their mini-project once the paper has been selected. \nFor example, they can reimplement the technique in the paper and combine it with \nmethodologies from other works we discussed in lecture, or they can apply their paper\u2019s \nmethodology to a new domain, datasets, or setup, where the technique may offer interesting \nand potentially novel insights. We will ask all students to submit a report in a workshop format of \nup to 4,000 words. This report, due roughly a week after Lent term ends, should describe their \nmethodology, experiments, and results. A crucial aspect of this report involves explaining the \nrationale behind different choices in methodology and experiments, as well as elaborating on \nthe choices made and hypotheses tested throughout their mini-project (potentially showing a \ndeep understanding of the work they are basing their mini-project on). To aid students with \nselecting their projects and making progress on them, we will hold regular office hours when \nstudents can come to discuss their progress and questions with us. \n \nRecommended reading \nTextbooks \n \n* Christoph Molnar, \u201cInterpretable Machine Learning\u201d. (2022): \nhttps://christophm.github.io/interpretable-ml-book/ \n \n\nOnline Courses and Tutorial \n \n* Su-in Lee and Ian Cover, \u201cCSEP 590B Explainable AI\u201d University of Washington* Explaining \nMachine Learning Predictions: State-of-the-art, Challenges, and Opportunities \n \n* On Explainable AI: From Theory to Motivation, Industrial Applications, XAI Coding & \nEngineering Practices - AAAI 2022 Turorial \n \nSurvey papers \n \n* Arrieta, Alejandro Barredo, et al. \"Explainable Artificial Intelligence (XAI): Concepts, \ntaxonomies, opportunities and challenges toward responsible AI.\" Information fusion 58 (2020): \n82-115. \n \n* Rudin, Cynthia, et al. \"Interpretable machine learning: Fundamental principles and 10 grand \nchallenges.\" Statistics Surveys 16 (2022): 1-85.\n \n\nFEDERATED LEARNING: THEORY AND PRACTICE \nPrerequisites: It is strongly recommended that students have previously (and successfully) \ncompleted an undergraduate machine learning course - or have equivalent experience through \nopen-source material (e.g., lectures or similar). An example course would be: Part1B \"Artificial \nIntelligence\". Students should feel comfortable with SGD and optimization methods used to train \ncurrent popular neural networks and simple forms of neural network architectures. \nMoodle, timetable \n \nObjectives \nThis course aims to extend the machine learning knowledge available to students in Part I (or \npresent in typical undergraduate degrees in other universities), and allow them to understand \nhow these concepts can manifest in a decentralized setting. The course will consider both \ntheoretical (e.g., decentralized optimization) and practical (e.g., networking efficiency) aspects \nthat combine to define this growing area of machine learning.  \n \nAt the end of the course students should: \n \nUnderstand popular methods used in federated learning \nBe able to construct and scale a simple federated system \nHave gained an appreciation of the core limitations to existing methods, and the approaches \navailable to cope with these issues \nDeveloped an intuition for related technologies like differential privacy and secure aggregation, \nand are able to use them within typical federated settings  \nCan reason about the privacy and security issues with federated systems \nLectures \nCourse Overview. Introduction to Federated Learning. \nDecentralized Optimization. \nStatistical and Systems Heterogeneity. \nVariations of Federated Aggregation. \nSecure Aggregation. \nDifferential Privacy within Federated Systems. \nExtensions to Federated Analytics. \nApplications to Speech, Video, Images and Robotics. \nLab sessions \nFederating a Centralized ML Classifier. \nBehaviour under Heterogeneity. \nScaling a Federated Implementation. \nExploring Privacy with Federated Settings \nRecommended Reading \nReadings will be assigned for each lecture. Readings will be taken from either research papers, \ntutorials, source code or blogs that provide more comprehensive treatment of taught concepts. \n \nAssessment - Part II Students \n\nFour labs are performed during the course, and students receive 10% of their total grade for \nwork done as part of each lab. (For a total of 40% of the total grade from lab work alone). Labs \nwill primarily provide hands-on teaching opportunities, that are then utilized within the lab \nassignment which is completed outside of the lab contact time. MPhil and Part III students will \nbe given additional questions to answer within their version of the lab assignment which will \ndiffer from the assignment given to Part II CST students. \n \nThe remainder of the course grade (60%) will be given based on a hands-on project that applies \nthe concepts taught in lectures and labs. This hands-on project will be assessed based on upon \na combination of source code, related documentation and brief 8-minute pre-recorded talk that \nsummarizes key project elements (any slides used are also submitted as part of the project). \nPlease note, that in the case of Part II CST students, the talk itself is not examinable -- as such \nwill be made optional to those students. \n \nA range of possible practical projects will be described and offered to students to select from, or \nalternatively students may propose their own. MPhil and Part III students will select from a \nproject pool that is separate from those offered to Part II CST students. MPhil and Part III \nprojects will contain a greater emphasis on a research element, and the pre-recorded talks \nprovided by this student group will focus on this research contribution. The project will be \nassessed on the level of student understanding demonstrated, the degree of difficulty, \ncorrectness of implementation -- and for Part III/MPhil students the additional criteria of the \nquality and execution of the research methodology, and depth and quality of results analysis. \n \nThis project can be done individually or in groups -- although individual projects will be strongly \nencouraged. It will be required the project is performed using a code repository that also will \ncontain all documentation -- access to this repository will be shared with course staff (e.g., \nlecturer and TAs). Where needed, marks assigned to students within a group will be \ndifferentiated using this repository as an input. Furthermore if groups are formed, members must \nbe either entirely from Part III/MPhil students or Part II CST, i.e., these two student groups \nshould not mix to form a project group. \n \nProjects will be made available publicly. A maximum word count for written contributions for the \nproject will be enforced.  \n \n  \nAssessment - MPhil / Part III Students \n50% final mini-project. This hands-on project will be assessed based on upon a combination of \nsource code, related documentation and brief 8-minute pre-recorded talk that summarizes key \nproject elements (any slides used are also submitted as part of the project). \n \n30% midterm lab report (a written report of a bit more depth of thought than required in a lab, \nthe report provides the results of experiments -- with the skills to perform these experiments \ncoming from lab sessions),  \n \n\n20% for two individual hands-on labs. Labs will primarily provide hands-on teaching \nopportunities, that are then utilized within the lab assignment which is completed outside of the \nlab contact time. There will also be additional questions to answer within their version of the lab \nassignment. \n \nFor the mini-project, a range of possible practical projects will be described and offered to \nstudents to select from, or alternatively students may propose their own. The project will be \nassessed on the level of student understanding demonstrated, the degree of difficulty, \ncorrectness of implementation, the quality and execution of the research methodology, and \ndepth and quality of results analysis. \n \nThe project can be done individually or in groups -- although individual projects will be strongly \nencouraged. It will be required the project is performed using a code repository that also will \ncontain all documentation -- access to this repository will be shared with course staff (e.g., \nlecturer and TAs). Where needed, marks assigned to students within a group will be \ndifferentiated using this repository as an input. Furthermore if groups are formed, members must \nbe either entirely from Part III or entirely from MPhil students, i.e., these two student groups \nshould not mix to form a project group. \n \nProjects will be made available publicly. A maximum word count for written contributions for the \nproject will be enforced.   \n \nAdvanced Material sessions - MPhil / Part III Students \nThese sessions are mandatory for the MPhil and Part III students. Part II CST students may \nattend if they wish \n \nTopics and announced the first week of class. Selected to extend beyond topics covered in \nlectures and labs. Content is a mixture of sessions run in the lecture room the are either (1) \npresentations by guest lectures by an invited domain expert or (2) class-wide discussions \nregarding one or more related academic papers. Paper discussions will require students to read \nthe paper ahead of the lecture, and a brief discussion primer presentation will be given students \nbefore discussions begin.  \n \nThis module is shared with Part II of the Computer Science Tripos. Assessment will be adjusted \nfor the two groups of students to be at an appropriate level for whichever course the student is \nenrolled on. Further information about assessment and practicals will follow at the first lecture.\n\nGEOMETRIC DEEP LEARNING \n \nPrerequisites: Experience with machine learning and deep neural networks is recommended. \nKnowledge of concepts from graph theory and group theory is useful, although the relevant \nparts will be explicitly retaught. \nMoodle, timetable \n \nAims and Objectives \nMost of the patterns we see in nature can be elegantly reasoned about using spatial \nsymmetries\u2014transformations that leave underlying objects unchanged. This observation has \nhad direct implications for the development of modern deep learning architectures that are \nseemingly able to escape the \u201ccurse of dimensionality\u201d and fit complex high-dimensional tasks \nfrom noisy real-world data. Beyond this, one of the most generic symmetries\u2014the \npermutation\u2014will prove remarkably powerful in building models that reason over graph \nstructured-data, which is an excellent abstraction to reason about naturally-occurring, \nirregularly-structured data. Prominent examples include molecules (represented as graphs of \natoms and bonds, with three-dimensional coordinates provided), social networks and \ntransportation networks. Several already-impacted application areas include traffic forecasting, \ndrug discovery, social network analysis and recommender systems. The module will provide the \nstudents the capability to analyse irregularly- and nontrivially-structured data in an effective way, \nand position geometric deep learning in a proper context with related fields. The main aim of the \ncourse is to enable students to make direct contributions to the field, thoroughly assimilate the \nkey concepts in the area, and draw relevant connections to various other fields (such as NLP, \nFourier Analysis and Probabilistic Graphical Models). We assume only a basic background in \nmachine learning with deep neural networks. \n \nLearning outcomes \nThe framework of geometric deep learning, and its key building blocks: symmetries, \nrepresentations, invariance and equivariance \nFundamentals of processing data on graphs, as well as impactful application areas for graph \nrepresentation learning \nTheoretical principles of graph machine learning: permutation invariance and equivariance \nThe three \"flavours\" of spatial graph neural networks (GNNs) (convolutional, attentional, \nmessage passing) and their relative merits. The Transformer architecture as a special case. \nAttaching symmetries to graphs: CNNs on images, spheres and manifolds, Geometric Graphs \nand E(n)-equivariant GNNs \nRelevant connections of geometric deep learning to various other fields (such as NLP, Fourier \nAnalysis and Probabilistic Graphical Models) \nLectures \nThe lectures will cover the following topics: \n \nLearning with invariances and symmetries: geometric deep learning. Foundations of group \ntheory and representation theory. \n\nWhy study data on graphs? Success stories: drug screening, travel time estimation, \nrecommender systems. Fundamentals of graph data processing: network science, spectral \nclustering, node embeddings. \nPermutation invariance and equivariance on sets and graphs. The principal tasks of node, edge \nand graph classification. Neural networks for point clouds: Deep Sets, PointNet; universal \napproximation properties. \nThe three flavours of spatial GNNs: convolutional, attentional, message passing. Prominent \nexamples: GCN, SGC, ChebyNets, MoNet, GAT, GATv2, IN, MPNN, GraphNets. Tradeoffs of \nusing different GNN variants. \nGraph Rewiring: how to apply GNNs when there is no graph? Links to natural language \nprocessing---Transformers as a special case of attentional GNNs. Representative \nmethodologies for graph rewiring: GDC, SDRF, EGP, DGCNN. \nExpressive power of graph neural networks: the Weisfeiler-Lehman hierarchy. GINs as a \nmaximally expressive GNN. Links between GNNs and graph algorithms: neural algorithmic \nreasoning. \nCombining spatial symmetries with GNNs: E(n)-equivariant GNNs, TFNs, SE(3)-Transformer. A \ndeep dive into AlphaFold 2. \nWorked examples: Circulant matrices on grids, the discrete Fourier transform, and convolutional \nnetworks on spheres. Graph Fourier transform and the Laplacian eigenbasis. \nPracticals \nThe practical is designed to complement the knowledge learnt in lectures and teach students to \nderive additional important results and architectures not directly shown in lectures. The practical \nwill be given as a series of individual exercises (each either code implementation or \nproof/derivation). Each of these exercises can be individually assessed based on a specified \nmark budget. \n \nPossible practical topics include the study of higher-order GNNs and equivariant message \npassing. \n \nAssessment \n(60%) Group Mini-project (writeup) at the end of the course. The mini projects can either be \nself-proposed, or the students can express their preference for one of the provided topics, the \nlist of which will be announced at the start of term. The projects will consist of implementing \nand/or extending graph representation learning models in the literature, applying them to \npublicly available datasets. Students will undertake the project in pairs and submit a joint \nwriteup limited to 4,000 words (in line with other modules); appendix of work logs to be included \nbut ungraded; \n \n(10%) Short presentation and viva: students will give a short presentation to explain their \nindividual contribution to the mini-project and there will be a short viva following. \n \n(30%) Practical work completion. Completing the exercises specified in the practical to a \nsatisfactory standard. The practical assessor should be satisfied that the student derived their \nanswers using insight gained from the course; coupled with original thought, not by simple \n\ncopy-pasting of relevant related work. The students would submit code and a short report, which \nwould then be marked in line with the predetermined mark budget for each practical item. \nThe students will learn how to run advanced architectures on GPU but no specific need for \ndedicated GPU resources. Practicals will be made possible to do on CPU; if required, students \ncan use GPUs on publicly available free services (such as Colab) for their mini-project work. \n \nReferences \nThe course will be based on the following literature: \n \n\"Geometric Deep Learning: Grids, Graphs, Groups, Geodesics, and Gauges\", by Michael \nBronstein, Joan Bruna, Taco Cohen and Petar Veli\u010dkovi\u0107 \n\"Graph Representation Learning\", by Will Hamilton \n\"Deep Learning\", by Ian Goodfellow, Yoshua Bengio and Aaron Courville.\n \n\nMOBILE HEALTH \nAims \nThe course aims to explore how wearable and mobile systems sensors can be used to gather \ndata relevant to understand health, how the data can be analysed with advanced signal \nprocessing and machine learning and the performance of these systems in terms of diagnostics \nand disease progression detection. \n \nSyllabus \nCourse Overview. Introduction to Mobile Health. Evaluation metrics and methodology. Basics of \nSignal Processing. \nInertial Measurement Units, Human Activity Recognition (HAR) and Gait Analysis and Machine \nLearning for IMU data. \nRadios, Bluetooth, GPS and Cellular. Epidemiology and contact tracing, Social interaction \nsensing and applications. Location tracking in health monitoring and public health. \nAudio Signal Processing. Voice and Speech Analysis: concepts and data analysis. Body \nSounds analysis. \nPhotoplethysmogram and Light sensing for health (heart and sleep) \nContactless and wireless behaviour and physiological monitoring \nGenerative Models for Wearable Health Data \nTopical Guest Lectures \nObjectives \nThe course aims to explore how wearable and mobile systems sensors can be used to gather \ndata relevant to understand health, how the data can be analysed with advanced signal \nprocessing and machine learning and the performance of these systems in terms of diagnostics \nand disease progression detection. \n \nRoughly, each lecture contains a theory part about the working of \u201csensor signals\u201d or \u201cdata \nanalysis methods\u201d and an application part which contextualises the concepts. \n \nAt the end of the course students should: Understand how mobile/wearable sensors capture \ndata and their working. Understand different approaches to acquiring and analysing sensor data \nfrom different types of sensors. Understand the concept of signal processing applied to time \nseries data and their practical application in health. Be able to extract sensor data and analyse it \nwith basic signal processing and machine learning techniques. Be aware of the different health \napplications of the various sensor techniques. The course will also touch on privacy and ethics \nimplications of the approaches developed in an orthogonal fashion. \n \nRecommended Reading \nPlease see Course Materials for recommended reading for each session. \n \nAssessment - Part II students \nTwo assignments will be based on two datasets which will be provided to the students: \n \n\nAssignment 1 (shared for Part II and Part III/MPhil): this will be based on a dataset and will be \nworth 40% of the final mark. The task of the assessment will be to perform pre-processing and \nbasic data analysis in a \"colab\" and an answer sheet of no more than 1000 words. \n \nAssignment 2 (Part II): This assignment (worth 60% of the final mark) will be a fuller analysis of \na dataset focusing on machine learning algorithms and metrics. Discussion and interpretation of \nthe findings will be reported in a colab and a report of no more than 1200 words. \n \nAssessment  - Part III and MPhil students \nTwo assignments will be based on datasets which will be provided to the students: \n \nAssignment 1: this will be based on a dataset and will be worth 40% of the final mark. The task \nof the assessment will be to perform pre-processing and basic data analysis in a \"colab\" and an \nanswer sheet of no more than 1000 words. \n \nAssignment 2: The second assignment (worth 60% of the final mark) will be based on multiple \ndatasets. The students will be asked to compare and contract ML algorithms/solutions (trained \nand tested on the different data) and discuss the findings and interpretation in terms of health \ncontext. This will be in the form of a colab and a report of 1800 words. \n \nFurther Information \nCurrent Cambridge undergraduate students who are continuing onto Part III or the MPhil in \nAdvanced Computer Science may only take this module if they did NOT take it as a Unit of \nAssessment in Part II. \n \nThis module is shared with Part II of the Computer Science Tripos. Assessment will be adjusted \nfor the two groups of students to be at an appropriate level for whichever course the student is \nenrolled on. Further information about assessment and practicals will follow at the first lecture.\n\nMULTICORE SEMANTICS AND PROGRAMMING \nPrerequisites: Some familiarity with discrete mathematics (sets, partial orders, etc.) and with \nsequential Java programming will be assumed. Experience with operational semantics and with \nsome concurrent programming would be helpful. \nMoodle, timetable \n \nAims \nIn recent years multiprocessors have become ubiquitous, but building reliable concurrent \nsystems with good performance remains very challenging. The aim of this module is to \nintroduce some of the theory and the practice of concurrent programming, from hardware \nmemory models and the design of high-level programming languages to the correctness and \nperformance properties of concurrent algorithms. \n \nLectures \nPart 1: Introduction and relaxed-memory concurrency [Professor P. Sewell] \n \nIntroduction. Sequential consistency, atomicity, basic concurrent problems. [1 block] \nConcurrency on real multiprocessors: the relaxed memory model(s) for x86, ARM, and IBM \nPower, and theoretical tools for reasoning about x86-TSO programs. [2 blocks] \nHigh-level languages. An introduction to C/C++11 and Java shared-memory concurrency. [1 \nblock] \nPart 2: Concurrent algorithms [Dr T. Harris] \n \nConcurrent programming. Simple algorithms (readers/writers, stacks, queues) and correctness \ncriteria (linearisability and progress properties). Advanced synchronisation patterns (e.g. some \nof the following: optimistic and lazy list algorithms, hash tables, double-checked locking, RCU, \nhazard pointers), with discussion of performance and on the interaction between algorithm \ndesign and the underlying relaxed memory models. [3 blocks] \nResearch topics, likely to include one hour on transactional memory and one guest lecture. [1 \nblock] \nObjectives \nBy the end of the course students should: \n \nhave a good understanding of the semantics of concurrent programs, both at the multprocessor \nlevel and the C/Java programming language level; \nhave a good understanding of some key concurrent algorithms, with practical experience. \nAssessment - Part II Students \nTwo assignments each worth 50% \n \nRecommended reading \nHerlihy, M. and Shavit, N. (2008). The art of multiprocessor programming. Morgan Kaufmann. \n \nCoursework \nCoursework will consist of assessed exercises. \n\n \nPractical work \nPart 2 of the course will include a practical exercise sheet.  The practical exercises involve \nbuilding concurrent data structures and measuring their performance.  The work can be \ncompleted in C, Java, or similar languages. \n \nAssessment - Part III and MPhil Students \nAssignment 1, for 50% of the overall grade. Written coursework comprising a series of questions \non hardware and software relaxed-memory concurrency semantics. \nAssignment 2, for 50% of the overall grade. Written coursework comprising a series of questions \non the design of mutual exclusion locks and shared-memory data structures. \nFurther Information \nCurrent Cambridge undergraduate students who are continuing onto Part III or the MPhil in \nAdvanced Computer Science may only take this module if they did NOT take it as a Unit of \nAssessment in Part II. \n \nThis module is shared with Part II of the Computer Science Tripos. Assessment will be adjusted \nfor the two groups of students to be at an appropriate level for whichever course the student is \nenrolled on. Further information about assessment and practicals will follow at the first lecture.\n\nREINFORCEMENT LEARNING \n \nAims \nThe aim of this module is to introduce students to the foundations of the field of reinforcement \nlearning (RL), discuss state-of-the-art RL methods, incentivise students to produce critical \nanalysis of recent methods, and prompt students to propose novel solutions that address \nshortcomings of existing methods. If judged by the headlines, RL has seen an unprecedented \nsuccess in recent years. However, the vast majority of RL methods still have shortcomings that \nmight not be apparent at first glance. The aim of the course is to inspire students by \ncommunicating the promising aspects of RL, but ensure that students develop the ability to \nproduce a critical analysis of current RL limitations. \n \nObjectives \nStudents will learn the following technical concepts: \n \nfundamental RL terminology and mathematical formalism; a brief history of RL and its \nconnection to neuroscience and biological systems \nRL methods for discrete action spaces, e.g. deep Q-learning and large-scale Monte Carlo Tree \nSearch \nmethods for exploration, modelling uncertainty, and partial observability for RL \nmodern policy gradient and actor-critic methods \nconcepts needed to construct model-based RL and Model Predictive Control methods \napproaches to make RL data-efficient and ways to enable simulation-to-reality transfer \nexamples of fine-tuning foundation models and large language models (LLMs) with human \nfeedback; safe RL concepts; examples of using RL for safety validation \nexamples of using RL for scientific discovery \nStudents will also gain experience with analysing RL methods to uncover their strengths and \nshortcomings, as well as proposing extensions to improve performance. \n \nFinally, students will gain skills needed to create and deliver a successful presentation in a \nformat similar to that of conference presentations. \n \nWith all of the above, students who take part in this module will be well-prepared to start \nconducting research in the field of reinforcement learning. \n  \n \nSyllabus \nTopic 1: Introduction and Fundamentals \n \nOverview of RL: foundational ideas, history, and books; connection to neuroscience and \nbiological systems, recent industrial applications and research demonstrations \nMathematical fundamentals: Markov decision processes, Bellman equations, policy and value \niteration, temporal difference learning \nTopic 2: RL in Discrete Action Spaces \n\n \nQ-learning, function approximation and deep Q-learning; nonstationarity in RL and its \nimplications for deep learning; example applications (video games; initial example: Atari) \nMonte Carlo Tree Search; example applications (AlphaGo) \nTopic 3: Exploration, Uncertainty, Partial Observability \n \nMulti-armed bandits, Bayesian optimisation, regret analysis \nPartially observable Markov decision process; belief, memory, and sequence modelling \n(probabilistic methods, recurrent networks, transformers) \nTopic 4: Policy Gradient and Actor-critic Methods for Continuous Action Spaces \n \nImportance sampling, policy gradient theorem, actor-critic methods (SPG, DDPG) \nProximal policy optimisation; example applications \nTopic 5: Model-based RL and Model Predictive Control \n \nLearning dynamics models (graph networks, stochastic processes, diffusion models, \nphysics-based models, ensembles); planning with learned models \nModel predictive control; example applications (real-time control) \nTopic 6: Data-efficient RL and Simulation-to-reality Transfer \n \nData-efficient learning with probabilistic methods from real data (e.g. policy search in robotics), \nreal-to-sim inference and differentiable simulation, data-efficient simulation-to-reality transfer \nRL for physical systems (successful examples in locomotion, open problems in contact-rich \nmanipulation, applications to logistics, energy, and transport systems); examples of RL for \nhealthcare. \nTopic 7: RL with Human Feedback ; Safe RL and RL for Validation \n \nFine-tuning large language models (LLMs) and other foundation models with human feedback \n(TRLX,RL4LMs, a light-weight overview of RLHF) \nA review of SafeRL, example: optimising commercial HVAC systems using policy improvement \nwith constraints; improving safety using RL for validation: examples in autonomous driving and \nautonomous flying and aircraft collision avoidance \nTopic 8: RL for Scientific Discovery; Student Presentations \n \nExamples of RL for molecular design and drug discovery, active learning for synthesising new \nmaterials, RL for nuclear fusion experiments \nStudent presentations (based on essays and mini-projects) for other topics in RL, e.g. \nmulti-agent RL, hierarchical RL, RL for hyperparameter optimisation and NN architecture \nsearch, RL for multi-task transfer, lifelong RL, RL in biological systems, etc. \nAssessment \nThe assessment for this module consists of: \n \nEssay and mini-project (60%)  \n\nStudents will choose a concrete RL method from the literature, then complete a 2-part essay \ndescribed below: \nEssay Part 1 (1500 words, 20%): Introduce a formal description of the method and explain how \nthe method extended the state-of-the-art at the time of its publication \nEssay Part 2 or a mini-project (2500 words, 40%): Provide a critical analysis of the chosen RL \nmethod. \nPresentation of the essay and mini-project results (15%)  \nstudents will be expected to create and record a 15-minute video presentation of the analysis \ndescribed in their essay and mini-project. \nShort test of RL theory fundamentals (15%)  \nto test the understanding of RL fundamentals (~15 minutes, in-class, closed book) \nParticipation in seminar discussions (10%)  \ntake an active part in the seminars by asking clarifying questions and mentioning related works. \nRecommended Reading \nBooks \n \n[S&B] Reinforcement Learning: An Introduction (second print edition). Richard S. Sutton, \nAndrew G. Barto. [Available from the book\u2019s website as a free PDF updated in 2022] \n \n[CZ] Algorithms for Reinforcement Learning. Csaba Szepesvari. [Available from the book\u2019s \nwebsite as a free PDF updated in 2019] \n \n[MK] Algorithms for Decision Making. Mykel J. Kochenderfer Tim A. WheelerKyle H. Wray. \n[Available from the book\u2019s website as a free PDF updated in 2023] \n \n[DB] Reinforcement Learning and Optimal Control. Dimitri Bertsekas. [Available from the book\u2019s \nwebsite as a free PDF updated in 2023] \n \nPresentation guidelines \n \n[KN] Ten simple rules for effective presentation slides. Kristen M.Naegle. PLoS computational \nbiology 17, no. 12 (2021).\n \n\nTHEORIES OF SOCIO-DIGITAL DESIGN FOR HUMAN-CENTRED AI \n \nPrerequisites: This course is appropriate to any ACS student, and will assist in the development \nof critical thinking, argument and long-form writing skills. Prior experience of essay-based \nhumanities subjects at university or secondary school will be beneficial, but not required. \nMoodle, timetable \n \nAims \nThis module is a theoretically-oriented advanced introduction to the broad field of \nhuman-computer interaction, extending to consider topics such as intelligent user interfaces, \ncritical and speculative design, participatory design, cognitive models of users, human-centred, \nas well as more-than-human-centred design, and others. The course will not address purely \nengineering approaches to the development of user interfaces (unless there is a clear \ntheoretical question being addressed), but allow students to effectively link the political, social, \nand ethical considerations in HCI to specific technical and conceptual design challenges. The \nmodule builds on the Practical Research in Human-Centred AI course offered in Michaelmas \n(developed with researchers at Microsoft Research Cambridge) and a collaboration with the \nLeverhulme Centre for the Future of Intelligence at Cambridge. Participants may include visitors \nfrom these groups and/or interdisciplinary research students and academic guests from other \nUniversity departments, including Cambridge Digital Humanities. \n \nSyllabus \nThe syllabus will remain broadly within the area of human-computer interaction, including \ntheories of design practice and the social contexts of technology use. Individual seminar topics \nwill be selected in response to contemporary and recent research developments, in consultation \nwith members of the class and visiting contributors. \n \nRepresentative topics in the next year are likely to include: \n \nResponsible AI and HCI \nIntersectional design for social justice \nParticipatory design and co-design \nSustainable AI and more-than-human-centred design \nUsefulness and usability of ethical design toolkits \nThe EU AI Act in design practice \nAI and interface design for transparency \nDesigning otherwise: on anarchist HCI \n  \nObjectives \nOn completion of this module, students should have developed facility in discussing and \ncritiquing the aims of their research, especially for an audience drawn from other academic \ndisciplines, including the following skills: \n \nTheoretical motivation and defence of a research question. \n\nConsideration of a research proposal from one or more alternative theoretical perspectives. \nPotential critique of the theoretical basis for a programme of research. \nCoursework \nIn advance of each seminar, all participants must read the essential reading - generally one \nlong, plus one or two short papers. Further reading is suggested for some seminars. Some \nweeks, instead of shorter papers, students will be asked to explore specific design tools or \nresources. \n \nBefore seminars 2-8, each student will submit a written commentary on the paper, either \ngenerated using a large language model (LLM), or written in the style of an LLM, supplemented \nby a short original observation. \n \nEach member of the class must select one of the 8 seminar themes as the subject for a more \nextended critical review essay. The critical review should be written as an academic research \nessay, not in LLM style. \n \nEach seminar will include an informal design exercise to be carried out in small groups. The \npurpose of these exercises is to reflect on a variety of practical design resources, by applying \nthose resources to concrete case studies. \n \nThroughout the course, students will be expected to keep a \"reflective diary\", briefly reporting \nthemes that arise in discussion, reflections on the design exercises, and ways in which the \nreadings relate to their own research interests. \n  \n \nPractical work \nThere is no practical work element in this course. \n \nAssessment \nWritten critical review of a single discussion text (20%) \nReflective diary submitted at the end of the module (60%) \nIn advance of the session, each student will submit a written commentary on the paper, either \ngenerated using a large language model (LLM), or written in the style of an LLM. A short original \nobservation should be added (20%) \nRecommended reading \nTo be assigned during the module, as discussed above. Most set readings will be available \neither from the ACM Digital Library, or from institutional repositories of the authors. \n \nFurther Information \nStudents from the MPhil in the Ethics of AI, Data and Algorithms, MSt in AI, and MPhil in Digital \nHumanities courses also attend the lectures for this module.\n \n\nTHEORY OF DEEP LEARNING \nPrerequisites: A strong background in calculus, probability theory and linear algebra, familiarity \nwith differential equations, optimization and information theory is required. Students need to \nhave studied Deep Neural Networks, familiarity with deep learning terminology (e.g. \narchitectures, benchmark problems) as well as deep learning frameworks (pytorch, jax or \nsimilar) is assumed \nMoodle, timetable \n \nAims \nThe objectives of this course is to expose you to one of the most active contemporary research \ndirections within machine learning: the theory of deep learning (DL). While the first wave of \nmodern DL has focussed on empirical breakthroughs and ever more complex techniques, the \nattention is now shifting to building a solid mathematical understanding of why these techniques \nwork so well in the first place. The purpose of this course is to review this recent progress \nthrough a mixture of reading group sessions and invited talks by leading researchers in the \ntopic, and prepare you to embark on a PhD in modern deep learning research. Compared to \ntypical, non-mathematical courses on deep learning, this advanced module will appeal to those \nwho have strong foundations in mathematics and theoretical computer science. In a way, this \ncourse is our answer to the question \u201cWhat should the world\u2019s best computer science students \nknow about deep learning in 2023?\u201d \n \nLearning Outcomes \nThis course should prepare the best students to start a PhD in the theory and mathematics of \ndeep learning, and to start formulating their own hypotheses in this space. You will be \nintroduced to a range of empirical and mathematical tools developed in recent years for the \nstudy of deep learning behaviour, and you will build an awareness of the main open questions \nand current lines of attack. At the end of the course you will: \n \nbe able to explain why classical learning theory is insufficient to describe the phenomenon of \ngeneralization in DL \nbe able to design and interpret empirical studies aimed at understanding generalization \nbe able to explain the role of overparameterization: be able to use deep linear models as a \nmodel to study implicit regularisation of gradient-based learning \nbe able to state PAC-Bayes and Information-theoretic bounds, and apply them to DL \nbe able to explain the connection between Gaussian processes and neural networks, and will \nbe able to study learning dynamics in the neural tangent kernel (NTK) regime. \nbe able to formulate your own hypotheses about DL and choose tools to prove/test them \nleverage your deeper theoretical understanding to produce more robust, rigorous and \nreproducible solutions to practical machine learning problems. \nSyllabus \nEach week we'll have two to four student-lead presentations about a research paper chosen \nfrom a reading list. The reading list loosely follows the weekly breakdown below (but we adapt it \neach year based as this is an active research area): \n \n\nWeek 1: Introduction to the topic \nWeek 2: Empirical Studies of Deep Learning Phenomena \nWeek 3: Interpolation Regime and \u201cDouble Descent\u201d Phenomena \nWeek 4: Implicit Regularization in Deep Linear Models \nWeek 5: Approximation Theory \nWeek 6: Networks in the Infinite Width Limit \nWeek 7: PAC-Bayes and Information Theoretic Bounds for SGD \nWeek 8: Discussion and Coursework Spotlight Session \n \nAssessment \nStudents will be assessed on the following basis: \n \n20% for presentation/content contributed to the module: Each student will have an opportunity \nto present one of the recommended papers during Weeks 1-7 (30 minute slot including Q&A). \nFor the presentation, students should aim to communicate the core ideas behind the paper, and \nclearly present the results, conclusions, and future directions. Where possible, students are \nencouraged to comment on how the work itself fits into broader research goals. \n10% for active participation (regular attendance and contribution to discussions during the Q&A \nsessions). \n70% for a group project report, with a word limit of 4000. Either (1) an original research \nproposal/report with a hypothesis, review of related literature, and ideally preliminary findings, \nor, (2) reproduction and ideally extension of an existing relevant paper. \nCoursework reports are marked in line with general ACS guidelines, reports receiving top marks \nwill have have demonstrable research value (contain an original research idea, extension of \nexisting work, or a thorough reproduction effort which is valuable to the research community). \nAdditionally, some projects will be suggested during the first weeks of the course, although \nstudents are encouraged to come up with their own ideas. Students may be required to \nparticipate in group projects, with groups of size 2-3 (the class groups will be separated). For \nany given project, individual contributions would be noted for assessment though a viva \ncomponent. \nRelationship with related modules \nThis course can be considered as an advanced follow-up to the Part IIB course on Deep Neural \nNetworks. That course introduces some high level concepts that this course significantly \nexpands on. \n \nThis module complements L48: Machine Learning in the Physical World and L46: Principles of \nMachine Learning Systems, which focus on applications and hardware/systems aspects of ML \nrespectively. \n \nRecommended reading \nPNAS Colloquium on the Science of Deep Learning \nMathematics of Machine Learning book by Marc Deisenroth, Aldo Faisal and Cheng Soon Ong. \nProbabilistic Machine Learning: An Introduction book by Kevin Murphy \nMatus Telgarsky's lecture notes on deep learning theory  \n\nThese are in addition to the papers which will be discussed in the lectures.\n \n\nUNDERSTANDING NETWORKED-SYSTEMS PERFORMANCE \nPrerequisites: A student must possess knowledge comparable to the undergraduate subjects on \nUnix Tools, C/C++ programming and Computer Networks. Optionally; a student will be \nadvantaged by doing an introduction to Computer Systems Modelling; such as the Part 2 \nComputer Systems Modelling subject as well as having a background in measurement \nmethodologies such as that of L50: Introduction to networking and systems measurements \nprovided in the ACS/Part III curriculum. \nMoodle, timetable \n \nAims \nThis is a practical course, actively building and extending software tools to observe the detailed \nbehaviour of transaction-oriented datacenter-like software. Students will observe and \nunderstand sources of user-facing tail latency, including that stemming \nfrom resource contention, cross-program interference, bad software locking, and simple design \nerrors. \nA 2-hour weekly hybrid, lecture/lab format permits students continuous monitored progress with \ncomplexity of tasks building naturally upon the previous weeks learning. \n \nObjectives \nUpon successful completion of the course, students will be able to: \n \nMake order-of-magnitude estimates of software, hardware, and I/O speeds \nMake valid measurements of actual software, hardware, and I/O speeds \nCreate observation facilities, including logs and dashboards, as part of a software system \ndesign \nCreate tracing facilities to fully observe the execution of complex software \nTime-align traces across multiple computers \nDisplay dense tracing information in meaningful ways \nReason about the sources of real-time and transaction delays, including cross-program \nresource interference, remote procedure call delays, and software locking surprises \nFix programs based on the above reasoning, making their response times faster and more \nrobust \n \nSyllabus \nMeasurement \n   Week 1 Intro, Measuring CPU time, rdtsc, Measuring memory access times, Measuring disks, \n   Week 2 gettimeofday, logs Measuring networks, Remote Procedure Calls, Multi-threads, locks \nObservation \n   Week 3 RPC, logs, displaying traces, interference \n   Week 4 Antagonist programs, Logging, dashboards, profiling. \nKUtrace \n   Week 5 Kernel patches, hello world, post-processing \n   Week 6 KUtrace multi-CPU time display \n   Week 7 Client-server KUtrace, with antagonists and interference \n\n   Week 8 Other trace mysteries \n \n \nAssessment \nThis is a Lab-based module; assessment is based upon reports covering guided laboratory work \nperformed each week. Assessment will be via two submissions: \n \n20% assignment deadline week 4 based upon Lab work from Weeks 1-4; word target 1000, limit \n2000 \n80% assignment deadline week 8 based upon lab work from weeks 5-8; word target 2000, limit \n4000 \nThe intent of the first assessment point is to provide rich feedback to students based upon a \n20% assignment permitting focussed and improved work to be executedfor the final assignment. \n \nAll work in this module is expected to be the effort of the student. Enough equipment is \nprovisioned to allow each student an independent set of apparatus. While classmembers may \nfind sharing operational experience valuable all assessment is based upon a students sole \nsubmission based upon. their own experiments and findings. \n \nReading Material \nCore text \n \nRichard L. Sites, Understanding Software Dynamics, Addison-Wesley Professional Computing \nSeries, 2022 \nFurther reading: papers and presentations that you might find interesting. None are required \nreading \u2013 they are well-written and/or informative. \n \nJohn K. Ousterhout et al., A trace-driven analysis of the UNIX 4.2 BSD file system ACM \nSIGOPS Operating Systems Review, Vol. 19, No. 5, Dec, 1985 \nhttps://dl.acm.org/doi/pdf/10.1145/323627.323631 \nLuiz Andr\u00b4e Barroso, Jimmy Clidaras, Urs H\u00a8olzle, The Datacenter as a Computer: An \nIntroduction to the Design of Warehouse-Scale Machines, Second Edition \nJohn L. Hennessy, David A. Patterson, Computer Architecture, A Quantitative Approach 5th \nEdition \nGeorge Varghese, Network Algorithmics. Morgan Kaufmann \nActually keeping datacenter software up and running \u2013 how Google runs production systems \nSite Reliability Engineering Edited by Betsy Beyer, Chris Jones, Jennifer Petoff and Niall \nRichard Murphy free PDF: https://landing.google.com/sre/book.html \nHow NOT to run datacenters (27 minute video, funny/sad) dotScale 2014 - Robert Kennedy - \nLife in the Trenches of healthcare.gov https://www.youtube.com/watch?v=GLQyj-kBRdo \nAn extended (optional) reading list will also be provided. \n \n"
    ],
    "semesters_2690463961427311183": [
        "1st Semester \n \nDATABASES \n \nAims \nThis course introduces basic concepts for database systems as seen from the perspective of \napplication designers. That is, the focus is on the abstractions supported by database \nmanagement systems and not on how those abstractions are implemented. \n \nThe database world is currently undergoing swift and dramatic transformations largely driven by \nInternet-oriented applications and services. Today many more options are available to database \napplication developers than in the past and so it is becoming increasingly difficult to sort fact \nfrom fiction. The course attempts to cut through the fog with a practical approach that \nemphasises engineering tradeoffs that underpin these recent developments and also guide our \nselection of \u201cthe right tool for the job.\u201d \n \nThis course covers three approaches. First, the traditional mainstay of the database industry -- \nthe relational approach -- is described with emphasis on eliminating logical redundancy in data. \nThen two representatives of recent trends are presented -- graph-oriented and \ndocument-oriented databases. The lectures are supported with two Help and Tick sessions, \nwhere students gain hands-on experience and guidance with the Assessed Exercises (ticks). \n \nLectures \nL1 Introduction. What is a database system? What is a data model? Typical DBMS \nconfigurations and variations (fast queries or reliable update). In-core, in secondary store or \ndistributed. \nL2 Conceptual modelling. Entities, relations, E/R diagrams and implementation-independent \nmodelling, weak entities, cardinality. \nL3+4 The relational database model. Implementing E/R models with relational tables. Relational \nalgebra and SQL. Basic query primitives. Update anomalies caused by redundancy. Minimising \nredundancy with normalised schemas. \nL5 Transactions. On-Line Transaction Processing. On-line Analytical Processing. Reliability, \nthroughput, normal forms, ACID, BASE, eventual consistency. \nL6 Documents and semi-structured data. The NoSQL, schema-free movement. XML/JSON. \nKey/value stores. Embracing data redundancy: representing data for fast, application-specific \naccess. Path queries (if time permits). \nL7 Further SQL. Multisets, NULL values, aggregates, transitive closure, expressibility (nested \nqueries and recursive SQL). \nL8 Graph databases. Optimised for processing enormous numbers of nodes and edges. \nImplementing E/R models in a graph-oriented database. Comparison of the presented models. \nObjectives \nAt the end of the course students should \n\n \nbe able to design entity-relationship diagrams to represent simple database application \nscenarios \nknow how to convert entity-relationship diagrams to relational- and graph-oriented \nimplementations \nunderstand the fundamental tradeoff between the ease of updating data and the response time \nof complex queries \nunderstand that no single data architecture can be used to meet all data management \nrequirements \nbe familiar with recent trends in the database area. \nRecommended reading \nLemahieu, W., Broucke, S. van den and Baesens, B. (2018) Principles of database \nmanagement. Cambridge University Press.\n \n\nDIGITAL ELECTRONICS \n \nAims \nThe aims of this course are to present the principles of combinational and sequential digital logic \ndesign and optimisation at a gate level. The use of n and p channel MOSFETs for building logic \ngates is also introduced. \n \nTopics \nIntroduction. Semiconductors to computers. Logic variables. Examples of simple logic. Logic \ngates. Boolean algebra. De Morgan\u2019s theorem. \nLogic minimisation. Truth tables and normal forms. Karnaugh maps. Quine-McCluskey method. \nBinary adders. Half adder, full adder, ripple carry adder, fast carry generation. \nCombinational logic design: further considerations. Multilevel logic. Gate propagation delay. An \nintroduction to timing diagrams. Hazards and hazard elimination. Other ways to implement \ncombinational logic. \nIntroduction to practical classes. Prototyping box. Breadboard and Dual in line (DIL) packages. \nWiring. \nSequential logic. Memory elements. RS latch. Transparent D latch. Master-slave D flip-flop. T \nand JK flip-flops. Setup and hold times. \nSequential logic. Counters: Ripple and synchronous. Shift registers. System timing - setup time \nconstraint, clock skew, metastability. \nSynchronous State Machines. Moore and Mealy finite state machines (FSMs). Reset and self \nstarting. State transition diagrams. Elimination of redundant states - row matching and state \nequivalence/implication table. \nFurther state machines. State assignment: sequential, sliding, shift register, one hot. \nImplementation of FSMs. \nIntroduction to microprocessors. Microarchitecture, fetch, register access, memory access, \nbranching, execution time. \nElectronics, Devices and Circuits. Current and voltage, conductors/insulators/semiconductors, \nresistance, basic circuit theory, the potential divider. Solving non-linear circuits. P-N junction \n(forward and reverse bias), N and p channel MOSFETs (operation and characteristics) and \nn-MOSFET logic, e.g., n-MOSFET inverter. Power consumption and switching time problems \nproblems in n-MOSFET logic. CMOS logic (NOT, NAND and NOR gates), logic families, noise \nmargin. \nObjectives \nAt the end of the course students should \n \nunderstand the relationships between combination logic and boolean algebra, and between \nsequential logic and finite state machines; \nbe able to design and minimise combinational logic; \nappreciate tradeoffs in complexity and speed of combinational designs; \nunderstand how state can be stored in a digital logic circuit; \nknow how to design a simple finite state machine from a specification and be able to implement \nthis in gates and edge triggered flip-flops; \n\nunderstand how to use MOSFETs to build digital logic circuits. \nRecommended reading \n* Harris, D.M. and Harris, S.L. (2013). Digital design and computer architecture. Morgan \nKaufmann (2nd ed.). The first edition is still relevant. \nKatz, R.H. (2004). Contemporary logic design. Benjamin/Cummings. The 1994 edition is more \nthan sufficient. \nHayes, J.P. (1993). Introduction to digital logic design. Addison-Wesley. \n \nBooks for reference: \n \nHorowitz, P. and Hill, W. (1989). The art of electronics. Cambridge University Press (2nd ed.) \n(more analog). \nWeste, N.H.E. and Harris, D. (2005). CMOS VLSI Design - a circuits and systems perspective. \nAddison-Wesley (3rd ed.). \nMead, C. and Conway, L. (1980). Introduction to VLSI systems. Addison-Wesley. \nCrowe, J. and Hayes-Gill, B. (1998). Introduction to digital electronics. Butterworth-Heinemann. \nGibson, J.R. (1992). Electronic logic circuits. Butterworth-Heinemann.\n \n\nDISCRETE MATHEMATICS \n \nAims \nThe course aims to introduce the mathematics of discrete structures, showing it as an essential \ntool for computer science that can be clever and beautiful. \n \nLectures \nProof [5 lectures]. \nProofs in practice and mathematical jargon. Mathematical statements: implication, bi-implication, \nuniversal quantification, conjunction, existential quantification, disjunction, negation. Logical \ndeduction: proof strategies and patterns, scratch work, logical equivalences. Proof by \ncontradiction. Divisibility and congruences. Fermat\u2019s Little Theorem. \n \nNumbers [5 lectures]. \nNumber systems: natural numbers, integers, rationals, modular integers. The Division Theorem \nand Algorithm. Modular arithmetic. Sets: membership and comprehension. The greatest \ncommon divisor, and Euclid\u2019s Algorithm and Theorem. The Extended Euclid\u2019s Algorithm and \nmultiplicative inverses in modular arithmetic. The Diffie-Hellman cryptographic method. \nMathematical induction: Binomial Theorem, Pascal\u2019s Triangle, Fundamental Theorem of \nArithmetic, Euclid\u2019s infinity of primes. \n \nSets [9 lectures]. \nExtensionality Axiom: subsets and supersets. Separation Principle: Russell\u2019s Paradox, the \nempty set. Powerset Axiom: the powerset Boolean algebra, Venn and Hasse diagrams. Pairing \nAxiom: singletons, ordered pairs, products. Union axiom: big unions, big intersections, disjoint \nunions. Relations: composition, matrices, directed graphs, preorders and partial orders. Partial \nand (total) functions. Bijections: sections and retractions. Equivalence relations and set \npartitions. Calculus of bijections: characteristic (or indicator) functions. Finite cardinality and \ncounting. Infinity axiom. Surjections. Enumerable and countable sets. Axiom of choice. \nInjections. Images: direct and inverse images. Replacement Axiom: set-indexed constructions. \nSet cardinality: Cantor-Schoeder-Bernstein Theorem, unbounded cardinality, diagonalisation, \nfixed-points. Foundation Axiom. \n \nFormal languages and automata [5 lectures]. \nIntroduction to inductive definitions using rules and proof by rule induction. Abstract syntax \ntrees. Regular expressions and their algebra. Finite automata and regular languages: Kleene\u2019s \ntheorem and the Pumping Lemma. \n \nObjectives \nOn completing the course, students should be able to \n \nprove and disprove mathematical statements using a variety of techniques; \napply the mathematical principle of induction; \nknow the basics of modular arithmetic and appreciate its role in cryptography; \n\nunderstand and use the language of set theory in applications to computer science; \ndefine sets inductively using rules and prove properties about them; \nconvert between regular expressions and finite automata; \nuse the Pumping Lemma to prove that a language is not regular. \nRecommended reading \nBiggs, N.L. (2002). Discrete mathematics. Oxford University Press (Second Edition). \nDavenport, H. (2008). The higher arithmetic: an introduction to the theory of numbers. \nCambridge University Press. \nHammack, R. (2013). Book of proof. Privately published (Second edition). Available at: \nhttp://www.people.vcu.edu/ rhammack/BookOfProof/index.html \nHouston, K. (2009). How to think like a mathematician: a companion to undergraduate \nmathematics. Cambridge University Press. \nKozen, D.C. (1997). Automata and computability. Springer. \nLehman, E.; Leighton, F.T.; Meyer, A.R. (2014). Mathematics for computer science. Available \non-line. \nVelleman, D.J. (2006). How to prove it: a structured approach. Cambridge University Press \n(Second Edition).\n \n\nFOUNDATIONS OF COMPUTER SCIENCE \nAims \nThe main aim of this course is to present the basic principles of programming. As the \nintroductory course of the Computer Science Tripos, it caters for students from all backgrounds. \nTo those who have had no programming experience, it will be comprehensible; to those \nexperienced in languages such as C, it will attempt to correct any bad habits that they have \nlearnt. \n \nA further aim is to introduce the principles of data structures and algorithms. The course will \nemphasise the algorithmic side of programming, focusing on problem-solving rather than on \nhardware-level bits and bytes. Accordingly it will present basic algorithms for sorting, searching, \netc., and discuss their efficiency using O-notation. Worked examples (such as polynomial \narithmetic) will demonstrate how algorithmic ideas can be used to build efficient applications. \n \nThe course will use a functional language (OCaml). OCaml is particularly appropriate for \ninexperienced programmers, since a faulty program cannot crash and OCaml\u2019s unobtrusive type \nsystem captures many program faults before execution. The course will present the elements of \nfunctional programming, such as curried and higher-order functions. But it will also introduce \ntraditional (procedural) programming, such as assignments, arrays and references. \n \nLectures \nIntroduction to Programming. The role of abstraction and representation. Introduction to integer \nand floating-point arithmetic. Declaring functions. Decisions and booleans. Example: integer \nexponentiation. \nRecursion and Efficiency. Examples: Exponentiation and summing integers. Iteration versus \nrecursion. Examples of growth rates. Dominance and O-Notation. The costs of some \nrepresentative functions. Cost estimation. \nLists. Basic list operations. Append. Na\u00efve versus efficient functions for length and reverse. \nStrings. \nMore on lists. The utilities take and drop. Pattern-matching: zip, unzip. A word on polymorphism. \nThe \u201cmaking change\u201d example. \nSorting. A random number generator. Insertion sort, mergesort, quicksort. Their efficiency. \nDatatypes and trees. Pattern-matching and case expressions. Exceptions. Binary tree traversal \n(conversion to lists): preorder, inorder, postorder. \nDictionaries and functional arrays. Functional arrays. Dictionaries: association lists (slow) versus \nbinary search trees. Problems with unbalanced trees. \nFunctions as values. Nameless functions. Currying. The \u201capply to all\u201d functional, map. \nExamples: The predicate functionals filter and exists. \nSequences, or lazy lists. Non-strict functions such as IF. Call-by-need versus call-by-name. Lazy \nlists. Their implementation in OCaml. Applications, for example Newton-Raphson square roots. \nQueues and search strategies. Depth-first search and its limitations. Breadth-first search (BFS). \nImplementing BFS using lists. An efficient representation of queues. Importance of efficient data \nrepresentation. \n\nElements of procedural programming. Address versus contents. Assignment versus binding. \nOwn variables. Arrays, mutable or not. Introduction to linked lists. \nObjectives \nAt the end of the course, students should \n \nbe able to write simple OCaml programs; \nunderstand the fundamentals of using a data structure to represent some mathematical \nabstraction; \nbe able to estimate the efficiency of simple algorithms, using the notions of average-case, \nworse-case and amortised costs; \nknow the comparative advantages of insertion sort, quick sort and merge sort; \nunderstand binary search and binary search trees; \nknow how to use currying and higher-order functions; \nunderstand how OCaml combines imperative and functional programming in a single language. \nRecommended reading \nJohn Whitington. OCaml from the Very Beginning. (http://ocaml-book.com). \n \nOkasaki, C. (1998). Purely functional data structures. Cambridge University Press.\n \n\nHARDWARE PRACTICAL CLASSES \nThe Hardware Practical Classes accompany the Digital Electronics series of lectures. The aim \nof the Practical Classes is to enable students to get hands-on experience of designing, building, \nand testing and debugging of digital electronic circuits. The labs will take place in the Intel Lab. \nlocated in the William Gates Building (WGB). \n \nThe Digital Electronics lecture series occupies 12 lectures in the first 4 weeks of the Michaelmas \nTerm, while the Practical Classes occupy the latter 6 weeks of the Michaelmas Term and the \nfirst 6 weeks of the Lent Term. If required, extra sessions will be available for any students \nneeding to 'catch-up' on missed sessions or to complete any remaining practical work. \n \nThe Practical Classes take the form of 4 workshops, specifically: \n \nWorkshop 1 \u2013 Electronic Die; \nWorkshop 2 \u2013 Shaft Position Encoder; \nWorkshop 3 \u2013 Debouncing a Switch; \nWorkshop 4 \u2013 Framestore for an LED Array. \nIn general, the workshops require some preparatory work to be done prior to the practical \nsession. These tasks are highlighted at the beginning of each Worksheet. Typically this involves \npreparing a design that you will then build, test and modify during the practical class. Note that \ninsufficient preparation prior to the practical classes may compromise effective use of your time \nin the Intel lab. \n \nIn the Practical Classes you will usually work on your own, and you are expected to complete \none Workshop in each of your scheduled sessions. Demonstrators are available during the \nsessions to assist you with any queries or problems you may have. \n \nImportant: Remember to get your work ticked.\n \n\nINTRODUCTION TO GRAPHICS \nAims \nTo introduce the necessary background, the basic algorithms, and the applications of computer \ngraphics and graphics hardware. \n \nLectures \nBackground. What is an image? Resolution and quantisation. Storage of images in memory. [1 \nlecture] \nRendering. Perspective. Reflection of light from surfaces and shading. Geometric models. Ray \ntracing. [2 lectures] \nGraphics pipeline. Polygonal mesh models. Transformations using matrices in 2D and 3D. \nHomogeneous coordinates. Projection: orthographic and perspective. [1 lecture] \nGraphics hardware and modern OpenGL. GPU rendering. GPU frameworks and APIs. Vertex \nprocessing. Rasterisation. Fragment processing. Working with meshes and textures. Z-buffer. \nDouble-buffering and frame synchronization. [2 lectures] \n  \nHuman vision, colour and tone mapping. Perception of colour. Tone mapping. Colour spaces. [2 \nlectures] \nObjectives \nBy the end of the course students should be able to: \n \nunderstand and apply in practice basic concepts of ray-tracing: ray-object intersection, \nreflections, refraction, shadow rays, distributed ray-tracing, direct and indirect illumination; \ndescribe and explain the following algorithms: z-buffer, texture mapping, double buffering, \nmip-map, and normal-mapping; \nuse matrices and homogeneous coordinates to represent and perform 2D and 3D \ntransformations; understand and use 3D to 2D projection, the viewing volume, and 3D clipping; \nimplement OpenGL code for rendering of polygonal objects, control camera and lighting, work \nwith vertex and fragment shaders; \ndescribe a number of colour spaces and their relative merits. \nexplain the need for tone mapping and colour processing in rendering pipeline. \nRecommended reading \n* Shirley, P. and Marschner, S. (2009). Fundamentals of Computer Graphics. CRC Press (3rd \ned.). \n \nFoley, J.D., van Dam, A., Feiner, S.K. and Hughes, J.F. (1990). Computer graphics: principles \nand practice. Addison-Wesley (2nd ed.). \n \nKessenich, J.M., Sellers, G. and Shreiner, D (2016). OpenGL Programming Guide: The Official \nGuide to Learning OpenGL, Version 4.5 with SPIR-V, [seventh edition and later]\n \n\nOBJECT-ORIENTED PROGRAMMING \n \nAims \nThe goal of this course is to provide students with an understanding of Object-Oriented \nProgramming. Concepts are demonstrated in multiple languages, but the primary language is \nJava. \n \nLecture syllabus \nTypes, Objects and Classes Moving from functional to imperative. Functions, methods. Control \nflow. values, variables and types. Primitive Types. Classes as custom types. Objects vs \nClasses. Class definition, constructors. Static data and methods. \nDesigning Classes Identifying classes. UML class diagrams. Modularity. Encapsulation/data \nhiding. Immutability. Access modifiers. Parameterised types (Generics). \nPointers, References and Memory Pointers and references. Reference types in Java. The call \nstack. The heap. Iteration and recursion. Pass-by-value and pass-by-reference. \nInheritance Inheritance. Casting. Shadowing. Overloading. Overriding. Abstract Methods and \nClasses. \nPolymorphism and Multiple Inheritance Polymorphism in ML and Java. Multiple inheritance. \nInterfaces in Java. \nLifecycle of an Object Constructors and chaining. Destructors. Finalizers. Garbage Collection: \nreference counting, tracing. \nJava Collections and Object Comparison Java Collection interface. Key classes. Collections \nclass. Iteration options and the use of Iterator. Comparing primitives and objects. Operator \noverloading. \nError Handling Types of errors. Limitations of return values. Deferred error handling. Exceptions. \nCustom exceptions. Checked vs unchecked. Inappropriate use of exceptions. Assertions. \nDesignLanguage evolution Need for languages to evolve. Generics in Java. Type erasure. \nIntroduction to Java 8: Lambda functions, functions as values, method references, streams. \nDesign Patterns Introduction to design patterns. Open-closed principle. Examples of Singleton, \nDecorator, State, Composite, Strategy, Observer. [2 lectures] \nObjectives \nAt the end of the course students should \n \nProvide an overview of key OOP concepts that are transferable in mainstream programming \nlanguages; \nGain an understanding of software development approaches adopted in the industry including \nmaintainability, testing, and software design patterns; \nIncrease programming familiarity with Java; \nRecommended reading \n1. Real-World Software Development: A Project-Driven Guide to Fundamentals in Java [Urma et \nal.] \n2. Modern Java in Action (2nd edition) [Urma et al.]. \n3. Java How to Program: early objects [Deitel et al.]\n \n\nOCAML PRACTICAL CLASSES \n \nThe Role of Tickers \nA ticking session is a short (5 minute) one-to-one session with a ticker conducted on Thursday \nafternoons. The role of the ticker is twofold: \n \nto check you understand your solution (and didn\u2019t just have some lucky guesses or copy code \nfrom elsewhere) \nto give you general feedback on ways to improve your code. \nTo those ends a Ticker will typically ask you questions related to the core material of the tick and \ndiscuss your code directly. \n \nDeadline Extensions \nDeadline extensions can be granted for illness or similar reasons. To obtain an extension please \nemail Jon Ludlam in the first instance, clearly stating your justification for an extension and \nCCing your Director of Studies, who will need to support your request.\n \n\nSCIENTIFIC COMPUTING \n \nAims \nThis course is a hands-on introduction to using computers to investigate scientific models and \ndata. \n \nSyllabus \nPython notebooks. Overview of the Python programming language. Use of notebooks for \nscientific computing. \nNumerical computation. Writing fast vectorized code in numpy. Optimization and fitting. \nSimulation. \nWorking with data. Data import. Common ways to summarize and plot data, for univariate and \nmultivariate analysis. \nObjectives \nAt the end of the course students should \n \nbe able to import data, plot it, and summarize it appropriately \nbe able to write fast vectorized code for scientific / data work\n \n\n",
        "2nd Semester \nALGORITHMS 1 \nAims \nThe aim of this course is to provide an introduction to computer algorithms and data structures, \nwith an emphasis on foundational material. \n \nLectures \nSorting. Review of complexity and O-notation. Trivial sorting algorithms of quadratic complexity. \nReview of merge sort and quicksort, understanding their memory behaviour on statically \nallocated arrays. Heapsort. Stability. Other sorting methods including sorting in linear time. \nMedian and order statistics. [Ref: CLRS3 chapters 1, 2, 3, 6, 7, 8, 9] [about 4 lectures] \nStrategies for algorithm design. Dynamic programming, divide and conquer, greedy algorithms \nand other useful paradigms. [Ref: CLRS3 chapters 4, 15, 16] [about 3 lectures] \nData structures. Elementary data structures: pointers, objects, stacks, queues, lists, trees. \nBinary search trees. Red-black trees. B-trees. Hash tables. Priority queues and heaps. [Ref: \nCLRS3 chapters 6, 10, 11, 12, 13, 18] [about 5 lectures]. \nObjectives \nAt the end of the course students should: \n \nhave a thorough understanding of several classical algorithms and data structures; \nbe able to analyse the space and time efficiency of most algorithms; \nhave a good understanding of how a smart choice of data structures may be used to increase \nthe efficiency of particular algorithms; \nbe able to design new algorithms or modify existing ones for new applications and reason about \nthe efficiency of the result. \nRecommended reading \n* Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest and Clifford Stein Introduction to \nAlgorithms, Fourth Edition ISBN 9780262046305 Published: April 5, 2022. \n \nSedgewick, R., Wayne, K. (2011). Algorithms. Addison-Wesley. ISBN 978-0-321-57351-3. \n \nKleinberg, J. and Tardos, \u00c9. (2006). Algorithm design. Addison-Wesley. ISBN \n978-0-321-29535-4. \n \nKnuth, D.A. (2011). The Art of Computer Programming. Addison-Wesley. ISBN \n978-0-321-75104-1.\n \n\nALGORITHMS 2 \n \nAims \nThe aim of this course is to provide an introduction to computer algorithms and data structures, \nwith an emphasis on foundational material. \n \nLectures \nGraphs and path-finding algorithms. Graph representations. Breadth-first and depth-first search. \nSingle-source shortest paths: Bellman-Ford and Dijkstra algorithms. All-pairs shortest paths: \ndynamic programming and Johnson\u2019s algorithms. [About 4 lectures] \nGraphs and subgraphs. Maximum flow: Ford-Fulkerson method, Max-flow min-cut theorem. \nMatchings in bipartite graphs. Minimum spanning tree: Kruskal and Prim algorithms. Topological \nsort. [About 4 lectures] \nAdvanced data structures. Binomial heap. Amortized analysis: aggregate analysis, potential \nmethod. Fibonacci heaps. Disjoint sets. [About 4 lectures] \nObjectives \nAt the end of the course students should: \n \nhave a thorough understanding of several classical algorithms and data structures; \nbe able to analyse the space and time efficiency of most algorithms; \nhave a good understanding of how a smart choice of data structures may be used to increase \nthe efficiency of particular algorithms; \nbe able to design new algorithms or modify existing ones for new applications and reason about \nthe efficiency of the result. \nRecommended reading \n* Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest and Clifford Stein Introduction to \nAlgorithms, Fourth Edition ISBN 9780262046305 Published: April 5, 2022. \n \nSedgewick, R., Wayne, K. (2011). Algorithms. Addison-Wesley. ISBN 978-0-321-57351-3. \n \nKleinberg, J. and Tardos, \u00c9. (2006). Algorithm design. Addison-Wesley. ISBN \n978-0-321-29535-4. \n \nKnuth, D.A. (2011). The Art of Computer Programming. Addison-Wesley. ISBN \n978-0-321-75104-1.\n \n\nMACHINE LEARNING AND REAL-WORLD DATA \n \nAims \nThis course introduces students to machine learning algorithms as used in real-world \napplications, and to the experimental methodology necessary to perform statistical analysis of \nlarge-scale data from unpredictable processes. Students will perform 3 extended practicals, as \nfollows: \n \nStatistical classification: Determining movie review sentiment using Naive Bayes; \nSequence Analysis: Hidden Markov Modelling and its application to a task from biology \n(predicting protein interactions with a cell membrane); \nAnalysis of social networks, including detection of cliques and central nodes. \nSyllabus \nTopic One: Statistical Classification \nIntroduction to sentiment classification. \nNaive Bayes parameter estimation. \nStatistical laws of language. \nStatistical tests for classification tasks. \nCross-validation and test sets. \nUncertainty and human agreement. \nTopic Two: Sequence Analysis  \nHidden Markov Models (HMM) and HMM training. \nThe Viterbi algorithm. \nUsing an HMM in a biological application. \nTopic Three: Social Networks  \nProperties of networks: Degree, Diameter. \nBetweenness Centrality. \nClustering using betweenness centrality. \nObjectives \nBy the end of the course students should be able to: \n \nunderstand and program two simple supervised machine learning algorithms; \nuse these algorithms in statistically valid experiments, including the design of baselines, \nevaluation metrics, statistical testing of results, and provision against overtraining; \nvisualise the connectivity and centrality in large networks; \nuse clustering (i.e., a type of unsupervised machine learning) for detection of cliques in \nunstructured networks. \nRecommended reading \nJurafsky, D. and Martin, J. (2008). Speech and language processing. Prentice Hall. \n \nEasley, D. and Kleinberg, J. (2010). Networks, crowds, and markets: reasoning about a highly \nconnected world. Cambridge University Press.\n \n\nOPERATING SYSTEMS \n \nAims \nThe overall aim of this course is to provide a general understanding of the structure and key \nfunctions of the operating system. Case studies will be used to illustrate and reinforce \nfundamental concepts. \n \nLectures \nIntroduction to operating systems. Elementary computer organisaton. Abstract view of an \noperating system. Key concepts: layering, multiplexing, performance, caching, buffering, policies \nversus mechanisms. OS evolution: multi-programming, time-sharing. Booting. [1 lecture] \nProtection. Dual-mode operation. Protecting CPU, memory, I/O, storage. Kernels, micro-kernels, \nand modules. System calls. Virtual machines and containers. Subjects and objects. \nAuthentication. Access matrix: ACLs and capabilities. Covert channels. [1 lecture] \nProcesses. Job/process concepts. Process lifecycle. Process management. Context switching. \nInter-process communication. [1 lecture] \nScheduling. CPU-I/O burst cycle. Scheduling basics: preemption and non-preemption. \nScheduling algorithms: FCFS, SJF, SRTF, Priority, and Round Robin. Multilevel scheduling. [2 \nlectures] \nMemory management. Processes in memory. Logical versus physical addresses. Partitions: \nstatic versus dynamic, free space management, external fragmentation. Segmented memory. \nPaged memory: concepts, internal fragmentation, page tables. Demand paging/segmentation. \nPage replacement strategies: FIFO, OPT, and LRU with approximations including NRU, LFU, \nMFU, MRU. Working set schemes. Thrashing. [3 lectures] \nI/O subsystem. General structure. Polled mode versus interrupt-driven I/O. Programmed I/O \n(PIO) versus Direct Memory Access (DMA). Application I/O interface: block and character \ndevices, buffering, blocking, non-blocking, asynchronous, and vectored I/O. Other issues: \ncaching, scheduling, spooling, performance. [1 lecture] \nFile management. File concept. Directory and storage services. File names and metadata. \nDirectory name-space: hierarchies, DAGs, hard and soft links. File operations. Access control. \nExistence and concurrency control. [1 lecture] \nUnix case study. History. General structure. Unix file system: file abstraction, directories, mount \npoints, implementation details. Processes: memory image, lifecycle, start of day. The shell: \nbasic operation, commands, standard I/O, redirection, pipes, signals. Character and block I/O. \nProcess scheduling. [2 lectures] \nObjectives \nAt the end of the course students should be able to: \n \ndescribe the general structure and purpose of an operating system; \nexplain the concepts of process, address space, and file; \ncompare and contrast various CPU scheduling algorithms; \ncompare and contrast mechanisms involved in memory and storage management; \ncompare and contrast polled, interrupt-driven, and DMA-based access to I/O devices. \nRecommended reading \n\n* Silberschatz, A., Galvin, P.C., and Gagne, G. (2018). Operating systems concepts. Wiley (10th \ned.). Prior editions, at least back to 8th ed., should also be acceptable. \nAnderson, T. and Dahlin, M. (2014). Operating Systems: Principles and Practice. Recursive \nBooks (2nd ed.). \nBacon, J. and Harris, T. (2003). Operating systems. Addison-Wesley (3rd ed.). Currently \nappears out-of-print. \nLeffler, S. (1989). The design and implementation of the 4.3BSD Unix operating system. \nAddison-Wesley. \nMcKusick, M.K., Neville-Neil, G.N. and Watson, R.N.M. (2014) The Design and Implementation \nof the FreeBSD Operating System. Pearson Education. (2nd ed.).\n \n\nINTERACTION DESIGN \nAims \nThe aim of this course is to provide an introduction to interaction design, with an emphasis on \nunderstanding and experiencing the user-centred design process, from conducting user \nresearch and requirements development to implementation and evaluation, while understanding \nthe background to human-computer interaction. \n \nLectures \nCourse overview and user research methods. Introduction to the course and practicals. \nUser-Centred Design. User research methods. \nKeeping usera in mind. User research data analysis. Identifying users and stakeholders. \nRepresenting user goals and activities. Identifying and establishing requirements. \nDesign and prototyping. Methods for exploring the design space. Prototyping and different kinds \nof prototypes. \nVisual and interaction design. Memory, perception, attention, and their implications for \ninteraction design. Modalities of interaction, interaction design patterns, information architecture, \nand their implications for interaction design. \nEvaluation. Practical methods for evaluating designs. Evaluation methods without users. \nEvaluation methods with users. \nCase studies from industry and research. Guest lectures (the topics of these lectures are \nsubject to change). \nObjectives \nBy the end of the course students should \n \nhave a thorough understanding of the user-centred design process and be able to apply it to \ninteraction design; \nbe able to design new user interfaces that are informed by principles of good design, and the \nprinciples of human visual perception, cognition and communication; \nbe able to prototype and implement interactive user interfaces with a strong emphasis on users, \nusability and appearance; \nbe able to evaluate existing or new user interfaces using multiple techniques; \nbe able to compare and contrast different design techniques and to critique their applicability to \nnew domains. \nRecommended reading \n* Preece, J., Rogers, Y. and Sharp, H. (2015). Interaction design. Wiley (4th ed.).\n \n\nINTRODUCTION TO PROBABILITY \n \nAims \nThis course provides an elementary introduction to probability and statistics with applications. \nProbability theory and the related field of statistical inference provide the foundations for \nanalysing and making sense of data. The focus of this course is to introduce the language and \ncore concepts of probability theory. The course will also cover some applications of statistical \ninference and algorithms in order to equip students with the ability to represent problems using \nthese concepts and analyse the data within these principles. \n \nLectures \nPart 1 - Introduction to Probability \n \nIntroduction. Counting/Combinatorics (revision), Probability Space, Axioms, Union Bound. \nConditional probability. Conditional Probabilities and Independence, Bayes\u2019Theorem, Partition \nTheorem \nPart 2 - Discrete Random Variables \n \nRandom variables. Definition of a Random Variable, Probability Mass Function, Cumulative \nDistribution, Expectation. \nProbability distributions. Definition and Properties of Expectation, Variance, different ways of \ncomputing them, Examples of important Distributions (Bernoulli, Binomial, Geometric, Poisson), \nPrimer on Continuous Distributions including Normal and Exponential Distributions. \nMultivariate distributions. Multiple Random Variables, Joint and Marginal Distributions, \nIndependence of Random Variables, Covariance. \nPart 3 - Moments and Limit Theorems \n \nIntroduction. Law of Average, Useful inequalities (Markov and Chebyshef), Weak Law of Large \nNumbers (including Proof using Chebyshef\u2019s inequality), Examples. \nMoments and Central Limit Theorem. Introduction to Moments of Random Variables, Central \nLimit Theorem (Proof using Moment Generating functions), Example. \nPart 4 - Applications/Statistics \n \nStatistics. Classical Parameter Estimation (Maximum-Likelihood-Estimation), bias, sample \nmean, sample variance), Examples (Collision-Sampling, Estimating Population Size). \nAlgorithms. Online Algorithms (Secretary Problem, Odd\u2019s Algorithm). \nObjectives \nAt the end of the course students should \n \nunderstand the basic principles of probability spaces and random variables \nbe able to formulate problems using concepts from probability theory and compute or estimate \nprobabilities \nbe familiar with more advanced concepts such as moments, limit theorems and applications \nsuch as parameter estimation \n\nRecommended reading \n* Ross, S.M. (2014). A First course in probability. Pearson (9th ed.). \n \nBertsekas, D.P. and Tsitsiklis, J.N. (2008). Introduction to probability. Athena Scientific. \n \nGrimmett, G. and Welsh, D. (2014). Probability: an Introduction. Oxford University Press (2nd \ned.). \n \nDekking, F.M., et. al. (2005) A modern introduction to probability and statistics. Springer.\n\nSOFTWARE AND SECURITY ENGINEERING \n \nAims \nThis course aims to introduce students to software and security engineering, and in particular to \nthe problems of building large systems, safety-critical systems and systems that must withstand \nattack by capable opponents. Case histories of failure are used to illustrate what can go wrong, \nwhile current software and security engineering practice is studied as a guide to how failures \ncan be avoided. \n \nLectures \n1. What is a security policy or a safety case? Definitions and examples; one-way flows for both \nconfidentiality and safety properties; separation of duties. Top-down and bottom-up analysis \nmethods. What architecture can do, versus benefits of decoupling policy from mechanism. \n \n2. Examples of safety and security policies. Safety and security usability; the pyramid of harms. \nPredicting and mitigating user errors. The prevention of fraud and error in accounting systems; \nthe safety usability of medical devices. \n \n3. Attitudes to risk: expected  utility, prospect theory, framing, status quo bias. Authority, \nconformity and gender; mental models, affordances and defaults. The characteristics of human \nmemory; forgetting passwords versus guessing them. \n \n4. Security protocols; how to enforce policy using  structured human interaction, cryptography or \nboth. Middleperson attacks.The role of verification and its limitations. \n \n5. Attacks on TLS, from rogue CAs through side channels to Heartbleed. Other types of \nsoftware bugs: syntactic, timing, concurrency, code injection, buffer overflows. Defensive \nprogramming: secure coding, contracts. Fuzzing. \n \n6. The software crisis. Examples of large-scale project failure, such as the London Ambulance \nService system and the NHS National Programme for IT. Intrinsic difficulties with complex \nsoftware. \n \n7. Software engineering as the management of complexity. The software life cycle; requirements \nanalysis methods; modular design; the role of prototyping; the waterfall, spiral and agile models. \n \n8. The economics of software as a Service (SaaS); the impact SaaS has on software \nengineering. Continuous integration, release engineering, behavioural analytics and experiment \nframeworks, rearchitecting systems while in operation. \n \n9. Critical systems: safety as an emergent system property. Examples of catastrophic failure: \nfrom Therac-25 to the Boeing 737Max. The problems of managing redundancy. The overall \nprocess of safety engineering. \n \n\n10. Managing the development of critical systems: tools and methods, individual versus group \nproductivity, economics of testing and agile development, measuring outcomes versus process, \nthe technical and human aspects of management, post-market surveillance and coordinated \ndisclosure. The sustainability of products with software components. \n \nAt the end of the course students should know how writing programs with tough assurance \ntargets, in large teams, or both, differs from the programming exercises they have engaged in \nso far. They should understand the different models of software development described in the \ncourse as well as the value of various development and management tools. They should \nunderstand the development life cycle and its basic economics. They should understand the \nvarious types of bugs, vulnerabilities and hazards, how to find them, and how to avoid \nintroducing them. Finally, they should be prepared for the organizational aspects of their Part IB \ngroup project. \n \nRecommended reading \nAnderson, R. (Third Edition 2020). Security engineering (Part 1 and Chapters 27-28). Wiley. \nAvailable at: http://www.cl.cam.ac.uk/users/rja14/book.html\n \n\n",
        "3rd Semester \nCONCURRENT AND DISTRIBUTED SYSTEMS \n \nAims \nThis course considers two closely related topics, Concurrent Systems and Distributed Systems, \nover 16 lectures. The aim of the first half of the course is to introduce concurrency control \nconcepts and their implications for system design and implementation. The aims of the latter \nhalf of the course are to study the fundamental characteristics of distributed systems, including \ntheir models and architectures; the implications for software design; some of the techniques that \nhave been used to build them; and the resulting details of good distributed algorithms and \napplications. \n \nLectures: Concurrency \nIntroduction to concurrency, threads, and mutual exclusion. Introduction to concurrent systems; \nthreads; interleaving; preemption; parallelism; execution orderings; processes and threads; \nkernel vs. user threads; M:N threads; atomicity; mutual exclusion; and mutual exclusion locks \n(mutexes). \nAutomata Composition. Synchronous and asynchronous parallelism; sequential consistency; \nrendezvous. Safety, liveness and deadlock; the Dining Philosophers; Hardware foundations for \natomicity: test-and-set, load-linked/store-conditional and fence instructions. Lamport bakery \nalgorithm. \nCommon design patterns: semaphores, producer-consumer, and MRSW. Locks and invariants; \nsemaphores; condition synchronisation; N-resource allocation; two-party and generalised \nproducer-consumer; Multi-Reader, Single-Write (MRSW) locks. \nCCR, monitors, and concurrency in practice. Conditional critical regions (CCR); monitors; \ncondition variables; signal-wait vs. signal-continue semantics; concurrency in practice (kernels, \npthreads, Java, Cilk, OpenMP). \nDeadlock and liveness guarantees Offline vs. online; model checking; resource allocation \ngraphs; lock order checking; deadlock prevention, avoidance, detection, and recovery; livelock; \npriority inversion; auto parallelisation. \nConcurrency without shared data; transactions. Active objects; message passing; tuple spaces; \nCSP; and actor models. Composite operations; transactions; ACID; isolation; and serialisability. \nFurther transactions History graphs; good and bad schedules; isolation vs. strict isolation; \n2-phase locking; rollback; timestamp ordering (TSO); and optimistic concurrency control (OCC). \nCrash recovery, lock-free programming, and transactional memory. Write-ahead logging, \ncheckpoints, and recovery. Lock-free programming. Hardware and software transactional \nmemories. \n  \n \nLectures: Distributed Systems \nIntroduction to distributed systems; RPC. Avantages and challenges of distributed systems; \nunbounded delay and partial failure; network protocols; transparency; client-server systems; \nremote procedure call (RPC); marshalling; interface definition languages (IDLs). \n\nSystem models and faults. Synchronous, partially synchronous, and asynchronous network \nmodels; crash-stop, crash-recovery, and Byzantine faults; failures, faults, and fault tolerance; \ntwo generals problem. \nTime, clocks, and ordering of events. Physical clocks; leap seconds; UTC; clock synchronisation \nand drift; Network Time Protocol (NTP). Causality; happens-before relation. \nLogical time; Lamport clocks; vector clocks. Broadcast (FIFO, causal, total order); gossip \nprotocols. \nReplication. Quorums; idempotence; replica consistency; read-after-write consistency. State \nmachine replication; leader-based replication. \nConsensus and total order broadcast. FLP result; leader election; the Raft consensus algorithm. \nReplica consistency. Two-phase commit; relationship between 2PC and consensus; \nlinearizability; ABD algorithm; eventual consistency; CAP theorem. \nCase studies. Conflict-free Replicated Data Types (CRDTs); collaborative text editing. Google's \nSpanner; TrueTime. \nObjectives \nAt the end of Concurrent Systems portion of the course, students should: \n \nunderstand the need for concurrency control in operating systems and applications, both mutual \nexclusion and condition synchronisation; \nunderstand how multi-threading can be supported and the implications of different approaches; \nbe familiar with the support offered by various programming languages for concurrency control \nand be able to judge the scope, performance implications and possible applications of the \nvarious approaches; \nbe aware that dynamic resource allocation can lead to deadlock; \nunderstand the concept of transaction; the properties of transactions, how they can be \nimplemented, and how their performance can be optimised based on optimistic assumptions; \nunderstand how the persistence properties of transactions are addressed through logging; and \nhave a high-level understanding of the evolution of software use of concurrency in the \noperating-system kernel case study. \nAt the end of the Distributed Systems portion of the course, students should: \n \nunderstand the difference between shared-memory concurrency and distributed systems; \nunderstand the fundamental properties of distributed systems and their implications for system \ndesign; \nunderstand notions of time, including logical clocks, vector clocks, and physical time \nsynchronisation; \nbe familiar with various approaches to data and service replication, as well as the concept of \nreplica consistency; \nunderstand the effects of large scale on the provision of fundamental services and the tradeoffs \narising from scale; \nappreciate the implications of individual node and network communications failures on \ndistributed computation; \nbe aware of a variety of programming models and abstractions for distributed systems, such as \nRPC, middleware, and total order broadcast; \n\nbe familiar with a range of distributed algorithms, such as consensus, causal broadcast, and \ntwo-phase commit. \nRecommended reading \nModern Operating Systems (free PDF available online) by Andrew S Tanenbaum, Herbert Bos \nJava Concurrency in Practice' (2006 but still highly relevant) by Brian Goetz \nKleppmann, M. (2017). Designing data-intensive applications. O\u2019Reilly. \nTanenbaum, A.S. and van Steen, M. (2017). Distributed systems, 3rd edition. available online. \nCachin, C., Guerraoui, R. and Rodrigues, L. (2011) Introduction to Reliable and Secure \nDistributed Programming. Springer (2nd edition).\n \n\nDATA SCIENCE \n \nAims \nThis course introduces fundamental tools for describing and reasoning about data. There are \ntwo themes: designing probability models to describe systems; and drawing conclusions based \non data generated by such systems. \n \nLectures \nSpecifying and fitting probability models. Random variables. Maximum likelihood estimation. \nGenerative and supervised models. Goodness of fit. \nFeature spaces. Vector spaces, bases, inner products, projection. Linear models. Model fitting \nas projection. Design of features. \nHandling probability models. Handling pdf and cdf. Bayes\u2019s rule. Monte Carlo estimation. \nEmpirical distribution. \nInference. Bayesianism. Frequentist confidence intervals, hypothesis testing. Bootstrap \nresampling. \nRandom processes. Markov chains. Stationarity, and drift analysis. Processes with memory. \nLearning a random process. \nObjectives \nAt the end of the course students should \n \nbe able to formulate basic probabilistic models, including discrete time Markov chains and linear \nmodels \nbe familiar with common random variables and their uses, and with the use of empirical \ndistributions rather than formulae \nunderstand different types of inference about noisy data, including model fitting, hypothesis \ntesting, and making predictions \nunderstand the fundamental properties of inner product spaces and orthonormal systems, and \ntheir application to modelling \nRecommended reading \n* F.M. Dekking, C. Kraaikamp, H.P. Lopuha\u00e4, L.E. Meester (2005). A modern introduction to \nprobability and statistics: understanding why and how. Springer. \n \nS.M. Ross (2002). Probability models for computer science. Harcourt / Academic Press. \n \nM. Mitzenmacher and E. Upfal (2005). Probability and computing: randomized algorithms and \nprobabilistic analysis. Cambridge University Press.\n \n\nECAD AND ARCHITECTURE PRACTICAL CLASSES \nAims \nThe aims of this course are to enable students to apply the concepts learned in the Computer \nDesign course. In particular a web based tutor is used to introduce the SystemVerilog hardware \ndescription language, while the remaining practical classes will then allow students to implement \nthe design of components in this language. \n \nPractical Classes \nWeb tutor The first class uses a web based tutor to rapidly teach the SystemVerilog language. \nFPGA design flow Test driven hardware development for FPGA including an embedded \nprocessor and peripherals [3 classes] \nEmbedded system implementation Embedded system implementation on FPGA [3-4 classes] \nObjectives \nGain experience in electronic computer aided design (ECAD) through learning a design-flow for \nfield programmable gate arrays (FPGAs). \nLearn how to interface to peripherals like a touch screen. \nLearn how to debug hardware and software systems in simulation. \nUnderstand how to construct and program a heterogeneous embedded system. \nRecommended reading \n* Harris, D.M. and Harris, S.L. (2007). Digital design and computer architecture: from gates to \nprocessors. Morgan Kaufmann. \n \nPointers to sources of more specialist information are included on the associated course web \npage. \n \nIntroduction \nThe ECAD and Architecture Laboratory sessions are a companion to the Introduction to \nComputer Architecture course. The objective is to provide experience of hardware design for \nFPGA including use of a small embedded processor. It covers hardware design in \nSystemVerilog, embedded software design in RISC-V assembly and C, and use of FPGA tools. \n \nPrerequisites \nThis course assumes familiarity with the material from Part IA Digital Electronics, although we \nwill use automated tools to perform many of the steps you might have done there by hand (for \nexample, logic minimisation). \n \nWe will be using a Linux command-line environment. We provide scripts and guidance on what \nyou need, however you may find it useful to review Unix Tools, in particular basic navigation \nsuch as ls, cd, mkdir, cp, mv. We'll be using the GCC compiler and Makefiles, described in parts \n31-32. We will also be using git for revision control and submission of work for ticking - you \nshould review at least parts 23-25 and 29. \n \nPracticalities \n\nThe course runs over the 8 weeks of Michaelmas term. The course has been designed so you \ncan do most of the work at home or at any time you wish. You should be able to run all the \nnecessary tools on your own machine, through the use of virtual machines and Docker \ncontainers. We have a number of routes to do this in case some of them aren't suitable for the \nmachine you have. \n \nWe're running the course in a hybrid mode this year. We will provide online help sessions via \nMicrosoft Teams as well as in-person lab sessions on Fridays and Tuesdays 2-5pm during term. \n \nThe core components of the course, being the RISC-V architecture and software and ticked \nexercise, can be done entirely online, without needing access to hardware. They can be \nundertaken with any of the platforms we support, including MCS Linux (on the Intel Lab \nworkstations and remotely) if your own machine isn't suitable. We expect everyone to complete \nthese parts. \n \nThe rest of the course is optional. The hardware simulation and FPGA compilation can be done \non your machine without access to hardware, if you are able to run the virtual machine. \n \nThe parts that require access to an FPGA board will need the ability to run the virtual machine \nand you be able to collect and return an FPGA board from the Department. That process will be \ndescribed when you come to those parts. There is an optional starred tick available for \ncompletion of this part, which will need assessment by a demonstrator in person. \n \nWe'll do our best to support all the different platforms and variations, but please bear with us - \nit's quite possible we'll come across a problem we haven't seen before! \n \nWe have provided four routes you can use different tools to complete the course. Some routes \nnecessarily exclude some material but this material is optional. \n \nAssessment \nIn a similar way to other practical courses, this course carries one 'tick'. Everyone should expect \nto receive this tick, and those who do not should expect to lose marks on their final exams. \n \nThis year we have adopted the 'chime' autoticker used by Further Java. You will check out the \ninitial files as a git repository from the chime server, commit your work, and push the repository \nback to submit it. You can then press a button to run tests against your code and the autoticker \nwill tell you whether you have passed. Additionally you may be selected for a (real/video) \ninterview with a ticker to discuss your code and understanding of the material. \n \nTicking procedures will only be available during weeks 1-8 of Michaelmas term. The deadline for \nsubmitting Tick 1 is 5pm on the Tuesday of week 6 (19 November 2024). After passing the \nautoticker you may be asked for a tick interview with a demonstrator (online or in person). You \ncan gain Tick 1* during the timetabled lab sessions, assuming demonstrators are available. \n \n\nIf you do not submit a successful Tick 1 to the autoticker by 5pm on the Tuesday of week 6 you \ncan self-certify an extension. If you need more time than this you will need to ask your Directory \nof Studies to organise a further extension. The hard deadline by which all ticks must be \ncompleted is noon on Friday 31 January 2025 (the Head of Department Notices is the definitive \ndocument). Extensions beyond this date due to exceptional circumstances require a formal \napplication from your college to the Examiners. \n \nScheduling \nWhile the full course contains 8 exercises, there is not a tight binding to doing one exercise per \nweek. Students should expect to spend about three hours per week on the course, however it is \nrecommended that you make a start on the next exercise if you have time in hand. \n \nDemonstrator help will be focused on the Tuesday and Friday 2-5pm slots during term the lab \nwould normally operate in, so you may find it helpful to work in these times as that will provide \nthe quickest response to questions. \n \nCollaboration \nWhen we have done these labs in person, we have often found they worked well when doing \nthem collaboratively. While your work must be your own, students can work together and help \neach other, and demonstrators contribute advice and experience with the tools. \n \nFor the timetabled sessions, which are Tuesdays and Fridays 2-5pm UK time during term, there \nwill be demonstrators available in the lab and at the same time we'll use the ECAD group on \nMicrosoft Teams. With this we can provide online audio/video help, screensharing and (for the \nWindows and possibly Mac Teams apps) optional remote control of your development \nenvironment. You will be added to the Team in week 1 - if you believe you have been missed \nplease post in the Moodle forum. In Teams there are a number of Helpdesk Channels (A-C) - if \nyou need help during lab times, post in Help Centre and a demonstrator can ask you to go to a \nspecific channel. There are also Student Breakout Channels (D-F) which are available for \nstudents to chat amongst themselves. \n \nFor help outside of timetabled sessions we've set up a Moodle forum where you can post \nquestions - we'll keep an eye on this at other times, but students are encouraged to use it to \nsupport each other. \n \nStudents are of course free to use other platforms to help each other. If you find anything useful \nthat we might use in future, please let us know! \n \nIf you are having difficulty accessing both Moodle and Teams, please email theo.markettos at \ncl.cam.ac.uk. \n \nFeedback \nWe revise the course each year, which may cause new bugs in the code or the notes. The \ncreators (Theo Markettos and Simon Moore) appreciate constructive feedback. \n\n \nCredits \nThis course was written by Theo Markettos and Simon Moore, with portions developed by \nRobert Eady, Jonas Fiala, Paul Fox, Vlad Gavrila, Brian Jones and Roy Spliet. We would like to \nthank Altera, Intel and Terasic for funding and other contributions.\n \n\nECONOMICS, LAW AND ETHICS \n \nAims \nThis course aims to give students an introduction to some basic concepts in economics, law and \nethics. \n \nLectures \nClassical economics and consumer theory. Prices and markets; Pareto efficiency; preferences; \nutility; supply and demand; the marginalist revolution; elasticity; the welfare theorems; \ntransaction costs. \nInformation economics. The discriminating monopolist; marginal costs; effects of technology on \nsupply and demand; competition and information; lock in; real and virtual networks; Metcalfe\u2019s \nlaw; the dominant firm model; price discrimination; bundling; income distribution. \nMarket failure and behavioural economics. Market failure: the business cycle; recession and \ntechnology; tragedy of the commons; externalities; monopoly rents; asymmetric information: the \nmarket for lemons; adverse selection; moral hazard; signalling. Behavioural economics: \nbounded rationality, heuristics and biases; nudge theory; the power of defaults; agency effects. \nAuction theory and game theory. Auction theory: types of auctions; strategic equivalence; the \nrevenue equivalence theorem; the winner\u2019s curse; problems with real auctions; mechanism \ndesign and the combinatorial auction; applicability of auction mechanisms in computer science; \nadvertising auctions. Game theory: the choice between cooperation and conflict; strategic \nforms; dominant strategy equilibrium; Nash equilibrium; the prisoners\u2019 dilemma; evolution of \nstrategies; stag hunt; volunteer\u2019s dilemma; chicken; iterated games; hawk-dove; application to \ncomputer science. \nPrinciples of law. Criminal and civil law; contract law; choice of law and jurisdiction; arbitration; \ntort; negligence; defamation; intellectual property rights. \nLaw and the Internet. Computer evidence; the General Data Protection Regulation; UK laws that \nspecifically affect the Internet; e-commerce regulations; privacy and electronic communications. \nPhilosophies of ethics. Authority, intuitionist, egoist and deontological theories; utilitarian and \nRawlsian models; morality; insights from evolutionary psychology, neurology, and experimental \nethics; professional codes of ethics; research ethics. \nContemporary ethical issues. The Internet and social policy; current debates on privacy, \nsurveillance, and censorship; responsible vulnerability disclosure; algorithmic bias; predictive \npolicing; gamification and engagement; targeted political advertising; environmental impacts. \nObjectives \nOn completion of this course, students should be able to: \n \nReflect on and discuss professional, economic, social, environmental, moral and ethical issues \nrelating to computer science \nDefine and explain economic and legal terminology and arguments \nApply the philosophies and theories covered to computer science problems and scenarios \nReflect on the main constraints that market, legislation and ethics place on firms dealing in \ninformation goods and services \nRecommended reading \n\n* Shapiro, C. & Varian, H. (1998). Information rules. Harvard Business School Press. \nHare, S. (2022). Technology is not neutral: A short guide to technology ethics. London \nPublishing Partnership. \n \nFurther reading: \nSmith, A. (1776). An inquiry into the nature and causes of the wealth of nations, available at    \nhttp://www.econlib.org/library/Smith/smWN.html \nThaler, R.H. (2016). Misbehaving. Penguin. \nGalbraith, J.K. (1991). A history of economics. Penguin. \nPoundstone, W. (1992). Prisoner\u2019s dilemma. Anchor Books. \nPinker, S (2011). The Better Angels of our Nature. Penguin. \nAnderson, R. (2008). Security engineering (Chapter 7). Wiley. \nVarian, H. (1999). Intermediate microeconomics - a modern approach. Norton. \nNuffield Council on Bioethics (2015) The collection, linking and use of data in biomedical \nresearch and health care.\n \n\nFURTHER GRAPHICS \n \nAims \nThe course introduces fundamental concepts of modern graphics pipelines. \n \nLectures \nThe order and content of lectures is provisional and subject to change. \n \nGeometry Representations. Parametric surfaces, implicit surfaces, meshes, point-set surfaces, \ngeometry processing pipeline. [1 lecture] \nDiscrete Differential Geometry. Surface normal and curvature, Laplace-Beltrami operator, heat \ndiffusion. [1 lecture] \nGeometry Processing.  Parametrization, filtering, 3D capture. [1 lecture] \nAnimation I. Animation types, animation pipeline, rigging/skinning, character animation. [1 \nlecture] \nAnimation II. Blending transformations, pose-space animation, controllers. [1 lecture] \nThe Rendering Equation. Radiosity, reflection models, BRDFs, local vs. global illumination. [1 \nlecture] \nDistributed Ray Tracing. Quadrature, importance sampling, recursive ray tracing. [1 lecture] \nInverse Rendering. Differentiable rendering, inverse problems. [1 lecture] \nObjectives \nOn completing the course, students should be able to \n \nunderstand and use fundamental 3D geometry/scene representations and operations; \nlearn animation techniques and controls; \nlearn how light transport is modeled and simulated in rendering; \nunderstand how graphics and rendering can be used to solve computer perception problems. \nRecommended reading \nStudents should expect to refer to one or more of these books, but should not find it necessary \nto purchase any of them. \n \nShirley, P. and Marschner, S. (2009). Fundamentals of Computer Graphics. CRC Press (3rd \ned.). \nWatt, A. (2000). 3D Computer Graphics. Addison-Wesley (3rd ed). \nHughes, van Dam, McGuire, Skalar, Foley, Feiner and Akeley (2013). Computer Graphics: \nPrinciples and Practice. Addison-Wesley (3rd edition) \nAkenine-M\u00f6ller, et. al. (2018). Real-time rendering. CRC Press (4th ed.).\n \n\nFURTHER JAVA \n \nAims \nThe goal of this course is to provide students with the ability to understand the advanced \nprogramming features available in the Java programming language, completing the coverage of \nthe language started in the Programming in Java course. The course is designed to \naccommodate students with diverse programming backgrounds; consequently Java is taught \nfrom first principles in a practical class setting where students can work at their own pace from a \ncourse handbook. \n \nObjectives \n \nAt the end of the course students should \n \nunderstand different mechanisms for communication between distributed applications and be \nable to evaluate their trade-offs; \nbe able to use Java generics and annotations to improve software usability, readability and \nsafety; \nunderstand and be able to exploit the Java class-loading mechansim; \nunderstand and be able to use concurrency control correctly; \nbe able to implement a vector clock algorithm and the happens-before relation. \nRecommended reading \n* Goetz, B. (2006). Java concurrency in practice. Addison-Wesley. \nGosling, J., Joy, B., Steele, G., Bracha, G. and Buckley, A. (2014). The Java language \nspecification, Java SE 8 Edition. Addison-Wesley. \nhttp://docs.oracle.com/javase/specs/jls/se8/html/\n \n\nGROUP PROJECTS \nExample Risk Report \nThe risk report should focus on the risks to your project succeeding on time (and not, say, on \ngeneral health and safety points). It is only 50 words. Example: \n \n\"Identified risks: \n * Training data requires access authorisation that has not been approved, and we don\u2019t know if \nclient can do this \n * Two group members have unreliable network connections and may miss meetings \n * One group member is in timezone UTC+8, and will have to attend meetings late at night \n * Bug reports on Github for a library we plan to use suggest maintenance updates will be \nnecessary for recent Android release\"\n \n\nINTRODUCTION TO COMPUTER ARCHITECTURE \n \nAims \nThe aims of this course are to introduce a hardware description language (SystemVerilog) and \ncomputer architecture concepts in order to design computer systems. The parallel ECAD+Arch \npractical classes will allow students to apply the concepts taught in lectures. \n \nLectures \nPart 1 - Gates to processors  \n \nTechnology trends and design challenges. Current technology, technology trends, ECAD trends, \nchallenges. [1 lecture] \nDigital system design. Practicalities of mapping SystemVerilog descriptions of hardware \n(including a processor) onto an FPGA board. Tips and pitfalls when generating larger modular \ndesigns. [1 lecture] \nEight great ideas in computer architecture. [1 lecture] \nReduced instruction set computers and RISC-V. Introduction to the RISC-V processor design. [1 \nlecture] \nExecutable and synthesisable models. [1 lecture] \nPipelining. [2 lectures] \nMemory hierarchy and caching. Caching, etc. [1 lecture] \nSupport for operating systems. Memory protection, exceptions, interrupts, etc. [1 lecture] \nOther instruction set architectures. CISC, stack, accumulator. [1 lecture] \nPart 2  \n \nOverview of Systems-on-Chip (SoCs) and DRAM. [1 lecture] High-level SoCs, DRAM storage \nand accessing. \nMulticore Processors. [2 lectures] Communication, cache coherence, barriers and \nsynchronisation primitives. \nGraphics processing units (GPUs) [2 lectures] Basic GPU architecture and programming. \nFuture Directions [1 lecture] Where is computer architecture heading? \nObjectives \nAt the end of the course students should \n \nbe able to read assembler given a guide to the instruction set and be able to write short pieces \nof assembler if given an instruction set or asked to invent an instruction set; \nunderstand the differences between RISC and CISC assembler; \nunderstand what facilities a processor provides to support operating systems, from memory \nmanagement to software interrupts; \nunderstand memory hierarchy including different cache structures and coherency needed for \nmulticore systems; \nunderstand how to implement a processor in SystemVerilog; \nappreciate the use of pipelining in processor design; \nhave an appreciation of control structures used in processor design; \n\nhave an appreciation of system-on-chips and their components; \nunderstand how DRAM stores data; \nunderstand how multicore processors communicate; \nunderstand how GPUs work and have an appreciation of how to program them. \nRecommended reading \n* Patterson, D.A. and Hennessy, J.L. (2017). Computer organization and design: The \nhardware/software interface RISC-V edition. Morgan Kaufmann. ISBN 978-0-12-812275-4. \n \nRecommended further reading: \n \nHarris, D.M. and Harris, S.L. (2012). Digital design and computer architecture. Morgan \nKaufmann. ISBN 978-0-12-394424-5. \nHennessy, J. and Patterson, D. (2006). Computer architecture: a quantitative approach. Elsevier \n(4th ed.). ISBN 978-0-12-370490-0. (Older versions of the book are also still generally relevant.) \n \nPointers to sources of more specialist information are included in the lecture notes and on the \nassociated course web page\n \n\nPROGRAMMING IN C AND C++ \n \nAims \nThis course aims \n \nto provide a solid introduction to programming in C and to provide an overview of the principles \nand constraints that affect the way in which the C programming language has been designed \nand is used; \nto introduce the key additional features of C++. \n  \n \nLectures \nIntroduction to the C language. Background and goals of C. Types, expressions, control flow \nand strings. \n(continued) Functions. Multiple compilation units. Scope. Segments. Incremental compilation. \nPreprocessor. \n(continued) Pointers and pointer arithmetic. Function pointers. Structures and Unions. \n(continued) Const and volatile qualifiers. Typedefs. Standard input/output. Heap allocation. \nMiscellaneous Features, Hints and Tips. \nC semantics and tools. Undefined vs implementation-defined behaviour. Buffer and integer \noverflows. ASan, MSan, UBsan, Valgrind checkers. \nMemory allocation, data structures and aliasing. Malloc/free, tree and DAG examples and their \ndeallocation using a memory arena. \nFurther memory management. Reference Counting and Garbage Collection \nMemory hierarchy and cache optimization. Data structure layouts. Intrusive lists. Array-of-structs \nvs struct-of-arrays representations. Loop blocking. \nDebugging. Using print statements, assertions and an interactive debugger. Unit testing and \nregression. \nIntroduction to C++. Goals of C++. Differences between C and C++. References versus \npointers. Overloaded functions. \nObjects in C++. Classes and structs. Destructors. Resource Acquisition is Initialisation. Operator \noverloading. Virtual functions. Casts. Multiple inheritance. Virtual base classes. \nOther C++ concepts. Templates and meta-programming. \nObjectives \nAt the end of the course students should \n \nbe able to read and write C programs; \nunderstand the interaction between C programs and the host operating system; \nbe familiar with the structure of C program execution in machine memory; \nunderstand the potential dangers of writing programs in C; \nunderstand the object model and main additional features of C++. \nRecommended reading \n* Kernighan, B.W. and Ritchie, D.M. (1988). The C programming language. Prentice Hall (2nd \ned.).\n \n\nSEMANTICS OF PROGRAMMING LANGUAGES \n \nAims \nThe aim of this course is to introduce the structural, operational approach to programming \nlanguage semantics. It will show how to specify the meaning of typical programming language \nconstructs, in the context of language design, and how to reason formally about semantic \nproperties of programs. \n \nLectures \nIntroduction. Transition systems. The idea of structural operational semantics. Transition \nsemantics of a simple imperative language. Language design options. [2 lectures] \nTypes. Introduction to formal type systems. Typing for the simple imperative language. \nStatements of desirable properties. [2 lectures] \nInduction. Review of mathematical induction. Abstract syntax trees and structural induction. \nRule-based inductive definitions and proofs. Proofs of type safety properties. [2 lectures] \nFunctions. Call-by-name and call-by-value function application, semantics and typing. Local \nrecursive definitions. [2 lectures] \nData. Semantics and typing for products, sums, records, references. [1 lecture] \nSubtyping. Record subtyping and simple object encoding. [1 lecture] \nSemantic equivalence. Semantic equivalence of phrases in a simple imperative language, \nincluding the congruence property. Examples of equivalence and non-equivalence. [1 lecture] \nConcurrency. Shared variable interleaving. Semantics for simple mutexes; a serializability \nproperty. [1 lecture] \nObjectives \nAt the end of the course students should \n \nbe familiar with rule-based presentations of the operational semantics and type systems for \nsome simple imperative, functional and interactive program constructs; \nbe able to prove properties of an operational semantics using various forms of induction \n(mathematical, structural, and rule-based); \nbe familiar with some operationally-based notions of semantic equivalence of program phrases \nand their basic properties. \nRecommended reading \n* Pierce, B.C. (2002). Types and programming languages. MIT Press. \nHennessy, M. (1990). The semantics of programming languages. Wiley. Out of print, but \navailable on the web at \nhttp://www.cs.tcd.ie/matthew.hennessy/splexternal2015/resources/sembookWiley.pdf \nWinskel, G. (1993). The formal semantics of programming languages. MIT Press.\n \n\nUNIX TOOLS \n \nAims \nThis video-lecture course gives students who have already basic Unix/Linux experience some \nadditional practical software-engineering knowledge: how to use the shell and related tools as \nan efficient working environment, how to automate routine tasks, and how version-control and \nautomated-build tools can help to avoid confusion and accidents, especially when working in \nteams. These are essential skills, both in industrial software development and student projects. \n \nLectures \nUnix concepts. Brief review of Unix history and design philosophy, documentation, terminals, \ninter-process communication mechanisms and conventions, shell, command-line arguments, \nenvironment variables, file descriptors. \nShell concepts. Program invocation, redirection, pipes, file-system navigation, argument \nexpansion, quoting, job control, signals, process groups, variables, locale, history and alias \nfunctions, security considerations. \nScripting. Plain-text formats, executables, #!, shell control structures and functions. Startup \nscripts. \nText, file and networking tools. sed, grep, chmod, find, ssh, rsync, tar, zip, etc. \nVersion control. diff, patch, RCS, Subversion, git. \nSoftware development tools. C compiler, linker, debugger, make. \nPerl. Introduction to a powerful scripting and text-manipulation language. [2 lectures] \nObjectives \nAt the end of the course students should \n \nbe confident in performing routine user tasks on a POSIX system, understand command-line \nuser-interface conventions and know how to find more detailed documentation; \nappreciate how simple tools can be combined to perform a large variety of tasks; \nbe familiar with the most common tools, file formats and configuration practices; \nbe able to understand, write, and maintain shell scripts and makefiles; \nappreciate how using a version-control system and fully automated build processes help to \nmaintain reproducibility and audit trails during software development; \nknow enough about basic development tools to be able to install, modify and debug C source \ncode; \nhave understood the main concepts of, and gained initial experience in, writing Perl scripts \n(excluding the facilities for object-oriented programming). \nRecommended reading \nRobbins, A. (2005). Unix in a nutshell. O\u2019Reilly (4th ed.). \nSchwartz, R.L., Foy, B.D. and Phoenix, T. (2011). Learning Perl. O\u2019Reilly (6th ed.).\n \n\n",
        "4th Semester \n \nCOMPILER CONSTRUCTION \n \nAims \nThis course aims to cover the main concepts associated with implementing compilers for \nprogramming languages. We use a running example called SLANG (a Small LANGuage) \ninspired by the languages described in 1B Semantics. A toy compiler (written in ML) is provided, \nand students are encouraged to extend it in various ways. \n \nLectures \nOverview of compiler structure The spectrum of interpreters and compilers; compile-time and \nrun-time. Compilation as a sequence of translations from higher-level to lower-level intermediate \nlanguages, where each translation preserves semantics. The structure of a simple compiler: \nlexical analysis and syntax analysis, type checking, intermediate representations, optimisations, \ncode generation. Overview of run-time data structures: stack and heap. Virtual machines. [1 \nlecture] \nLexical analysis and syntax analysis. Lexical analysis based on regular expressions and finite \nstate automata. Using LEX-tools. How does LEX work? Parsing based on context-free \ngrammars and push-down automata. Grammar ambiguity, left- and right-associativity and \noperator precedence. Using YACC-like tools. How does YACC work? LL(k) and LR(k) parsing \ntheory. [3 lectures] \nCompiler Correctness Recursive functions can be transformed into iterative functions using the \nContinuation-Passing Style (CPS) transformation. CPS applied to a (recursive) SLANG \ninterpreter to derive, in a step-by-step manner, a correct stack-based compiler. [3 lectures] \nData structures, procedures/functions Representing tuples, arrays, references. Procedures and \nfunctions: calling conventions, nested structure, non-local variables. Functions as first-class \nvalues represented as closures. Simple optimisations: inline expansion, constant folding, \nelimination of tail recursion, peephole optimisation. [5 lectures] \nAdvanced topics Run-time memory management (garbage collection). Static and dynamic \nlinking. Objects and inheritance; implementation of method dispatch. Try-catch exception \nmechanisms. Compiling a compiler via bootstrapping. [4 lectures] \nObjectives \nAt the end of the course students should understand the overall structure of a compiler, and will \nknow significant details of a number of important techniques commonly used. They will be \naware of the way in which language features raise challenges for compiler builders. \n \nRecommended reading \n* Aho, A.V., Sethi, R. and Ullman, J.D. (2007). Compilers: principles, techniques and tools. \nAddison-Wesley (2nd ed.). \nMogensen, T. \u00c6. (2011). Introduction to compiler design. Springer. http://www.diku.dk/ \ntorbenm/Basics.\n \n\nCOMPUTATION THEORY \n \nAims \nThe aim of this course is to introduce several apparently different formalisations of the informal \nnotion of algorithm; to show that they are equivalent; and to use them to demonstrate that there \nare uncomputable functions and algorithmically undecidable problems. \n \nLectures \nIntroduction: algorithmically undecidable problems. Decision problems. The informal notion of \nalgorithm, or effective procedure. Examples of algorithmically undecidable problems. [1 lecture] \nRegister machines. Definition and examples; graphical notation. Register machine computable \nfunctions. Doing arithmetic with register machines. [1 lecture] \nUniversal register machine. Natural number encoding of pairs and lists. Coding register machine \nprograms as numbers. Specification and implementation of a universal register machine. [2 \nlectures] \nUndecidability of the halting problem. Statement and proof. Example of an uncomputable partial \nfunction. Decidable sets of numbers; examples of undecidable sets of numbers. [1 lecture] \nTuring machines. Informal description. Definition and examples. Turing computable functions. \nEquivalence of register machine computability and Turing computability. The Church-Turing \nThesis. [2 lectures] \nPrimitive and partial recursive functions. Definition and examples. Existence of a recursive, but \nnot primitive recursive function. A partial function is partial recursive if and only if it is \ncomputable. [2 lectures] \nLambda-Calculus. Alpha and beta conversion. Normalization. Encoding data. Writing recursive \nfunctions in the lambda-calculus. The relationship between computable functions and \nlambda-definable functions. [3 lectures] \nObjectives \nAt the end of the course students should \n \nbe familiar with the register machine, Turing machine and lambda-calculus models of \ncomputability; \nunderstand the notion of coding programs as data, and of a universal machine; \nbe able to use diagonalisation to prove the undecidability of the Halting Problem; \nunderstand the mathematical notion of partial recursive function and its relationship to \ncomputability. \nRecommended reading \n* Hopcroft, J.E., Motwani, R. and Ullman, J.D. (2001). Introduction to automata theory, \nlanguages, and computation. Addison-Wesley (2nd ed.). \n* Hindley, J.R. and Seldin, J.P. (2008). Lambda-calculus and combinators, an introduction. \nCambridge University Press (2nd ed.). \nCutland, N.J. (1980). Computability: an introduction to recursive function theory. Cambridge \nUniversity Press. \nDavis, M.D., Sigal, R. and Weyuker, E.J. (1994). Computability, complexity and languages. \nAcademic Press (2nd ed.). \n\nSudkamp, T.A. (2005). Languages and machines. Addison-Wesley (3rd ed.).\n \n\nCOMPUTER NETWORKING \n \nAims \nThe aim of this course is to introduce key concepts and principles of computer networks. The \ncourse will use a top-down approach to study the Internet and its protocol stack. Instances of \narchitecture, protocol, application-examples will include email, web and media-streaming. We \nwill cover communications services (e.g., TCP/IP) required to support such network \napplications. The implementation and deployment of communications services in practical \nnetworks: including wired and wireless LAN environments, will be followed by a discussion of \nissues of network-management. Throughout the course, the Internet\u2019s architecture and \nprotocols will be used as the primary examples to illustrate the fundamental principles of \ncomputer networking. \n \nLectures \nIntroduction. Overview of networking using the Internet as an example. LANs and WANs. OSI \nreference model, Internet TCP/IP Protocol Stack. Circuit-switching, packet-switching, Internet \nstructure, networking delays and packet loss. [3 lectures] \nLink layer and local area networks. Link layer services, error detection and correction, Multiple \nAccess Protocols, link layer addressing, Ethernet, hubs and switches, Point-to-Point Protocol. [2 \nlectures] \nWireless and mobile networks. Wireless links and network characteristics, Wi-Fi: IEEE 802.11 \nwireless LANs. [1 lecture] \nNetwork layer addressing. Network layer services, IP, IP addressing, IPv4, DHCP, NAT, ICMP, \nIPv6. [3 lectures] \nNetwork layer routing. Routing and forwarding, routing algorithms, routing in the Internet, \nmulticast. [3 lectures] \nTransport layer. Service models, multiplexing/demultiplexing, connection-less transport (UDP), \nprinciples of reliable data transfer, connection-oriented transport (TCP), TCP congestion control, \nTCP variants. [6 lectures] \nApplication layer. Client/server paradigm, WWW, HTTP, Domain Name System, P2P. [1.5 \nlectures] \nMultimedia networking. Networked multimedia applications, multimedia delivery requirements, \nmultimedia protocols (SIP), content distribution networks. [0.5 lecture] \nObjectives \nAt the end of the course students should \n \nbe able to analyse a communication system by separating out the different functions provided \nby the network; \nunderstand that there are fundamental limits to any communications system; \nunderstand the general principles behind multiplexing, addressing, routing, reliable transmission \nand other stateful protocols as well as specific examples of each; \nunderstand what FEC is; \nbe able to compare communications systems in how they solve similar problems; \n\nhave an informed view of both the internal workings of the Internet and of a number of common \nInternet applications and protocols. \nRecommended reading \n* Peterson, L.L. and Davie, B.S. (2011). Computer networks: a systems approach. Morgan \nKaufmann (5th ed.). ISBN 9780123850591 \nKurose, J.F. and Ross, K.W. (2009). Computer networking: a top-down approach. \nAddison-Wesley (5th ed.). \nComer, D. and Stevens, D. (2005). Internetworking with TCP-IP, vol. 1 and 2. Prentice Hall (5th \ned.). \nStevens, W.R., Fenner, B. and Rudoff, A.M. (2003). UNIX network programming, Vol.I: The \nsockets networking API. Prentice Hall (3rd ed.).\n \n\nFURTHER HUMAN\u2013COMPUTER INTERACTION \n \nAims \nThis aim of this course is to provide an introduction to the theoretical foundations of Human \nComputer Interaction, and an understanding of how these can be applied to the design of \ncomplex technologies. \n \nLectures \nTheory driven approaches to HCI. What is a theory in HCI? Why take a theory driven approach \nto HCI? \nDesign of visual displays. Segmentation and variables of the display plane. Modes of \ncorrespondence. \nGoal-oriented interaction. Using cognitive theories of planning, learning and understanding to \nunderstand user behaviour, and what they find hard. \nDesigning smart systems. Using statistical methods to anticipate user needs and actions with \nBayesian strategies. \nDesigning efficient systems. Measuring and optimising human performance through quantitative \nexperimental methods. \nDesigning meaningful systems. Qualitative research methods to understand social context and \nrequirements of user experience. \nEvaluating interactive system designs. Approaches to evaluation in systems research and \nengineering, including Part II Projects. \nDesigning complex systems. Worked case studies of applying the theories to a hard HCI \nproblem. Research directions in HCI. \nObjectives \nAt the end of the course students should be able to apply theories of human performance and \ncognition to system design, including selection of appropriate techniques to analyse, observe \nand improve the usability of a wide range of technologies. \n \nRecommended reading \n* Preece, J., Sharp, H. and Rogers, Y. (2015). Interaction design: beyond human-computer \ninteraction. Wiley (Currently in 4th edition, but earlier editions will suffice). \n \nFurther reading: \n \nCarroll, J.M. (ed.) (2003). HCI models, theories and frameworks: toward a multi-disciplinary \nscience. Morgan Kaufmann.\n \n\nLOGIC AND PROOF \n \nAims \nThis course will teach logic, especially the predicate calculus. It will present the basic principles \nand definitions, then describe a variety of different formalisms and algorithms that can be used \nto solve problems in logic. Putting logic into the context of Computer Science, the course will \nshow how the programming language Prolog arises from the automatic proof method known as \nresolution. It will introduce topics that are important in mechanical verification, such as binary \ndecision diagrams (BDDs), SAT solvers and modal logic. \n \nLectures \nIntroduction to logic. Schematic statements. Interpretations and validity. Logical consequence. \nInference. \nPropositional logic. Basic syntax and semantics. Equivalences. Normal forms. Tautology \nchecking using CNF. \nThe sequent calculus. A simple (Hilbert-style) proof system. Natural deduction systems. \nSequent calculus rules. Sample proofs. \nFirst order logic. Basic syntax. Quantifiers. Semantics (truth definition). \nFormal reasoning in FOL. Free versus bound variables. Substitution. Equivalences for \nquantifiers. Sequent calculus rules. Examples. \nClausal proof methods. Clause form. A SAT-solving procedure. The resolution rule. Examples. \nRefinements. \nSkolem functions, Unification and Herbrand\u2019s theorem. Prenex normal form. Skolemisation. \nMost general unifiers. A unification algorithm. Herbrand models and their properties. \nResolution theorem-proving and Prolog. Binary resolution. Factorisation. Example of Prolog \nexecution. Proof by model elimination. \nSatisfiability Modulo Theories. Decision problems and procedures. How SMT solvers work. \nBinary decision diagrams. General concepts. Fast canonical form algorithm. Optimisations. \nApplications. \nModal logics. Possible worlds semantics. Truth and validity. A Hilbert-style proof system. \nSequent calculus rules. \nTableaux methods. Simplifying the sequent calculus. Examples. Adding unification. \nSkolemisation. The world\u2019s smallest theorem prover? \nObjectives \nAt the end of the course students should \n \nbe able to manipulate logical formulas accurately; \nbe able to perform proofs using the presented formal calculi; \nbe able to construct a small BDD; \nunderstand the relationships among the various calculi, e.g. SAT solving, resolution and Prolog; \nunderstand the concept of a decision procedure and the basic principles of \u201csatisfiability modulo \ntheories\u201d. \nbe able to apply the unification algorithm and to describe its uses. \nRecommended reading \n\n* Huth, M. and Ryan, M. (2004). Logic in computer science: modelling and reasoning about \nsystems. Cambridge University Press (2nd ed.). \nBen-Ari, M. (2001). Mathematical logic for computer science. Springer (2nd ed.).\n \n\nPROLOG \n \nAims \nThe aim of this course is to introduce programming in the Prolog language. Prolog encourages \na different programming style to Java or ML and particular focus is placed on programming to \nsolve real problems that are suited to this style. Practical experimentation with the language is \nstrongly encouraged. \n \nLectures \nIntroduction to Prolog. The structure of a Prolog program and how to use the Prolog interpreter. \nUnification. Some simple programs. \nArithmetic and lists. Prolog\u2019s support for evaluating arithmetic expressions and lists. The space \ncomplexity of program evaluation discussed with reference to last-call optimisation. \nBacktracking, cut, and negation. The cut operator for controlling backtracking. Negation as \nfailure and its uses. \nSearch and cut. Prolog\u2019s search method for solving problems. Graph searching exploiting \nProlog\u2019s built-in search mechanisms. \nDifference structures. Difference lists: introduction and application to example programs. \nBuilding on Prolog. How particular limitations of Prolog programs can be addressed by \ntechniques such as Constraint Logic Programming (CLP) and tabled resolution. \nObjectives \nAt the end of the course students should \n \nbe able to write programs in Prolog using techniques such as accumulators and difference \nstructures; \nknow how to model the backtracking behaviour of program execution; \nappreciate the unique perspective Prolog gives to problem solving and algorithm design; \nunderstand how larger programs can be created using the basic programming techniques used \nin this course. \nRecommended reading \n* Bratko, I. (2001). PROLOG programming for artificial intelligence. Addison-Wesley (3rd or 4th \ned.). \nSterling, L. and Shapiro, E. (1994). The art of Prolog. MIT Press (2nd ed.). \n \nFurther reading: \n \nO\u2019Keefe, R. (1990). The craft of Prolog. MIT Press. [This book is beyond the scope of this \ncourse, but it is very instructive. If you understand its contents, you\u2019re more than prepared for \nthe examination.]\n \n\nARTIFICIAL INTELLIGENCE \n \nPrerequisites: Algorithms 1, Algorithms 2. In addition the course requires some mathematics, in \nparticular some use of vectors and some calculus. Part IA Natural Sciences Mathematics or \nequivalent and Discrete Mathematics are likely to be helpful although not essential. Similarly, \nelements of Machine Learning and Real World Data, Foundations of Data Science, Logic and \nProof, Prolog and Complexity Theory are likely to be useful. This course is a prerequisite for the \nPart II courses Machine Learning and Bayesian Inference and Natural Language Processing. \nThis course is a prerequisite for: Natural Language Processing \nExam: Paper 7 Question 1, 2 \nPast exam questions, Moodle, timetable \n \nAims \nThe aim of this course is to provide an introduction to some fundamental issues and algorithms \nin artificial intelligence (AI). The course approaches AI from an algorithmic, computer \nscience-centric perspective; relatively little reference is made to the complementary \nperspectives developed within psychology, neuroscience or elsewhere. The course aims to \nprovide some fundamental tools and algorithms required to produce AI systems able to exhibit \nlimited human-like abilities, particularly in the form of problem solving by search, game-playing, \nrepresenting and reasoning with knowledge, planning, and learning. \n \nLectures \nIntroduction. Alternate ways of thinking about AI. Agents as a unifying view of AI systems. [1 \nlecture] \nSearch I. Search as a fundamental paradigm for intelligent problem-solving. Simple, uninformed \nsearch algorithms. Tree search and graph search. [1 lecture] \nSearch II. More sophisticated heuristic search algorithms. The A* algorithm and its properties. \nImproving memory efficiency: the IDA* and recursive best first search algorithms. Local search \nand gradient descent. [1 lecture] \nGame-playing. Search in an adversarial environment. The minimax algorithm and its \nshortcomings. Improving minimax using alpha-beta pruning. [1 lecture] \nConstraint satisfaction problems (CSPs). Standardising search problems to a common format. \nThe backtracking algorithm for CSPs. Heuristics for improving the search for a solution. Forward \nchecking, constraint propagation and arc consistency. [1 lecture] \nBackjumping in CSPs. Backtracking, backjumping using Gaschnig\u2019s algorithm, graph-based \nbackjumping. [1 lecture] \nKnowledge representation and reasoning I. How can we represent and deal with commonsense \nknowledge and other forms of knowledge? Semantic networks, frames and rules. How can we \nuse inference in conjunction with a knowledge representation scheme to perform reasoning \nabout the world and thereby to solve problems? Inheritance, forward and backward chaining. [1 \nlecture] \nKnowledge representation and reasoning II. Knowledge representation and reasoning using first \norder logic. The frame, qualification and ramification problems. The situation calculus. [1 lecture] \n\nPlanning I. Methods for planning in advance how to solve a problem. The STRIPS language. \nAchieving preconditions, backtracking and fixing threats by promotion or demotion: the \npartial-order planning algorithm. [1 lecture] \nPlanning II. Incorporating heuristics into partial-order planning. Planning graphs. The \nGRAPHPLAN algorithm. Planning using propositional logic. Planning as a constraint satisfaction \nproblem. [1 lecture] \nNeural Networks I. A brief introduction to supervised learning from examples. Learning as fitting \na curve to data. The perceptron. Learning by gradient descent. [1 lecture] \nNeural Networks II. Multilayer perceptrons and the backpropagation algorithm. [1 lecture] \nObjectives \nAt the end of the course students should: \n \nappreciate the distinction between the popular view of the field and the actual research results; \nappreciate the fact that the computational complexity of most AI problems requires us regularly \nto deal with approximate techniques; \nbe able to design basic problem solving methods based on AI-based search, knowledge \nrepresentation, reasoning, planning, and learning algorithms. \nRecommended reading \nThe recommended text is: \n \n* Russell, S. and Norvig, P. (2010). Artificial intelligence: a modern approach. Prentice Hall (3rd \ned.). \n \nThere are many good books available on artificial intelligence; one alternative is: \n \nPoole, D. L. and Mackworth, A. K. (2017). Artificial intelligence: foundations of computational \nagents. Cambridge University Press (2nd ed.). \n \nFor some of the material you might find it useful to consult more specialised texts, in particular: \n \nDechter, R. (2003). Constraint processing. Morgan Kaufmann. \n \nCawsey, A. (1998). The essence of artificial intelligence. Prentice Hall. \n \nGhallab, M., Nau, D. and Traverso, P. (2004). Automated planning: theory and practice. Morgan \nKaufmann. \n \nBishop, C.M. (2006). Pattern recognition and machine learning. Springer. \n \nBrachman, R.J and Levesque, H.J. (2004). Knowledge Representation and Reasoning. Morgan \nKaufmann.\n \n\nCOMPLEXITY THEORY \n \nAims \nThe aim of the course is to introduce the theory of computational complexity. The course will \nexplain measures of the complexity of problems and of algorithms, based on time and space \nused on abstract models. Important complexity classes will be defined, and the notion of \ncompleteness established through a thorough study of NP-completeness. Applications to \ncryptography will be considered. \n \nSyllabus \nIntroduction to Complexity Theory. Decidability and complexity; lower and upper bounds; the \nChurch-Turing hypothesis. \nTime complexity. Time complexity classes; polynomial time computation and the class P. \nNon-determinism. Non-deterministic machines; the class NP; the problem 3SAT. \nNP-completeness. Reductions and completeness; the Cook-Levin theorem; graph-theoretic \nproblems. \nMore NP-complete problems. 3-colourability; scheduling, matching, set covering, and knapsack.  \ncoNP. Complement classes. NP \u2229 coNP; primality and factorisation. \nCryptography. One-way functions; the class UP. \nSpace complexity. Space complexity classes; Savitch\u2019s theorem. \nHierarchy theorems. Time and space hierarchy theorems. \nRandomised algorithms. The class BPP; derandomisation; error-correcting codes, \ncommunication complexity. \nQuantum Complexity. The classes BQP and QMA. \nInteractive Proofs. IP=PSPACE; Zero Knowledge Proofs; Probabilistically Checkable Proofs \n(PCPs).  \n  \nObjectives \nAt the end of the course students should \n \nbe able to analyse practical problems and classify them according to their complexity; \nbe familiar with the phenomenon of NP-completeness, and be able to identify problems that are \nNP-complete; \nbe aware of a variety of complexity classes and their interrelationships; \nunderstand the role of complexity analysis in cryptography. \nRecommended reading \nA Survey of P vs. NP by Scott Aaronson. \nComputational Complexity: A Modern Approach by Arora and Barak \nComputational Complexity: A Conceptual Perspective by Oded Goldreich \nMathematics and Computation by Avi Wigderson\n \n\nCYBERSECURITY \n \nAims \nIn today\u2019s digital society, computer security is vital for commercial competitiveness, national \nsecurity and privacy of individuals. Protection against both criminal and state-sponsored attacks \nwill need a large cohort of skilled individuals with an understanding of the principles of security \nand with practical experience of the application of these principles. We want you to become one \nof them. In this adversarial game, to defeat the bad guys, the good guys have to be at least as \nskilled at them. Therefore this course has a strong foundation of becoming proficient at actual \nattacks. A theoretical understanding is of course necessary, but without some practice the bad \nguys will run circles around the good guys. In 12 hours I can\u2019t bring you from zero to the stage \nwhere you can invent new attacks and countermeasures, but at least by practicing and \ndissecting common attacks (akin to \u201cstudying the classics\u201d) I\u2019ll give you a feeling for the kind of \nadversarial thinking that is required in this field. Large portions of this course are hands-on: you \nwill need to acquire the relevant skills rather than just reading about it. The recommended \ncourse textbook will be especially helpful to those with no prior low-level hacking experience. \n \nLectures \nIntroduction. \nThe adversarial nature of security; thinking like the attacker; confidentiality, integrity and \navailability; systems-level thinking; Trusted Computing Base; role of cryptography. \n \nFundamentals of access control (chapter 1). \nDiscretionary vs mandatory access control; discretionary access control in POSIX; file \npermissions, file ownership, groups, permission bits. \n \nSoftware security (spanning 4 book chapters) \nSetuid (chapter 2): privileged programs, attack surfaces, exploitable vulnerabilities, privilege \nescalation from regular user to root. \nBuffer overflow (chapter 4): stack memory layout, exploiting a buffer overflow vulnerability, \nguessing unknown addresses, payload, countermeasures and counter-countermeasures. \nReturn to libc and return-oriented programming (chapter 5): exploiting a non-executable stack, \nchaining function calls, chaining ROP gadgets. \nSQL injection (chapter 14): vulnerability and its exploitation, countermeasures, input filtering, \nprepared statements. \n \nAuthentication. \nSomething you know, have, are; passwords, their dominance, their shortcomings and the many \nattempts at replacing them; password cracking; tokens; biometrics; taxonomy of Single Sign-On \nsystems; password managers. \n \nWeb and internet security (spanning 3 book chapters). \nCross-Site Request Forgery (chapter 12): why cross-site requests, CSRF attacks on HTTP GET \nand HTTP POST, countermeasures. \n\nCross-Site Scripting (chapter 13): non-persistent vs persistent XSS, javascript, self-propagating \nXSS worm, countermeasures; \n \nHuman factors. \nUsers are not the enemy; Compliance budget; Prospect theory; 7 principles for systems \nsecurity. \n \nAdditional topics. \nViruses, worms and quines; lockpicking; privilege escalation in physical locks; conclusions. \n \nTo complete the course successfully, students must acquire the practical ability to carry out (as \nopposed to just describing) the exploits mentioned in the syllabus, given a vulnerable system. \nThe low-level hands-on portions of the course are supported by the very helpful course \ntextbook. Those who choose not to study on the recommended textbook will be severely \ndisadvantaged. \n \nRecommended textbook: Wenliang Du. Computer Security: A Hands-on Approach. 3rd Edition. \nISBN: 978-17330039-5-7. Published: 1 May 2022. \n \nhttps://www.handsonsecurity.net. Some chapters freely available online. \n \n \nOptional additional books (not substitutes): \n \nRoss Anderson, Security Engineering 3rd Ed, Wiley, 2020. ISBN: 978-1-119-64278-7. \nhttps://www.cl.cam.ac.uk/~rja14/book.html. Some chapters freely available online. \n \nPaul van Oorschot, Computer Security and the Internet, Springer, 2020. ISBN: \n978-3-030-33648-6 (hardcopy), 978-3-030-33649-3 (eBook). \nhttps://people.scs.carleton.ca/~paulv/toolsjewels.html. All chapters freely available online.\n\nFORMAL MODELS OF LANGUAGE \n \nAims \nThis course studies formal models of language and considers how they might be relevant to the \nprocessing and acquisition of natural languages. The course will extend knowledge of formal \nlanguage theory; introduce several new grammars; and use concepts from information theory to \ndescribe natural language. \n \nLectures \nNatural language and the Chomsky hierarchy 1. Recap classes of language. Closure properties \nof language classes. Recap pumping lemma for regular languages. Discussion of relevance (or \nnot) to natural languages (example embedded clauses in English). \nNatural language and the Chomsky hierarchy 2. Pumping lemma for context free languages. \nDiscussion of relevance (or not) to natural languages (example Swiss-German cross serial \ndependancies). Properties of minimally context sensitive languages. Introduction to tree \nadjoining grammars. \nLanguage processing and context free grammar parsing 1. Recap of context free grammar \nparsing. Language processing predictions based on top down parsing models (example Yngve\u2019s \nlanguage processing predictions). Language processing predictions based on probabilistic \nparsing (example Halle\u2019s language processing predictions). \nLanguage processing and context free grammar parsing 2. Introduction to context free grammar \nequivalent dependancy grammars. Language processing predictions based on Shift-Reduce \nparsing (examples prosodic look-ahead parsers, Parsey McParseface). \nGrammar induction of language classes. Introduction to grammar induction. Discussion of \nrelevance (or not) to natural language acquisition. Gold\u2019s theorem. Introduction to context free \ngrammar equivalent categorial grammars and their learnable classes. \nNatural language and information theory 1. Entropy and natural language typology. Uniform \ninformation density as a predictor for language processing. \nNatural language and information theory 2. Noisy channel encoding as a model for spelling \nerror, translation and language processing. \nVector space models and word vectors. Introduction to word vectors (example Word2Vec). Word \nvectors as predictors for semantic language processing. \nObjectives \nAt the end of the course students should \n \nunderstand how known natural languages relate to formal languages in the Chomsky hierarchy; \nhave knowledge of several context free grammars equivalents; \nunderstand how we might make predictions about language processing and language \nacquisition from formal models; \nknow how to use information theoretic concepts to describe aspects of natural language. \nRecommended reading \n* Jurafsky, D. and Martin, J. (2008). Speech and language processing. Prentice Hall. \nManning, C.D. and Schutze, H. (1999) Foundations of statistical natural language processing. \nMIT Press. \n\nRuslan, M. (2003) The Oxford handbook of computational linguistics. Oxford University Press. \nClark, A., Fox, C. and Lappin, S. (2010) The handbook of computational linguistics and natural \nlanguage processing. Wiley-Blackwell. \nKozen, D. (1997) Automata and computibility. Springer.\n \n\n",
        "5th Semester \nBIOINFORMATICS \nAims \nThis course focuses on algorithms used in Bioinformatics and System Biology. Most of the \nalgorithms are general and can be applied in other fields on multidimensional and noisy data. All \nthe necessary biological terms and concepts useful for the course and the examination will be \ngiven in the lectures. The most important software implementing the described algorithms will be \ndemonstrated. \n \nLectures \nIntroduction to biological data: Bioinformatics as an interesting field in computer science. \nComputing and storing information with DNA (including Adleman\u2019s experiment). \nDynamic programming. Longest common subsequence, DNA global and local alignment, linear \nspace alignment, Nussinov algorithm for RNA, heuristics for multiple alignment. (Vol. 1, chapter \n5) \nSequence database search. Blast. (see notes and textbooks) \nGenome sequencing. De Bruijn graph. (Vol. 1, chapter 3) \nPhylogeny. Distance based algorithms (UPGMA, Neighbour-Joining). Parsimony-based \nalgorithms. Examples in Computer Science. (Vol. 2, chapter 7) \nClustering. Hard and soft K-means clustering, use of Expectation Maximization in clustering, \nHierarchical clustering, Markov clustering algorithm. (Vol. 2, chapter 8) \nGenomics Pattern Matching. Suffix Tree String Compression and the Burrows-Wheeler \nTransform. (Vol. 2, chapter 9) \nHidden Markov Models. The Viterbi algorithm, profile HMMs for sequence alignment, classifying \nproteins with profile HMMs, soft decoding problem, Baum-Welch learning. (Vol. 2, chapter 10) \nObjectives \nAt the end of this course students should \n \nunderstand Bioinformatics terminology; \nhave mastered the most important algorithms in the field; \nbe able to work with bioinformaticians and biologists; \nbe able to find data and literature in repositories. \nRecommended reading \n* Compeau, P. and Pevzner, P.A. (2015). Bioinformatics algorithms: an active learning approach. \nActive Learning Publishers. \nDurbin, R., Eddy, S., Krough, A. and Mitchison, G. (1998). Biological sequence analysis: \nprobabilistic models of proteins and nucleic acids. Cambridge University Press. \nJones, N.C. and Pevzner, P.A. (2004). An introduction to bioinformatics algorithms. MIT Press. \nFelsenstein, J. (2003). Inferring phylogenies. Sinauer Associates.\n \n\nBUSINESS STUDIES \n \nAims \nHow to start and run a computer company; the aims of this course are to introduce students to \nall the things that go to making a successful project or product other than just the programming. \nThe course will survey some of the issues that students are likely to encounter in the world of \ncommerce and that need to be considered when setting up a new computer company. \n \nSee also Business Seminars in the Easter Term. \n \nLectures \nSo you\u2019ve got an idea? Introduction. Why are you doing it and what is it? Types of company. \nMarket analysis. The business plan. \nMoney and tools for its management. Introduction to accounting: profit and loss, cash flow, \nbalance sheet, budgets. Sources of finance. Stocks and shares. Options and futures. \nSetting up: legal aspects. Company formation. Brief introduction to business law; duties of \ndirectors. Shares, stock options, profit share schemes and the like. Intellectual Property Rights, \npatents, trademarks and copyright. Company culture and management theory. \nPeople. Motivating factors. Groups and teams. Ego. Hiring and firing: employment law. \nInterviews. Meeting techniques. \nProject planning and management. Role of a manager. PERT and GANTT charts, and critical \npath analysis. Estimation techniques. Monitoring. \nQuality, maintenance and documentation. Development cycle. Productization. Plan for quality. \nPlan for maintenance. Plan for documentation. \nMarketing and selling. Sales and marketing are different. Marketing; channels; marketing \ncommunications. Stages in selling. Control and commissions. \nGrowth and exit routes. New markets: horizontal and vertical expansion. Problems of growth; \nsecond system effects. Management structures. Communication. Exit routes: acquisition, \nfloatation, MBO or liquidation. Futures: some emerging ideas for new computer businesses. \nSummary. Conclusion: now you do it! \nObjectives \nAt the end of the course students should \n \nbe able to write and analyse a business plan; \nknow how to construct PERT and GANTT diagrams and perform critical path analysis; \nappreciate the differences between profitability and cash flow, and have some notion of budget \nestimation; \nhave an outline view of company formation, share structure, capital raising, growth and exit \nroutes; \nhave been introduced to concepts of team formation and management; \nknow about quality documentation and productization processes; \nunderstand the rudiments of marketing and the sales process. \nRecommended reading \n\nLang, J. (2001). The high-tech entrepreneur\u2019s handbook: how to start and run a high-tech \ncompany. FT.COM/Prentice Hall. \n \nStudents will be expected to be able to use Microsoft Excel and Microsoft Project. \n \nFor additional reading on a lecture-by-lecture basis, please see the course website. \n \nStudents are strongly recommended to enter the CU Entrepreneurs Business Ideas Competition \nhttp://www.cue.org.uk/\n \n\nDENOTATIONAL SEMANTICS \n \nAims \nThe aims of this course are to introduce domain theory and denotational semantics, and to \nshow how they provide a mathematical basis for reasoning about the behaviour of programming \nlanguages. \n \nLectures \nIntroduction.The denotational approach to the semantics of programming \nlanguages.Recursively defined objects as limits of successive approximations. \nLeast fixed points.Complete partial orders (cpos) and least elements.Continuous functions and \nleast fixed points. \nConstructions on domains.Flat domains.Product domains. Function domains. \nScott induction.Chain-closed and admissible subsets of cpos and domains.Scott\u2019s fixed-point \ninduction principle. \nPCF.The Scott-Plotkin language PCF.Evaluation. Contextual equivalence. \nDenotational semantics of PCF.Denotation of types and terms. Compositionality. Soundness \nwith respect to evaluation. [2 lectures]. \nRelating denotational and operational semantics.Formal approximation relation and its \nfundamental property.Computational adequacy of the PCF denotational semantics with respect \nto evaluation. Extensionality properties of contextual equivalence. [2 lectures]. \nFull abstraction.Failure of full abstraction for the domain model. PCF with parallel or. \nObjectives \nAt the end of the course students should \n \nbe familiar with basic domain theory: cpos, continuous functions, admissible subsets, least fixed \npoints, basic constructions on domains; \nbe able to give denotational semantics to simple programming languages with simple types; \nbe able to apply denotational semantics; in particular, to understand the use of least fixed points \nto model recursive programs and be able to reason about least fixed points and simple \nrecursive programs using fixed point induction; \nunderstand the issues concerning the relation between denotational and operational semantics, \nadequacy and full abstraction, especially with respect to the language PCF. \nRecommended reading \nWinskel, G. (1993). The formal semantics of programming languages: an introduction. MIT \nPress. \nGunter, C. (1992). Semantics of programming languages: structures and techniques. MIT Press. \nTennent, R. (1991). Semantics of programming languages. Prentice Hall.\n \n\nINFORMATION THEORY \n \nAims \nThis course introduces the principles and applications of information theory: how information is \nmeasured in terms of probability and various entropies, how these are used to calculate the \ncapacity of communication channels, with or without noise, and to measure how much random \nvariables reveal about each other. Coding schemes including error correcting codes are studied \nalong with data compression, spectral analysis, transforms, and wavelet coding. Applications of \ninformation theory are reviewed, from astrophysics to pattern recognition. \n \nLectures \nInformation, probability, uncertainty, and surprise. How concepts of randomness and uncertainty \nare related to information. How the metrics of information are grounded in the rules of \nprobability. Shannon Information. Weighing problems and other examples. \nEntropy of discrete variables. Definition and link to Shannon information. Joint entropy, Mutual \ninformation. Visual depictions of the relationships between entropy types. Why entropy gives \nfundamental measures of information content. \nSource coding theorem and data compression; prefix, variable-, and fixed-length codes. \nInformation rates; Asymptotic equipartition principle; Symbol codes; Huffman codes and the \nprefix property. Binary symmetric channels. Capacity of a noiseless discrete channel. Stream \ncodes. \nNoisy discrete channel coding. Joint distributions; mutual information; Conditional Entropy; \nError-correcting codes; Capacity of a discrete channel. Noisy channel coding theorem. \nEntropy of continuous variables. Differential entropy; Mutual information; Channel Capacity; \nGaussian channels. \nEntropy for comparing probability distributions and for machine learning. Relative entropy/KL \ndivergence; cross-entropy; use as loss function. \nApplications of information theory in other sciences.  \nObjectives \nAt the end of the course students should be able to \n \ncalculate the information content of a random variable from its probability distribution; \nrelate the joint, conditional, and marginal entropies of variables in terms of their coupled \nprobabilities; \ndefine channel capacities and properties using Shannon\u2019s Theorems; \nconstruct efficient codes for data on imperfect communication channels; \ngeneralize the discrete concepts to continuous signals on continuous channels; \nunderstand encoding and communication schemes in terms of the spectral properties of signals \nand channels; \ndescribe compression schemes, and efficient coding using wavelets and other representations \nfor data. \nRecommended reading \n  Mackay's Information Theory, inference and Learning Algorithms \n \n\n Stone's Information Theory: A Tutorial Introduction.\n \n\nLATEX AND JULIA \n \nAims \nIntroduction to two widely-used languages for typesetting dissertations and scientific \npublications, for prototyping numerical algorithms and to visualize results. \n \nLectures \nLATEX. Workflow example, syntax, typesetting conventions, non-ASCII characters, document \nstructure, packages, mathematical typesetting, graphics and figures, cross references, build \ntools. \nJulia. Tools for technical computing and visualization. The Array type and its operators, 2D/3D \nplotting, functions and methods, notebooks, packages, vectorized audio demonstration. \nObjectives \nStudents should be able to avoid the most common LATEX mistakes, to prototype simple image \nand signal-processing algorithms in Julia, and to visualize the results. \n \nRecommended reading \n* Lamport, L. (1994). LATEX \u2013 a documentation preparation system user\u2019s guide and reference \nmanual. Addison-Wesley (2nd ed.). \nMittelbach, F., et al. (2023). The LATEX companion. Addison-Wesley (3rd ed.).\n \n\nPRINCIPLES OF COMMUNICATIONS \n \nAims \nThis course aims to provide a detailed understanding of the underlying principles for how \ncommunications systems operate. Practical examples (from wired and wireless \ncommunications, the Internet, and other communications systems) are used to illustrate the \nprinciples. \n \nLectures \nIntroduction. Course overview. Abstraction, layering. Review of structure of real networks, links, \nend systems and switching systems. [1 lecture] \nRouting. Central versus Distributed Routing Policy Routing. Multicast Routing Circuit Routing [6 \nlectures] \nFlow control and resource optimisation. 1[Control theory] is a branch of engineering familiar to \npeople building dynamic machines. It can be applied to network traffic. Stemming the flood, at \nsource, sink, or in between? Optimisation as a model of networkand user. TCP in the wild. [3 \nlectures] \nPacket Scheduling. Design choices for scheduling and queue management algorithms for \npacket forwarding, and fairness. [2 lectures] \nThe big picture for managing traffic. Economics and policy are relevant to networks in many \nways. Optimisation and game theory are both relevant topics discussed here. [2 lectures] \nSystem Structures and Summary. Abstraction, layering. The structure of real networks, links, \nend systems and switching. [2 lectures] \n1 Control theory was not taught and will not be the subject of any exam question. \n \nObjectives \nAt the end of the course students should be able to explain the underlying design and behaviour \nof protocols and networks, including capacity, topology, control and use. Several specific \nmathematical approaches are covered (control theory, optimisation). \n \nRecommended reading \n* Keshav, S. (2012). Mathematical Foundations of Computer Networking. Addison Wesley. ISBN \n9780321792105 \nBackground reading: \nKeshav, S. (1997). An engineering approach to computer networking. Addison-Wesley (1st ed.). \nISBN 0201634422 \nStevens, W.R. (1994). TCP/IP illustrated, vol. 1: the protocols. Addison-Wesley (1st ed.). ISBN \n0201633469\n \n\nTYPES \n \nAims \nThe aim of this course is to show by example how type systems for programming languages can \nbe defined and their properties developed, using techniques that were introduced in the Part IB \ncourse on Semantics of Programming Languages. The emphasis is on type systems for \nfunctional languages and their connection to constructive logic. \n \nLectures \nIntroduction. The role of type systems in programming languages. Review of rule-based \nformalisation of type systems. [1 lecture] \nPropositions as types. The Curry-Howard correspondence between intuitionistic propositional \ncalculus and simply-typed lambda calculus. Inductive types and iteration. Consistency and \ntermination. [2 lectures] \nPolymorphic lambda calculus (PLC). PLC syntax and reduction semantics. Examples of \ndatatypes definable in the polymorphic lambda calculus. Type inference. [3 lectures] \nMonads and effects. Explicit versus implicit effects. Using monadic types to control effects. \nReferences and polymorphism. Recursion and looping. [2 lectures] \nContinuations and classical logic. First-class continuations and control operators. Continuations \nas Curry-Howard for classical logic. Continuation-passing style. [2 lectures] \nDependent types. Dependent function types. Indexed datatypes. Equality types and combining \nproofs with programming. [2 lectures] \nObjectives \nAt the end of the course students should \n \nbe able to use a rule-based specification of a type system to carry out type checking and type \ninference; \nunderstand by example the Curry-Howard correspondence between type systems and logics; \nunderstand how types can be used to control side-effects in programming; \nappreciate the expressive power of parametric polymorphism and dependent types. \nRecommended reading \n* Pierce, B.C. (2002). Types and programming languages. MIT Press. \nPierce, B. C. (Ed.) (2005). Advanced Topics in Types and Programming Languages. MIT Press. \nGirard, J-Y. (tr. Taylor, P. and Lafont, Y.) (1989). Proofs and types. Cambridge University Press.\n\nADVANCED DATA SCIENCE \n \nPracticals \nThe module will have a practical session for each of the three stages of the pipeline. Each of the \nparts will be tied into an aspect of the final project. \n \nObjectives \nAt the end of the course students will be familiar with the purpose of data science, how it differs \nfrom the closely related fields of machine learning, statistics and artificial intelligence and what a \ntypical data analysis pipeline looks like in practice. As well as emphasising the importance of \nanalysis methods we will introduce a formalism for organising how data science is done in \npractice and what the different aspects the data scientist faces when giving data-driven answers \nto questions of interest. \n \nRecommended reading \nSimon Rogers et al. (2016). A First Course in Machine Learning, Second Edition. [] Chapman \nand Hall/CRC, nil \nShai Shalev-Shwartz et al. (2014). Understanding Machine Learning: From Theory to \nAlgorithms. New York, NY, USA: Cambridge  University Press \nChristopher M. Bishop (2006). Pattern Recognition and Machine Learning (Information Science \nand Statistics). Secaucus, NJ, USA: Springer-Verlag New York, Inc. \nAssessment \nPractical There will be four practicals contributing 20% to the final module mark. Data science is \na topic where there is rarely a single \ncorrect answer therefore the important thing is that an informed attempt has been made to \naddress the questions that can be supported and motivated. \nReport The course will be concluded by an individual report covering the material in the course \nthrough \npractical exercise working with real data. This report will make up 80% of the mark for the \ncourse. \n \n \n\nAFFECTIVE ARTIFICIAL INTELLIGENCE \n \nSynopsis \nAffective Artificial Intelligence (Affective AI) aims to imbue machines with social and emotional \nintelligence (EQ). More specifically, Affective AI aims to create artificially intelligent systems and \nmachines that can recognize, interpret, process, and simulate human social signals and \nbehaviours, expressions, and emotions, to enhance human-AI interaction and communication. \n\\r\\n\\r\\nTo achieve this goal, Affective AI draws upon various scientific disciplines, including \nmachine learning, computer vision, speech / natural language / signal processing, psychology \nand cognitive science, and ethics and social sciences. \n \n  \nBackground & Aims \nAffective Artificial Intelligence (Affective AI) aims to imbue machines with social and emotional \nintelligence (EQ). More specifically, Affective AI aims to create artificially intelligent systems and \nmachines that can recognize, interpret, process, and simulate human social signals and \nbehaviours, expressions, and emotions, to enhance human-AI interaction and communication.  \n \nTo achieve this goal, Affective AI draws upon various scientific disciplines, including machine \nlearning, computer vision, speech / natural language / signal processing, psychology and \ncognitive science, and ethics and social sciences. \n \nAffective AI has direct applications in and implications for the design of innovative interactive \ntechnology (e.g., interaction with chat bots, virtual agents, robots), single and multi-user smart \nenvironments ( e.g., in-car/ virtual / augmented / mixed reality, serious games), public speaking \nand cognitive training, and clinical and biomedical studies (e.g., autism, depression, pain). \n \nThe aim of this module is to impart knowledge and ability needed to make informed choices of \nmodels, data, and machine learning techniques for sensing, recognition, and generation of \naffective and social behaviour (e.g., smile, frown, head nodding/shaking, \nagreement/disagreement) in order to create Affectively intelligent AI systems, with a \nconsideration for various ethical issues (e.g., privacy, bias) arising from the real-world \ndeployment of these systems. \n \n  \n \nSyllabus \nThe following list provides a representative list of topics: \n \nIntroduction, definitions, and overview \nTheories from various disciplines \nSensing from multiple modalities (e.g., vision, audio, bio signals, text) \nData acquisition and annotation \nSignal processing / feature extraction \n\nLearning / prediction / recognition and evaluation \nBehaviour synthesis / generation (e.g., for embodied agents / robots) \nAdvanced topics and ethical considerations (e.g., bias and fairness) \nCross-disciplinary applications (via seminar presentations and discussions) \nGuest lectures (diverse topics \u2013 changes each year) \nHands-on research and programming work (i.e., mini project & report)  \n \n  \n \nObjectives \nOn completion of this module, students will: \n \nunderstand and demonstrate knowledge in key characteristics of affectively intelligent AI, which \ninclude: \nRecognition: How to equip Affective AI systems with capabilities of analysing facial expressions, \nvocal intonations, gestures, and other physiological signals to infer human affective states? \nGeneration: How to enable affectively intelligent AI simulate expressions of emotions in \nmachines, allowing them to respond in a more human-like manner, such as virtual agents and \nhumanoid robots, showing empathy or sympathy? \nAdaptation and Personalization: How to enable affectively intelligent AI adapt system responses \nbased on users' affective states and/or needs, or tailor interactions and experiences to individual \nusers based on their expressivity / emotional profiles, personalities, and past interactions? \nEmpathetic Communication: How to design Affective AI systems to communicate with users in a \nway that demonstrates empathy, understanding, and sensitivity to their affective states and \nneeds? \nEthical and societal considerations: What are the various human differences, ethical guidelines, \nand societal impacts that need to be considered when designing and deploying Affective AI to \nensure that these systems respect users' privacy, autonomy, and well-being? \ncomprehend and apply (appropriate) methods for collection, analysis, representation, and \nevaluation of human affective and communicative behavioural data. \nenhance programming skills for creating and implementing (components of) Affective AI \nsystems. \ndemonstrate critical thinking, analysis and synthesis while deciding on 'when' and 'how' to \nincorporate human affect and social signals in a specific AI system context and gain practical \nexperience in proposing and justifying computational solution(s) of suitable nature and scope. \n  \n \nAssessment - Part II Students \nSeminar presentation: 20% \nParticipating in Q&A and discussions: 10% \nMid-term mini project report and presentation (as a team of 2): 10%  \nFinal mini project report & code (as a team of 2): 60% \n \n\nCATEGORY THEORY \n \nAims \nCategory theory provides a unified treatment of mathematical properties and constructions that \ncan be expressed in terms of 'morphisms' between structures. It gives a precise framework for \ncomparing one branch of mathematics (organized as a category) with another and for the \ntransfer of problems in one area to another. Since its origins in the 1940s motivated by \nconnections between algebra and geometry, category theory has been applied to diverse fields, \nincluding computer science, logic and linguistics. This course introduces the basic notions of \ncategory theory: adjunction, natural transformation, functor and category. We will use category \ntheory to organize and develop the kinds of structure that arise in models and semantics for \nlogics and programming languages. \n \nSyllabus \nIntroduction; some history. Definition of category. The category of sets and functions. \nCommutative diagrams. Examples of categories: preorders and monotone functions; monoids \nand monoid homomorphisms; a preorder as a category; a monoid as a category. Definition of \nisomorphism. Informal notion of a 'category-theoretic' property. \nTerminal objects. The opposite of a category and the duality principle. Initial objects. Free \nmonoids as initial objects. \nBinary products and coproducts. Cartesian categories. \nExponential objects: in the category of sets and in general. Cartesian closed categories: \ndefinition and examples. \nIntuitionistic Propositional Logic (IPL) in Natural Deduction style. Semantics of IPL in a cartesian \nclosed preorder. \nSimply Typed Lambda Calculus (STLC). The typing relation. Semantics of STLC types and \nterms in a cartesian closed category (ccc). The internal language of a ccc. The \nCurry-Howard-Lawvere correspondence. \nFunctors. Contravariance. Identity and composition for functors. \nSize: small categories and locally small categories. The category of small categories. Finite \nproducts of categories. \nNatural transformations. Functor categories. The category of small categories is cartesian \nclosed. \nHom functors. Natural isomorphisms. Adjunctions. Examples of adjoint functors. Theorem \ncharacterising the existence of right (respectively left) adjoints in terms of a universal property. \nDependent types. Dependent product sets and dependent function sets as adjoint functors. \nEquivalence of categories. Example: the category of I-indexed sets and functions is equivalent \nto the slice category Set/I. \nPresheaves. The Yoneda Lemma. Categories of presheaves are cartesian closed. \nMonads. Modelling notions of computation as monads. Moggi's computational lambda calculus. \nObjectives \nOn completion of this module, students should: \n \n\nbe familiar with some of the basic notions of category theory and its connections with logic and \nprogramming language semantics \nAssessment \na graded exercise sheet (25% of the final mark), and \na take-home test (75%) \nRecommended reading \nAwodey, S. (2010). Category theory. Oxford University Press (2nd ed.). \n \nCrole, R. L. (1994). Categories for types. Cambridge University Press. \n \nLambek, J. and Scott, P. J. (1986). Introduction to higher order categorical logic. Cambridge \nUniversity Press. \n \nPitts, A. M. (2000). Categorical Logic. Chapter 2 of S. Abramsky, D. M. Gabbay and T. S. E. \nMaibaum (Eds) Handbook of Logic in Computer Science, Volume 5. Oxford University Press. \n(Draft copy available here.) \n \nClass Size \nThis module can accommodate upto 15 Part II students plus 15 MPhil / Part III students.\n\nDIGITAL SIGNAL PROCESSING \n \nAims \nThis course teaches basic signal-processing principles necessary to understand many modern \nhigh-tech systems, with examples from audio processing, image coding, radio communication, \nradar, and software-defined radio. Students will gain practical experience from numerical \nexperiments in programming assignments (in Julia, MATLAB or NumPy). \n \nLectures \nSignals and systems. Discrete sequences and systems: types and properties. Amplitude, \nphase, frequency, modulation, decibels, root-mean square. Linear time-invariant systems, \nconvolution. Some examples from electronics, optics and acoustics. \nPhasors. Eigen functions of linear time-invariant systems. Review of complex arithmetic. \nPhasors as orthogonal base functions. \nFourier transform. Forms and properties of the Fourier transform. Convolution theorem. Rect \nand sinc. \nDirac\u2019s delta function. Fourier representation of sine waves, impulse combs in the time and \nfrequency domain. Amplitude-modulation in the frequency domain. \nDiscrete sequences and spectra. Sampling of continuous signals, periodic signals, aliasing, \ninterpolation, sampling and reconstruction, sample-rate conversion, oversampling, spectral \ninversion. \nDiscrete Fourier transform. Continuous versus discrete Fourier transform, symmetry, linearity, \nFFT, real-valued FFT, FFT-based convolution, zero padding, FFT-based resampling, \ndeconvolution exercise. \nSpectral estimation. Short-time Fourier transform, leakage and scalloping phenomena, \nwindowing, zero padding. Audio and voice examples. DTFM exercise. \nFinite impulse-response filters. Properties of filters, implementation forms, window-based FIR \ndesign, use of frequency-inversion to obtain high-pass filters, use of modulation to obtain \nband-pass filters. \nInfinite impulse-response filters. Sequences as polynomials, z-transform, zeros and poles, some \nanalog IIR design techniques (Butterworth, Chebyshev I/II, elliptic filters, second-order cascade \nform). \nBand-pass signals. Band-pass sampling and reconstruction, IQ up and down conversion, \nsuperheterodyne receivers, software-defined radio front-ends, IQ representation of AM and FM \nsignals and their demodulation. \nDigital communication. Pulse-amplitude modulation. Matched-filter detector. Pulse shapes, \ninter-symbol interference, equalization. IQ representation of ASK, BSK, PSK, QAM and FSK \nsignals. [2 hours] \nRandom sequences and noise. Random variables, stationary and ergodic processes, \nautocorrelation, cross-correlation, deterministic cross-correlation sequences, filtered random \nsequences, white noise, periodic averaging. \nCorrelation coding. Entropy, delta coding, linear prediction, dependence versus correlation, \nrandom vectors, covariance, decorrelation, matrix diagonalization, eigen decomposition, \n\nKarhunen-Lo\u00e8ve transform, principal component analysis. Relation to orthogonal transform \ncoding using fixed basis vectors, such as DCT. \nLossy versus lossless compression. What information is discarded by human senses and can \nbe eliminated by encoders? Perceptual scales, audio masking, spatial resolution, colour \ncoordinates, some demonstration experiments. \nQuantization, image coding standards. Uniform and logarithmic quantization, A/\u00b5-law coding, \ndithering, JPEG. \nObjectives \napply basic properties of time-invariant linear systems; \nunderstand sampling, aliasing, convolution, filtering, the pitfalls of spectral estimation; \nexplain the above in time and frequency domain representations; \nuse filter-design software; \nvisualize and discuss digital filters in the z-domain; \nuse the FFT for convolution, deconvolution, filtering; \nimplement, apply and evaluate simple DSP applications; \nfamiliarity with a number of signal-processing concepts used in digital communication systems \nRecommended reading \nLyons, R.G. (2010). Understanding digital signal processing. Prentice Hall (3rd ed.). \nOppenheim, A.V. and Schafer, R.W. (2007). Discrete-time digital signal processing. Prentice \nHall (3rd ed.). \nStein, J. (2000). Digital signal processing \u2013 a computer science perspective. Wiley. \n \nClass size \nThis module can accommodate a maximum of 24 students (16 Part II students and 8 MPhil \nstudents) \n \nAssessment - Part II students \nThree homework programming assignments, each comprising 20% of the mark \nWritten test, comprising 40% of the total mark.\n \n\nMACHINE VISUAL PERCEPTION \n \nAims \nThis course aims at introducing the theoretical foundations and practical techniques for machine \nperception, the capability of computers to interpret data resulting from sensor measurements. \nThe course will teach the fundamentals and modern techniques of machine perception, i.e. \nreconstructing the real world starting from sensor measurements with a focus on machine \nperception for visual data. The topics covered will be image/geometry representations for \nmachine perception, semantic segmentation, object detection and recognition, geometry \ncapture, appearance modeling and acquisition, motion detection and estimation, \nhuman-in-the-loop machine perception, select topics in applied machine perception. \n \nMachine perception/computer vision is a rapidly expanding area of research with real-world \napplications. An understanding of machine perception is also important for robotics, interactive \ngraphics (especially AR/VR), applied machine learning, and several other fields and industries. \nThis course will provide a fundamental understanding of and practical experience with the \nrelevant techniques. \n \nLearning outcomes \nStudents will understand the theoretical underpinnings of the modern machine perception \ntechniques for reconstructing models of reality starting from an incomplete and imperfect view of \nreality. \nStudents will be able to apply machine perception theory to solve practical problems, e.g. \nclassification of images, geometry capture. \nStudents will gain an understanding of which machine perception techniques are appropriate for \ndifferent tasks and scenarios. \nStudents will have hands-on experience with some of these techniques via developing a \nfunctional machine perception system in their projects. \nStudents will have practical experience with the current prominent machine perception \nframeworks. \nSyllabus \nThe fundamentals of machine learning for machine perception \nDeep neural networks and frameworks for machine perception \nSemantic segmentation of objects and humans \nObject detection and recognition \nMotion estimation, tracking and recognition \n3D geometry capture \nAppearance modeling and acquisition \nSelect topics in applied machine perception \nAssessment \nA practical exercise, worth 20% of the mark. This will cover the basics and theory of machine \nperception and some of the practical techniques the students will likely use in their projects. This \nis individual work. No GPU hours will be needed for the practical work. \nA machine perception project worth 80% of the marks: \n\nCourse projects will be selected by the students following the possible project themes proposed \nby the lecturer, and will be checked by the lecturer for appropriateness.  \nThe students will form groups of 2-3 to design, implement, report, and present a project to tackle \na given task in machine perception. \nThe final mark will be composed of an implementation/report mark (60%) and a presentation \nmark (20%). Each team member will be evaluated based on her/his contribution. \nEach project will have extensions to be completed only by the ACS students. Each student will \nwrite a different part of the report, whose author will be clearly marked. Each student will further \nsummarise her/his contributions to the project in the same report. \nRecommended Reading List \nComputer Vision: Algorithms and Applications, Richard Szeliski, Springer, 2010. \nDeep Learning, Ian Goodfellow, Yoshua Bengio, and Aaron Courville, MIT Press, 2016. \nMachine Learning and Visual Perception, Baochang Zhang, De Gruyter, 2020.\n \n\nNATURAL LANGUAGE PROCESSING \n \nAims \nThis course introduces the fundamental techniques of natural language processing. It aims to \nexplain the potential and the main limitations of these techniques. Some current research issues \nare introduced and some current and potential applications discussed and evaluated. Students \nwill also be introduced to practical experimentation in natural language processing. \n \nLectures \nOverview. Brief history of NLP research, some current applications, components of NLP \nsystems. \nMorphology and Finite State Techniques. Morphology in different languages, importance of \nmorphological analysis in NLP, finite-state techniques in NLP. \nPart-of-Speech Tagging and Log-Linear Models. Lexical categories, word tagging, corpora and \nannotations, empirical evaluation. \nPhrase Structure and Structure Prediction. Phrase structures, structured prediction, context-free \ngrammars, weights and probabilities. Some limitations of context-free grammars. \nDependency Parsing. Dependency structure, grammar-free parsing, incremental processing.  \nGradient Descent and Neural Nets. Parameter optimisation by gradient descent. Non-linear \nfunctions with neural network layers. Log-linear model as softmax layer. Current findings of \nNeural NLP. \nWord representations. Representing words with vectors, count-based and prediction-based \napproaches, similarity metrics. \nRecurrent Neural Networks. Modelling sequences, parameter sharing in recurrent neural \nnetworks, neural language models, word prediction. \nCompositional Semantics. Logical representations, compositional semantics, lambda calculus, \ninference and robust entailment. \nLexical Semantics. Semantic relations, WordNet, word senses. \nDiscourse. Discourse relations, anaphora resolution, summarization. \nNatural Language Generation. Challenges of natural language generation (NLG), tasks in NLG, \nsurface realisation. \nPractical and assignments. Students will build a natural language processing system which will \nbe trained and evaluated on supplied data. The system will be built from existing components, \nbut students will be expected to compare approaches and some programming will be required \nfor this. Several assignments will be set during the practicals for assessment. \nObjectives \nBy the end of the course students should: \n \nbe able to discuss the current and likely future performance of several NLP applications; \nbe able to describe briefly a fundamental technique for processing language for several \nsubtasks, such as morphological processing, parsing, word sense disambiguation etc.; \nunderstand how these techniques draw on and relate to other areas of computer science. \nRecommended reading \n\nJurafsky, D. & Martin, J. (2023). Speech and language processing. Prentice Hall (3rd ed. draft, \nonline). \n \nAssessment - Part II Students \nAssignment 1 - 10% of marks \nAssignment 2 - 60% of marks \nAssignment 3 - 30% of marks\n \n\nPRACTICAL RESEARCH IN HUMAN-CENTRED AI \n \nAims \nThis is an advanced course in human-computer interaction, with a specialist focus on intelligent \nuser interfaces and interaction with machine-learning and artificial intelligence technologies. The \nformat will be largely Practical, with students carrying out a mini-project involving empirical \nresearch investigation. These studies will investigate human interaction with some kind of \nmodel-based system for planning, decision-making, automation etc. Possible study formats \nmight include: System evaluation, Field observation, Hypothesis testing experiment, Design \nintervention or Corpus analysis, following set examples from recent research publications. \nProject work will be formally evaluated through a report and presentation. \n \nLectures \n(note that Lectures 2-7 also include one hour class discussion of practical work) \n        \u2022 Current research themes in intelligent user interfaces \n        \u2022 Program synthesis \n        \u2022 Mixed initiative interaction \n        \u2022 Interpretability / explainable AI \n        \u2022 Labelling as a fundamental problem \n        \u2022 Machine learning risks and bias \n        \u2022 Visualisation and visual analytics \n        \u2022 Student research presentations \n \nObjectives \nBy the end of the course students should: \n        \u2022 be familiar with current state of the art in intelligent interactive systems \n        \u2022 understand the human factors that are most critical in the design of such systems \n        \u2022 be able to evaluate evidence for and against the utility of novel systems \n        \u2022 have experience of conducting user studies meeting the quality criteria of this field \n        \u2022 be able to write up and present user research in a professional manner \n \nClass Size \nThis module can accommodate upto 20 Part II, Part III and MPhil students. \n \nRecommended reading \nBrad A. Myers and Richard McDaniel (2000). Demonstrational Interfaces: Sometimes You Need \na Little Intelligence, Sometimes You Need a Lot. \n \nAlan Blackwell (2024). Moral Codes: Designing alternatives to AI \n \nAssessment - Part II Students \nThe format will be largely practical, with students carrying out an individual mini-project involving \nempirical research investigation. \n \n\nAssignment 1: six incremental submissions which together contribute 20% to the final module \nmark. \n \nAssignment 2: Final report - 80% of the final module mark \n \n\n",
        "6th Semester \nADVANCED COMPUTER ARCHITECTURE \nAims \nThis course examines the techniques and underlying principles that are used to design \nhigh-performance computers and processors. Particular emphasis is placed on understanding \nthe trade-offs involved when making design decisions at the architectural level. A range of \nprocessor architectures are explored and contrasted. In each case we examine their merits and \nlimitations and how ultimately the ability to scale performance is restricted. \n \nLectures \nIntroduction. The impact of technology scaling and market trends. \nFundamentals of Computer Design. Amdahl\u2019s law, energy/performance trade-offs, ISA design. \nAdvanced pipelining. Pipeline hazards; exceptions; optimal pipeline depth; branch prediction; \nthe branch target buffer [2 lectures] \nSuperscalar techniques. Instruction-Level Parallelism (ILP); superscalar processor architecture \n[2 lectures] \nSoftware approaches to exploiting ILP. VLIW architectures; local and global instruction \nscheduling techniques; predicated instructions and support for speculative compiler \noptimisations. \nMultithreaded processors. Coarse-grained, fine-grained, simultaneous multithreading \nThe memory hierarchy. Caches; programming for caches; prefetching [2 lectures] \nVector processors. Vector machines; short vector/SIMD instruction set extensions; stream \nprocessing \nChip multiprocessors. The communication model; memory consistency models; false sharing; \nmultiprocessor memory hierarchies; cache coherence protocols; synchronization [2 lectures] \nOn-chip interconnection networks. Bus-based interconnects; on-chip packet switched networks \nSpecial-purpose architectures. Converging approaches to computer design \nObjectives \nAt the end of the course students should \n \nunderstand what determines processor design goals; \nappreciate what constrains the design process and how architectural trade-offs are made within \nthese constraints; \nbe able to describe the architecture and operation of pipelined and superscalar processors, \nincluding techniques such as branch prediction, register renaming and out-of-order execution; \nhave an understanding of vector, multithreaded and multi-core processor architectures; \nfor the architectures discussed, understand what ultimately limits their performance and \napplication domain. \nRecommended reading \n* Hennessy, J. and Patterson, D. (2012). Computer architecture: a quantitative approach. \nElsevier (5th ed.) ISBN 9780123838728. (the 3rd and 4th editions are also good)\n \n\nCRYPTOGRAPHY \n \nAims \nThis course provides an overview of basic modern cryptographic techniques and covers \nessential concepts that users of cryptographic standards need to understand to achieve their \nintended security goals. \n \nLectures \nCryptography. Overview, private vs. public-key ciphers, MACs vs. signatures, certificates, \ncapabilities of adversary, Kerckhoffs\u2019 principle. \nClassic ciphers. Attacks on substitution and transposition ciphers, Vigen\u00e9re. Perfect secrecy: \none-time pads. \nPrivate-key encryption. Stream ciphers, pseudo-random generators, attacking \nlinear-congruential RNGs and LFSRs. Semantic security definitions, oracle queries, advantage, \ncomputational security, concrete-security proofs. \nBlock ciphers. Pseudo-random functions and permutations. Birthday problem, random \nmappings. Feistel/Luby-Rackoff structure, DES, TDES, AES. \nChosen-plaintext attack security. Security with multiple encryptions, randomized encryption. \nModes of operation: ECB, CBC, OFB, CNT. \nMessage authenticity. Malleability, MACs, existential unforgeability, CBC-MAC, ECBC-MAC, \nCMAC, birthday attacks, Carter-Wegman one-time MAC. \nAuthenticated encryption. Chosen-ciphertext attack security, ciphertext integrity, \nencrypt-and-authenticate, authenticate-then-encrypt, encrypt-then-authenticate, padding oracle \nexample, GCM. \nSecure hash functions. One-way functions, collision resistance, padding, Merkle-Damg\u00e5rd \nconstruction, sponge function, duplex construct, entropy pool, SHA standards. \nApplications of secure hash functions. HMAC, stream authentication, Merkle tree, commitment \nprotocols, block chains, Bitcoin. \nKey distribution problem. Needham-Schroeder protocol, Kerberos, hardware-security modules, \npublic-key encryption schemes, CPA and CCA security for asymmetric encryption. \nNumber theory, finite groups and fields. Modular arithmetic, Euclid\u2019s algorithm, inversion, \ngroups, rings, fields, GF(2n), subgroup order, cyclic groups, Euler\u2019s theorem, Chinese \nremainder theorem, modular roots, quadratic residues, modular exponentiation, easy and \ndifficult problems. [2 lectures] \nDiscrete logarithm problem. Baby-step-giant-step algorithm, computational and decision \nDiffie-Hellman problem, DH key exchange, ElGamal encryption, hybrid cryptography, Schnorr \ngroups, elliptic-curve systems, key sizes. [2 lectures] \nTrapdoor permutations. Security definition, turning one into a public-key encryption scheme, \nRSA, attacks on \u201ctextbook\u201d RSA, RSA as a trapdoor permutation, optimal asymmetric \nencryption padding, common factor attacks. \nDigital signatures. One-time signatures, RSA signatures, Schnorr identification scheme, \nElGamal signatures, DSA, PS3 hack, certificates, PKI. \nObjectives \nBy the end of the course students should \n\n \nbe familiar with commonly used standardized cryptographic building blocks; \nbe able to match application requirements with concrete security definitions and identify their \nabsence in naive schemes; \nunderstand various adversarial capabilities and basic attack algorithms and how they affect key \nsizes; \nunderstand and compare the finite groups most commonly used with discrete-logarithm \nschemes; \nunderstand the basic number theory underlying the most common public-key schemes, and \nsome efficient implementation techniques. \nRecommended reading \nKatz, J., Lindell, Y. (2015). Introduction to modern cryptography. Chapman and Hall/CRC (2nd \ned.).\n \n\nE-COMMERCE \n \nAims \nThis course aims to give students an outline of the issues involved in setting up an e-commerce \nsite. \n \nLectures \nThe history of electronic commerce. Mail order; EDI; web-based businesses, credit card \nprocessing, PKI, identity and other hot topics. \nNetwork economics. Real and virtual networks, supply-side versus demand-side scale \neconomies, Metcalfe\u2019s law, the dominant firm model, the differentiated pricing model Data \nProtection Act, Distance Selling regulations, business models. \nWeb site design. Stock and price control; domain names, common mistakes, dynamic pages, \ntransition diagrams, content management systems, multiple targets. \nWeb site implementation. Merchant systems, system design and sizing, enterprise integration, \npayment mechanisms, CRM and help desks. Personalisation and internationalisation. \nThe law and electronic commerce. Contract and tort; copyright; binding actions; liabilities and \nremedies. Legislation: RIP; Data Protection; EU Directives on Distance Selling and Electronic \nSignatures. \nPutting it into practice. Search engine interaction, driving and analysing traffic; dynamic pricing \nmodels. Integration with traditional media. Logs and audit, data mining modelling the user. \ncollaborative filtering and affinity marketing brand value, building communities, typical behaviour. \nFinance. How business plans are put together. Funding Internet ventures; the recent hysteria; \nmaximising shareholder value. Future trends. \nUK and International Internet Regulation. Data Protection Act and US Privacy laws; HIPAA, \nSarbanes-Oxley, Security Breach Disclosure, RIP Act 2000, Electronic Communications Act \n2000, Patriot Act, Privacy Directives, data retention; specific issues: deep linking, Inlining, brand \nmisuse, phishing. \nObjectives \nAt the end of the course students should know how to apply their computer science skills to the \nconduct of e-commerce with some understanding of the legal, security, commercial, economic, \nmarketing and infrastructure issues involved. \n \nRecommended reading \nShapiro, C. and Varian, H. (1998). Information rules. Harvard Business School Press. \n \nAdditional reading: \n \nStandage, T. (1999). The Victorian Internet. Phoenix Press. Klemperer, P. (2004). Auctions: \ntheory and practice. Princeton Paperback ISBN 0-691-11925-2.\n \n\nMACHINE LEARNING AND BAYESIAN INFERENCE \nAims \nThe Part 1B course Artificial Intelligence introduced simple neural networks for supervised \nlearning, and logic-based methods for knowledge representation and reasoning. This course \nhas two aims. First, to provide a rigorous introduction to machine learning, moving beyond the \nsupervised case and ultimately presenting state-of-the-art methods. Second, to provide an \nintroduction to the wider area of probabilistic methods for representing and reasoning with \nknowledge. \n \nLectures \nIntroduction to learning and inference. Supervised, unsupervised, semi-supervised and \nreinforcement learning. Bayesian inference in general. What the naive Bayes method actually \ndoes. Review of backpropagation. Other kinds of learning and inference. [1 lecture] \nHow to classify optimally. Treating learning probabilistically. Bayesian decision theory and Bayes \noptimal classification. Likelihood functions and priors. Bayes theorem as applied to supervised \nlearning. The maximum likelihood and maximum a posteriori hypotheses. What does this teach \nus about the backpropagation algorithm? [2 lectures] \nLinear classifiers I. Supervised learning via error minimization. Iterative reweighted least \nsquares. The maximum margin classifier. [2 lectures] \nGaussian processes. Learning and inference for regression using Gaussian process models. [2 \nlectures] \nSupport vector machines (SVMs). The kernel trick. Problem formulation. Constrained \noptimization and the dual problem. SVM algorithm. [2 lectures] \nPractical issues. Hyperparameters. Measuring performance. Cross-validation. Experimental \nmethods. [1 lecture] \nLinear classifiers II. The Bayesian approach to neural networks. [1 lecture] \nUnsupervised learning I. The k-means algorithm. Clustering as a maximum likelihood problem. \n[1 lecture] \nUnsupervised learning II. The EM algorithm and its application to clustering. [1 lecture] \nBayesian networks I. Representing uncertain knowledge using Bayesian networks. Conditional \nindependence. Exact inference in Bayesian networks. [2 lectures] \nBayesian networks II. Markov random fields. Approximate inference. Markov chain Monte Carlo \nmethods. [1 lecture] \nObjectives \nAt the end of this course students should: \n \nUnderstand how learning and inference can be captured within a probabilistic framework, and \nknow how probability theory can be applied in practice as a means of handling uncertainty in AI \nsystems. \nUnderstand several algorithms for machine learning and apply those methods in practice with \nproper regard for good experimental practice. \nRecommended reading \nIf you are going to buy a single book for this course I recommend: \n \n\n* Bishop, C.M. (2006). Pattern recognition and machine learning. Springer. \n \nThe course text for Artificial Intelligence I: \n \nRussell, S. and Norvig, P. (2010). Artificial intelligence: a modern approach. Prentice Hall (3rd \ned.). \n \ncovers some relevant material but often in insufficient detail. Similarly: \n \nMitchell, T.M. (1997). Machine Learning. McGraw-Hill. \n \ngives a gentle introduction to some of the course material, but only an introduction. \n \nRecently a few new books have appeared that cover a lot of relevant ground well. For example: \n \nBarber, D. (2012). Bayesian Reasoning and Machine Learning. Cambridge University Press. \nFlach, P. (2012). Machine Learning: The Art and Science of Algorithms that Make Sense of \nData. Cambridge University Press. \nMurphy, K.P. (2012). Machine Learning: A Probabilistic Perspective. MIT Press.\n \n\nOPTIMISING COMPILERS \n \nAims \nThe aims of this course are to introduce the principles of program optimisation and related \nissues in decompilation. The course will cover optimisations of programs at the abstract syntax, \nflowgraph and target-code level. It will also examine how related techniques can be used in the \nprocess of decompilation. \n \nLectures \nIntroduction and motivation. Outline of an optimising compiler. Optimisation partitioned: analysis \nshows a property holds which enables a transformation. The flow graph; representation of \nprogramming concepts including argument and result passing. The phase-order problem. \nKinds of optimisation. Local optimisation: peephole optimisation, instruction scheduling. Global \noptimisation: common sub-expressions, code motion. Interprocedural optimisation. The call \ngraph. \nClassical dataflow analysis. Graph algorithms, live and avail sets. Register allocation by register \ncolouring. Common sub-expression elimination. Spilling to memory; treatment of \nCSE-introduced temporaries. Data flow anomalies. Static Single Assignment (SSA) form. \nHigher-level optimisations. Abstract interpretation, Strictness analysis. Constraint-based \nanalysis, Control flow analysis for lambda-calculus. Rule-based inference of program properties, \nTypes and effect systems. Points-to and alias analysis. \nTarget-dependent optimisations. Instruction selection. Instruction scheduling and its phase-order \nproblem. \nDecompilation. Legal/ethical issues. Some basic ideas, control flow and type reconstruction. \nObjectives \nAt the end of the course students should \n \nbe able to explain program analyses as dataflow equations on a flowgraph; \nknow various techniques for high-level optimisation of programs at the abstract syntax level; \nunderstand how code may be re-scheduled to improve execution speed; \nknow the basic ideas of decompilation. \nRecommended reading \n* Nielson, F., Nielson, H.R. and Hankin, C.L. (1999). Principles of program analysis. Springer. \nGood on part A and part B. \nAppel, A. (1997). Modern compiler implementation in Java/C/ML (3 editions). \nMuchnick, S. (1997). Advanced compiler design and implementation. Morgan Kaufmann. \nWilhelm, R. (1995). Compiler design. Addison-Wesley. \nAho, A.V., Sethi, R. and Ullman, J.D. (2007). Compilers: principles, techniques and tools. \nAddison-Wesley (2nd ed.).\n \n\nQUANTUM COMPUTING \n \nAims \nThe principal aim of the course is to introduce students to the basics of the quantum model of \ncomputation. The model will be used to study algorithms for searching, factorisation and \nquantum chemistry as well as other important topics in quantum information such as \ncryptography and super-dense coding. Issues in the complexity of computation will also be \nexplored. A second aim of the course is to introduce student to near-term quantum computing. \nTo this end, error-correction and adiabatic quantum computing are studied. \n \nLectures \nBits and qubits. Introduction to quantum states and measurements with motivating examples. \nComparison with discrete classical states. \nLinear algebra. Review of linear algebra: vector spaces, linear operators, Dirac notation, the \ntensor product. \nThe postulates of quantum mechanics. Postulates of quantum mechanics, incl. evolution and \nmeasurement. \nImportant concepts in quantum mechanics. Entanglement, distinguishing orthogonal and \nnon-orthogonal quantum states, no-cloning and no signalling. \nThe quantum circuit model. The circuit model of quantum computation. Quantum gates and \ncircuits. Universality of the quantum circuit model, and efficient simulation of arbitrary two-qubit \ngates with a standard universal set of gates. \nSome applications of quantum information. Applications of quantum information (other than \nquantum computation): quantum key distribution, superdense coding and quantum teleportation. \nDeutsch-Jozsa algorithm. Introducing Deutsch\u2019s problem and Deutsch\u2019s algorithm leading onto \nits generalisation, the Deutsch-Jozsa algorithm. \nQuantum search. Grover\u2019s search algorithm: analysis and lower bounds. \nQuantum Fourier Transform and Quantum Phase Estimation. Definition of the Quantum Fourier \nTransform (QFT), and efficient representation thereof as a quantum circuit. Application of the \nQFT to enable Quantum Phase Estimation (QPE). \nApplication 1 of QFT / QPE: Factoring. Shor\u2019s algorithm: reduction of factoring to period finding \nand then using the QFT for period finding. \nApplication 2 of QFT / QPE: Quantum Chemistry. Efficient simulation of quantum systems, and \napplications to real-world problems in quantum chemistry. \nQuantum complexity. Quantum complexity classes and their relationship to classical complexity. \nComparison with probabilistic computation. \nQuantum error correction. Introducing the concept of quantum error correction required for the \nfollowing lecture on fault-tolerance. \nFault tolerant quantum computing. Elements of fault tolerant computing; the threshold theorem \nfor efficient suppression of errors. \nAdiabatic quantum computing and quantum optimisation. The quantum adiabatic theorem, and \nadiabatic optimisation. Quantum annealing and D-Wave. \nCase studies in near-term quantum computation. Examples of state-of-the-art quantum \nalgorithms and computers, including superconducting and networked quantum computers. \n\nObjectives \nAt the end of the course students should: \n \nunderstand the quantum model of computation and the basic principles of quantum mechanics; \nbe familiar with basic quantum algorithms and their analysis; \nbe familiar with basic quantum protocols such as teleportation and superdense coding; \nsee how the quantum model relates to classical models of deterministic and probabilistic \ncomputation. \nappreciate the importance of efficient error-suppression if quantum computation is to yield an \nadvantage over classical computation. \ngain a general understanding of the important topics in near-term quantum computing, including \nadiabatic quantum computing. \nRecommended reading \nBooks: \nNielsen M.A., Chuang I.L. (2010). Quantum Computation and Quantum Information. Cambridge \nUniversity Press. \nMermin N.D. (2007). Quantum Computer Science: An Introduction. Cambridge University Press. \nMcGeoch, C. (2014). Adiabatic Quantum Computation and Quantum Annealing Theory and \nPractice. Morgan and Claypool. https://ieeexplore.ieee.org/document/7055969 \n \nPapers: \n \nBraunstein S.L. (2003). Quantum computation tutorial. Available at: \nhttps://www-users.cs.york.ac.uk/~schmuel/comp/comp_best.pdf \nAharonov D., Quantum computation [arXiv:quant-ph/9812037] \nAlbash T., Adiabatic Quantum Computing https://arxiv.org/pdf/1611.04471.pdf \nMcCardle S. et al, Quantum computational chemistry https://arxiv.org/abs/1808.10402 \n \nOther lecture notes: \n \nAndrew Childs (University of Maryland): http://cs.umd.edu/~amchilds/qa/ \n \n\nRANDOMISED ALGORITHMS \n \nAims: \nThe aim of this course is to introduce advanced techniques in the design and analysis \nalgorithms, with a strong focus on randomised algorithms. It covers essential tools and ideas \nfrom probability, optimisation and graph theory, and develops this knowledge through a variety \nof examples of algorithms and processes. This course will demonstrate that randomness is not \nonly an important design technique which often leads to simpler and more elegant algorithms, \nbut may also be essential in settings where time and space are restricted.   \n \nLectures: \nIntroduction. Course Overview. Why Randomised Algorithms? A first Randomised Algorithm for \nthe MAX-CUT problem. [1 Lecture]  \n \nConcentration Inequalities. Moment-Generating Functions and Chernoff Bounds. Extension: \nMethod of Bounded Independent Differences. Applications: Balls-into-Bins, Quick-Sort and Load \nBalancing. [approx. 2 Lectures] \n \nMarkov Chains and Mixing Times. Random Walks on Graphs. Application: Randomised \nAlgorithm for the 2-SAT problem. Mixing Times of Markov Chains. Application: Load Balancing \non Networks. [approx. 2 Lectures] \n \nLinear Programming and Applications. Definitions and Applications. Formulating Linear \nPrograms. The Simplex Algorithm. Finding Initial Solutions. How to use Linear Programs and \nBranch & Bound to Solve a Classical TSP instance. [approx. 3 Lectures] \n \nRandomised Approximation Algorithms. Randomised Approximation Schemes. Linearity of \nExpectations, Derandomisation. Deterministic and Randomised Rounding of Linear Programs. \nApplications: MAX3-SAT problem, Weighted Vertex Cover, Weighted Set Cover, MAX-SAT. \n[approx. 2 Lectures] \n \nSpectral Graph Theory and Clustering. Eigenvalues of Graphs and Matrices: Relations between \nEigenvalues and Graph Properties, Spectral Graph Drawing. Spectral Clustering: Conductance, \nCheeger's Inequality. Spectral Partitioning Algorithm [approx. 2 Lectures] \n \nObjectives: \nBy the end of the course students should be able to: \n \nlearn how to use randomness in the design of algorithms, in particular, approximation \nalgorithms; \nlearn the basics of linear programming, integer programming and randomised rounding; \napply randomisation to various problems coming from optimisation, machine learning and data \nscience; \nuse results from probability theory to analyse the performance of randomised algorithms. \n\nRecommended reading \n* Michael Mitzenmacher and Eli Upfal. Probability and Computing: Randomized Algorithms and \nProbabilistic Analysis., Cambridge University Press, 2nd edition. \n \n* David P. Williamson and David B. Shmoys. The Design of Approximation Algorithms, \nCambridge University Press, 2011 \n \n* Cormen, T.H., Leiserson, C.D., Rivest, R.L. and Stein, C. (2009). Introduction to Algorithms. \nMIT Press (3rd ed.). ISBN 978-0-262-53305-8 \n \n \n\nCLOUD COMPUTING \nAims \nThis module aims to teach students the fundamentals of Cloud Computing covering topics such \nas virtualization, data centres, cloud resource management, cloud storage and popular cloud \napplications including batch and data stream processing. Emphasis is given on the different \nbackend technologies to build and run efficient clouds and the way clouds are used by \napplications to realise computing on demand. The course will include practical tutorials on cloud \ninfrastructure technologies. Students will be assessed via a Cloud-based coursework project. \n \nLectures \nIntroduction to Cloud Computing \nData centres \nVirtualization I \nVirtualization II \nMapReduce \nMapReduce advanced \nResource management for virtualized data centres \nCloud storage \nCloud-based data stream processing \nObjectives \nBy the end of the course students should: \n \nunderstand how modern clouds operate and provide computing on demand; \nunderstand about cloud availability, performance, scalability and cost; \nknow about cloud infrastructure technologies including virtualization, data centres, resource \nmanagement and storage; \nknow how popular applications such as batch and data stream processing run efficiently on \nclouds; \nAssessment \nAssignment 1, worth 55% of the final mark. Develop batch processing applications running on a \ncluster assessed through automatic testing, code inspection and a report. \nAssignment 2, worth 30% of the final mark: Design and develop a framework for running the \nbatch processing applications from Assignment 1 on a cluster according to certain resource \nallocation policies. This assigment will be assessed by an expert, testing, code inspection and a \nreport. \nAssignment 3, worth 15% of the final mark: Write two individual project reports describing your \nwork for assignments 1 and 2. Write two sets of scripts to manage and run your code. Reports \nand scripts will be submitted with each assignment. \nRecommended reading \nMarinescu, D.C. Cloud Computing, Theory and Practice. Morgan Kaufmann. \nBarham, P., et. al. (2003). \u201cXen and the Art of Virtualization\u201d. In Proceedings of SOSP 2003. \nCharkasova, L., Gupta, D. and Vahdat, A. (2007). \u201cComparison of the Three CPU Schedulers in \nXen\u201d. In SIGMETRICS 2007. \n\nDean, J. and Ghemawat, S. (2004). \u201cMapReduce: Simplified Data Processing on Large \nClusters\u201d. In Proceedings of OSDI 2004. \nZaharia, M, et al. (2008). \u201cImproving MapReduce Performance in Heterogeneous \nEnvironments\u201d. In Proceedings of OSDI 2008. \nHindman, A., et al. (2011). \u201cMesos: A Platform for Fine-Grained Resource Sharing in Data \nCenter\u201d. In Proceedings of NSDI 2011. \nSchwarzkopf, M., et al. (2013). \u201cOmega: Flexible, Scalable Schedulers for Large Compute \nClusters\u201d. In EuroSys 2013. \nGhemawat, S. (2003). \u201cThe Google File System\u201d. In Proceedings of SOSP 2003. \nChang, F. (2006). \u201cBigtable: A Distributed Storage System for Structured Data\u201d. In Proceedings \nof OSDI 2006. \nFernandez, R.C., et al. (2013). \u201cIntegrating Scale Out and Fault Tolerance in Stream Processing \nusing Operator State Management\u201d. In SIGMOD 2013.\n \n\nCOMPUTER SYSTEMS MODELLING \n \nAims \n \nThe aims of this course are to introduce the concepts and principles of mathematical modelling \n \nand simulation, with particular emphasis on using queuing theory and control theory for \nunderstanding the behaviour of computer and communications systems. \n \nSyllabus \n \n1.     Overview of computer systems modeling using both analytic techniques and simulation. \n \n2.     Introduction to discrete event simulation. \n \na.     Simulation techniques \n \nb.     Random number generation methods \n \nc.     Statistical aspects: confidence intervals, stopping criteria \n \nd.     Variance reduction techniques. \n \n3.     Stochastic processes (builds on starred material in Foundations of Data Science) \n \na.     Discrete and continuous stochastic processes. \n \nb.     Markov processes and Chapman-Kolmogorov equations. \n \nc.     Discrete time Markov chains. \n \nd.     Ergodicity and the stationary distribution. \n \ne.     Continuous time Markov chains. \n \nf.      Birth-death processes, flow balance equations. \n \ng.     The Poisson process. \n \n4.     Queueing theory \n \na.     The M/M/1 queue in detail. \n \n\nb.     The equilibrium distribution with conditions for existence and common performance \nmetrics. \n \nc.     Extensions of the M/M/1 queue: the M/M/k queue, the M/M/infinity queue. \n \nd.     Queueing networks. Jacksonian networks. \n \ne.     The M/G/1 queue. \n \n5.     Signals, systems, and transforms \n \na.     Discrete- and continuous-time convolution. \n \nb.     Signals. The complex exponential signal. \n \nc.     Linear Time-Invariant Systems. Modeling practical systems as an LTI system. \n \nd.     Fourier and Laplace transforms. \n \n6.     Control theory. \n \na.     Controlled systems. Modeling controlled systems. \n \nb.     State variables. The transfer function model. \n \nc.     First-order and second-order systems. \n \nd.     Feedback control. PID control. \n \ne.     Stability. BIBO stability. Lyapunov stability. \n \nf.      Introduction to Model Predictive Control. \n \nObjectives \n \nAt the end of the course students should \n \n\u00b7       Be aware of different approaches to modeling a computer system; their pros and cons \n \n\u00b7       Be aware of the issues in building a simulation of a computer system and analysing the \nresults obtained \n \n\u00b7       Understand the concept of a stochastic process and how they arise in practice \n \n\n\u00b7       Be able to build simple Markov models and understand the critical modelling assumptions \n \n\u00b7       Be able to solve simple birth-death processes \n \n\u00b7       Understand and use M/M/1 queues to model computer systems \n \n\u00b7       Be able to model a computer system as a linear time-invariant system \n \n\u00b7       Understand the dynamics of a second-order controlled system \n \n\u00b7       Design a PID control for an LTI system \n \n\u00b7       Understand what is meant by BIBO and Lyapunov stability \n \nAssessment \n \nThere will be one programming assignment to build a discrete event simulator and using it to \nsimulate an M/M/1 and an M/D/1 queue (worth 15% of the final marks). There will also be two \none-hour in-person assessments for the course on: Stochastic processes and Queueing theory \n(40%) and Signals, Systems, Transforms, and Control Theory (45%). \n \nReference books \n \nKeshav, S. (2012)*. Mathematical Foundations of Computer Networking. Addison-Wesley. A \ncustomized PDF will be made available to class participants. \n \nKleinrock, L. (1975). Queueing systems, vol. 1. Theory. Wiley. \n \nKraniauskas, Peter. Transforms in signals and systems. Addison-Wesley Longman, 1992. \n \nJain, R. (1991). The art of computer systems performance analysis. Wiley. \n \n \n\nCOMPUTING EDUCATION \n \nAims \nThis module considers various aspects of the teaching and learning of computer science in \nschool, including practical experience, and provides an introduction to the field of computing \neducation research. It is offered by the Raspberry Pi Computing Education Research Centre, \npart of the Department of Computer Science and Technology, in conjunction with local schools \nin the Cambridge area. Students will have the opportunity to observe, plan and deliver a lesson, \nand explore the teaching of computing in a secondary education setting. The lesson taught will \nbe on a topic within the computing/computer science curriculum in England but will be \nnegotiated with the teacher mentoring the school placement. In the sessions in the Department, \nstudents will be introduced to elements of pedagogy and assessment alongside current issues \nin computing education. The course can also serve as a useful introduction to research in \ncomputing education for those interested in this area. Assessment will include lesson planning, \npresentations, an in-person viva and a 3000-word report which will include evidence of \nengagement with relevant research underpinning the lessons and activities undertaken. An \ninformation session relating to the module will be available in the preceding Easter Term. The \narrangement of school placements and DBS checks will be carried out in Michaelmas Term, so \nstudents should be prepared to engage in these preparatory activities. \n \nLectures \nThis is not a traditional lecture course and the term will be structured as follows: \n \nWeek 1: Introduction to computing education & visit to school placement \nWeek 2: In school (observation, supporting teacher) \nWeek 3: In school (observation, supporting teacher) \nWeek 4: In school (co-teach / small group teaching) \nWeek 5: Computing pedagogy and assessment (school half-term) \nWeek 6: In school (co-teach / small group teaching) \nWeek 7: In school (teach part/whole lesson) \nWeek 8: Presentations \nObjectives \nBy the end of the course students should: \n \ngain experience of computing education in practice \ndemonstrate an ability to communicate an aspect of computer science clearly and effectively \nbe able to plan and design a teaching activity/lesson with consideration of the audience \nbe able to thoroughly evaluate the design and implementation of a teaching/activity lesson \nbe able to demonstrate an understanding of relevant theories of learning and pedagogy from the \nliterature and how they apply to the learning/teaching of the topic under consideration \ngain an understanding of the breadth and depth of the field of computing education research \nClass Size \nThis module can accommodate up to 10 Part II students. \n \n\nRecommended reading \nBrown, N. C., Sentance, S., Crick, T., & Humphreys, S. (2014). Restart: The resurgence of \ncomputer science in UK schools. ACM Transactions on Computing Education (TOCE), 14(2), \n1-22. https://doi.org/10.1145/2602484 \n \nSentance, S., Barendsen, E., Howard, N. R., & Schulte, C. (Eds.). (2023). Computer science \neducation: Perspectives on teaching and learning in school. Bloomsbury Publishing. \n \nGrover, S. (Ed). 2020. Computer Science in K-12: An A-to-Z Handbook on Teaching \nProgramming. Edfinity. https://www.shuchigrover.com/atozk12cs/ \n \nRaspberry Pi Foundation (2022). Hello World Big Book of Pedagogy. \nhttps://www.raspberrypi.org/hello-world/issues/the-big-book-of-computing-pedagogy \n \nAssessment \nWeek 2: Reflections on an observed lesson (10%, graded out of 20) \nWeek 4: Submission of a lesson plan outline (10% graded out of 20) \nWeek 8: Delivery of an oral presentation relating to the lesson delivery (10% graded out of 20) \nAfter course completion: 3000 word report (60%), graded out of 20 \nViva with assessor (10%) (pass/fail) \nFurther Information \nWe will find placements for students in secondary schools within cycling or bus distance from \nthe Department/centre of Cambridge. There will be a timetabled slot for the course that students \ncan use to go to their school placement, but students may need to be flexible and liaise with the \nschool to find the best time for working with a specific class. \n \n \n\nDEEP NEURAL NETWORKS \n \nObjectives \nYou will gain detailed knowledge of \n \nCurrent understanding of generalization in neural networks vs classical statistical models. \nOptimization procedures for neural network models such as stochastic gradient descent and \nADAM. \nAutomatic differentiation and at least one software framework (PyTorch, TensorFolow) as well as \nan overview of other software approaches. \nArchitectures that are deployed to deal with different data types such as images or sequences \nincluding a. convolutional networks b. recurrent networks and c. transformers. \nIn addition you will gain knowledge of more advanced topics reflecting recent research in \nmachine learning. These change from year to year, examples include: \n \nApproaches to unsupervised learning including autoencoders and self-supervised learning.. \nTechniques for deploying models in low data regimes such as transfer learning and \nmeta-learning. \nTechniques for propagating uncertainty such as Bayesian neural networks. \nReinforcement learning and its applications to robotics or games. \nGraph Neural Networks and Geometric Deep Learning. \nTeaching Style \nThe start of the course will focus on the latest undertanding of current theory of neural networks, \ncontrasting with previous classical understandings of generalization performance. Then we will \nmove to practical examples of network architectures and deployment. We will end with more \nadvanced topics reflecting current research, mostly delivered by guest lecturers from the \nDepartment and beyond. \n \nSchedule \nWeek 1 \nTwo lectures: Generalization and Approximation with Neural Networks \n \nWeek 2 \nTwo lectures: Optimization: Stochastic Gradient Descent and ADAM \n \nWeek 3 \nTwo lectures: Hardware Acceleration and Convolutional Neural Networks \n \nWeek 4 \nTwo lectures: Vanishing Gradients, Recurrent and Residual Networks, Sequence Modeling. \n \nWeek 5 \nTwo lectures: Attention, The Transformer Architecture, Large Language Models \n \n\nWeek 6-8 \nLectures and guest lectures from the special topics below. \n \nSpecial topics \nSelf-Supervised Learning, AutoEncoders, Generative Modeling of Images \nHardware Implementations \nReinforcement learning \nTransfer learning and meta-learning \nUncertainty and Bayesian Neural Networks \nGraph Neural Networks \nAssessment \nAssessment involves solving a number of exercises in a jupyter notebook environment. \nFamiliarity with the python programming language is assumed. The exercises rely on the \npytorch deep learning framework, the syntax of which we don\u2019t cover in detail in the lectures, \nstudents are referred to documentation and tutorials to learn this component of the assessment. \n \nAssignment 1 \n \n30% of the total marks, with guided exercises. \n \nAssignment 2 \n \n70% of the total marks, a combination of guided exercises and a more exploratory mini-project.\n\nEXTENDED REALITY \n \nSyllabus & Schedule \nWeek 1 \nIntroduction \nCourse schedule and concept \nThe history of AR/VR/XR \nApplications of AR/VR/XR \nOverview of the XR pipeline \u2013 3 main components of XR \nDisplay technologies and rendering \nMachine perception and input interfaces \nContent generation \u2013 geometry, appearance, motion \nOverview of the AR/VR/XR frameworks \nPractical 1 \nProject structure \nProject themes \nIntroduction to the XR programming platform \nWeek 2 \n3D scene representations and rendering \nObject representations with polygonal meshes \nEfficient rendering via rasterization \nPractical XR rendering of textured geometries \nTracking and pose estimation for XR \nRigid transforms \nCamera pose estimation \nBody/ hand tracking \nPractical 2 \nProject draft proposal due \nProject feedback \nRendering and displaying a first object \nWeek 3 \n3D scene capture and understanding \nDepth estimation \n3D geometry capture \nAppearance acquisition \nLighting estimation and relighting \nLighting models for 3D scenes \nEstimating lighting in a scene \nRelighting rendered models \nPractical 3 \nAcquiring and utilizing depth from sensors \nRelighting a displayed object \nWeek 4 \nPhysically-based simulation of rigid objects \n\nRigid-body dynamics \nCollision detection \nTime integration \nPhysically-based simulation of non-rigid volumetric phenomena \nParticle systems \nBasic fluid dynamics \nPractical 4 \nPractical due \nProject prototype due \nProject check-in \nSimulating and rendering a simple object \nWeek 5 \nAdvanced topics in XR I: AR/VR displays technologies \nMicro LED-based displays \nWaveguide-based displays \nNext generation displays \nWeek 6 \nAdvanced topics in XR II \u2013 Mobile AR \nComputer vision on limited hardware \nUX/UI challenges \nContent challenges \nProject due \n  \n \nAssessment \nA practical exercise, worth 20% of the mark. This will cover the basics of AR/VR development \nand some of the practical techniques the students will use in their projects. This is individual \nwork. No mobile device or GPU hours are needed. \nAn AR/VR project worth 80% of the marks: \nCourse projects will be designed by the students following the possible project themes proposed \nby the lecturer and will be checked by the lecturer for appropriateness. \nThe students will form groups of 2-3 to design, implement, report, and present the course \nproject. \nThe final mark will be an implementation mark (40%), a report mark (20%), and a \npresentation/demo mark (20%). Each team member will be evaluated based on their \ncontribution. \nEach project will have extensions to be completed only by the ACS students. Each student will \nwrite a different part of the report, whose author will be clearly marked. Each student will further \nsummarise her/his contributions to the project in the same report. \nNote: Submission is by one submission per group but work by each individual must be clearly \nlabelled.\n \n\nFEDERATED LEARNING: THEORY AND PRACTICE \n \nObjectives \nThis course aims to extend the machine learning knowledge available to students in Part I (or \npresent in typical undergraduate degrees in other universities), and allow them to understand \nhow these concepts can manifest in a decentralized setting. The course will consider both \ntheoretical (e.g., decentralized optimization) and practical (e.g., networking efficiency) aspects \nthat combine to define this growing area of machine learning.  \n \nAt the end of the course students should: \n \nUnderstand popular methods used in federated learning \nBe able to construct and scale a simple federated system \nHave gained an appreciation of the core limitations to existing methods, and the approaches \navailable to cope with these issues \nDeveloped an intuition for related technologies like differential privacy and secure aggregation, \nand are able to use them within typical federated settings  \nCan reason about the privacy and security issues with federated systems \nLectures \nCourse Overview. Introduction to Federated Learning. \nDecentralized Optimization. \nStatistical and Systems Heterogeneity. \nVariations of Federated Aggregation. \nSecure Aggregation. \nDifferential Privacy within Federated Systems. \nExtensions to Federated Analytics. \nApplications to Speech, Video, Images and Robotics. \nLab sessions \nFederating a Centralized ML Classifier. \nBehaviour under Heterogeneity. \nScaling a Federated Implementation. \nExploring Privacy with Federated Settings \nRecommended Reading \nReadings will be assigned for each lecture. Readings will be taken from either research papers, \ntutorials, source code or blogs that provide more comprehensive treatment of taught concepts. \n \nAssessment - Part II Students \nFour labs are performed during the course, and students receive 10% of their total grade for \nwork done as part of each lab. (For a total of 40% of the total grade from lab work alone). Labs \nwill primarily provide hands-on teaching opportunities, that are then utilized within the lab \nassignment which is completed outside of the lab contact time. MPhil and Part III students will \nbe given additional questions to answer within their version of the lab assignment which will \ndiffer from the assignment given to Part II CST students. \n \n\nThe remainder of the course grade (60%) will be given based on a hands-on project that applies \nthe concepts taught in lectures and labs. This hands-on project will be assessed based on upon \na combination of source code, related documentation and brief 8-minute pre-recorded talk that \nsummarizes key project elements (any slides used are also submitted as part of the project). \nPlease note, that in the case of Part II CST students, the talk itself is not examinable -- as such \nwill be made optional to those students. \n \nA range of possible practical projects will be described and offered to students to select from, or \nalternatively students may propose their own. MPhil and Part III students will select from a \nproject pool that is separate from those offered to Part II CST students. MPhil and Part III \nprojects will contain a greater emphasis on a research element, and the pre-recorded talks \nprovided by this student group will focus on this research contribution. The project will be \nassessed on the level of student understanding demonstrated, the degree of difficulty, \ncorrectness of implementation -- and for Part III/MPhil students the additional criteria of the \nquality and execution of the research methodology, and depth and quality of results analysis. \n \nThis project can be done individually or in groups -- although individual projects will be strongly \nencouraged. It will be required the project is performed using a code repository that also will \ncontain all documentation -- access to this repository will be shared with course staff (e.g., \nlecturer and TAs). Where needed, marks assigned to students within a group will be \ndifferentiated using this repository as an input. Furthermore if groups are formed, members must \nbe either entirely from Part III/MPhil students or Part II CST, i.e., these two student groups \nshould not mix to form a project group. \n \nProjects will be made available publicly. A maximum word count for written contributions for the \nproject will be enforced.\n \n\nMOBILE HEALTH \n \nAims \nThe course aims to explore how wearable and mobile systems sensors can be used to gather \ndata relevant to understand health, how the data can be analysed with advanced signal \nprocessing and machine learning and the performance of these systems in terms of diagnostics \nand disease progression detection. \n \nSyllabus \nCourse Overview. Introduction to Mobile Health. Evaluation metrics and methodology. Basics of \nSignal Processing. \nInertial Measurement Units, Human Activity Recognition (HAR) and Gait Analysis and Machine \nLearning for IMU data. \nRadios, Bluetooth, GPS and Cellular. Epidemiology and contact tracing, Social interaction \nsensing and applications. Location tracking in health monitoring and public health. \nAudio Signal Processing. Voice and Speech Analysis: concepts and data analysis. Body \nSounds analysis. \nPhotoplethysmogram and Light sensing for health (heart and sleep) \nContactless and wireless behaviour and physiological monitoring \nGenerative Models for Wearable Health Data \nTopical Guest Lectures \nObjectives \nThe course aims to explore how wearable and mobile systems sensors can be used to gather \ndata relevant to understand health, how the data can be analysed with advanced signal \nprocessing and machine learning and the performance of these systems in terms of diagnostics \nand disease progression detection. \n \nRoughly, each lecture contains a theory part about the working of \u201csensor signals\u201d or \u201cdata \nanalysis methods\u201d and an application part which contextualises the concepts. \n \nAt the end of the course students should: Understand how mobile/wearable sensors capture \ndata and their working. Understand different approaches to acquiring and analysing sensor data \nfrom different types of sensors. Understand the concept of signal processing applied to time \nseries data and their practical application in health. Be able to extract sensor data and analyse it \nwith basic signal processing and machine learning techniques. Be aware of the different health \napplications of the various sensor techniques. The course will also touch on privacy and ethics \nimplications of the approaches developed in an orthogonal fashion. \n \nRecommended Reading \nPlease see Course Materials for recommended reading for each session. \n \nAssessment - Part II students \nTwo assignments will be based on two datasets which will be provided to the students: \n \n\nAssignment 1 (shared for Part II and Part III/MPhil): this will be based on a dataset and will be \nworth 40% of the final mark. The task of the assessment will be to perform pre-processing and \nbasic data analysis in a \"colab\" and an answer sheet of no more than 1000 words. \n \nAssignment 2 (Part II): This assignment (worth 60% of the final mark) will be a fuller analysis of \na dataset focusing on machine learning algorithms and metrics. Discussion and interpretation of \nthe findings will be reported in a colab and a report of no more than 1200 words.\n \n\nMULTICORE SEMANTICS AND PROGRAMMING \n \nAims \nIn recent years multiprocessors have become ubiquitous, but building reliable concurrent \nsystems with good performance remains very challenging. The aim of this module is to \nintroduce some of the theory and the practice of concurrent programming, from hardware \nmemory models and the design of high-level programming languages to the correctness and \nperformance properties of concurrent algorithms. \n \nLectures \nPart 1: Introduction and relaxed-memory concurrency [Professor P. Sewell] \n \nIntroduction. Sequential consistency, atomicity, basic concurrent problems. [1 block] \nConcurrency on real multiprocessors: the relaxed memory model(s) for x86, ARM, and IBM \nPower, and theoretical tools for reasoning about x86-TSO programs. [2 blocks] \nHigh-level languages. An introduction to C/C++11 and Java shared-memory concurrency. [1 \nblock] \nPart 2: Concurrent algorithms [Dr T. Harris] \n \nConcurrent programming. Simple algorithms (readers/writers, stacks, queues) and correctness \ncriteria (linearisability and progress properties). Advanced synchronisation patterns (e.g. some \nof the following: optimistic and lazy list algorithms, hash tables, double-checked locking, RCU, \nhazard pointers), with discussion of performance and on the interaction between algorithm \ndesign and the underlying relaxed memory models. [3 blocks] \nResearch topics, likely to include one hour on transactional memory and one guest lecture. [1 \nblock] \nObjectives \nBy the end of the course students should: \n \nhave a good understanding of the semantics of concurrent programs, both at the multprocessor \nlevel and the C/Java programming language level; \nhave a good understanding of some key concurrent algorithms, with practical experience. \nAssessment - Part II Students \nTwo assignments each worth 50% \n \nRecommended reading \nHerlihy, M. and Shavit, N. (2008). The art of multiprocessor programming. Morgan Kaufmann.\n\nBUSINESS STUDIES SEMINARS \nAims \nThis course is a series of seminars by former members and friends of the Laboratory about their \nreal-world experiences of starting and running high technology companies. It is a follow on to \nthe Business Studies course in the Michaelmas Term. It provides practical examples and case \nstudies, and the opportunity to network with and learn from actual entrepreneurs. \n \nLectures \nEight lectures by eight different entrepreneurs. \n \nObjectives \nAt the end of the course students should have a better knowledge of the pleasures and pitfalls \nof starting a high tech company. \n \nRecommended reading \nLang, J. (2001). The high-tech entrepreneur\u2019s handbook: how to start and run a high-tech \ncompany. FT.COM/Prentice Hall. \nMaurya, A. (2012). Running Lean: Iterate from Plan A to a Plan That Works. O\u2019Reilly. \nOsterwalder, A. and Pigneur, Y. (2010). Business Model Generation: A Handbook for \nVisionaires, Game Changers, and Challengers. Wiley. \nKim, W. and Mauborgne, R. (2005). Blue Ocean Strategy. Harvard Business School Press. \n \nSee also the additional reading list on the Business Studies web page.\n \n\nHOARE LOGIC AND MODEL CHECKING \n \nAims \nThe course introduces two verification methods, Hoare Logic and Temporal Logic, and uses \nthem to formally specify and verify imperative programs and systems. \n \nThe first aim is to introduce Hoare logic for a simple imperative language and then to show how \nit can be used to formally specify programs (along with discussion of soundness and \ncompleteness), and also how to use it in a mechanised program verifier. \n \nThe second aim is to introduce model checking: to show how temporal models can be used to \nrepresent systems, how temporal logic can describe the behaviour of systems, and finally to \nintroduce model-checking algorithms to determine whether properties hold, and to find \ncounter-examples. \n \nCurrent research trends also will be outlined. \n \nLectures \nPart 1: Hoare logic. Formal versus informal methods. Specification using preconditions and \npostconditions. \nAxioms and rules of inference. Hoare logic for a simple language with assignments, sequences, \nconditionals and while-loops. Syntax-directedness. \nLoops and invariants. Various examples illustrating loop invariants and how they can be found. \nPartial and total correctness. Hoare logic for proving termination. Variants. \nSemantics and metatheory Mathematical interpretation of Hoare logic. Semantics and \nsoundness of Hoare logic. \nSeparation logic Separation logic as a resource-aware reinterpretation of Hoare logic to deal \nwith aliasing in programs with pointers. \nPart 2: Model checking. Models. Representation of state spaces. Reachable states. \nTemporal logic. Linear and branching time. Temporal operators. Path quantifiers. CTL, LTL, and \nCTL*. \nModel checking. Simple algorithms for verifying that temporal properties hold. \nApplications and more recent developments Simple software and hardware examples. CEGAR \n(counter-example guided abstraction refinement). \nObjectives \nAt the end of the course students should \n \nbe able to prove simple programs correct by hand and implement a simple program verifier; \nbe familiar with the theory and use of separation logic; \nbe able to write simple models and specify them using temporal logic; \nbe familiar with the core ideas of model checking, and be able to implement a simple model \nchecker. \nRecommended reading \n\nHuth, M. and Ryan M. (2004). Logic in Computer Science: Modelling and Reasoning about \nSystems. Cambridge University Press (2nd ed.).\n \n\n",
        "7th Semester \nADVANCED TOPICS IN COMPUTER ARCHITECTURE \nAims \nThis course aims to provide students with an introduction to a range of advanced topics in \ncomputer architecture. It will explore the current and future challenges facing the architects of \nmodern computers. These will also be used to illustrate the many different influences and \ntrade-offs involved in computer architecture. \n \nObjectives \nOn completion of this module students should: \n \nunderstand the challenges of designing and verifying modern microprocessors \nbe familiar with recent research themes and emerging challenges \nappreciate the complex trade-offs at the heart of computer architecture \nSyllabus \nEach seminar will focus on a different topic. The proposed topics are listed below but there may \nbe some minor changes to this: \n \nTrends in computer architecture \nState-of-the-art microprocessor design \nMemory system design \nHardware reliability \nSpecification, verification and test (may be be replaced with a different topic) \nHardware security (2) \nHW accelerators and accelerators for machine learning \nEach two hour seminar will include three student presentations (15mins) questions (5mins) and \na broader discussion of the topics (around 30mins). The last part of the seminar will include a \nshort scene setting lecture (around 20mins) to introduce the following week's topic. \n \nAssessment \nEach week students will compare and contrast two of the main papers and submit a written \nsummary and review in advance of each seminar (except when presenting). \n \nStudents will be expected to give a number of 15 minute presentations. \n \nEssays and presentations will be marked out of 10. After dropping the lowest mark, the \nremaining marks will be scaled to give a final score out of 100. \n \nStudents will give at least one presentation during the course. They will not be required to \nsubmit an essay during the weeks they are presenting. \n \nEach presentation will focus on a single paper from the reading list. Marks will be awarded for \nclarity and the communication of the paper's key ideas, an analysis of the work's strengths and \nweaknesses and the work\u2019s relationship to related work and broader trends and constraints. \n\n \nRecommended prerequisite reading \nPatterson, D. A., Hennessy, J. L. (2017). Computer organization and design: The \nHardware/software interface RISC-V edition Morgan Kaufmann. ISBN 978-0-12-812275-4. \n \nHennessy, J. and Patterson, D. (2012). Computer architecture: a quantitative approach. Elsevier \n(5th ed.) ISBN 9780123838728. (the 3rd and 4th editions are also relevant)\n \n\nADVANCED TOPICS IN PROGRAMMING LANGUAGES \nAims \nThis module explores various topics in programming languages beyond the scope of \nundergraduate courses. It aims to introduce students to ideas, results and techniques found in \nthe literature and prepare them for research in the field. \n \nSyllabus and structure \nThe module consists of eight two-hour seminars, each on a particular topic. Topics will vary from \nyear to year, but may include, for example, \n \nAbstract interpretation \nVerified software \nMetaprogramming \nBehavioural types \nProgram synthesis \nVerified compilation \nPartial evaluation \nGarbage collection \nDependent types \nAutomatic differentiation \nDelimited continuations \nModule systems \nThere will be three papers assigned for each topic, which students are expected to read before \nthe seminar. \nEach seminar will include three 20 minute student presentations (15 minutes + 5 minutes \nquestions), time for general discussion of the topic, and a brief overview lecture for the following \nweek\u2019s topic. \nBefore each seminar, except in weeks in which they give presentations, students will submit a \nshort essay about two of the papers. \n \n \nObjectives \nOn completion of this module, students should \n \nbe able to identify some major themes in programming language research \nbe familiar with some classic papers and recent advances \nhave an understanding of techniques used in the field \n \nAssessment \nAssessment consists of: \n \nPresentation of one of the papers from the reading list (typically once or twice in total for each \nstudent, depending on class numbers) \nOne essay per week (except on the first week and on presentation weeks) \n\nAll essays and presentations carry equal numbers of marks. \n \nEssay marks are awarded for understanding, for insight and analysis, and for writing quality. \nEssays should be around 1500 words (with a lower limit of 1450 and upper limit of 1650). \nPresentation marks are awarded for clarity, for effective communication, and for selection and \norganisation of topics. \n \nThere will be seven submissions (essays or presentations) in total and, as in other courses, the \nlowest mark for each student will be disregarded when computing the final mark. \n \nMarking, deadlines and extensions will be handled in accordance with the MPhil Assessment \nGuidelines. \n \nRecommended reading material and resources \nResearch and survey papers from programming language conferences and journals (e.g. \nPOPL, PLDI, TOPLAS, FTPL) will be assigned each week. General background material may \nbe found in: \n \n\u2022 Types and Programming Languages (Benjamin C. Pierce) \n   The MIT Press \n   ISBN 0-262-16209-1 \n \n\u2022 Advanced Topics in Types and Programming Languages (ed. Benjamin C. Pierce) \n   The MIT Press \n   ISBN 0-262-16228-8 \n \n\u2022 Practical Foundations for Programming Languages (Robert Harper) \n   Cambridge University Press \n   9781107150300\n \n\nAFFECTIVE ARTIFICIAL INTELLIGENCE \n \nSynopsis \nAffective Artificial Intelligence (Affective AI) aims to imbue machines with social and emotional \nintelligence (EQ). More specifically, Affective AI aims to create artificially intelligent systems and \nmachines that can recognize, interpret, process, and simulate human social signals and \nbehaviours, expressions, and emotions, to enhance human-AI interaction and communication. \n\\r\\n\\r\\nTo achieve this goal, Affective AI draws upon various scientific disciplines, including \nmachine learning, computer vision, speech / natural language / signal processing, psychology \nand cognitive science, and ethics and social sciences. \n \n  \nBackground & Aims \nAffective Artificial Intelligence (Affective AI) aims to imbue machines with social and emotional \nintelligence (EQ). More specifically, Affective AI aims to create artificially intelligent systems and \nmachines that can recognize, interpret, process, and simulate human social signals and \nbehaviours, expressions, and emotions, to enhance human-AI interaction and communication.  \n \nTo achieve this goal, Affective AI draws upon various scientific disciplines, including machine \nlearning, computer vision, speech / natural language / signal processing, psychology and \ncognitive science, and ethics and social sciences. \n \nAffective AI has direct applications in and implications for the design of innovative interactive \ntechnology (e.g., interaction with chat bots, virtual agents, robots), single and multi-user smart \nenvironments ( e.g., in-car/ virtual / augmented / mixed reality, serious games), public speaking \nand cognitive training, and clinical and biomedical studies (e.g., autism, depression, pain). \n \nThe aim of this module is to impart knowledge and ability needed to make informed choices of \nmodels, data, and machine learning techniques for sensing, recognition, and generation of \naffective and social behaviour (e.g., smile, frown, head nodding/shaking, \nagreement/disagreement) in order to create Affectively intelligent AI systems, with a \nconsideration for various ethical issues (e.g., privacy, bias) arising from the real-world \ndeployment of these systems. \n \n  \n \nSyllabus \nThe following list provides a representative list of topics: \n \nIntroduction, definitions, and overview \nTheories from various disciplines \nSensing from multiple modalities (e.g., vision, audio, bio signals, text) \nData acquisition and annotation \nSignal processing / feature extraction \n\nLearning / prediction / recognition and evaluation \nBehaviour synthesis / generation (e.g., for embodied agents / robots) \nAdvanced topics and ethical considerations (e.g., bias and fairness) \nCross-disciplinary applications (via seminar presentations and discussions) \nGuest lectures (diverse topics \u2013 changes each year) \nHands-on research and programming work (i.e., mini project & report)  \n \n  \n \nObjectives \nOn completion of this module, students will: \n \nunderstand and demonstrate knowledge in key characteristics of affectively intelligent AI, which \ninclude: \nRecognition: How to equip Affective AI systems with capabilities of analysing facial expressions, \nvocal intonations, gestures, and other physiological signals to infer human affective states? \nGeneration: How to enable affectively intelligent AI simulate expressions of emotions in \nmachines, allowing them to respond in a more human-like manner, such as virtual agents and \nhumanoid robots, showing empathy or sympathy? \nAdaptation and Personalization: How to enable affectively intelligent AI adapt system responses \nbased on users' affective states and/or needs, or tailor interactions and experiences to individual \nusers based on their expressivity / emotional profiles, personalities, and past interactions? \nEmpathetic Communication: How to design Affective AI systems to communicate with users in a \nway that demonstrates empathy, understanding, and sensitivity to their affective states and \nneeds? \nEthical and societal considerations: What are the various human differences, ethical guidelines, \nand societal impacts that need to be considered when designing and deploying Affective AI to \nensure that these systems respect users' privacy, autonomy, and well-being? \ncomprehend and apply (appropriate) methods for collection, analysis, representation, and \nevaluation of human affective and communicative behavioural data. \nenhance programming skills for creating and implementing (components of) Affective AI \nsystems. \ndemonstrate critical thinking, analysis and synthesis while deciding on 'when' and 'how' to \nincorporate human affect and social signals in a specific AI system context and gain practical \nexperience in proposing and justifying computational solution(s) of suitable nature and scope. \n  \n \nAssessment - Part II Students \nSeminar presentation: 20% \nParticipating in Q&A and discussions: 10% \nMid-term mini project report and presentation (as a team of 2): 10%  \nFinal mini project report & code (as a team of 2): 60% \n  \n \n\nAssessment - MPhil / Part III Students \nSeminar presentation: 20% \nParticipating in Q&A and discussions: 10% \nMid-term mini project report and presentation: 10%  \nFinal mini project report & code: 60% \nFurther Information \nCurrent Cambridge undergraduate students who are continuing onto Part III or the MPhil in \nAdvanced Computer Science may only take this module if they did NOT take it as a Unit of \nAssessment in Part II. \n \nThis module is shared with ACS. Assessment will be adjusted for the two groups of students to \nbe at an appropriate level for whichever course the student is enrolled on. More information will \nfollow at the first lecture. \n \nThis module is shared with Part II of the Computer Science Tripos. Assessment will be adjusted \nfor the two groups of students to be at an appropriate level for whichever course the student is \nenrolled on. Further information about assessment and practicals will follow at the first lecture.\n\nCATEGORY THEORY \n \nAims \nCategory theory provides a unified treatment of mathematical properties and constructions that \ncan be expressed in terms of 'morphisms' between structures. It gives a precise framework for \ncomparing one branch of mathematics (organized as a category) with another and for the \ntransfer of problems in one area to another. Since its origins in the 1940s motivated by \nconnections between algebra and geometry, category theory has been applied to diverse fields, \nincluding computer science, logic and linguistics. This course introduces the basic notions of \ncategory theory: adjunction, natural transformation, functor and category. We will use category \ntheory to organize and develop the kinds of structure that arise in models and semantics for \nlogics and programming languages. \n \nSyllabus \nIntroduction; some history. Definition of category. The category of sets and functions. \nCommutative diagrams. Examples of categories: preorders and monotone functions; monoids \nand monoid homomorphisms; a preorder as a category; a monoid as a category. Definition of \nisomorphism. Informal notion of a 'category-theoretic' property. \nTerminal objects. The opposite of a category and the duality principle. Initial objects. Free \nmonoids as initial objects. \nBinary products and coproducts. Cartesian categories. \nExponential objects: in the category of sets and in general. Cartesian closed categories: \ndefinition and examples. \nIntuitionistic Propositional Logic (IPL) in Natural Deduction style. Semantics of IPL in a cartesian \nclosed preorder. \nSimply Typed Lambda Calculus (STLC). The typing relation. Semantics of STLC types and \nterms in a cartesian closed category (ccc). The internal language of a ccc. The \nCurry-Howard-Lawvere correspondence. \nFunctors. Contravariance. Identity and composition for functors. \nSize: small categories and locally small categories. The category of small categories. Finite \nproducts of categories. \nNatural transformations. Functor categories. The category of small categories is cartesian \nclosed. \nHom functors. Natural isomorphisms. Adjunctions. Examples of adjoint functors. Theorem \ncharacterising the existence of right (respectively left) adjoints in terms of a universal property. \nDependent types. Dependent product sets and dependent function sets as adjoint functors. \nEquivalence of categories. Example: the category of I-indexed sets and functions is equivalent \nto the slice category Set/I. \nPresheaves. The Yoneda Lemma. Categories of presheaves are cartesian closed. \nMonads. Modelling notions of computation as monads. Moggi's computational lambda calculus. \nObjectives \nOn completion of this module, students should: \n \n\nbe familiar with some of the basic notions of category theory and its connections with logic and \nprogramming language semantics \nAssessment \na graded exercise sheet (25% of the final mark), and \na take-home test (75%) \nRecommended reading \nAwodey, S. (2010). Category theory. Oxford University Press (2nd ed.). \n \nCrole, R. L. (1994). Categories for types. Cambridge University Press. \n \nLambek, J. and Scott, P. J. (1986). Introduction to higher order categorical logic. Cambridge \nUniversity Press. \n \nPitts, A. M. (2000). Categorical Logic. Chapter 2 of S. Abramsky, D. M. Gabbay and T. S. E. \nMaibaum (Eds) Handbook of Logic in Computer Science, Volume 5. Oxford University Press. \n(Draft copy available here.) \n \nClass Size \nThis module can accommodate upto 15 Part II students plus 15 MPhil / Part III students. \n \nPre-registration evaluation form  \nStudents who are interested in taking this module will be asked to complete a pre-registration \nevaluation form to determine whether they have enough mathematical background to take the \nmodule.  \n \nThis will take place during the week Monday 12 August - Friday 16 August.  \n \nCoursework \nThe coursework in this module will consist of a graded exercise sheet. \n \nFurther Information \nCurrent Cambridge undergraduate students who are continuing onto Part III or the MPhil in \nAdvanced Computer Science may only take this module if they did NOT take 'Catgeory Theory' \nas a Unit of Assessment in Part II. \n \nThis module is shared with Part II of the Computer Science Tripos. Assessment will be adjusted \nfor the two groups of students to be at an appropriate level for whichever course the student is \nenrolled on. Further information about assessment and practicals will follow at the first lecture.\n\nDIGITAL MONEY AND DECENTRALISED FINANCE \n \nAims \nOur society is evolving towards digital payments and the elimination of physical cash. Digital \nalternatives to cash require trade-offs between various properties including unforgeability, \ntraceability, divisibility, transferability without intermediaries, privacy, redeemability, control over \nthe money supply and many more, often in conflict with each other. Since the 1980s, \ncryptographers have proposed a variety of clever technical solutions addressing some of these \nissues but it is only with the appearance of Bitcoin, blockchain and a plethora of copycat \ncryptocurrencies that a new asset class has now emerged, worth in excess of two trillion dollars \n(although plagued by extreme volatility). Meanwhile, most major countries have been planning \nthe introduction of Central Bank Digital Currencies in an attempt to retain control. This \nseminar-style interactive module encourages the students to understand what's happening, \nwhat the underlying problems and opportunities are (technical, social and economic) and where \nwe should go from here. We are at a historical turning point where an enterprising candidate \nmight disrupt the status quo and truly make a difference. \n \nSyllabus \nUnderstanding money: from gold standard to digital cash \nIntro to the Decentralised Finance (DeFi) ecosystem: Bitcoin, blockchain, wallets, smart \ncontracts \nStablecoins and Central Bank Digital Currencies \nDecentralised Autonomous Organisations and Automated Market Makers \nYield Farming, Perpetual Futures and Maximal Extractable Value \nWeb3, Non Fungible Tokens \nCryptocurrency crashes; crimes facilitated by cryptocurrencies \nNew areas of development. Where from here? \nThe module is structured as a reading club with a high degree of interactivity. Aside from the first \nand last session, which are treated differently, the regular two-hour sessions have the following \nstructure (not necessarily in this order). \n \nTwo half-hour debates on each of two papers \nTwo rapid-fire presentations on open questions previously assigned by the lecturers. \nHalf an hour of presentation by the lecturers. \nAbout ten minutes of buffer that can account for switchover time or be absorbed into the \nlecturers' presentation. \nThe first session is scene-setting by the lecturers, with no student presentations. The last \nsession has no paper debates, only one rapid-fire presentation on an open question from each \nof the students. \n \nObjectives \nOn completion of this module, students will gain an in-depth understanding of digital money \nalternatives as well as many modern applications and components of Decentralised Finance. By \ncomparing DeFi with current processes in traditional banking, students will be able to discuss \n\nthe merits and limitations of these new technologies as well as to identify potential new areas of \ndevelopment. \n \nThrough having to write, present and defend very brief extended abstracts, the students will \nimprove their communication skills and in particular the ability to distil and convey the most \nimportant points forcefully and concisely. \n \n \nAssessment \nThe module is an interactive reading club. Each student produces four output triplets throughout \nthe course. Each output triplet consists of three parts: a 1200-word essay, a 200-word extended \nabstract and a 7-minute presentation. \n \nThe essay and abstract must be submitted in advance, using the LaTeX templates provided. \nThe abstract must consist of full sentences, not bullet points, and must fit on a single slide. \nDuring the presentation, the slide with the abstract will be projected on screen as the only visual \naid; the essay will not be accessible to either presenter or audience. Reading out the slide \nverbatim will obviously not count as a great presentation. Presenters must be able to expand \nand defend their ideas live, and answer questions from lecturers and audience. \n \nThe essays, the abstracts and the presentations are graded separately, each out of 10, and \nassigned weights of 60%, 20%, 20%, respectively, within the triplet. \n \nIn a regular 2-hour session (all but the first and last) there will be two paper slots and two \ninvestigation slots, plus a lecturer slot, described next. \n \nEach paper slot (30 min) involves a \u201csupporter\u201d and a \u201cdissenter\u201d for the paper, each \ncontributing one output triplet, and then a panel Q&A where all the other participants quiz the \nsupporter and dissenter. \n \nEach investigation slot (10 min) involves a single student contributing an output triplet on a \ngiven theme. \n \nRoles, papers and themes are assigned by the lecturers the previous week. \n \nIn the first session there will be no student presentations, only lecturer-led exploration. In the \nlast session there will be 12 investigation slots and no paper slots. \n \nOverall, each student will be supporter once, dissenter once and investigator twice, for a total of \n4 output triplets. The output triplets of the last session will be weighed twice as much as the \nothers, i.e. 40% each, while the others 20% each. \n \nAttendance is required at all sessions. Missing a session in which the candidate was due to \npresent will result in the loss of the corresponding presentation marks, while the written work will \n\nstill have to be submitted and will be marked as usual. Missing a session in which the candidate \nwas not due to present will result in the loss of 3%. \n \nRecommended reading \nThere isn't a textbook covering all the material in this course. The following textbook, also \nadopted by R47 Distributed Ledger Technologies (also freely available online), is a thorough \nintroduction to Bitcoin and Blockchain, but we will complement it with research papers and \nindustry white papers for each lecture. \n \nNarayanan, A. , Bonneau, J., Felten, E., Miller, A. and Goldfeder, S. (2016). Bitcoin and \nCryptocurrency Technologies: A Comprehensive Introduction. Princeton University Press. \n(2016 Draft available here: \nhttps://d28rh4a8wq0iu5.cloudfront.net/bitcointech/readings/princeton_bitcoin_book.pdf)\n \n\nDIGITAL SIGNAL PROCESSING \n \nAims \nThis course teaches basic signal-processing principles necessary to understand many modern \nhigh-tech systems, with examples from audio processing, image coding, radio communication, \nradar, and software-defined radio. Students will gain practical experience from numerical \nexperiments in programming assignments (in Julia, MATLAB or NumPy). \n \nLectures \nSignals and systems. Discrete sequences and systems: types and properties. Amplitude, \nphase, frequency, modulation, decibels, root-mean square. Linear time-invariant systems, \nconvolution. Some examples from electronics, optics and acoustics. \nPhasors. Eigen functions of linear time-invariant systems. Review of complex arithmetic. \nPhasors as orthogonal base functions. \nFourier transform. Forms and properties of the Fourier transform. Convolution theorem. Rect \nand sinc. \nDirac\u2019s delta function. Fourier representation of sine waves, impulse combs in the time and \nfrequency domain. Amplitude-modulation in the frequency domain. \nDiscrete sequences and spectra. Sampling of continuous signals, periodic signals, aliasing, \ninterpolation, sampling and reconstruction, sample-rate conversion, oversampling, spectral \ninversion. \nDiscrete Fourier transform. Continuous versus discrete Fourier transform, symmetry, linearity, \nFFT, real-valued FFT, FFT-based convolution, zero padding, FFT-based resampling, \ndeconvolution exercise. \nSpectral estimation. Short-time Fourier transform, leakage and scalloping phenomena, \nwindowing, zero padding. Audio and voice examples. DTFM exercise. \nFinite impulse-response filters. Properties of filters, implementation forms, window-based FIR \ndesign, use of frequency-inversion to obtain high-pass filters, use of modulation to obtain \nband-pass filters. \nInfinite impulse-response filters. Sequences as polynomials, z-transform, zeros and poles, some \nanalog IIR design techniques (Butterworth, Chebyshev I/II, elliptic filters, second-order cascade \nform). \nBand-pass signals. Band-pass sampling and reconstruction, IQ up and down conversion, \nsuperheterodyne receivers, software-defined radio front-ends, IQ representation of AM and FM \nsignals and their demodulation. \nDigital communication. Pulse-amplitude modulation. Matched-filter detector. Pulse shapes, \ninter-symbol interference, equalization. IQ representation of ASK, BSK, PSK, QAM and FSK \nsignals. [2 hours] \nRandom sequences and noise. Random variables, stationary and ergodic processes, \nautocorrelation, cross-correlation, deterministic cross-correlation sequences, filtered random \nsequences, white noise, periodic averaging. \nCorrelation coding. Entropy, delta coding, linear prediction, dependence versus correlation, \nrandom vectors, covariance, decorrelation, matrix diagonalization, eigen decomposition, \n\nKarhunen-Lo\u00e8ve transform, principal component analysis. Relation to orthogonal transform \ncoding using fixed basis vectors, such as DCT. \nLossy versus lossless compression. What information is discarded by human senses and can \nbe eliminated by encoders? Perceptual scales, audio masking, spatial resolution, colour \ncoordinates, some demonstration experiments. \nQuantization, image coding standards. Uniform and logarithmic quantization, A/\u00b5-law coding, \ndithering, JPEG. \nObjectives \napply basic properties of time-invariant linear systems; \nunderstand sampling, aliasing, convolution, filtering, the pitfalls of spectral estimation; \nexplain the above in time and frequency domain representations; \nuse filter-design software; \nvisualize and discuss digital filters in the z-domain; \nuse the FFT for convolution, deconvolution, filtering; \nimplement, apply and evaluate simple DSP applications; \nfamiliarity with a number of signal-processing concepts used in digital communication systems \nRecommended reading \nLyons, R.G. (2010). Understanding digital signal processing. Prentice Hall (3rd ed.). \nOppenheim, A.V. and Schafer, R.W. (2007). Discrete-time digital signal processing. Prentice \nHall (3rd ed.). \nStein, J. (2000). Digital signal processing \u2013 a computer science perspective. Wiley. \n \nClass size \nThis module can accommodate a maximum of 24 students (16 Part II students and 8 MPhil \nstudents) \n \nAssessment - Part II students \nThree homework programming assignments, each comprising 20% of the mark \nWritten test, comprising 40% of the total mark. \nAssessment - Part III and MPhil students \nThree homework programming exercises, each comprising 10% of the mark. \nWritten test, comprising 20% of the total mark. \nIndividual implementation project with 8-page written report, topic agreed with lecturer, \ncomprising 50% of the mark. \nFurther Information \nCurrent Cambridge undergraduate students who are continuing onto Part III or the MPhil in \nAdvanced Computer Science may only take this module if they did NOT take it as a Unit of \nAssessment in Part II. \n \nThis module is shared with Part II of the Computer Science Tripos. Assessment will be adjusted \nfor the two groups of students to be at an appropriate level for whichever course the student is \nenrolled on. Further information about assessment and practicals will follow at the first lecture.\n\nINTRODUCTION TO COMPUTATIONAL SEMANTICS \n \nAims \nThis is a lecture-style course that introduces students to various aspects of the semantics of \nNatural Languages (mainly English): \n \nLexical Semantics, with an emphasis on theory and phenomenology  \nCompositional Semantics \nDiscourse and pragmatics-related aspects of semantics \nObjectives \nGive an operational definition of what is meant by \u201cmeaning\u201d (for instance, above and beyond \nsyntax); \nName the types of phenomena in language that require semantic consideration, in terms of \nlexical, compositional and discourse/pragmatic aspects, in other words, argue why semantics is \nimportant; \nDemonstrate an understanding of the basics of various semantic representations, including \nlogic-based and graph-based semantic representations, their properties, how they are used and \nwhy they are important, and how they are different from syntactic representations; \nKnow how such semantic representations are derived during or after parsing, and how they can \nbe analysed and mapped to surface strings; \nUnderstand applications of semantic representations e.g. reasoning, validation, and methods \nhow these are approached. \nWhen designing NL tasks that clearly require semantic processing (e.g. knowledge-based QA), \nto be aware of and reuse semantic representations and algorithms when designing the task, \nrather than reinventing the wheel. \nPractical advantages of this course for NLP students \nKnowledge of underlying semantic effects helps improve NLP evaluation, for instance by \nproviding more meaningful error analysis. You will be able to link particular errors to design \ndecisions inside your system. \nYou will learn methods for better benchmarking of your system, whatever the task may be. \nSupervised ML systems (in particular black-box systems such as Deep Learning) are only as \nclever as the datasets they are based on. In this course, you will learn to design datasets so that \nthey are harder to trick without real understanding, or critique existing datasets. \nYou will be able to design tests for ML systems that better pinpoint which aspects of language \nan end-to-end system has \u201cunderstood\u201d. \nYou will learn to detect ambiguity and ill-formed semantics in human-human communication. \nThis can serve to write more clearly and logically. \nYou will learn about decomposing complex semantics-reliant tasks sensibly so that you can \nreuse the techniques underlying semantic analyzers in a modular way. In this way, rather than \nbeing forced to treat complex tasks in an end-to-end manner, you will be able to profit from \npartial explanations and a better error analysis already built into the system. \nSyllabus \nOverview of the course \nEvents and semantic role labelling \n\nReferentiality and coreference resolution \nTruth-conditional semantics \nGraph-based meaning representation \nCompositional semantics \nContext-free Graph Rewriting \nSurface realisation \nNegation and psychological approach to semantics \nDynamic semantics \nGricean pragmatics \nVector space models \nCross-modality \nAcquisition of semantics \nDiachronic change of semantics \nSummary of the course \n  \n \nAssessment \n5 take-home exercises worth 20% each: \n \nTake-home assignment 1 is given in Lecture 4 and due is Lecture 6 \n \nStudents are given 10 English sentences and asked to provide their semantic analysis \naccording to truth-conditions. Students will be expected to return 10 logical expressions as their \nanswers. No word limit. Assessment criteria: correctness of the 10 logical expressions; 2 points \nfor each logical expression. \n \nTake-home assignment 2 is given in Lecture 7 and due is Lecture 9  \n \nStudents are given 10 English sentences and asked to provide their syntactico-semantic \nderivations according to the compositionality principle. Students will be expected to return \nderivation graphs as their answers. No word limit. Assessment criteria: correctness of the 10 \nderivation graphs; 2 points for each derivation. \n \nTake-home assignment 3 is given in Lecture 11 and due is Lecture 13  \n \nAll students are assigned with a paper on modelling common ground in dialogue system. \nStudents will receive related but different papers. Each student will write a review of their \nassigned paper, including a comprehensive summary and their own thoughts. Word limit: 1000 \nwords. Assessment criteria: 15 points on whether a student understands the paper correctly; 5 \npoints on whether a student is able to think critically. \n \nTake-home assignment 4 is given in Lecture 13 and due is Lecture 15 \n \n\nAll students are assigned with a paper on language-vision interaction. Students will receive \nrelated but different papers. Each student will write a review of their assigned paper, including a \ncomprehensive summary and their own thoughts. Word limit: 1000 words. Assessment criteria: \n15 points on whether a student understands the paper correctly; 5 points on whether a student \nis able to think critically. \n \nTake-home assignment 5 is given in Lecture 14 and due is two weeks later. \n \nContent: All students are assigned with a paper on bootstrapping language acquisition. All \nstudents will receive the same paper. Each student will write a review of the paper, including a \ncomprehensive summary and their own thoughts. Word limit: 1000 words. Assessment criteria: \n15 points on whether a student understands the paper correctly; 5 points on whether a student \nis able to think critically.\n \n\nINTRODUCTION TO NATURAL LANGUAGE SYNTAX AND PARSING \n \nAims \nThis module aims to provide a brief introduction to linguistics for computer scientists and then \ngoes on to cover some of the core tasks in natural language processing (NLP), focussing on \nstatistical parsing of sentences to yield syntactic representations. We will look at how to \nevaluate parsers and see how well state-of-the-art tools perform given current techniques. \n \nSyllabus \nLinguistics for NLP focusing on morphology and syntax \nGrammars and representations \nStatistical and neural parsing \nTreebanks and evaluation \nObjectives \nOn completion of this module, students should: \n \nunderstand the basic properties of human languages and be familiar with descriptive and \ntheoretical frameworks for handling these properties; \nunderstand the design of tools for NLP tasks such as parsing and be able to apply them to text \nand evaluate their performance; \nPractical work \nWeek 1-7: There will be non-assessed practical exercises between sessions and during \nsessions. \nWeek 8: Download and apply two parsers to a designated text. Evaluate the performance of the \ntools quantitatively and qualitatively. \nAssessment \nThere will be one presentation worth 10% of the final mark; presentation topics will be allocated \nat the start of term. \nAn assessed practical report of not more 8 pages in ACL conference format. It will contribute \n90% of the final mark. \nRecommended reading \nJurafsky, D. and Martin, J. (2008). Speech and Language Processing. Prentice-Hall (2nd ed.). \n(See also 3rd ed. available online.)\n \n\nINTRODUCTION TO NETWORKING AND SYSTEMS MEASUREMENTS \n \nAims \nSystems research refers to the study of a broad range of behaviours arising from complex \nsystem design, including: resource sharing and scheduling; interactions between hardware and \nsoftware; network topology, protocol and device design and implementation; low-level operating \nsystems; Interconnect, storage and more. This module will: \n \nTeach performance characteristics and performance measurement methodology and practice \nthrough profiling experiments; \nExpose students to real-world systems artefacts evident through different measurement tools; \nDevelop scientific writing skills through a series of laboratory reports; \nProvide research skills for characterization and modelling of systems and networks using \nmeasurements. \nPrerequisites \nIt is strongly recommended that students have previously (and successfully) completed an \nundergraduate networking course -- or have equivalent experience through project or \nopen-source work. \n \nSyllabus \nIntroduction to performance measurements, performance characteristics [1 lecture] \nPerformance measurements tools and techniques [2 lectures + 2 lab sessions] \nReproducible experiments [1 lecture + 1 lab session] \nCommon pitfalls in measurements [1 lecture] \nDevice and system characterisation [1 lecture + 2 lab sessions] \nObjectives \nOn completion of this module, students should: \n \nDescribe the objectives of measurements, and what they can achieve; \nCharacterise and model a system using measurements; \nPerform reproducible measurements experiments; \nEvaluate the performance of a system using measurements; \nOperate measurements tools and be aware of their limitations; \nDetect anomalies in the network and avoid common measurements pitfalls; \nWrite system-style performance evaluations. \nPractical work \nFive 2-hour in-classroom labs will ask students to develop and use skills in performance \nmeasurements as applied to real-world systems and networking artefacts. Results from these \nlabs (and follow-up work by students outside of the classroom) will by the primary input to lab \nreports. \n \nThe first three labs will provide an introduction and hands on experience with measurement \ntools and measurements methodologies, while the last two labs will focus on practical \nmeasurements and evaluation of specific platforms. Students may find it useful to work in pairs \n\nwithin the lab, but must prepare lab reports independently. The module lecturer will give a short \nintroductory at the start of each lab, and instructors will be on-hand throughout labs to provide \nassistance. \n \nLab participation is not directly included in the final mark, but lab work is a key input to lab \nreports that are assessed. Guided lab experiments resulting in practical write ups. \n \nAssessment \nEach student will write two lab reports. The first lab report will summarise the experiments done \nin the first three labs (20%). The second will be a lab report (5000 words) summarising the \nevaluation of a device or a system (80%). \n \nRecommended reading \nThe following list provides some background to the course materials, but is not mandatory. A \nreading list, including research papers, will be provided in the course materials. \n \nGeorge Varghese. Network algorithmics. Chapman and Hall/CRC, 2010. \nMark Crovella and Balachander Krishnamurthy. Internet measurement: infrastructure, traffic and \napplications. John Wiley and Sons, Inc., 2006. \nBrendan Gregg. Systems Performance: Enterprise and the Cloud, Prentice Hall Press, Upper \nSaddle River, NJ, USA, October 2013. \nRaj Jain, The Art of Computer Systems Performance Analysis: Techniques for Experimental \nDesign, Measurement, Simulation, and Modeling, Wiley - Interscience, New York, NY, USA, \nApril 1991.\n \n\nLARGE-SCALE DATA PROCESSING AND OPTIMISATION \n \nAims \nThis module provides an introduction to large-scale data processing, optimisation, and the \nimpact on computer system's architecture. Large-scale distributed applications with high volume \ndata processing such as training of machine learning will grow ever more in importance. \nSupporting the design and implementation of robust, secure, and heterogeneous large-scale \ndistributed systems is essential. To deal with distributed systems with a large and complex \nparameter space, tuning and optimising computer systems is becoming an important and \ncomplex task, which also deals with the characteristics of input data and algorithms used in the \napplications. Algorithm designers are often unaware of the constraints imposed by systems and \nthe best way to consider these when designing algorithms with massive volume of data. On the \nother hand, computer systems often miss advances in algorithm design that can be used to cut \ndown processing time and scale up systems in terms of the size of the problem they can \naddress. Integrating machine learning approaches (e.g. Bayesian Optimisation, Reinforcement \nLearning) for system optimisation will be explored in this course. \n \nSyllabus \nThis course provides perspectives on large-scale data processing, including data-flow \nprogramming, graph data processing, probabilistic programming and computer system \noptimisation, especially using machine learning approaches, thus providing a solid basis to work \non the next generation of distributed systems. \n \nThe module consists of 8 sessions, with 5 sessions on specific aspects of large-scale data \nprocessing research. Each session discusses 3-4 papers, led by the assigned students. One \nsession is a hands-on tutorial on on dataflow programming fundamentals. The first session \nadvises on how to read/review a paper together with a brief introduction on different \nperspectives in large-scale data \nprocessing and optimisation. The last session is dedicated to the student presentation of \nopensource project studies. \n \nIntroduction to large-scale data processing and optimisation \nData flow programming: Map/Reduce to TensorFlow \nLarge-scale graph data processing: storage, processing model and parallel processing \nDataflow programming hands-on tutorial \nProbabilistic Programming \nOptimisations in ML Compiler \nOptimisations of Computer systems using ML \nPresentation of Open Source Project Study \nObjectives \nOn completion of this module, students should: \n \nUnderstand key concepts of scalable data processing approaches in future computer systems. \n\nObtain a clear understanding of building distributed systems using data centric programming \nand large-scale data processing. \nUnderstand a large and complex parameter space in computer system's optimisation and \napplicability of Machine Learning approach. \nCoursework \nReading Club: \nThe preparation for the reading club will involve 1-3 papers every week. At each session, \naround 3-4 papers are selected under the given topic, and the students present their review \nwork. \nHands-on tutorial session of data flow programming including writing an application of \nprocessing streaming in Twitter data and/or Deep Neural Networks using Google TensorFlow \nusing cluster computing. \nReports \nThe following three reports are required, which could be extended from the assignment of the \nreading club, within the scope of data centric systems. \n \nReview report on a full length paper (max 1800 words) \nDescribe the contribution of the paper in depth with criticisms \nCrystallise the significant novelty in contrast to other related work \nSuggestions for future work \nSurvey report on sub-topic in large-scale data processing and optimisation (max 2000 words) \nPick up to 5 papers as core papers in the survey scope \nRead the above and expand reading through related work \nComprehend the view and finish an own survey paper \nProject study and exploration of a prototype (max 2500 words) \nWhat is the significance of the project in the research domain? \nCompare with similar and succeeding projects \nDemonstrate the project by exploring its prototype \nReports 1 should be handed in by the end of 5th week; Report 2 in the 8th week and Report 3 \non the first day of Lent Term. \n \nAssessment \nThe final grade for the course will be provided as a percentage, and the assessment will consist \nof two parts: \n \n25%: for reading club (participation, presentation) \n75%: for the three reports: \n15%: Intensive review report \n25%: Survey report \n35%: Project study \nRecommended reading \nM. Abadi et al. TensorFlow: A System for Large-Scale Machine Learning, OSDI, 2016. \nD. Aken et al.: Automatic Database Management System Tuning Through Large-scale Machine \nLearning, SIGMOD, 2017. \n\nJ. Ansel et al. Opentuner: an extensible framework for program autotuning. PACT, 2014. \nV. Dalibard, M. Schaarschmidt, E. Yoneki. BOAT: Building Auto-Tuners with Structured Bayesian \nOptimization, WWW, 2017. \nJ. Dean et al. Large scale distributed deep networks. NIPS, 2012. \nG. Malewicz, M. Austern, A. Bik, J. Dehnert, I. Horn, N. Leiser, G. Czajkowski. Pregel: A System \nfor Large-Scale Graph Processing, SIGMOD, 2010. \nA. Mirhoseini et al. Device Placement Optimization with Reinforcement Learning, ICML, 2017. \nD. Murray, F. McSherry, R. Isaacs, M. Isard, P. Barham, M. Abadi. Naiad: A Timely Dataflow \nSystem, SOSP, 2013. \nM. Schaarschmidt, S. Mika, K. Fricke and E. Yoneki: RLgraph: Modular Computation Graphs for \nDeep Reinforcement Learning, SysML, 2019. \nZ. Jia, O. Padon, J. Thomas, T. Warszawski, M. Zaharia,  A. Aiken: TASO: Optimizing Deep \nLearning Computation with Automated Generation of Graph Substitutions: SOSP, 2019. \nH. Mao et al.: Park: An Open Platform for Learning-Augmented Computer Systems, \nOpenReview, 2019. \nJ. Shao, et al.: Tensor Program Optimization with Probabilistic Programs, NeurIPS, 2022.  \nA complete list can be found on the course material web page. See also 2023-2024 course \nmaterial on the previous course Large-Scale Data Processing and Optimisation.\n \n\nMACHINE LEARNING AND THE PHYSICAL WORLD \nAims \nThe module \u201cMachine Learning and the Physical World\u201d is focused on machine learning \nsystems that interact directly with the real world. Building artificial systems that interact with the \nphysical world have significantly different challenges compared to the purely digital domain. In \nthe real world data is scares, often uncertain and decisions can have costly and irreversible \nconsequences. However, we also have the benefit of centuries of scientific knowledge that we \ncan draw from. This module will provide the methodological background to machine learning \napplied in this scenario. We will study how we can build models with a principled treatment of \nuncertainty, allowing us to leverage prior knowledge and provide decisions that can be \ninterrogated. \n \nThere are three principle points about machine learning in the real world that will concern us. \n \nWe often have a mechanistic understanding of the real world which we should be able to \nbootstrap to make decisions. For example, equations from physics or an understanding of \neconomics. \nReal world decisions have consequences which may have costs, and often these cost functions \nneed to be assimilated into our machine learning system. \nThe real world is surprising, it does things that you do not expect and accounting for these \nchallenges requires us to build more robust and or interpretable systems. \nDecision making in the real world hasn\u2019t begun only with the advent of machine learning \ntechnologies. There are other domains which take these areas seriously, physics, environmental \nscientists, econometricians, statisticians, operational researchers. This course identifies how \nmachine learning can contribute and become a tool within these fields. It will equip you with an \nunderstanding of methodologies based on uncertainty and decision making functions for \ndelivering on these challenges. \n \nObjectives \nYou will gain detailed knowledge of \n \nsurrogate models and uncertainty \nsurrogate-based optimization \nsensitivity analysis \nexperimental design \nYou will gain knowledge of \n \ncounterfactual analysis \nsurrogate-based quadrature \nSchedule \nWeek 1: Introduction to the unit and foundation of probabilistic modelling \n \nWeek 2: Gaussian processes and probablistic inference \n \n\nWeek 3: Simulation and Sequential decision making under uncertainty \n \nWeek 4: Emulation and Experimental Design \n \nWeek 5: Sensitivity Analysis and Multifidelity Modelling \n \nWeek 6-8: Case studies of applications and Projects \n \nPractical work \nDuring the first five weeks of the unit we will provide a weekly worksheet that will focus on \nimplementation and practical exploration of the material covered in the lectures. The worksheets \nwill allow you to build up a these methods without relying on extensive external libraries. You are \nfree to use any programming language of choice however we highly recommended the use of \n=Python=. \n \nAssessment \nTwo individual tasks (15% each) \nGroup Project (70%). Each group will work on an application of uncertainty that covers the \nmaterial of the first 5 weeks of lectures in the unit. Each group will submit a report which will \nform the basis of the assessment. In addition to the report each group will also attend a short \noral examination based on the material covered both in the report and the taught material. \nRecommended Reading \nRasmussen, C. E. and Williams, C. K. I. (2006). Gaussian Processes for Machine Learning. MIT \nPress \n \nBishop, C. (2006). Pattern recognition and machine learning. Springer. \nhttps://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-an\nd-Machine-Learning-2006.pdf \n \nLaplace, P. S. (1902). A Philosophical Essay on Probabilities. John Wiley & Sons. \nhttps://archive.org/details/philosophicaless00lapliala\n \n\nMACHINE VISUAL PERCEPTION \nAims \nThis course aims at introducing the theoretical foundations and practical techniques for machine \nperception, the capability of computers to interpret data resulting from sensor measurements. \nThe course will teach the fundamentals and modern techniques of machine perception, i.e. \nreconstructing the real world starting from sensor measurements with a focus on machine \nperception for visual data. The topics covered will be image/geometry representations for \nmachine perception, semantic segmentation, object detection and recognition, geometry \ncapture, appearance modeling and acquisition, motion detection and estimation, \nhuman-in-the-loop machine perception, select topics in applied machine perception. \n \nMachine perception/computer vision is a rapidly expanding area of research with real-world \napplications. An understanding of machine perception is also important for robotics, interactive \ngraphics (especially AR/VR), applied machine learning, and several other fields and industries. \nThis course will provide a fundamental understanding of and practical experience with the \nrelevant techniques. \n \nLearning outcomes \nStudents will understand the theoretical underpinnings of the modern machine perception \ntechniques for reconstructing models of reality starting from an incomplete and imperfect view of \nreality. \nStudents will be able to apply machine perception theory to solve practical problems, e.g. \nclassification of images, geometry capture. \nStudents will gain an understanding of which machine perception techniques are appropriate for \ndifferent tasks and scenarios. \nStudents will have hands-on experience with some of these techniques via developing a \nfunctional machine perception system in their projects. \nStudents will have practical experience with the current prominent machine perception \nframeworks. \nSyllabus \nThe fundamentals of machine learning for machine perception \nDeep neural networks and frameworks for machine perception \nSemantic segmentation of objects and humans \nObject detection and recognition \nMotion estimation, tracking and recognition \n3D geometry capture \nAppearance modeling and acquisition \nSelect topics in applied machine perception \nAssessment \nA practical exercise, worth 20% of the mark. This will cover the basics and theory of machine \nperception and some of the practical techniques the students will likely use in their projects. This \nis individual work. No GPU hours will be needed for the practical work. \nA machine perception project worth 80% of the marks: \n\nCourse projects will be selected by the students following the possible project themes proposed \nby the lecturer, and will be checked by the lecturer for appropriateness.  \nThe students will form groups of 2-3 to design, implement, report, and present a project to tackle \na given task in machine perception. \nThe final mark will be composed of an implementation/report mark (60%) and a presentation \nmark (20%). Each team member will be evaluated based on her/his contribution. \nEach project will have extensions to be completed only by the ACS students. Each student will \nwrite a different part of the report, whose author will be clearly marked. Each student will further \nsummarise her/his contributions to the project in the same report. \nRecommended Reading List \nComputer Vision: Algorithms and Applications, Richard Szeliski, Springer, 2010. \nDeep Learning, Ian Goodfellow, Yoshua Bengio, and Aaron Courville, MIT Press, 2016. \nMachine Learning and Visual Perception, Baochang Zhang, De Gruyter, 2020. \nFurther Information \nCurrent Cambridge undergraduate students who are continuing onto Part III or the MPhil in \nAdvanced Computer Science may only take this module if they did NOT take it as a Unit of \nAssessment in Part II. \n \nThis module is shared with Part II of the Computer Science Tripos. Assessment will be adjusted \nfor the two groups of students to be at an appropriate level for whichever course the student is \nenrolled on. Further information about assessment and practicals will follow at the first lecture.\n\nMOBILE, WEARABLE SYSTEMS AND MACHINE LEARNING \n \nAims \nThis module aims to introduce the latest research advancements in mobile systems and mobile \ndata machine learning, spanning a range of domains including systems, data gathering, \nanalytics and machine learning, on device machine learning and applications such as health, \ntransportation, behavior monitoring, cyber-physical systems, autonomous vehicles, drones. The \ncourse will cover current and seminal research papers in the area of research. \n \nSyllabus \nThe course will consist of one introductory lecture, seven two-hour, and one three-hour, \nsessions covering a variety of topics roughly including the following material (some variation in \nthe topics might happen from year to year): \n \nSystem, Energy and Security \nBackscatter Communication, Battery Free and Energy Harvesting Devices \nNew Sensing Modalities \nMachine Learning on Wearable Data \nOn Device Machine Learning \nMobile and Wearable Health \nMobile and Wearable Systems of Sustainability \nEach week, three class participants will be assigned to introduce assigned three papers via \n20-minute presentations, conference-style and highlighting critically its features. Each \npresentation will be followed by 10 minutes of questions. This will be followed by 10 minutes of \ngeneral discussion. Slides will be used for presentation. \n \nStudents will give one or more presentations each term. Each student will submit a paper review \neach week for one of the three papers presented except for the week they will be presenting \nslides. Each review will follow a template and be up to 1,000 words. Each review will receive a \nmaximum of 10 points. As a result, each student will produce 6-7 reviews and at least one \npresentation, probably two. All participants are expected to attend and participate in every class; \nthe instructor must be notified of any absences in advance. \n \nObjectives \nOn completion of this module students should have an understanding of the recent key research \nin mobile and sensor systems and mobile analytics as well as an improved critical thinking over \nresearch papers. \n \nAssessment \nAggregate mark for 7 assignments from 6-7 reports and 1-2 presentations. Each report or \npresentation will contribute one seventh of the course mark. \nA tick for presence and participation to each class will also be awarded. \nRecommended reading \n\nReadings from the most recent conferences such as AAAI, ACM KDD, ACM MobiCom, ACM \nMobiSys, ACM SenSys, ACM UbiComp, ICLR, ICML Neurips, and WWW pertinent to mobile \nsystems and data.\n \n\nNETWORK ARCHITECTURES \nAims \nThis module aims to provide the world with more network architects. The 2011-2012 version \nwas oriented around the evolution of IP to support new services like multicast, mobility, \nmultihoming, pub/sub and, in general, data oriented networking. The course is a paper reading \nwhich puts the onus on the student to do the work. \n \nSyllabus \nIPng [2 lectures, Jon Crowcroft] \nNew Architectures [2 lectures, Jon Crowcroft] \nMulticast [2 lectures, Jon Crowcroft] \nContent Distribution and Content Centric Networks [2 lectures, Jon Crowcroft] \nResource Pooling [2 lectures, Jon Crowcroft] \nGreen Networking [2 lectures, Jon Crowcroft] \nAlternative Router Implementions [2 lectures, Jon Crowcroft] \nData Center Networks [2 Lectures, Jon Crowcroft] \nObjectives \nOn completion of this module, students should be able to: \n \ncontribute to new network system designs; \nengineer evolutionary changes in network systems; \nidentify and repair architectural design flaws in networked systems; \nsee that there are no perfect solutions (aside from academic ones) for routing, addressing, \nnaming; \nunderstand tradeoffs in modularisation and other pressures on clean software systems \nimplementation, and see how the world is changing the proper choices in protocol layering, or \nnon layered or cross-layered. \nCoursework \nAssessment is through three graded essays (each chosen individually from a number of \nsuggested or student-chosen topics), as follows: \n \nAnalysis of two different architectures for a particular scenario in terms of cost/performance \ntradeoffs for some functionality and design dimension, for example: \nATM \u2013 e.g. for hardware versus software tradeoff \nIP \u2013 e.g. for mobility, multi-homing, multicast, multipath \n3GPP \u2013 e.g. for plain complexity versus complicatedness \nA discursive essay on a specific communications systems component, in a particular context, \nsuch as ad hoc routing, or wireless sensor networks. \nA bespoke network design for a narrow, well specified specialised target scenario, for example: \nA customer baggage tracking network for an airport. \nin-flight entertainment system. \nin-car network for monitoring and control. \ninter-car sensor/control network for automatic highways. \nPractical work \n\nThis course does not feature any implementation work due to time constraints. \n \nAssessment \nThree 1,200-word essays (worth 25% each), and \nan annotated bibliography (25%). \nRecommended reading \nPre-course reading: \n \nKeshav, S. (1997). An engineering approach to computer networking. Addison-Wesley (1st ed.). \nISBN 0201634422 \nPeterson, L.L. and Davie, B.S. (2007). Computer networks: a systems approach. Morgan \nKaufmann (4th ed.). \n \nDesign patterns: \n \nDay, John (2007). Patterns in network architecture: a return to fundamentals. Prentice Hall. \n \nExample systems: \n \nKrishnamurthy, B. and Rexford, J. (2001). Web protocols and practice: HTTP/1.1, Networking \nprotocols, caching, and traffic measurement. Addison-Wesley. \n \nEconomics and networks: \n \nFrank, Robert H. (2008). The economic naturalist: why economics explains almost everything. \n \nPapers: \n \nCertainly, a collection of papers (see ACM CCR which publishes notable network researchers' \nfavourite ten papers every 6 months or so).\n \n\nOVERVIEW OF NATURAL LANGUAGE PROCESSING \n \nAims \nThis course introduces the fundamental techniques of natural language processing. It aims to \nexplain the potential and the main limitations of these techniques. Some current research issues \nare introduced and some current and potential applications discussed and evaluated. Students \nwill also be introduced to practical experimentation in natural language processing. \n \nLectures \nOverview. Brief history of NLP research, some current applications, components of NLP \nsystems. \nMorphology and Finite State Techniques. Morphology in different languages, importance of \nmorphological analysis in NLP, finite-state techniques in NLP. \nPart-of-Speech Tagging and Log-Linear Models. Lexical categories, word tagging, corpora and \nannotations, empirical evaluation. \nPhrase Structure and Structure Prediction. Phrase structures, structured prediction, context-free \ngrammars, weights and probabilities. Some limitations of context-free grammars. \nDependency Parsing. Dependency structure, grammar-free parsing, incremental processing.  \nGradient Descent and Neural Nets. Parameter optimisation by gradient descent. Non-linear \nfunctions with neural network layers. Log-linear model as softmax layer. Current findings of \nNeural NLP. \nWord representations. Representing words with vectors, count-based and prediction-based \napproaches, similarity metrics. \nRecurrent Neural Networks. Modelling sequences, parameter sharing in recurrent neural \nnetworks, neural language models, word prediction. \nCompositional Semantics. Logical representations, compositional semantics, lambda calculus, \ninference and robust entailment. \nLexical Semantics. Semantic relations, WordNet, word senses. \nDiscourse. Discourse relations, anaphora resolution, summarization. \nNatural Language Generation. Challenges of natural language generation (NLG), tasks in NLG, \nsurface realisation. \nPractical and assignments. Students will build a natural language processing system which will \nbe trained and evaluated on supplied data. The system will be built from existing components, \nbut students will be expected to compare approaches and some programming will be required \nfor this. Several assignments will be set during the practicals for assessment. \nObjectives \nBy the end of the course students should: \n \nbe able to discuss the current and likely future performance of several NLP applications; \nbe able to describe briefly a fundamental technique for processing language for several \nsubtasks, such as morphological processing, parsing, word sense disambiguation etc.; \nunderstand how these techniques draw on and relate to other areas of computer science. \nRecommended reading \n\nJurafsky, D. & Martin, J. (2023). Speech and language processing. Prentice Hall (3rd ed. draft, \nonline). \n \nAssessment - Part II Students \nAssignment 1 - 10% of marks \nAssignment 2 - 60% of marks \nAssignment 3 - 30% of marks \nCoursework \nSubmit work for 3 assignments as part of practical sessions on information extraction. \n \nThese include an annotation exercise, a feature-based classifier along with code repository and \ndocumentation, and a 4,000-word report on results and analysis from an extended information \nextraction experiment. \n \nPractical work \nStudents will build a natural language processing system which will be trained and evaluated on \nsupplied data. The system will be built from existing components, but students will be expected \nto compare approaches and some programming will be required for this. \n \nAssessment - Part III and MPhil Students \nAssessment will be based on the practicals: \n \nFirst practical exercise (10%, ticked) \nSecond practical exercise (25%, code repository or notebook with documentation) \nFinal report (65%, 4,000 words, excluding references) \nFurther Information \nAlthough the lectures don't assume any exposure to linguistics, the course will be easier to \nfollow if students have some understanding of basic linguistic concepts. The following may be \nuseful for this: The Internet Grammar of English \n \nCurrent Cambridge undergraduate students who are continuing onto Part III or the MPhil in \nAdvanced Computer Science may only take this module if they did NOT take it as a Unit of \nAssessment in Part II. \n \nStudents from other departments may attend the lectures for this module if space allows. \nHowever students wanting to take it for credit will need to make arrangements for assessment \nwithin their own department. \n \nThis module is shared with Part II of the Computer Science Tripos. Assessment will be adjusted \nfor the two groups of students to be at an appropriate level for whichever course the student is \nenrolled on. Further information about assessment and practicals will follow at the first lecture.\n\nPRACTICAL RESEARCH IN HUMAN-CENTRED AI \n \nAims \nThis is an advanced course in human-computer interaction, with a specialist focus on intelligent \nuser interfaces and interaction with machine-learning and artificial intelligence technologies. The \nformat will be largely Practical, with students carrying out a mini-project involving empirical \nresearch investigation. These studies will investigate human interaction with some kind of \nmodel-based system for planning, decision-making, automation etc. Possible study formats \nmight include: System evaluation, Field observation, Hypothesis testing experiment, Design \nintervention or Corpus analysis, following set examples from recent research publications. \nProject work will be formally evaluated through a report and presentation. \n \nLectures \n(note that Lectures 2-7 also include one hour class discussion of practical work) \n        \u2022 Current research themes in intelligent user interfaces \n        \u2022 Program synthesis \n        \u2022 Mixed initiative interaction \n        \u2022 Interpretability / explainable AI \n        \u2022 Labelling as a fundamental problem \n        \u2022 Machine learning risks and bias \n        \u2022 Visualisation and visual analytics \n        \u2022 Student research presentations \n \nObjectives \nBy the end of the course students should: \n        \u2022 be familiar with current state of the art in intelligent interactive systems \n        \u2022 understand the human factors that are most critical in the design of such systems \n        \u2022 be able to evaluate evidence for and against the utility of novel systems \n        \u2022 have experience of conducting user studies meeting the quality criteria of this field \n        \u2022 be able to write up and present user research in a professional manner \n \nClass Size \nThis module can accommodate upto 20 Part II, Part III and MPhil students. \n \nRecommended reading \nBrad A. Myers and Richard McDaniel (2000). Demonstrational Interfaces: Sometimes You Need \na Little Intelligence, Sometimes You Need a Lot. \n \nAlan Blackwell (2024). Moral Codes: Designing alternatives to AI \n \nAssessment - Part II Students \nThe format will be largely practical, with students carrying out an individual mini-project involving \nempirical research investigation. \n \n\nAssignment 1: six incremental submissions which together contribute 20% to the final module \nmark. \n \nAssignment 2: Final report - 80% of the final module mark \n  \n \nAssessment - MPhil / Part III Students \nThere will be a minor assessment component (20%) in which students compile a reflective diary \nthroughout the term, reporting on the weekly sessions. Diary entries should include citations to \nany key references, notes of possible further reading, summary of key points, questions relevant \nto the personal project, and points of interest noted in relation to the work of other students. \n \nThe major assessment component (80%) will involve a report on research findings, in the style \nof a submission to the ACM CHI or IUI conferences. This work will be submitted incrementally \nthrough the term, in order that feedback can be provided before final assessment of the full \nreport. Phased submissions will cover the following aspects of the empirical study: \n \nResearch question \nMethod \nLiterature Review \nIntroduction \nResults \nDiscussion / Conclusion \nFeedback on each phased submission will include an indicative mark for guidance, but with the \nunderstanding that the final grade will be based on the final delivered report, and that this may \ngo up (or possibly down), depending on how well the student responds to earlier feedback. \n \nReflective diary entries will also be assessed, but graded at a relatively coarse granularity \ncorresponding to the ACS grading bands, and with minimal written feedback. Informal and \ngeneric feedback will be offered verbally in class, and also potentially supplemented with peer \nassessment. \n \nFurther Information \nCurrent Cambridge undergraduate students who are continuing onto Part III or the MPhil in \nAdvanced Computer Science may only take this module if they did NOT take it as a Unit of \nAssessment in Part II. \n \nThis module is shared with Part II of the Computer Science Tripos. Assessment will be adjusted \nfor the two groups of students to be at an appropriate level for whichever course the student is \nenrolled on. Further information about assessment and practicals will follow at the first lecture.\n\nPRINCIPLES OF MACHINE LEARNING SYSTEMS \n \nPrerequisites: It is recommended that students have successfully completed introductory \ncourses, at an undergraduate level, in: 1) operating systems, 2) computer architecture, and 3) \nmachine learning. In addition, this course will heavily focus on deep neural network \nmethodologies and assume familiarity with common neural architectures and related algorithms. \nIf these topics were not covered within the machine learning course taken, students should \nsupplement by reviewing material in a book like: \"Dive into Deep Learning\". Finally, students are \nassumed to be comfortable with programming in Python. \nMoodle, timetable \n \nAims \nThis course will examine the emerging principles and methodologies that underpin scalable and \nefficient machine learning systems. Primarily, the course will focus on an exciting cross-section \nof algorithms and system techniques that are used to support the training and inference of \nmachine learning models under a spectrum of computing systems that range from constrained \nembedded systems up to large-scale distributed systems. It will also touch upon the new \nengineering practices that are developing in support of such systems at scale. When needed to \nappreciate issues of scalability and efficiency, the course will drill down to certain aspects of \ncomputer architecture, systems software and distributed systems and explore how these \ninteract with the usage and deployment of state-of-the-art machine learning. \n \nSyllabus \nTopics covered may include the following, with confirmation a month before the course begins: \n \nSystem Performance Trade-offs \nDistributed Learning Algorithms  \nModel Compression  \nDeep Learning Compilers  \nFrameworks and Run-times  \nScalable Inference Serving  \nDevelopment Practices  \nAutomated Machine Learning  \nFederated Learning  \nPrimarily, topics are covered with conventional lectures. However, where appropriate, material \nwill be delivered through hands-on lab tutorials. Lab tutorials will make use of hardware \nincluding ARM microcontrollers and multi-GPU machines to explore forms of efficient machine \nlearning (any necessary equipment will be provided to students) \n \nAssessment \nEach student will be assessed on 3 labs which will be worth 30% of their grade. They will also \nundertake a written project report which will be worth 70% of the grade. This report will detail an \ninvestigation into a particular aspect of machine learning systems, this report will be made \navailable publicly.\n \n\nPROOF ASSISTANTS \nAims \nThis module introduces students to interactive theorem proving using Isabelle and Coq. It \nincludes techniques for specifying formal models of software and hardware systems and for \nderiving properties of these models. \n \n  \n \nSyllabus \nIntroduction to proof assistants and logic. \nReasoning in predicate logic and typed set theory. \nReasoning in dependent type theory. \nInductive definitions and recursive functions: modelling them in logic, reasoning about them. \nModelling operational semantics definitions and proving properties. \n  \n \nObjectives \nOn completion of this module, students should: \n \npossess good skills in the use of Isabelle and Coq; \nbe able to specify inductive definitions and perform proofs by induction; \nbe able to formalise and reason about a variety of specifications in a proof assistant. \nCoursework \nPractical sessions will allow students to develop skills. Some of the exercises will serve as the \nbasis for assessment. \n \nAssessment \nAssessment will be based on two marked assignments (due in weeks 5 and 8) with students \nperforming small formalisation and verification projects in a proof assistant together with a \nwrite-up explaining their work (word limit 2,500 per project for the write-up). Each of the \nassignments will be worth 100 marks, distributed as follows: \n \n50 marks for completing basic formalisation and verification tasks assessing grasp of the \nmaterial taught in the lecture. Students will submit their work as theory files for either the \nIsabelle or Coq proof assistant, and they will be assessed for correctness and completeness of \nthe specifications and proofs. \n20 marks for completing designated more challenging tasks, requiring the use of advanced \ntechniques or creative proof strategies. \n30 marks for a clear write-up explaining the design decisions made during the formalisation and \nthe strategy used for the proofs, where 10 of these marks will be reserved for write-ups of \nexceptional quality, e.g. demonstrating particular insight. \nThe main tasks in the assignments will be designed to assess the student's proficiency with the \nbasic material and techniques taught in the lectures and the practical sessions, while the more \nchallenging tasks will give exceptional students the opportunity to earn distinction marks \n\n \nRecommended reading \nNipkow, T., Klein, G. (2014). Concrete Semantics with Isabelle/HOL. (The first part of this book, \n\u201cProgramming and Proving in Isabelle/HOL\u201d, comes with the Isabelle distribution.)  \n \nNipkow, T., Paulson, L.C. and Wenzel, M. (2002). A proof assistant for higher-order logic. \nSpringer LNCS 2283.  \n \nThe Software Foundations series of books, in particular the first two: \n \nPierce, B., Azevedo de Amorim, A., Casinghino, C., Gaboardi, M., Greenberg, M., Hri\u0163cu, C., \nSj\u00f6berg, V., Yorgey, B. (2023). Logical Foundations  \nPierce, B., Azevedo de Amorim, A., Casinghino, C., Gaboardi, M., Greenberg, M., Hri\u0163cu, C., \nSj\u00f6berg, V., Tolmach, A., Yorgey, B. (2023). Programming Language Foundations  \nChlipala, A. (2022). Certified programming with dependent types: a pragmatic introduction to the \nCoq proof assistant. MIT Press.  \n \nSergey, I. (2014). Programs and Proofs: Mechanizing Mathematics with Dependent Types.  \n \nAll of these are freely available online.\n \n\nQUANTUM ALGORITHMS AND COMPLEXITY \n \nAims \nThis module is a research-focused introduction to the theory of quantum computing. The aim is \nto prepare the students to conduct research in quantum algorithms and quantum complexity \ntheory. \n \nSyllabus \nTopics will vary from year to year, based on developments in cutting edge research. \nRepresentative topics include: \n \nQuantum algorithms: \n \nQuantum learning theory \nQuantum property testing \nShadow tomography \nQuantum walks \nQuantum state/unitary synthesis. \nStructure vs randomness in quantum algorithms \nQuantum complexity theory: \n \nThe quantum PCP conjecture \nEntanglement and MIP \nState complexity, AdS/CFT, and quantum gravity \nQuantum locally testable codes \nPseudorandom states and unitaries \nQuantum zero-knowledge proofs \nObjectives \nOn completion of this module, students should: \n \nbe familiar with contemporary quantum algorithms \ndevelop an understanding of the power and limitations of quantum computation \nconduct research in quantum algorithms and complexity theory \nAssessment \nMini research project (70%) \nPresentation (20%) \nAttendance and participation (10%) \nTimelines for the assignment submissions:  \n \nsubmit questions/observations on a weekly basis (i.e., Weeks 2-8) \ndeliver talks in Weeks 5-8 \nsubmit their mini-project at the end of term \nRecommended reading material and resources \n\u201cQuantum Computing Since Democritus\u201d by Scott Aaronson \n\n \n\u201cIntroduction to Quantum Information Science\u201d by Scott Aaronson \n \n\u201cQuantum Computing: Lecture Notes\u201d by Ronald de Wolf \n \n\u201cQuantum Computation and Quantum Information\u201d by Nielsen and Chuang\n \n\nUNDERSTANDING QUANTUM ARCHITECTURE \nPrerequisites: Requires introductory linear algebra - concepts such as eigenvalues, Hermitian \nmatrices, unitary matrices. Taking the Part II Quantum Computing course (or similar) is helpful \nbut not required. Familiarity with computer architecture and compilers is helpful. \nMoodle, timetable \n \nAims \nThis course covers the architecture of a practical-scale quantum computer. We will examine the \nresource requirements of practical quantum applications, understand the different layers of the \nquantum stack, the techniques used in these layers and examine how these layers come \ntogether to enable practical quantum advantage over classical computing. \n \nSyllabus \nThe course has two parts: a series of lectures to cover important aspects of the quantum stack \nand a set of student presentations. The following are a list of representative topics: \n \n- Basics of quantum computing \n- The fault-tolerant quantum stack \n- Compilation \n- Instruction sets \n- Implementations of quantum error correction \n- Implementation of magic state distillation \n- Resource estimation \n \nStudent presentations will be based on a reading list of important papers in quantum \narchitecture.  \n  \n \nObjectives \nAt the end of the course, students will have a broad understanding of the quantum computing \nstack. They will understand how major qubit technologies work, design challenges in real \nquantum hardware, how quantum applications are mapped to a system and the importance of \nquantum error correction for scalability.  \n \nAssessment \nSeminar presentation: 20% \nRead one paper from a provided reading list \nPrepare a 20 minute presentation on it + 10 minutes of answering questions. \nThe student presentation should be similar to a conference presentation - convey the problem, \nwhat are prior solutions, what did the paper do, what are the results, what are future directions \nFor the 20% of the score, the score will be split as 15% and 5%. 15% based on how well the \nstudent understands and explains the paper to the rest of the audience. 5% based on the Q&A \nsession. \n \n\n  \n \nCourse project: 80% (split across a proposal, mid-term report, final report) \nA. Research proposal - at least 500 words (10% of total) \nB. Mid-term report - at least 1000 words (including aspects like the research problem, literature \nreview, what questions will be evaluated, any early ideas or methods (20% of total)  \nC. Final report - up to 4 pages double column in a conference paper style with 3000-4000 \nwords. In addition to the mid term report, should include aspects like results and future \ndirections. (50% of the total) \nD. Report on contributions (in case of group work by 2 students) - 1 paragraph on individual \ncontribution of the student. 1 paragraph on teammate's contribution. \nStudents may work alone in the project, in which case they need only components A-C. \nStudents may work in pairs of two, but they should in addition individually submit D. The \ninstructor may conduct a short viva in case contributions from both team members are not clear \nor imbalanced. \nRecommended reading \nNielsen M.A., Chuang I.L. (2010). Quantum Computation and Quantum Information. Cambridge \nUniversity Press. \n \nMermin N.D. (2007). Quantum Computer Science: An Introduction. Cambridge University Press. \n \nAssessing requirements to scale to practical quantum advantage (2022) Beverland et al.\n\n",
        "8th Semester \nADVANCED TOPICS IN CATEGORY THEORY \n \n \nTeaching \nThe teaching style will be lecture-based, but supported by a practical component where \nstudents will learn to use a proof assistant for higher category theory, and build a small portfolio \nof proofs. Towards the end of the course we will explore some of the exciting computer science \nresearch literature on monoidal and higher categories, and students will choose a paper and \npresent it to the class. \n \nAims \nThe module will introduce advanced topics in category theory. The aim is to train students to \nengage and start modern research on the mathematical foundations of higher categories, the \ngraphical calculus, monoids and representations, type theories, and their applications in \ntheoretical computer science, both classical and quantum. \n \nObjectives \nOn completion of this module, students should: \n \nBe familiar with the techniques of compositional category theory. \nHave a strong understanding of basic categorical semantic models. \nBegun exploring current research in monoidal categories and higher structures. \nSyllabus \nPart 1, lecture course: \nThe first part of the course introduces concepts from monoidal categories and higher categories, \nand explores their application in computer science. \n\u2010 Monoidal categories and the graphical calculus \n\u2010 The proof assistant homotopy.io \n\u2010 Coherence theorems and higher category theory \n\u2010 Linearity, superposition, duality, quantum entanglement \n\u2010 Monoids, Frobenius algebras and bialgebras \n\u2010 Type theory for higher category theory \n \nPart 2, exploring the research frontier: \nIn the second part of the course, students choose a research paper to study, and give a \npresentation to the class. \nThere is a nice varied literature related to the topics of the course, and the lecturer will supply a \nlist of suggested papers.  \n \nClasses \nThere will be three exercise sheets for homework, with accompanying classes by a teaching \nassistant to go over them. \n \n\nAssessment \nProblem sheets (50%) \nClass presentation (20%) \nPractical portfolio (30%) \nReading List \nChris Heunen and Jamie Vicary, \u201cCategory for Quantum Theory: An Introduction\u201d, Oxford \nUniversity Press\n \n\nADVANCED TOPICS IN COMPUTER SYSTEMS \n \nAims \nThis module will attempt to provide an overview of \u201csystems\u201d research. This is a very broad field \nwhich has existed for over 50 years and which has historically included areas such as operating \nsystems, database systems, file systems, distributed systems and networking, to name but a \nfew. The course will thus necessarily cover only a tiny subset of the field. \n \nMany good ideas in systems research are the result of discussing and debating previous work. \nA primary aim of this course therefore will be to educate students in the art of critical thinking: \nthe ability to argue for and/or against a particular approach or idea. This will be done by having \nstudents read and critique a set of papers each week. In addition, each week will include \npresentations from a number of participants which aim to advocate or criticise each piece of \nwork. \n \nSyllabus \nThe syllabus for this course will vary from year to year so as to cover a mixture of older and \nmore contemporary systems papers. Contemporary papers will be generally selected from the \npast 5 years, primarily drawn from high quality conferences such as SOSP, OSDI, ASPLOS, \nFAST, NSDI and EuroSys. Example topics might include: \n \nSystems Research and System Design \nOS Structure and Virtual Memory \nVirtualisation \nConsensus \nScheduling \nPrivacy \nData Intensive Computing \nBugs \nThe reading each week will involve a load equivalent to 3 full length papers. Students will be \nexpected to read these in detail and prepare a written summary and review. In addition, each \nweek will contain one or more short presentations by students for each paper. The types of \npresentation will include: \n \nOverview: a balanced presentation of the paper, covering both positive and negative aspects. \nAdvocacy: a positive spin on the paper, aiming to convince others of its value. \nCriticism: a negative take on the paper, focusing on its weak spots and omissions. \nThese presentation roles will be assigned in advance, regardless of the soi disant absolute merit \nof the paper or the preference of the student. Furthermore, all students \u2013 regardless of any \nassigned presentation role in a given week \u2013 will be expected to participate in the class by \nasking questions and generally entering into the debate. \n \nObjectives \n\nOn completion of this module students should have a broad understanding of some key papers \nand concepts in computer systems research, as well as an appreciation of how to argue for or \nagainst any particular idea. \n \nCoursework and practical work \nCoursework will be the production of the weekly paper reviews. Practical work will be presenting \npapers as appropriate, as well as ongoing participation in the class. \n \nAssessment \nAssessment consists of: \n \nOne essay per week for 7 weeks (10% each) \nPresentation (20%) \nParticipation in class over the term (10%) \nRecommended reading \nMost of the reading for this course will be in the form of the selected papers each week. \nHowever, the following may be useful background reading to refresh your knowledge from \nundergraduate courses: \n \nSilberschatz, A., Peterson, J.L. and Galvin, P.C. (2005). Operating systems concepts. \nAddison-Wesley (7th ed.). \n \nTanenbaum, A.S. (2008). Modern Operating Systems. Prentice-Hall (3rd ed.). \n \nBacon, J. and Harris, T. (2003). Operating systems. Addison-Wesley (3rd ed.). \n \nAnderson, T. and Dahlin, M. (2014). Operating Systems: Principles and Practice. Recursive \nBooks (2nd ed.). \n \nHennessy, J. and Patterson, D. (2006). Computer architecture: a quantitative approach. Elsevier \n(4th ed.). ISBN 978-0-12-370490-0. \n \nKleppmann M (2016) Designing Data-Intensive Applications, O'Reilly (1st ed.)\n \n\nADVANCED TOPICS IN MACHINE LEARNING \nAims \nThis course explores current research topics in machine learning in sufficient depth that, at the \nend of the course, participants will be in a position to contribute to research on their chosen \ntopics. Each topic will be introduced with a lecture which, building on the material covered in the \nprerequisite courses, will make the current research literature accessible. Each lecture will be \nfollowed by up to three sessions which will typically be run as a reading group with student \npresentations on recent papers from the literature followed by a discussion, or a practical, or \nsimilar. \n \nStructure \nEach student will attend 3 topics and each topic's sessions will be spread over 5 contact hours. \nStudents will be expected to undertake readings for their selected topics. There will be some \ngroup work. \n \nThere will be a briefing session in Michaelmas term. \n \nSyllabus \nStudents choose five topics in preferential order from a list to be published in Michaelmas term. \nThey will be assigned to three topics out of their list. Students are assessed on one of these \ntopics which may not necessarily be their first choice topic. \n \nThe topics to be offered in 2024-25 are yet to be decided but to give an indicative idea of the \ntypes of topics, the ones offered in 2023-24 were: \n \nImitation learning  Prof A. Vlachos \nThe Future of Large Language Models: data architectures ethics Dr M. Tomalin \nPhysics and Geometry in Machine Learning Dr C. Mishra \nDiffusion Models and SDEs Francisco Vargas, Dr C. H. Ek \nExplainable Artificial Intelligence M. Espinosa, Z. Shams, Prof M. Jamnik \nUnconventional approaches to AI Dr S. Banerjee \nNarratives in Artificial Intelligence and Machine Learning Prof N. Lawrence \nMultimodal Machine Learning K. Hemker, N. Simidjievski, Prof M. Jamnik \nDeep Reinforcement Learning S. Morad, Dr C. H. Ek \nAutomation in Proof Assistants A. Jiang, W. Li, Prof M. Jamnik \nOn completion of this module, students should: \n \nbe in a strong position to contribute to the research topics covered; \nunderstand the fundamental methods (algorithms, data analysis, specific tasks) underlying each \ntopic; \nand be familiar with recent research papers and advances in the field. \nCoursework \nStudents will typically work in groups to give a presentation on assigned papers. Alternatively, a \ntopic may include practical sessions. Each topic will typically consist of one preliminary lecture \n\nfollowed by 3 reading and discussion sessions, or several lectures followed by a practical \nsession. A typical topic can accommodate up to 9 students presenting papers. There will be at \nleast 10 minutes general discussion per session. \n \nFull coursework details will be published by October. \n \nAssessment \nCoursework will be marked by the topic leaders and second marked by the module conveners. \n \nParticipation in all assigned topics, 10% \nPresentation or practical work or similar (for one of the chosen topics), 20% \nTopic coursework (for one of the chosen topics), 70% \nIndividual topic coursework will be published late Michaelmas term. \n \nAssessment criteria for topic coursework will follow project assessment criteria here: \nhttps://www.cl.cam.ac.uk/teaching/exams/acs_project_marking.pdf \n \nPlease note that students will be assessed on one of their three chosen topics but this may not \nbe their first choice. \n \nRecommended reading \nTo be confirmed by each topic convenor.\n \n\nCOMPUTING FOR COLLECTIVE INTELLIGENCE \nPrerequisites: Familiarity with core machine learning paradigms - Basic calculus (analytical \nskills) - Basic discretre optimization (combinatorics) \nMoodle, timetable \n \nObjectives \nThere is a substantial body of academic work demonstrating that complex life is built by \ncooperation across scales: collections of genes cooperate to produce organisms, cells \ncooperate to produce multi-cellular organisms and multicellular animals cooperate to form \ncomplex social groups. Arguably, all intelligence is collective intelligence. Yet, to-date, many \nartificially intelligent agents (both embodied and virtual), are generally not conceived from the \nground up to interact with other intelligent agents (be it machines or humans). The canonical AI \nproblem is that of a monolithic and solitary machine confronting a non-social environment. This \ncourse aims to balance this trend by (i) equipping students with conceptual and practical \nknowledge on collective intelligence from a computational standpoint, and (ii) by conveying \nvarious computational paradigms by which collective intelligence can be modelled as well as \nsynthesized. \n \nAssessment \nThe assessment will be based on a reading-group paper presentation (individual or in pairs), \naccompanied by a 2-page summary, and a technical position paper (individual work) handed in \nafter the course. The position paper will be assessed based on its technical correctness, \nstrength or arguments, \nand clarity of research vision, and will be accompanied by a brief 5-minute pre-recorded talk that \nsummarizes key arguments (any slides used are also submitted as part of the project). \n \nPaper presentation and 1-page summary: 25% \nTechnical position paper: 75% (4000 word limit) \nRecommended reading \nSwarm Intelligence : From Natural to Artificial Systems, Bonabeau et al (1999) \nJoined-Up Thinking, Hannah Critchlow, (2024) \nSupercooperators, Martin Nowak (2011) \nA Course on Cooperative Game Theory, Chakravarty et al., (2015) \nGoverning the Commons: The Evolution of Institutions for Collective Action. Ostrom (1990).  \n \n\nCRYPTOGRAPHY AND PROTOCOL ENGINEERING \nAims \nWe all use cryptographic protocols every day: whenever we access an https:// website or send a \nmessage via WhatsApp or Signal, for example. In this module, students will get hands-on \nexperience of how those protocols are implemented. Students start by writing their own secure \nmessaging protocol from scratch, and then move on to more advanced examples of \ncryptographic protocols for security and privacy, such as private information retrieval (searching \nfor data without revealing what you\u2019re searching for). \n \nSyllabus \nThis is a practical course in which the first half of each of the 2-hour weekly sessions is a lecture \nthat introduces a topic, while the second half is lab time during which the students can work on \ntheir implementations, discuss the topics in small groups, and get help from the lecturers. The \nmodule is structured into three blocks as follows:  \n \nCryptographic primitives (2 weeks). Using common building blocks such as symmetric ciphers \nand hash functions. Implementing selected aspects of elliptic curve Diffie-Hellman and digital \nsignatures. Protocol security, Dolev-Yao model, side-channel attacks.  \nSecure communication (3 weeks). Authenticated key exchange (e.g. SIGMA, TLS), \npassword-authenticated key exchange (e.g. SPAKE2), forward secrecy, post-compromise \nsecurity, double ratchet, the Signal protocol.  \nPrivate database lookups (3 weeks). Lattice-based post-quantum cryptography, learning with \nerrors, homomorphic encryption, private information retrieval.  \nIn each block, students will be given a description of the algorithms and protocols covered (e.g. \na research paper or an RFC standards document), and a code template that the students \nshould use for their implementation. The implementation language will probably be Python (to \nbe confirmed after the practical materials have been developed ). Students will also be given a \nset of test cases to check their code, and will have the opportunity to test their implementation \ncommunicating with other students\u2019 implementations. Finally, for each block, students will write \nand submit a lab report explaining their approach and the findings from their implementation. \n \nObjectives \nOn completion of this module, students should: \n \nUnderstand that cryptography is not magic! The mathematics can look intimidating, but this \nmodule will show students that they needn\u2019t be afraid of cryptography. \nGain appreciation for the complexity and careful implementation of real-world cryptography \nlibraries ,and understand why one should prefer vetted implementations. \nBe familiar with the mathematical notation and concepts commonly used in descriptions of \ncryptographic protocols, and be able to understand and implement research papers and RFCs \nin this field. \nBe comfortable with cryptographic primitives commonly used in protocols, and able to correctly \nuse software libraries that implement them. \n\nHave gained hands-on experience of attacks on cryptographic protocols, and an appreciation \nfor the difficulty of making these protocols correct. \nHave been exposed to ideas and techniques from recent research, which may be useful for their \nown future research. \nHave practised technical writing through lab reports. \nAssessment \nStudents will be provided with code skeletons in one or two different programming languages \n(probably Python, maybe Rust) to avoid them struggling with setup issues. For each \nassignment, students are required to produce a working implementation of the protocols \ndiscussed in the course, using the programming language and skeleton provided. Here, \n\u201cworking\u201d means that the algorithms are functionally correct, but they are not production-quality \n(in particular, no side-channel countermeasures such as constant-time algorithms are required). \nFor some primitives (e.g. symmetric ciphers and hash functions) existing libraries should be \nused, whereas other cryptographic algorithms will be implemented from scratch. Students \nshould submit their code as a Docker container that can be run against specified test cases.  \n \nAfter each of the three blocks, students will be asked to submit a lab report of approximately \n2,000 words along with their code for that block. In the lab report, the students should explain \nhow their implementation works and why it is correct, as well as any findings from the work (e.g. \nany limitations or trade-offs they found with the protocols). The report structure and marking \nscheme will be specified in advance. The lab reports provide an opportunity for students to \nprovide critical insights and creativity. For instance, they might compare their implementation \nwith existing real-world implementations which provide additional features and guarantees.  \n \nEach of the three lab reports (along with the associated code) will be marked, forming the basis \nof assessment, and feedback on the reports will be given to the students before the next \nsubmission is due, so that students can take the feedback on board for their next submission. \nOf the three reports, the worst mark will be discarded, and the other two reports each contribute \n50% of the final mark. As such, students might decide to submit only two reports.  \n \nStudents may discuss their work with others, but the implementations and lab reports must be \nindividual work. \n \nRecommended reading \nTextbook: Jonathan Katz and Yehuda Lindell. Introduction to Modern Cryptography (3rd edition). \nCRC Press, 2020. \n \nThe textbook covers prerequisite knowledge on cryptographic primitives, such as ciphers, \nMACs, hash functions, and signatures. This book is also used in the Part II Cryptography \ncourse. For the protocols we cover in this course, we are not aware of a suitable textbook; \ninstead we will use research papers, lecture slides, and RFCs as resources, which will be \nprovided at the start of the module.\n \n\nDISTRIBUTED LEDGER TECHNOLOGIES: FOUNDATIONS AND APPLICATIONS \n \nAims \nThis reading group course examines foundations and current research into distributed ledger \n(blockchain) technologies and their applications. Students will read, review, and present seminal \nresearch papers in this area. Once completed, students should be able to integrate blockchain \ntechnologies into their own research and gain familiarity with a range of research skills. \n \nLectures \nIntroduction \nConsensus protocols \nBitcoin and its variants \nEthereum, smart contracts, and other permissionless DLTs \nHybrid and permissioned DLTs \nApplications \nLearning objectives \nThere are two broad objectives: to acquire familiarity with a body of work in the area of \ndistributed ledgers and to learn some specific research skills: \n \nHow to read a paper \nHow to review a paper \nHow to analyze a paper\u2019s strengths and weaknesses \nWritten and oral presentation skills \nAssessment \nYou are expected to read all assigned papers and submit paper reviews each week. Each \nreview must either follow the provided review form [PDF] [Latex source]. Each \u201creview\u201d is worth \n5% of your total mark, and is marked out of 100 with 60 a passing grade. Marks will be awarded \nand penalties for late submission applied according to ACS Assessment Guidelines.  \n \nOne paper review for the first week, then two paper reviews each week for 6 weeks (13 reviews, \n5% each) 65% (approx 600 words per review) \nSummative essay 25% (max 3000 words) \nPresentation 5% \n100% attendance in class 5% (2 marks deducted per missed class) \nRecommended Reading \nNarayanan, A. , Bonneau, J., Felten, E., Miller, A. and Goldfeder, S. (2016). Bitcoin and \nCryptocurrency Technologies: A Comprehensive Introduction. Princeton University Press. \n(2016 Draft available here: \nhttps://d28rh4a8wq0iu5.cloudfront.net/bitcointech/readings/princeton_bitcoin_book.pdf)\n \n\nEXPLAINABLE ARTIFICIAL INTELLIGENCE \nPrerequisites: A solid background in statistics, calculus and linear algebra. We strongly \nrecommend some experience with machine learning and deep neural networks (to the level of \nthe first chapters of Goodfellow et al.\u2019s \u201cDeep Learning\u201d). Students are expected to be \ncomfortable reading and writing Python code for the module\u2019s practical sessions. \nMoodle, timetable \n \nAims \nThe recent palpable introduction of Artificial Intelligence (AI) models to everyday \nconsumer-facing products, services, and tools brings forth several new technical challenges and \nethical considerations. Amongst these is the fact that most of these models are driven by Deep \nNeural Networks (DNNs), models that, although extremely expressive and useful, are \nnotoriously complex and opaque. This \u201cblack-box\u201d nature of DNNs limits their ability to be \nsuccessfully deployed in critical scenarios such as those in healthcare and law. Explainable \nArtificial Intelligence (XAI) is a fast-moving subfield of AI that aims to circumvent this crucial \nlimitation of DNNs by either  \n \n(i) constructing human-understandable explanations for their predictions,  \n \nor (ii) designing novel neural architectures that are interpretable by construction.  \n \nIn this module, we will introduce the key ideas behind XAI methods and discuss some of the \nimportant application areas of these methods (e.g., healthcare, scientific discovery, debugging, \nmodel auditing, etc.). We will approach this by focusing on the nature of what constitutes an \nexplanation, and discussing different ways in which explanations can be constructed or learnt to \nbe generated as a by-product of a model. The main aim of this module is to introduce students \nto several commonly used approaches in XAI, both theoretically in lectures and through \nhands-on exercises in practicals, while also bringing recent promising directions within this field \nto their attention. We hope that, by the end of this module, students will be able to directly \ncontribute to XAI research and will understand how methods discussed in this module may be \npowerful tools for their own work and research. \n \nSyllabus \nOverview and taxonomy of XAI (why is explainability needed, definition of terms, taxonomy of \nthe XAI space, etc.)  \nPerturbation-based feature attribution (e.g., LIME, Anchors, SHAP, etc.) \nPropagation-based feature attribution methods (e.g., Relevance Propagation, Saliency \nmethods, etc.) \nConcept-based explainability (Net2Vec, T-CAV, ACE, etc.) \nInterpretable architectures (CBMs and variants, Concept Whitening, SENNs, etc.) \nNeurosymbolic methods (DeepProbLog, Neural Reasoners, etc.) \nSample-based Explanations (Influence functions, ProtoPNets, etc.) \nCounterfactual explanations \nProposed Schedule \n\nThe 16 hours of lectures across 8 weeks will be divided as follows:  \nWeek 1: 1h lecture + 1h lecture  \nWeek 2: 1h lecture + 1h reading group & presentations  \nWeek 3: 1h lecture + 1h reading group & presentations  \nWeek 4: 2h practical  \nWeek 5: 1h lecture + 1h reading group & presentations  \nWeek 6: 2h practical  \nWeek 7: 1h lecture + 1h reading group & presentations  \nWeek 8: 1h reading group & presentation + 1h reading group & presentations \n \nIn weeks where lectures are planned, one-hour lecture slots will intercalate with one hour of \nstudent paper presentations. During each paper presentation session, three students will \npresent a paper related to the topic covered in the earlier lecture that week for about 10 minutes \neach + 5 minutes of questions. At the end of all paper presentations, there will be a discussion \non all the papers. The order of student presentations will be allocated randomly during the first \nweek so that students know in advance when they are expected to present. For the sake of \nfairness, we will release the paper to be presented by each student a week before their \npresentation slot. \n \nObjectives \nBy the end of this module, students should be able to: \n \nRecognise and identify key concepts in XAI together with their connection to related subfields in \nAI such as fairness, accountability, and trustworthy AI. \nUnderstand how to use, design, and deploy model-agnostic perturbation methods such as \nLIME, Anchors, and RISE. In particular, students should understand the connection between \nfeature importance and cooperative game theory, and its uses in methods such as SHAP. \nIdentify the uses and limitations of propagation-based feature importance methods such as \nSaliency, SmoothGrad, GradCAM, and Integrated Gradients. Students should be able to \nimplement each of these methods on their own and connect the theoretical ideas behind them \nto practical code, exploiting modern frameworks\u2019 auto-differentiation. \nUnderstand what concept learning is and what limitations it overcomes compared to traditional \nfeature-based methods. Specifically, students should understand how probing a DNN\u2019s latent \nspace may be exploited for learning useful concepts for explainability. \nReason about the key components of inherently interpretable architectures and neuro-symbolic \nmethods and understand how interpretable neural networks can be designed from first \nprinciples. \nElaborate on what sample-based explanations are and how influence functions and prototypical \narchitectures such as ProtoPNet can be used to construct such explanations. \nExplain what counterfactual explanations are, how they are related to causality, and under which \nconditions they may be useful. \nUpon completion of this module, students will have the technical background and tools to use \nXAI as part of their own research or partake in XAI research itself. Moreover, we hope that by \ndetailing a clear timeline of how this relatively young subfield has developed, students may be \n\nable to better understand what are some fundamental open questions in this area and what are \nsome promising directions that are currently actively being explored. \n  \n \nAssessment \n(10%) student presentation: each student will be randomly assigned a presentation slot at the \nbeginning of the course. We will then distribute a paper for them to present in their slot a week \nbefore the presentation\u2019s scheduled time. Students are expected to prepare a 10-minute \npresentation of their assigned paper where they will present the motivation of their assigned \nwork and discuss the main methodology and findings reported in that paper. We will encourage \nstudents to focus on fully communicating the intuition of the work in their assigned papers and \ntry and connect it with ideas that we have previously discussed in previous lectures. All students \nnot presenting each week will be asked to submit one question pertaining to each paper \npresented that week before the paper presentations. \n \n(20%) practical exercises: We will run two practical sessions where students will be asked to \nperform a series of exercises that require them to use concepts we have introduced in lecture \nup to that point. For each practical session, we will prepare a colab notebook to guide the \nstudent through exercises and we will ask the students to submit their solutions through this \ncolab notebook. We expect students to complete about \u2154 of the exercises in the practical class \nand complete the rest at home as homework. Each practical will be worth 10%. \n \n(70%) mini-project: At the end of week 3, we will hand out a list of papers for students to select \ntheir mini-projects from. Each mini-project will consist of a student selecting a paper from our list \nand reimplementing and expanding the key idea in the paper. We encourage students to be as \ncreative as they want with how they drive their mini-project once the paper has been selected. \nFor example, they can reimplement the technique in the paper and combine it with \nmethodologies from other works we discussed in lecture, or they can apply their paper\u2019s \nmethodology to a new domain, datasets, or setup, where the technique may offer interesting \nand potentially novel insights. We will ask all students to submit a report in a workshop format of \nup to 4,000 words. This report, due roughly a week after Lent term ends, should describe their \nmethodology, experiments, and results. A crucial aspect of this report involves explaining the \nrationale behind different choices in methodology and experiments, as well as elaborating on \nthe choices made and hypotheses tested throughout their mini-project (potentially showing a \ndeep understanding of the work they are basing their mini-project on). To aid students with \nselecting their projects and making progress on them, we will hold regular office hours when \nstudents can come to discuss their progress and questions with us. \n \nRecommended reading \nTextbooks \n \n* Christoph Molnar, \u201cInterpretable Machine Learning\u201d. (2022): \nhttps://christophm.github.io/interpretable-ml-book/ \n \n\nOnline Courses and Tutorial \n \n* Su-in Lee and Ian Cover, \u201cCSEP 590B Explainable AI\u201d University of Washington* Explaining \nMachine Learning Predictions: State-of-the-art, Challenges, and Opportunities \n \n* On Explainable AI: From Theory to Motivation, Industrial Applications, XAI Coding & \nEngineering Practices - AAAI 2022 Turorial \n \nSurvey papers \n \n* Arrieta, Alejandro Barredo, et al. \"Explainable Artificial Intelligence (XAI): Concepts, \ntaxonomies, opportunities and challenges toward responsible AI.\" Information fusion 58 (2020): \n82-115. \n \n* Rudin, Cynthia, et al. \"Interpretable machine learning: Fundamental principles and 10 grand \nchallenges.\" Statistics Surveys 16 (2022): 1-85.\n \n\nFEDERATED LEARNING: THEORY AND PRACTICE \nPrerequisites: It is strongly recommended that students have previously (and successfully) \ncompleted an undergraduate machine learning course - or have equivalent experience through \nopen-source material (e.g., lectures or similar). An example course would be: Part1B \"Artificial \nIntelligence\". Students should feel comfortable with SGD and optimization methods used to train \ncurrent popular neural networks and simple forms of neural network architectures. \nMoodle, timetable \n \nObjectives \nThis course aims to extend the machine learning knowledge available to students in Part I (or \npresent in typical undergraduate degrees in other universities), and allow them to understand \nhow these concepts can manifest in a decentralized setting. The course will consider both \ntheoretical (e.g., decentralized optimization) and practical (e.g., networking efficiency) aspects \nthat combine to define this growing area of machine learning.  \n \nAt the end of the course students should: \n \nUnderstand popular methods used in federated learning \nBe able to construct and scale a simple federated system \nHave gained an appreciation of the core limitations to existing methods, and the approaches \navailable to cope with these issues \nDeveloped an intuition for related technologies like differential privacy and secure aggregation, \nand are able to use them within typical federated settings  \nCan reason about the privacy and security issues with federated systems \nLectures \nCourse Overview. Introduction to Federated Learning. \nDecentralized Optimization. \nStatistical and Systems Heterogeneity. \nVariations of Federated Aggregation. \nSecure Aggregation. \nDifferential Privacy within Federated Systems. \nExtensions to Federated Analytics. \nApplications to Speech, Video, Images and Robotics. \nLab sessions \nFederating a Centralized ML Classifier. \nBehaviour under Heterogeneity. \nScaling a Federated Implementation. \nExploring Privacy with Federated Settings \nRecommended Reading \nReadings will be assigned for each lecture. Readings will be taken from either research papers, \ntutorials, source code or blogs that provide more comprehensive treatment of taught concepts. \n \nAssessment - Part II Students \n\nFour labs are performed during the course, and students receive 10% of their total grade for \nwork done as part of each lab. (For a total of 40% of the total grade from lab work alone). Labs \nwill primarily provide hands-on teaching opportunities, that are then utilized within the lab \nassignment which is completed outside of the lab contact time. MPhil and Part III students will \nbe given additional questions to answer within their version of the lab assignment which will \ndiffer from the assignment given to Part II CST students. \n \nThe remainder of the course grade (60%) will be given based on a hands-on project that applies \nthe concepts taught in lectures and labs. This hands-on project will be assessed based on upon \na combination of source code, related documentation and brief 8-minute pre-recorded talk that \nsummarizes key project elements (any slides used are also submitted as part of the project). \nPlease note, that in the case of Part II CST students, the talk itself is not examinable -- as such \nwill be made optional to those students. \n \nA range of possible practical projects will be described and offered to students to select from, or \nalternatively students may propose their own. MPhil and Part III students will select from a \nproject pool that is separate from those offered to Part II CST students. MPhil and Part III \nprojects will contain a greater emphasis on a research element, and the pre-recorded talks \nprovided by this student group will focus on this research contribution. The project will be \nassessed on the level of student understanding demonstrated, the degree of difficulty, \ncorrectness of implementation -- and for Part III/MPhil students the additional criteria of the \nquality and execution of the research methodology, and depth and quality of results analysis. \n \nThis project can be done individually or in groups -- although individual projects will be strongly \nencouraged. It will be required the project is performed using a code repository that also will \ncontain all documentation -- access to this repository will be shared with course staff (e.g., \nlecturer and TAs). Where needed, marks assigned to students within a group will be \ndifferentiated using this repository as an input. Furthermore if groups are formed, members must \nbe either entirely from Part III/MPhil students or Part II CST, i.e., these two student groups \nshould not mix to form a project group. \n \nProjects will be made available publicly. A maximum word count for written contributions for the \nproject will be enforced.  \n \n  \nAssessment - MPhil / Part III Students \n50% final mini-project. This hands-on project will be assessed based on upon a combination of \nsource code, related documentation and brief 8-minute pre-recorded talk that summarizes key \nproject elements (any slides used are also submitted as part of the project). \n \n30% midterm lab report (a written report of a bit more depth of thought than required in a lab, \nthe report provides the results of experiments -- with the skills to perform these experiments \ncoming from lab sessions),  \n \n\n20% for two individual hands-on labs. Labs will primarily provide hands-on teaching \nopportunities, that are then utilized within the lab assignment which is completed outside of the \nlab contact time. There will also be additional questions to answer within their version of the lab \nassignment. \n \nFor the mini-project, a range of possible practical projects will be described and offered to \nstudents to select from, or alternatively students may propose their own. The project will be \nassessed on the level of student understanding demonstrated, the degree of difficulty, \ncorrectness of implementation, the quality and execution of the research methodology, and \ndepth and quality of results analysis. \n \nThe project can be done individually or in groups -- although individual projects will be strongly \nencouraged. It will be required the project is performed using a code repository that also will \ncontain all documentation -- access to this repository will be shared with course staff (e.g., \nlecturer and TAs). Where needed, marks assigned to students within a group will be \ndifferentiated using this repository as an input. Furthermore if groups are formed, members must \nbe either entirely from Part III or entirely from MPhil students, i.e., these two student groups \nshould not mix to form a project group. \n \nProjects will be made available publicly. A maximum word count for written contributions for the \nproject will be enforced.   \n \nAdvanced Material sessions - MPhil / Part III Students \nThese sessions are mandatory for the MPhil and Part III students. Part II CST students may \nattend if they wish \n \nTopics and announced the first week of class. Selected to extend beyond topics covered in \nlectures and labs. Content is a mixture of sessions run in the lecture room the are either (1) \npresentations by guest lectures by an invited domain expert or (2) class-wide discussions \nregarding one or more related academic papers. Paper discussions will require students to read \nthe paper ahead of the lecture, and a brief discussion primer presentation will be given students \nbefore discussions begin.  \n \nThis module is shared with Part II of the Computer Science Tripos. Assessment will be adjusted \nfor the two groups of students to be at an appropriate level for whichever course the student is \nenrolled on. Further information about assessment and practicals will follow at the first lecture.\n\nGEOMETRIC DEEP LEARNING \n \nPrerequisites: Experience with machine learning and deep neural networks is recommended. \nKnowledge of concepts from graph theory and group theory is useful, although the relevant \nparts will be explicitly retaught. \nMoodle, timetable \n \nAims and Objectives \nMost of the patterns we see in nature can be elegantly reasoned about using spatial \nsymmetries\u2014transformations that leave underlying objects unchanged. This observation has \nhad direct implications for the development of modern deep learning architectures that are \nseemingly able to escape the \u201ccurse of dimensionality\u201d and fit complex high-dimensional tasks \nfrom noisy real-world data. Beyond this, one of the most generic symmetries\u2014the \npermutation\u2014will prove remarkably powerful in building models that reason over graph \nstructured-data, which is an excellent abstraction to reason about naturally-occurring, \nirregularly-structured data. Prominent examples include molecules (represented as graphs of \natoms and bonds, with three-dimensional coordinates provided), social networks and \ntransportation networks. Several already-impacted application areas include traffic forecasting, \ndrug discovery, social network analysis and recommender systems. The module will provide the \nstudents the capability to analyse irregularly- and nontrivially-structured data in an effective way, \nand position geometric deep learning in a proper context with related fields. The main aim of the \ncourse is to enable students to make direct contributions to the field, thoroughly assimilate the \nkey concepts in the area, and draw relevant connections to various other fields (such as NLP, \nFourier Analysis and Probabilistic Graphical Models). We assume only a basic background in \nmachine learning with deep neural networks. \n \nLearning outcomes \nThe framework of geometric deep learning, and its key building blocks: symmetries, \nrepresentations, invariance and equivariance \nFundamentals of processing data on graphs, as well as impactful application areas for graph \nrepresentation learning \nTheoretical principles of graph machine learning: permutation invariance and equivariance \nThe three \"flavours\" of spatial graph neural networks (GNNs) (convolutional, attentional, \nmessage passing) and their relative merits. The Transformer architecture as a special case. \nAttaching symmetries to graphs: CNNs on images, spheres and manifolds, Geometric Graphs \nand E(n)-equivariant GNNs \nRelevant connections of geometric deep learning to various other fields (such as NLP, Fourier \nAnalysis and Probabilistic Graphical Models) \nLectures \nThe lectures will cover the following topics: \n \nLearning with invariances and symmetries: geometric deep learning. Foundations of group \ntheory and representation theory. \n\nWhy study data on graphs? Success stories: drug screening, travel time estimation, \nrecommender systems. Fundamentals of graph data processing: network science, spectral \nclustering, node embeddings. \nPermutation invariance and equivariance on sets and graphs. The principal tasks of node, edge \nand graph classification. Neural networks for point clouds: Deep Sets, PointNet; universal \napproximation properties. \nThe three flavours of spatial GNNs: convolutional, attentional, message passing. Prominent \nexamples: GCN, SGC, ChebyNets, MoNet, GAT, GATv2, IN, MPNN, GraphNets. Tradeoffs of \nusing different GNN variants. \nGraph Rewiring: how to apply GNNs when there is no graph? Links to natural language \nprocessing---Transformers as a special case of attentional GNNs. Representative \nmethodologies for graph rewiring: GDC, SDRF, EGP, DGCNN. \nExpressive power of graph neural networks: the Weisfeiler-Lehman hierarchy. GINs as a \nmaximally expressive GNN. Links between GNNs and graph algorithms: neural algorithmic \nreasoning. \nCombining spatial symmetries with GNNs: E(n)-equivariant GNNs, TFNs, SE(3)-Transformer. A \ndeep dive into AlphaFold 2. \nWorked examples: Circulant matrices on grids, the discrete Fourier transform, and convolutional \nnetworks on spheres. Graph Fourier transform and the Laplacian eigenbasis. \nPracticals \nThe practical is designed to complement the knowledge learnt in lectures and teach students to \nderive additional important results and architectures not directly shown in lectures. The practical \nwill be given as a series of individual exercises (each either code implementation or \nproof/derivation). Each of these exercises can be individually assessed based on a specified \nmark budget. \n \nPossible practical topics include the study of higher-order GNNs and equivariant message \npassing. \n \nAssessment \n(60%) Group Mini-project (writeup) at the end of the course. The mini projects can either be \nself-proposed, or the students can express their preference for one of the provided topics, the \nlist of which will be announced at the start of term. The projects will consist of implementing \nand/or extending graph representation learning models in the literature, applying them to \npublicly available datasets. Students will undertake the project in pairs and submit a joint \nwriteup limited to 4,000 words (in line with other modules); appendix of work logs to be included \nbut ungraded; \n \n(10%) Short presentation and viva: students will give a short presentation to explain their \nindividual contribution to the mini-project and there will be a short viva following. \n \n(30%) Practical work completion. Completing the exercises specified in the practical to a \nsatisfactory standard. The practical assessor should be satisfied that the student derived their \nanswers using insight gained from the course; coupled with original thought, not by simple \n\ncopy-pasting of relevant related work. The students would submit code and a short report, which \nwould then be marked in line with the predetermined mark budget for each practical item. \nThe students will learn how to run advanced architectures on GPU but no specific need for \ndedicated GPU resources. Practicals will be made possible to do on CPU; if required, students \ncan use GPUs on publicly available free services (such as Colab) for their mini-project work. \n \nReferences \nThe course will be based on the following literature: \n \n\"Geometric Deep Learning: Grids, Graphs, Groups, Geodesics, and Gauges\", by Michael \nBronstein, Joan Bruna, Taco Cohen and Petar Veli\u010dkovi\u0107 \n\"Graph Representation Learning\", by Will Hamilton \n\"Deep Learning\", by Ian Goodfellow, Yoshua Bengio and Aaron Courville.\n \n\nMOBILE HEALTH \nAims \nThe course aims to explore how wearable and mobile systems sensors can be used to gather \ndata relevant to understand health, how the data can be analysed with advanced signal \nprocessing and machine learning and the performance of these systems in terms of diagnostics \nand disease progression detection. \n \nSyllabus \nCourse Overview. Introduction to Mobile Health. Evaluation metrics and methodology. Basics of \nSignal Processing. \nInertial Measurement Units, Human Activity Recognition (HAR) and Gait Analysis and Machine \nLearning for IMU data. \nRadios, Bluetooth, GPS and Cellular. Epidemiology and contact tracing, Social interaction \nsensing and applications. Location tracking in health monitoring and public health. \nAudio Signal Processing. Voice and Speech Analysis: concepts and data analysis. Body \nSounds analysis. \nPhotoplethysmogram and Light sensing for health (heart and sleep) \nContactless and wireless behaviour and physiological monitoring \nGenerative Models for Wearable Health Data \nTopical Guest Lectures \nObjectives \nThe course aims to explore how wearable and mobile systems sensors can be used to gather \ndata relevant to understand health, how the data can be analysed with advanced signal \nprocessing and machine learning and the performance of these systems in terms of diagnostics \nand disease progression detection. \n \nRoughly, each lecture contains a theory part about the working of \u201csensor signals\u201d or \u201cdata \nanalysis methods\u201d and an application part which contextualises the concepts. \n \nAt the end of the course students should: Understand how mobile/wearable sensors capture \ndata and their working. Understand different approaches to acquiring and analysing sensor data \nfrom different types of sensors. Understand the concept of signal processing applied to time \nseries data and their practical application in health. Be able to extract sensor data and analyse it \nwith basic signal processing and machine learning techniques. Be aware of the different health \napplications of the various sensor techniques. The course will also touch on privacy and ethics \nimplications of the approaches developed in an orthogonal fashion. \n \nRecommended Reading \nPlease see Course Materials for recommended reading for each session. \n \nAssessment - Part II students \nTwo assignments will be based on two datasets which will be provided to the students: \n \n\nAssignment 1 (shared for Part II and Part III/MPhil): this will be based on a dataset and will be \nworth 40% of the final mark. The task of the assessment will be to perform pre-processing and \nbasic data analysis in a \"colab\" and an answer sheet of no more than 1000 words. \n \nAssignment 2 (Part II): This assignment (worth 60% of the final mark) will be a fuller analysis of \na dataset focusing on machine learning algorithms and metrics. Discussion and interpretation of \nthe findings will be reported in a colab and a report of no more than 1200 words. \n \nAssessment  - Part III and MPhil students \nTwo assignments will be based on datasets which will be provided to the students: \n \nAssignment 1: this will be based on a dataset and will be worth 40% of the final mark. The task \nof the assessment will be to perform pre-processing and basic data analysis in a \"colab\" and an \nanswer sheet of no more than 1000 words. \n \nAssignment 2: The second assignment (worth 60% of the final mark) will be based on multiple \ndatasets. The students will be asked to compare and contract ML algorithms/solutions (trained \nand tested on the different data) and discuss the findings and interpretation in terms of health \ncontext. This will be in the form of a colab and a report of 1800 words. \n \nFurther Information \nCurrent Cambridge undergraduate students who are continuing onto Part III or the MPhil in \nAdvanced Computer Science may only take this module if they did NOT take it as a Unit of \nAssessment in Part II. \n \nThis module is shared with Part II of the Computer Science Tripos. Assessment will be adjusted \nfor the two groups of students to be at an appropriate level for whichever course the student is \nenrolled on. Further information about assessment and practicals will follow at the first lecture.\n\nMULTICORE SEMANTICS AND PROGRAMMING \nPrerequisites: Some familiarity with discrete mathematics (sets, partial orders, etc.) and with \nsequential Java programming will be assumed. Experience with operational semantics and with \nsome concurrent programming would be helpful. \nMoodle, timetable \n \nAims \nIn recent years multiprocessors have become ubiquitous, but building reliable concurrent \nsystems with good performance remains very challenging. The aim of this module is to \nintroduce some of the theory and the practice of concurrent programming, from hardware \nmemory models and the design of high-level programming languages to the correctness and \nperformance properties of concurrent algorithms. \n \nLectures \nPart 1: Introduction and relaxed-memory concurrency [Professor P. Sewell] \n \nIntroduction. Sequential consistency, atomicity, basic concurrent problems. [1 block] \nConcurrency on real multiprocessors: the relaxed memory model(s) for x86, ARM, and IBM \nPower, and theoretical tools for reasoning about x86-TSO programs. [2 blocks] \nHigh-level languages. An introduction to C/C++11 and Java shared-memory concurrency. [1 \nblock] \nPart 2: Concurrent algorithms [Dr T. Harris] \n \nConcurrent programming. Simple algorithms (readers/writers, stacks, queues) and correctness \ncriteria (linearisability and progress properties). Advanced synchronisation patterns (e.g. some \nof the following: optimistic and lazy list algorithms, hash tables, double-checked locking, RCU, \nhazard pointers), with discussion of performance and on the interaction between algorithm \ndesign and the underlying relaxed memory models. [3 blocks] \nResearch topics, likely to include one hour on transactional memory and one guest lecture. [1 \nblock] \nObjectives \nBy the end of the course students should: \n \nhave a good understanding of the semantics of concurrent programs, both at the multprocessor \nlevel and the C/Java programming language level; \nhave a good understanding of some key concurrent algorithms, with practical experience. \nAssessment - Part II Students \nTwo assignments each worth 50% \n \nRecommended reading \nHerlihy, M. and Shavit, N. (2008). The art of multiprocessor programming. Morgan Kaufmann. \n \nCoursework \nCoursework will consist of assessed exercises. \n\n \nPractical work \nPart 2 of the course will include a practical exercise sheet.  The practical exercises involve \nbuilding concurrent data structures and measuring their performance.  The work can be \ncompleted in C, Java, or similar languages. \n \nAssessment - Part III and MPhil Students \nAssignment 1, for 50% of the overall grade. Written coursework comprising a series of questions \non hardware and software relaxed-memory concurrency semantics. \nAssignment 2, for 50% of the overall grade. Written coursework comprising a series of questions \non the design of mutual exclusion locks and shared-memory data structures. \nFurther Information \nCurrent Cambridge undergraduate students who are continuing onto Part III or the MPhil in \nAdvanced Computer Science may only take this module if they did NOT take it as a Unit of \nAssessment in Part II. \n \nThis module is shared with Part II of the Computer Science Tripos. Assessment will be adjusted \nfor the two groups of students to be at an appropriate level for whichever course the student is \nenrolled on. Further information about assessment and practicals will follow at the first lecture.\n\nREINFORCEMENT LEARNING \n \nAims \nThe aim of this module is to introduce students to the foundations of the field of reinforcement \nlearning (RL), discuss state-of-the-art RL methods, incentivise students to produce critical \nanalysis of recent methods, and prompt students to propose novel solutions that address \nshortcomings of existing methods. If judged by the headlines, RL has seen an unprecedented \nsuccess in recent years. However, the vast majority of RL methods still have shortcomings that \nmight not be apparent at first glance. The aim of the course is to inspire students by \ncommunicating the promising aspects of RL, but ensure that students develop the ability to \nproduce a critical analysis of current RL limitations. \n \nObjectives \nStudents will learn the following technical concepts: \n \nfundamental RL terminology and mathematical formalism; a brief history of RL and its \nconnection to neuroscience and biological systems \nRL methods for discrete action spaces, e.g. deep Q-learning and large-scale Monte Carlo Tree \nSearch \nmethods for exploration, modelling uncertainty, and partial observability for RL \nmodern policy gradient and actor-critic methods \nconcepts needed to construct model-based RL and Model Predictive Control methods \napproaches to make RL data-efficient and ways to enable simulation-to-reality transfer \nexamples of fine-tuning foundation models and large language models (LLMs) with human \nfeedback; safe RL concepts; examples of using RL for safety validation \nexamples of using RL for scientific discovery \nStudents will also gain experience with analysing RL methods to uncover their strengths and \nshortcomings, as well as proposing extensions to improve performance. \n \nFinally, students will gain skills needed to create and deliver a successful presentation in a \nformat similar to that of conference presentations. \n \nWith all of the above, students who take part in this module will be well-prepared to start \nconducting research in the field of reinforcement learning. \n  \n \nSyllabus \nTopic 1: Introduction and Fundamentals \n \nOverview of RL: foundational ideas, history, and books; connection to neuroscience and \nbiological systems, recent industrial applications and research demonstrations \nMathematical fundamentals: Markov decision processes, Bellman equations, policy and value \niteration, temporal difference learning \nTopic 2: RL in Discrete Action Spaces \n\n \nQ-learning, function approximation and deep Q-learning; nonstationarity in RL and its \nimplications for deep learning; example applications (video games; initial example: Atari) \nMonte Carlo Tree Search; example applications (AlphaGo) \nTopic 3: Exploration, Uncertainty, Partial Observability \n \nMulti-armed bandits, Bayesian optimisation, regret analysis \nPartially observable Markov decision process; belief, memory, and sequence modelling \n(probabilistic methods, recurrent networks, transformers) \nTopic 4: Policy Gradient and Actor-critic Methods for Continuous Action Spaces \n \nImportance sampling, policy gradient theorem, actor-critic methods (SPG, DDPG) \nProximal policy optimisation; example applications \nTopic 5: Model-based RL and Model Predictive Control \n \nLearning dynamics models (graph networks, stochastic processes, diffusion models, \nphysics-based models, ensembles); planning with learned models \nModel predictive control; example applications (real-time control) \nTopic 6: Data-efficient RL and Simulation-to-reality Transfer \n \nData-efficient learning with probabilistic methods from real data (e.g. policy search in robotics), \nreal-to-sim inference and differentiable simulation, data-efficient simulation-to-reality transfer \nRL for physical systems (successful examples in locomotion, open problems in contact-rich \nmanipulation, applications to logistics, energy, and transport systems); examples of RL for \nhealthcare. \nTopic 7: RL with Human Feedback ; Safe RL and RL for Validation \n \nFine-tuning large language models (LLMs) and other foundation models with human feedback \n(TRLX,RL4LMs, a light-weight overview of RLHF) \nA review of SafeRL, example: optimising commercial HVAC systems using policy improvement \nwith constraints; improving safety using RL for validation: examples in autonomous driving and \nautonomous flying and aircraft collision avoidance \nTopic 8: RL for Scientific Discovery; Student Presentations \n \nExamples of RL for molecular design and drug discovery, active learning for synthesising new \nmaterials, RL for nuclear fusion experiments \nStudent presentations (based on essays and mini-projects) for other topics in RL, e.g. \nmulti-agent RL, hierarchical RL, RL for hyperparameter optimisation and NN architecture \nsearch, RL for multi-task transfer, lifelong RL, RL in biological systems, etc. \nAssessment \nThe assessment for this module consists of: \n \nEssay and mini-project (60%)  \n\nStudents will choose a concrete RL method from the literature, then complete a 2-part essay \ndescribed below: \nEssay Part 1 (1500 words, 20%): Introduce a formal description of the method and explain how \nthe method extended the state-of-the-art at the time of its publication \nEssay Part 2 or a mini-project (2500 words, 40%): Provide a critical analysis of the chosen RL \nmethod. \nPresentation of the essay and mini-project results (15%)  \nstudents will be expected to create and record a 15-minute video presentation of the analysis \ndescribed in their essay and mini-project. \nShort test of RL theory fundamentals (15%)  \nto test the understanding of RL fundamentals (~15 minutes, in-class, closed book) \nParticipation in seminar discussions (10%)  \ntake an active part in the seminars by asking clarifying questions and mentioning related works. \nRecommended Reading \nBooks \n \n[S&B] Reinforcement Learning: An Introduction (second print edition). Richard S. Sutton, \nAndrew G. Barto. [Available from the book\u2019s website as a free PDF updated in 2022] \n \n[CZ] Algorithms for Reinforcement Learning. Csaba Szepesvari. [Available from the book\u2019s \nwebsite as a free PDF updated in 2019] \n \n[MK] Algorithms for Decision Making. Mykel J. Kochenderfer Tim A. WheelerKyle H. Wray. \n[Available from the book\u2019s website as a free PDF updated in 2023] \n \n[DB] Reinforcement Learning and Optimal Control. Dimitri Bertsekas. [Available from the book\u2019s \nwebsite as a free PDF updated in 2023] \n \nPresentation guidelines \n \n[KN] Ten simple rules for effective presentation slides. Kristen M.Naegle. PLoS computational \nbiology 17, no. 12 (2021).\n \n\nTHEORIES OF SOCIO-DIGITAL DESIGN FOR HUMAN-CENTRED AI \n \nPrerequisites: This course is appropriate to any ACS student, and will assist in the development \nof critical thinking, argument and long-form writing skills. Prior experience of essay-based \nhumanities subjects at university or secondary school will be beneficial, but not required. \nMoodle, timetable \n \nAims \nThis module is a theoretically-oriented advanced introduction to the broad field of \nhuman-computer interaction, extending to consider topics such as intelligent user interfaces, \ncritical and speculative design, participatory design, cognitive models of users, human-centred, \nas well as more-than-human-centred design, and others. The course will not address purely \nengineering approaches to the development of user interfaces (unless there is a clear \ntheoretical question being addressed), but allow students to effectively link the political, social, \nand ethical considerations in HCI to specific technical and conceptual design challenges. The \nmodule builds on the Practical Research in Human-Centred AI course offered in Michaelmas \n(developed with researchers at Microsoft Research Cambridge) and a collaboration with the \nLeverhulme Centre for the Future of Intelligence at Cambridge. Participants may include visitors \nfrom these groups and/or interdisciplinary research students and academic guests from other \nUniversity departments, including Cambridge Digital Humanities. \n \nSyllabus \nThe syllabus will remain broadly within the area of human-computer interaction, including \ntheories of design practice and the social contexts of technology use. Individual seminar topics \nwill be selected in response to contemporary and recent research developments, in consultation \nwith members of the class and visiting contributors. \n \nRepresentative topics in the next year are likely to include: \n \nResponsible AI and HCI \nIntersectional design for social justice \nParticipatory design and co-design \nSustainable AI and more-than-human-centred design \nUsefulness and usability of ethical design toolkits \nThe EU AI Act in design practice \nAI and interface design for transparency \nDesigning otherwise: on anarchist HCI \n  \nObjectives \nOn completion of this module, students should have developed facility in discussing and \ncritiquing the aims of their research, especially for an audience drawn from other academic \ndisciplines, including the following skills: \n \nTheoretical motivation and defence of a research question. \n\nConsideration of a research proposal from one or more alternative theoretical perspectives. \nPotential critique of the theoretical basis for a programme of research. \nCoursework \nIn advance of each seminar, all participants must read the essential reading - generally one \nlong, plus one or two short papers. Further reading is suggested for some seminars. Some \nweeks, instead of shorter papers, students will be asked to explore specific design tools or \nresources. \n \nBefore seminars 2-8, each student will submit a written commentary on the paper, either \ngenerated using a large language model (LLM), or written in the style of an LLM, supplemented \nby a short original observation. \n \nEach member of the class must select one of the 8 seminar themes as the subject for a more \nextended critical review essay. The critical review should be written as an academic research \nessay, not in LLM style. \n \nEach seminar will include an informal design exercise to be carried out in small groups. The \npurpose of these exercises is to reflect on a variety of practical design resources, by applying \nthose resources to concrete case studies. \n \nThroughout the course, students will be expected to keep a \"reflective diary\", briefly reporting \nthemes that arise in discussion, reflections on the design exercises, and ways in which the \nreadings relate to their own research interests. \n  \n \nPractical work \nThere is no practical work element in this course. \n \nAssessment \nWritten critical review of a single discussion text (20%) \nReflective diary submitted at the end of the module (60%) \nIn advance of the session, each student will submit a written commentary on the paper, either \ngenerated using a large language model (LLM), or written in the style of an LLM. A short original \nobservation should be added (20%) \nRecommended reading \nTo be assigned during the module, as discussed above. Most set readings will be available \neither from the ACM Digital Library, or from institutional repositories of the authors. \n \nFurther Information \nStudents from the MPhil in the Ethics of AI, Data and Algorithms, MSt in AI, and MPhil in Digital \nHumanities courses also attend the lectures for this module.\n \n\nTHEORY OF DEEP LEARNING \nPrerequisites: A strong background in calculus, probability theory and linear algebra, familiarity \nwith differential equations, optimization and information theory is required. Students need to \nhave studied Deep Neural Networks, familiarity with deep learning terminology (e.g. \narchitectures, benchmark problems) as well as deep learning frameworks (pytorch, jax or \nsimilar) is assumed \nMoodle, timetable \n \nAims \nThe objectives of this course is to expose you to one of the most active contemporary research \ndirections within machine learning: the theory of deep learning (DL). While the first wave of \nmodern DL has focussed on empirical breakthroughs and ever more complex techniques, the \nattention is now shifting to building a solid mathematical understanding of why these techniques \nwork so well in the first place. The purpose of this course is to review this recent progress \nthrough a mixture of reading group sessions and invited talks by leading researchers in the \ntopic, and prepare you to embark on a PhD in modern deep learning research. Compared to \ntypical, non-mathematical courses on deep learning, this advanced module will appeal to those \nwho have strong foundations in mathematics and theoretical computer science. In a way, this \ncourse is our answer to the question \u201cWhat should the world\u2019s best computer science students \nknow about deep learning in 2023?\u201d \n \nLearning Outcomes \nThis course should prepare the best students to start a PhD in the theory and mathematics of \ndeep learning, and to start formulating their own hypotheses in this space. You will be \nintroduced to a range of empirical and mathematical tools developed in recent years for the \nstudy of deep learning behaviour, and you will build an awareness of the main open questions \nand current lines of attack. At the end of the course you will: \n \nbe able to explain why classical learning theory is insufficient to describe the phenomenon of \ngeneralization in DL \nbe able to design and interpret empirical studies aimed at understanding generalization \nbe able to explain the role of overparameterization: be able to use deep linear models as a \nmodel to study implicit regularisation of gradient-based learning \nbe able to state PAC-Bayes and Information-theoretic bounds, and apply them to DL \nbe able to explain the connection between Gaussian processes and neural networks, and will \nbe able to study learning dynamics in the neural tangent kernel (NTK) regime. \nbe able to formulate your own hypotheses about DL and choose tools to prove/test them \nleverage your deeper theoretical understanding to produce more robust, rigorous and \nreproducible solutions to practical machine learning problems. \nSyllabus \nEach week we'll have two to four student-lead presentations about a research paper chosen \nfrom a reading list. The reading list loosely follows the weekly breakdown below (but we adapt it \neach year based as this is an active research area): \n \n\nWeek 1: Introduction to the topic \nWeek 2: Empirical Studies of Deep Learning Phenomena \nWeek 3: Interpolation Regime and \u201cDouble Descent\u201d Phenomena \nWeek 4: Implicit Regularization in Deep Linear Models \nWeek 5: Approximation Theory \nWeek 6: Networks in the Infinite Width Limit \nWeek 7: PAC-Bayes and Information Theoretic Bounds for SGD \nWeek 8: Discussion and Coursework Spotlight Session \n \nAssessment \nStudents will be assessed on the following basis: \n \n20% for presentation/content contributed to the module: Each student will have an opportunity \nto present one of the recommended papers during Weeks 1-7 (30 minute slot including Q&A). \nFor the presentation, students should aim to communicate the core ideas behind the paper, and \nclearly present the results, conclusions, and future directions. Where possible, students are \nencouraged to comment on how the work itself fits into broader research goals. \n10% for active participation (regular attendance and contribution to discussions during the Q&A \nsessions). \n70% for a group project report, with a word limit of 4000. Either (1) an original research \nproposal/report with a hypothesis, review of related literature, and ideally preliminary findings, \nor, (2) reproduction and ideally extension of an existing relevant paper. \nCoursework reports are marked in line with general ACS guidelines, reports receiving top marks \nwill have have demonstrable research value (contain an original research idea, extension of \nexisting work, or a thorough reproduction effort which is valuable to the research community). \nAdditionally, some projects will be suggested during the first weeks of the course, although \nstudents are encouraged to come up with their own ideas. Students may be required to \nparticipate in group projects, with groups of size 2-3 (the class groups will be separated). For \nany given project, individual contributions would be noted for assessment though a viva \ncomponent. \nRelationship with related modules \nThis course can be considered as an advanced follow-up to the Part IIB course on Deep Neural \nNetworks. That course introduces some high level concepts that this course significantly \nexpands on. \n \nThis module complements L48: Machine Learning in the Physical World and L46: Principles of \nMachine Learning Systems, which focus on applications and hardware/systems aspects of ML \nrespectively. \n \nRecommended reading \nPNAS Colloquium on the Science of Deep Learning \nMathematics of Machine Learning book by Marc Deisenroth, Aldo Faisal and Cheng Soon Ong. \nProbabilistic Machine Learning: An Introduction book by Kevin Murphy \nMatus Telgarsky's lecture notes on deep learning theory  \n\nThese are in addition to the papers which will be discussed in the lectures.\n \n\nUNDERSTANDING NETWORKED-SYSTEMS PERFORMANCE \nPrerequisites: A student must possess knowledge comparable to the undergraduate subjects on \nUnix Tools, C/C++ programming and Computer Networks. Optionally; a student will be \nadvantaged by doing an introduction to Computer Systems Modelling; such as the Part 2 \nComputer Systems Modelling subject as well as having a background in measurement \nmethodologies such as that of L50: Introduction to networking and systems measurements \nprovided in the ACS/Part III curriculum. \nMoodle, timetable \n \nAims \nThis is a practical course, actively building and extending software tools to observe the detailed \nbehaviour of transaction-oriented datacenter-like software. Students will observe and \nunderstand sources of user-facing tail latency, including that stemming \nfrom resource contention, cross-program interference, bad software locking, and simple design \nerrors. \nA 2-hour weekly hybrid, lecture/lab format permits students continuous monitored progress with \ncomplexity of tasks building naturally upon the previous weeks learning. \n \nObjectives \nUpon successful completion of the course, students will be able to: \n \nMake order-of-magnitude estimates of software, hardware, and I/O speeds \nMake valid measurements of actual software, hardware, and I/O speeds \nCreate observation facilities, including logs and dashboards, as part of a software system \ndesign \nCreate tracing facilities to fully observe the execution of complex software \nTime-align traces across multiple computers \nDisplay dense tracing information in meaningful ways \nReason about the sources of real-time and transaction delays, including cross-program \nresource interference, remote procedure call delays, and software locking surprises \nFix programs based on the above reasoning, making their response times faster and more \nrobust \n \nSyllabus \nMeasurement \n   Week 1 Intro, Measuring CPU time, rdtsc, Measuring memory access times, Measuring disks, \n   Week 2 gettimeofday, logs Measuring networks, Remote Procedure Calls, Multi-threads, locks \nObservation \n   Week 3 RPC, logs, displaying traces, interference \n   Week 4 Antagonist programs, Logging, dashboards, profiling. \nKUtrace \n   Week 5 Kernel patches, hello world, post-processing \n   Week 6 KUtrace multi-CPU time display \n   Week 7 Client-server KUtrace, with antagonists and interference \n\n   Week 8 Other trace mysteries \n \n \nAssessment \nThis is a Lab-based module; assessment is based upon reports covering guided laboratory work \nperformed each week. Assessment will be via two submissions: \n \n20% assignment deadline week 4 based upon Lab work from Weeks 1-4; word target 1000, limit \n2000 \n80% assignment deadline week 8 based upon lab work from weeks 5-8; word target 2000, limit \n4000 \nThe intent of the first assessment point is to provide rich feedback to students based upon a \n20% assignment permitting focussed and improved work to be executedfor the final assignment. \n \nAll work in this module is expected to be the effort of the student. Enough equipment is \nprovisioned to allow each student an independent set of apparatus. While classmembers may \nfind sharing operational experience valuable all assessment is based upon a students sole \nsubmission based upon. their own experiments and findings. \n \nReading Material \nCore text \n \nRichard L. Sites, Understanding Software Dynamics, Addison-Wesley Professional Computing \nSeries, 2022 \nFurther reading: papers and presentations that you might find interesting. None are required \nreading \u2013 they are well-written and/or informative. \n \nJohn K. Ousterhout et al., A trace-driven analysis of the UNIX 4.2 BSD file system ACM \nSIGOPS Operating Systems Review, Vol. 19, No. 5, Dec, 1985 \nhttps://dl.acm.org/doi/pdf/10.1145/323627.323631 \nLuiz Andr\u00b4e Barroso, Jimmy Clidaras, Urs H\u00a8olzle, The Datacenter as a Computer: An \nIntroduction to the Design of Warehouse-Scale Machines, Second Edition \nJohn L. Hennessy, David A. Patterson, Computer Architecture, A Quantitative Approach 5th \nEdition \nGeorge Varghese, Network Algorithmics. Morgan Kaufmann \nActually keeping datacenter software up and running \u2013 how Google runs production systems \nSite Reliability Engineering Edited by Betsy Beyer, Chris Jones, Jennifer Petoff and Niall \nRichard Murphy free PDF: https://landing.google.com/sre/book.html \nHow NOT to run datacenters (27 minute video, funny/sad) dotScale 2014 - Robert Kennedy - \nLife in the Trenches of healthcare.gov https://www.youtube.com/watch?v=GLQyj-kBRdo \nAn extended (optional) reading list will also be provided. \n \n"
    ],
    "text_C:\\Users\\miafo\\downloads\\curriculum-skills\\tests\\sample_pdfs\\University of Cambridge.pdf": [
        "Course Outlines \n \n1st Semester \n \nDATABASES \n \nAims \nThis course introduces basic concepts for database systems as seen from the perspective of \napplication designers. That is, the focus is on the abstractions supported by database \nmanagement systems and not on how those abstractions are implemented. \n \nThe database world is currently undergoing swift and dramatic transformations largely driven by \nInternet-oriented applications and services. Today many more options are available to database \napplication developers than in the past and so it is becoming increasingly difficult to sort fact \nfrom fiction. The course attempts to cut through the fog with a practical approach that \nemphasises engineering tradeoffs that underpin these recent developments and also guide our \nselection of \u201cthe right tool for the job.\u201d \n \nThis course covers three approaches. First, the traditional mainstay of the database industry -- \nthe relational approach -- is described with emphasis on eliminating logical redundancy in data. \nThen two representatives of recent trends are presented -- graph-oriented and \ndocument-oriented databases. The lectures are supported with two Help and Tick sessions, \nwhere students gain hands-on experience and guidance with the Assessed Exercises (ticks). \n \nLectures \nL1 Introduction. What is a database system? What is a data model? Typical DBMS \nconfigurations and variations (fast queries or reliable update). In-core, in secondary store or \ndistributed. \nL2 Conceptual modelling. Entities, relations, E/R diagrams and implementation-independent \nmodelling, weak entities, cardinality. \nL3+4 The relational database model. Implementing E/R models with relational tables. Relational \nalgebra and SQL. Basic query primitives. Update anomalies caused by redundancy. Minimising \nredundancy with normalised schemas. \nL5 Transactions. On-Line Transaction Processing. On-line Analytical Processing. Reliability, \nthroughput, normal forms, ACID, BASE, eventual consistency. \nL6 Documents and semi-structured data. The NoSQL, schema-free movement. XML/JSON. \nKey/value stores. Embracing data redundancy: representing data for fast, application-specific \naccess. Path queries (if time permits). \nL7 Further SQL. Multisets, NULL values, aggregates, transitive closure, expressibility (nested \nqueries and recursive SQL). \nL8 Graph databases. Optimised for processing enormous numbers of nodes and edges. \nImplementing E/R models in a graph-oriented database. Comparison of the presented models. \nObjectives \nAt the end of the course students should \n",
        " \nbe able to design entity-relationship diagrams to represent simple database application \nscenarios \nknow how to convert entity-relationship diagrams to relational- and graph-oriented \nimplementations \nunderstand the fundamental tradeoff between the ease of updating data and the response time \nof complex queries \nunderstand that no single data architecture can be used to meet all data management \nrequirements \nbe familiar with recent trends in the database area. \nRecommended reading \nLemahieu, W., Broucke, S. van den and Baesens, B. (2018) Principles of database \nmanagement. Cambridge University Press.\n \n",
        "DIGITAL ELECTRONICS \n \nAims \nThe aims of this course are to present the principles of combinational and sequential digital logic \ndesign and optimisation at a gate level. The use of n and p channel MOSFETs for building logic \ngates is also introduced. \n \nTopics \nIntroduction. Semiconductors to computers. Logic variables. Examples of simple logic. Logic \ngates. Boolean algebra. De Morgan\u2019s theorem. \nLogic minimisation. Truth tables and normal forms. Karnaugh maps. Quine-McCluskey method. \nBinary adders. Half adder, full adder, ripple carry adder, fast carry generation. \nCombinational logic design: further considerations. Multilevel logic. Gate propagation delay. An \nintroduction to timing diagrams. Hazards and hazard elimination. Other ways to implement \ncombinational logic. \nIntroduction to practical classes. Prototyping box. Breadboard and Dual in line (DIL) packages. \nWiring. \nSequential logic. Memory elements. RS latch. Transparent D latch. Master-slave D flip-flop. T \nand JK flip-flops. Setup and hold times. \nSequential logic. Counters: Ripple and synchronous. Shift registers. System timing - setup time \nconstraint, clock skew, metastability. \nSynchronous State Machines. Moore and Mealy finite state machines (FSMs). Reset and self \nstarting. State transition diagrams. Elimination of redundant states - row matching and state \nequivalence/implication table. \nFurther state machines. State assignment: sequential, sliding, shift register, one hot. \nImplementation of FSMs. \nIntroduction to microprocessors. Microarchitecture, fetch, register access, memory access, \nbranching, execution time. \nElectronics, Devices and Circuits. Current and voltage, conductors/insulators/semiconductors, \nresistance, basic circuit theory, the potential divider. Solving non-linear circuits. P-N junction \n(forward and reverse bias), N and p channel MOSFETs (operation and characteristics) and \nn-MOSFET logic, e.g., n-MOSFET inverter. Power consumption and switching time problems \nproblems in n-MOSFET logic. CMOS logic (NOT, NAND and NOR gates), logic families, noise \nmargin. \nObjectives \nAt the end of the course students should \n \nunderstand the relationships between combination logic and boolean algebra, and between \nsequential logic and finite state machines; \nbe able to design and minimise combinational logic; \nappreciate tradeoffs in complexity and speed of combinational designs; \nunderstand how state can be stored in a digital logic circuit; \nknow how to design a simple finite state machine from a specification and be able to implement \nthis in gates and edge triggered flip-flops; \n",
        "understand how to use MOSFETs to build digital logic circuits. \nRecommended reading \n* Harris, D.M. and Harris, S.L. (2013). Digital design and computer architecture. Morgan \nKaufmann (2nd ed.). The first edition is still relevant. \nKatz, R.H. (2004). Contemporary logic design. Benjamin/Cummings. The 1994 edition is more \nthan sufficient. \nHayes, J.P. (1993). Introduction to digital logic design. Addison-Wesley. \n \nBooks for reference: \n \nHorowitz, P. and Hill, W. (1989). The art of electronics. Cambridge University Press (2nd ed.) \n(more analog). \nWeste, N.H.E. and Harris, D. (2005). CMOS VLSI Design - a circuits and systems perspective. \nAddison-Wesley (3rd ed.). \nMead, C. and Conway, L. (1980). Introduction to VLSI systems. Addison-Wesley. \nCrowe, J. and Hayes-Gill, B. (1998). Introduction to digital electronics. Butterworth-Heinemann. \nGibson, J.R. (1992). Electronic logic circuits. Butterworth-Heinemann.\n \n",
        "DISCRETE MATHEMATICS \n \nAims \nThe course aims to introduce the mathematics of discrete structures, showing it as an essential \ntool for computer science that can be clever and beautiful. \n \nLectures \nProof [5 lectures]. \nProofs in practice and mathematical jargon. Mathematical statements: implication, bi-implication, \nuniversal quantification, conjunction, existential quantification, disjunction, negation. Logical \ndeduction: proof strategies and patterns, scratch work, logical equivalences. Proof by \ncontradiction. Divisibility and congruences. Fermat\u2019s Little Theorem. \n \nNumbers [5 lectures]. \nNumber systems: natural numbers, integers, rationals, modular integers. The Division Theorem \nand Algorithm. Modular arithmetic. Sets: membership and comprehension. The greatest \ncommon divisor, and Euclid\u2019s Algorithm and Theorem. The Extended Euclid\u2019s Algorithm and \nmultiplicative inverses in modular arithmetic. The Diffie-Hellman cryptographic method. \nMathematical induction: Binomial Theorem, Pascal\u2019s Triangle, Fundamental Theorem of \nArithmetic, Euclid\u2019s infinity of primes. \n \nSets [9 lectures]. \nExtensionality Axiom: subsets and supersets. Separation Principle: Russell\u2019s Paradox, the \nempty set. Powerset Axiom: the powerset Boolean algebra, Venn and Hasse diagrams. Pairing \nAxiom: singletons, ordered pairs, products. Union axiom: big unions, big intersections, disjoint \nunions. Relations: composition, matrices, directed graphs, preorders and partial orders. Partial \nand (total) functions. Bijections: sections and retractions. Equivalence relations and set \npartitions. Calculus of bijections: characteristic (or indicator) functions. Finite cardinality and \ncounting. Infinity axiom. Surjections. Enumerable and countable sets. Axiom of choice. \nInjections. Images: direct and inverse images. Replacement Axiom: set-indexed constructions. \nSet cardinality: Cantor-Schoeder-Bernstein Theorem, unbounded cardinality, diagonalisation, \nfixed-points. Foundation Axiom. \n \nFormal languages and automata [5 lectures]. \nIntroduction to inductive definitions using rules and proof by rule induction. Abstract syntax \ntrees. Regular expressions and their algebra. Finite automata and regular languages: Kleene\u2019s \ntheorem and the Pumping Lemma. \n \nObjectives \nOn completing the course, students should be able to \n \nprove and disprove mathematical statements using a variety of techniques; \napply the mathematical principle of induction; \nknow the basics of modular arithmetic and appreciate its role in cryptography; \n",
        "understand and use the language of set theory in applications to computer science; \ndefine sets inductively using rules and prove properties about them; \nconvert between regular expressions and finite automata; \nuse the Pumping Lemma to prove that a language is not regular. \nRecommended reading \nBiggs, N.L. (2002). Discrete mathematics. Oxford University Press (Second Edition). \nDavenport, H. (2008). The higher arithmetic: an introduction to the theory of numbers. \nCambridge University Press. \nHammack, R. (2013). Book of proof. Privately published (Second edition). Available at: \nhttp://www.people.vcu.edu/ rhammack/BookOfProof/index.html \nHouston, K. (2009). How to think like a mathematician: a companion to undergraduate \nmathematics. Cambridge University Press. \nKozen, D.C. (1997). Automata and computability. Springer. \nLehman, E.; Leighton, F.T.; Meyer, A.R. (2014). Mathematics for computer science. Available \non-line. \nVelleman, D.J. (2006). How to prove it: a structured approach. Cambridge University Press \n(Second Edition).\n \n",
        "FOUNDATIONS OF COMPUTER SCIENCE \nAims \nThe main aim of this course is to present the basic principles of programming. As the \nintroductory course of the Computer Science Tripos, it caters for students from all backgrounds. \nTo those who have had no programming experience, it will be comprehensible; to those \nexperienced in languages such as C, it will attempt to correct any bad habits that they have \nlearnt. \n \nA further aim is to introduce the principles of data structures and algorithms. The course will \nemphasise the algorithmic side of programming, focusing on problem-solving rather than on \nhardware-level bits and bytes. Accordingly it will present basic algorithms for sorting, searching, \netc., and discuss their efficiency using O-notation. Worked examples (such as polynomial \narithmetic) will demonstrate how algorithmic ideas can be used to build efficient applications. \n \nThe course will use a functional language (OCaml). OCaml is particularly appropriate for \ninexperienced programmers, since a faulty program cannot crash and OCaml\u2019s unobtrusive type \nsystem captures many program faults before execution. The course will present the elements of \nfunctional programming, such as curried and higher-order functions. But it will also introduce \ntraditional (procedural) programming, such as assignments, arrays and references. \n \nLectures \nIntroduction to Programming. The role of abstraction and representation. Introduction to integer \nand floating-point arithmetic. Declaring functions. Decisions and booleans. Example: integer \nexponentiation. \nRecursion and Efficiency. Examples: Exponentiation and summing integers. Iteration versus \nrecursion. Examples of growth rates. Dominance and O-Notation. The costs of some \nrepresentative functions. Cost estimation. \nLists. Basic list operations. Append. Na\u00efve versus efficient functions for length and reverse. \nStrings. \nMore on lists. The utilities take and drop. Pattern-matching: zip, unzip. A word on polymorphism. \nThe \u201cmaking change\u201d example. \nSorting. A random number generator. Insertion sort, mergesort, quicksort. Their efficiency. \nDatatypes and trees. Pattern-matching and case expressions. Exceptions. Binary tree traversal \n(conversion to lists): preorder, inorder, postorder. \nDictionaries and functional arrays. Functional arrays. Dictionaries: association lists (slow) versus \nbinary search trees. Problems with unbalanced trees. \nFunctions as values. Nameless functions. Currying. The \u201capply to all\u201d functional, map. \nExamples: The predicate functionals filter and exists. \nSequences, or lazy lists. Non-strict functions such as IF. Call-by-need versus call-by-name. Lazy \nlists. Their implementation in OCaml. Applications, for example Newton-Raphson square roots. \nQueues and search strategies. Depth-first search and its limitations. Breadth-first search (BFS). \nImplementing BFS using lists. An efficient representation of queues. Importance of efficient data \nrepresentation. \n",
        "Elements of procedural programming. Address versus contents. Assignment versus binding. \nOwn variables. Arrays, mutable or not. Introduction to linked lists. \nObjectives \nAt the end of the course, students should \n \nbe able to write simple OCaml programs; \nunderstand the fundamentals of using a data structure to represent some mathematical \nabstraction; \nbe able to estimate the efficiency of simple algorithms, using the notions of average-case, \nworse-case and amortised costs; \nknow the comparative advantages of insertion sort, quick sort and merge sort; \nunderstand binary search and binary search trees; \nknow how to use currying and higher-order functions; \nunderstand how OCaml combines imperative and functional programming in a single language. \nRecommended reading \nJohn Whitington. OCaml from the Very Beginning. (http://ocaml-book.com). \n \nOkasaki, C. (1998). Purely functional data structures. Cambridge University Press.\n \n",
        "HARDWARE PRACTICAL CLASSES \nThe Hardware Practical Classes accompany the Digital Electronics series of lectures. The aim \nof the Practical Classes is to enable students to get hands-on experience of designing, building, \nand testing and debugging of digital electronic circuits. The labs will take place in the Intel Lab. \nlocated in the William Gates Building (WGB). \n \nThe Digital Electronics lecture series occupies 12 lectures in the first 4 weeks of the Michaelmas \nTerm, while the Practical Classes occupy the latter 6 weeks of the Michaelmas Term and the \nfirst 6 weeks of the Lent Term. If required, extra sessions will be available for any students \nneeding to 'catch-up' on missed sessions or to complete any remaining practical work. \n \nThe Practical Classes take the form of 4 workshops, specifically: \n \nWorkshop 1 \u2013 Electronic Die; \nWorkshop 2 \u2013 Shaft Position Encoder; \nWorkshop 3 \u2013 Debouncing a Switch; \nWorkshop 4 \u2013 Framestore for an LED Array. \nIn general, the workshops require some preparatory work to be done prior to the practical \nsession. These tasks are highlighted at the beginning of each Worksheet. Typically this involves \npreparing a design that you will then build, test and modify during the practical class. Note that \ninsufficient preparation prior to the practical classes may compromise effective use of your time \nin the Intel lab. \n \nIn the Practical Classes you will usually work on your own, and you are expected to complete \none Workshop in each of your scheduled sessions. Demonstrators are available during the \nsessions to assist you with any queries or problems you may have. \n \nImportant: Remember to get your work ticked.\n \n",
        "INTRODUCTION TO GRAPHICS \nAims \nTo introduce the necessary background, the basic algorithms, and the applications of computer \ngraphics and graphics hardware. \n \nLectures \nBackground. What is an image? Resolution and quantisation. Storage of images in memory. [1 \nlecture] \nRendering. Perspective. Reflection of light from surfaces and shading. Geometric models. Ray \ntracing. [2 lectures] \nGraphics pipeline. Polygonal mesh models. Transformations using matrices in 2D and 3D. \nHomogeneous coordinates. Projection: orthographic and perspective. [1 lecture] \nGraphics hardware and modern OpenGL. GPU rendering. GPU frameworks and APIs. Vertex \nprocessing. Rasterisation. Fragment processing. Working with meshes and textures. Z-buffer. \nDouble-buffering and frame synchronization. [2 lectures] \n  \nHuman vision, colour and tone mapping. Perception of colour. Tone mapping. Colour spaces. [2 \nlectures] \nObjectives \nBy the end of the course students should be able to: \n \nunderstand and apply in practice basic concepts of ray-tracing: ray-object intersection, \nreflections, refraction, shadow rays, distributed ray-tracing, direct and indirect illumination; \ndescribe and explain the following algorithms: z-buffer, texture mapping, double buffering, \nmip-map, and normal-mapping; \nuse matrices and homogeneous coordinates to represent and perform 2D and 3D \ntransformations; understand and use 3D to 2D projection, the viewing volume, and 3D clipping; \nimplement OpenGL code for rendering of polygonal objects, control camera and lighting, work \nwith vertex and fragment shaders; \ndescribe a number of colour spaces and their relative merits. \nexplain the need for tone mapping and colour processing in rendering pipeline. \nRecommended reading \n* Shirley, P. and Marschner, S. (2009). Fundamentals of Computer Graphics. CRC Press (3rd \ned.). \n \nFoley, J.D., van Dam, A., Feiner, S.K. and Hughes, J.F. (1990). Computer graphics: principles \nand practice. Addison-Wesley (2nd ed.). \n \nKessenich, J.M., Sellers, G. and Shreiner, D (2016). OpenGL Programming Guide: The Official \nGuide to Learning OpenGL, Version 4.5 with SPIR-V, [seventh edition and later]\n \n",
        "OBJECT-ORIENTED PROGRAMMING \n \nAims \nThe goal of this course is to provide students with an understanding of Object-Oriented \nProgramming. Concepts are demonstrated in multiple languages, but the primary language is \nJava. \n \nLecture syllabus \nTypes, Objects and Classes Moving from functional to imperative. Functions, methods. Control \nflow. values, variables and types. Primitive Types. Classes as custom types. Objects vs \nClasses. Class definition, constructors. Static data and methods. \nDesigning Classes Identifying classes. UML class diagrams. Modularity. Encapsulation/data \nhiding. Immutability. Access modifiers. Parameterised types (Generics). \nPointers, References and Memory Pointers and references. Reference types in Java. The call \nstack. The heap. Iteration and recursion. Pass-by-value and pass-by-reference. \nInheritance Inheritance. Casting. Shadowing. Overloading. Overriding. Abstract Methods and \nClasses. \nPolymorphism and Multiple Inheritance Polymorphism in ML and Java. Multiple inheritance. \nInterfaces in Java. \nLifecycle of an Object Constructors and chaining. Destructors. Finalizers. Garbage Collection: \nreference counting, tracing. \nJava Collections and Object Comparison Java Collection interface. Key classes. Collections \nclass. Iteration options and the use of Iterator. Comparing primitives and objects. Operator \noverloading. \nError Handling Types of errors. Limitations of return values. Deferred error handling. Exceptions. \nCustom exceptions. Checked vs unchecked. Inappropriate use of exceptions. Assertions. \nDesignLanguage evolution Need for languages to evolve. Generics in Java. Type erasure. \nIntroduction to Java 8: Lambda functions, functions as values, method references, streams. \nDesign Patterns Introduction to design patterns. Open-closed principle. Examples of Singleton, \nDecorator, State, Composite, Strategy, Observer. [2 lectures] \nObjectives \nAt the end of the course students should \n \nProvide an overview of key OOP concepts that are transferable in mainstream programming \nlanguages; \nGain an understanding of software development approaches adopted in the industry including \nmaintainability, testing, and software design patterns; \nIncrease programming familiarity with Java; \nRecommended reading \n1. Real-World Software Development: A Project-Driven Guide to Fundamentals in Java [Urma et \nal.] \n2. Modern Java in Action (2nd edition) [Urma et al.]. \n3. Java How to Program: early objects [Deitel et al.]\n \n",
        "OCAML PRACTICAL CLASSES \n \nThe Role of Tickers \nA ticking session is a short (5 minute) one-to-one session with a ticker conducted on Thursday \nafternoons. The role of the ticker is twofold: \n \nto check you understand your solution (and didn\u2019t just have some lucky guesses or copy code \nfrom elsewhere) \nto give you general feedback on ways to improve your code. \nTo those ends a Ticker will typically ask you questions related to the core material of the tick and \ndiscuss your code directly. \n \nDeadline Extensions \nDeadline extensions can be granted for illness or similar reasons. To obtain an extension please \nemail Jon Ludlam in the first instance, clearly stating your justification for an extension and \nCCing your Director of Studies, who will need to support your request.\n \n",
        "SCIENTIFIC COMPUTING \n \nAims \nThis course is a hands-on introduction to using computers to investigate scientific models and \ndata. \n \nSyllabus \nPython notebooks. Overview of the Python programming language. Use of notebooks for \nscientific computing. \nNumerical computation. Writing fast vectorized code in numpy. Optimization and fitting. \nSimulation. \nWorking with data. Data import. Common ways to summarize and plot data, for univariate and \nmultivariate analysis. \nObjectives \nAt the end of the course students should \n \nbe able to import data, plot it, and summarize it appropriately \nbe able to write fast vectorized code for scientific / data work\n \n",
        "2nd Semester \nALGORITHMS 1 \nAims \nThe aim of this course is to provide an introduction to computer algorithms and data structures, \nwith an emphasis on foundational material. \n \nLectures \nSorting. Review of complexity and O-notation. Trivial sorting algorithms of quadratic complexity. \nReview of merge sort and quicksort, understanding their memory behaviour on statically \nallocated arrays. Heapsort. Stability. Other sorting methods including sorting in linear time. \nMedian and order statistics. [Ref: CLRS3 chapters 1, 2, 3, 6, 7, 8, 9] [about 4 lectures] \nStrategies for algorithm design. Dynamic programming, divide and conquer, greedy algorithms \nand other useful paradigms. [Ref: CLRS3 chapters 4, 15, 16] [about 3 lectures] \nData structures. Elementary data structures: pointers, objects, stacks, queues, lists, trees. \nBinary search trees. Red-black trees. B-trees. Hash tables. Priority queues and heaps. [Ref: \nCLRS3 chapters 6, 10, 11, 12, 13, 18] [about 5 lectures]. \nObjectives \nAt the end of the course students should: \n \nhave a thorough understanding of several classical algorithms and data structures; \nbe able to analyse the space and time efficiency of most algorithms; \nhave a good understanding of how a smart choice of data structures may be used to increase \nthe efficiency of particular algorithms; \nbe able to design new algorithms or modify existing ones for new applications and reason about \nthe efficiency of the result. \nRecommended reading \n* Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest and Clifford Stein Introduction to \nAlgorithms, Fourth Edition ISBN 9780262046305 Published: April 5, 2022. \n \nSedgewick, R., Wayne, K. (2011). Algorithms. Addison-Wesley. ISBN 978-0-321-57351-3. \n \nKleinberg, J. and Tardos, \u00c9. (2006). Algorithm design. Addison-Wesley. ISBN \n978-0-321-29535-4. \n \nKnuth, D.A. (2011). The Art of Computer Programming. Addison-Wesley. ISBN \n978-0-321-75104-1.\n \n",
        "ALGORITHMS 2 \n \nAims \nThe aim of this course is to provide an introduction to computer algorithms and data structures, \nwith an emphasis on foundational material. \n \nLectures \nGraphs and path-finding algorithms. Graph representations. Breadth-first and depth-first search. \nSingle-source shortest paths: Bellman-Ford and Dijkstra algorithms. All-pairs shortest paths: \ndynamic programming and Johnson\u2019s algorithms. [About 4 lectures] \nGraphs and subgraphs. Maximum flow: Ford-Fulkerson method, Max-flow min-cut theorem. \nMatchings in bipartite graphs. Minimum spanning tree: Kruskal and Prim algorithms. Topological \nsort. [About 4 lectures] \nAdvanced data structures. Binomial heap. Amortized analysis: aggregate analysis, potential \nmethod. Fibonacci heaps. Disjoint sets. [About 4 lectures] \nObjectives \nAt the end of the course students should: \n \nhave a thorough understanding of several classical algorithms and data structures; \nbe able to analyse the space and time efficiency of most algorithms; \nhave a good understanding of how a smart choice of data structures may be used to increase \nthe efficiency of particular algorithms; \nbe able to design new algorithms or modify existing ones for new applications and reason about \nthe efficiency of the result. \nRecommended reading \n* Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest and Clifford Stein Introduction to \nAlgorithms, Fourth Edition ISBN 9780262046305 Published: April 5, 2022. \n \nSedgewick, R., Wayne, K. (2011). Algorithms. Addison-Wesley. ISBN 978-0-321-57351-3. \n \nKleinberg, J. and Tardos, \u00c9. (2006). Algorithm design. Addison-Wesley. ISBN \n978-0-321-29535-4. \n \nKnuth, D.A. (2011). The Art of Computer Programming. Addison-Wesley. ISBN \n978-0-321-75104-1.\n \n",
        "MACHINE LEARNING AND REAL-WORLD DATA \n \nAims \nThis course introduces students to machine learning algorithms as used in real-world \napplications, and to the experimental methodology necessary to perform statistical analysis of \nlarge-scale data from unpredictable processes. Students will perform 3 extended practicals, as \nfollows: \n \nStatistical classification: Determining movie review sentiment using Naive Bayes; \nSequence Analysis: Hidden Markov Modelling and its application to a task from biology \n(predicting protein interactions with a cell membrane); \nAnalysis of social networks, including detection of cliques and central nodes. \nSyllabus \nTopic One: Statistical Classification \nIntroduction to sentiment classification. \nNaive Bayes parameter estimation. \nStatistical laws of language. \nStatistical tests for classification tasks. \nCross-validation and test sets. \nUncertainty and human agreement. \nTopic Two: Sequence Analysis  \nHidden Markov Models (HMM) and HMM training. \nThe Viterbi algorithm. \nUsing an HMM in a biological application. \nTopic Three: Social Networks  \nProperties of networks: Degree, Diameter. \nBetweenness Centrality. \nClustering using betweenness centrality. \nObjectives \nBy the end of the course students should be able to: \n \nunderstand and program two simple supervised machine learning algorithms; \nuse these algorithms in statistically valid experiments, including the design of baselines, \nevaluation metrics, statistical testing of results, and provision against overtraining; \nvisualise the connectivity and centrality in large networks; \nuse clustering (i.e., a type of unsupervised machine learning) for detection of cliques in \nunstructured networks. \nRecommended reading \nJurafsky, D. and Martin, J. (2008). Speech and language processing. Prentice Hall. \n \nEasley, D. and Kleinberg, J. (2010). Networks, crowds, and markets: reasoning about a highly \nconnected world. Cambridge University Press.\n \n",
        "OPERATING SYSTEMS \n \nAims \nThe overall aim of this course is to provide a general understanding of the structure and key \nfunctions of the operating system. Case studies will be used to illustrate and reinforce \nfundamental concepts. \n \nLectures \nIntroduction to operating systems. Elementary computer organisaton. Abstract view of an \noperating system. Key concepts: layering, multiplexing, performance, caching, buffering, policies \nversus mechanisms. OS evolution: multi-programming, time-sharing. Booting. [1 lecture] \nProtection. Dual-mode operation. Protecting CPU, memory, I/O, storage. Kernels, micro-kernels, \nand modules. System calls. Virtual machines and containers. Subjects and objects. \nAuthentication. Access matrix: ACLs and capabilities. Covert channels. [1 lecture] \nProcesses. Job/process concepts. Process lifecycle. Process management. Context switching. \nInter-process communication. [1 lecture] \nScheduling. CPU-I/O burst cycle. Scheduling basics: preemption and non-preemption. \nScheduling algorithms: FCFS, SJF, SRTF, Priority, and Round Robin. Multilevel scheduling. [2 \nlectures] \nMemory management. Processes in memory. Logical versus physical addresses. Partitions: \nstatic versus dynamic, free space management, external fragmentation. Segmented memory. \nPaged memory: concepts, internal fragmentation, page tables. Demand paging/segmentation. \nPage replacement strategies: FIFO, OPT, and LRU with approximations including NRU, LFU, \nMFU, MRU. Working set schemes. Thrashing. [3 lectures] \nI/O subsystem. General structure. Polled mode versus interrupt-driven I/O. Programmed I/O \n(PIO) versus Direct Memory Access (DMA). Application I/O interface: block and character \ndevices, buffering, blocking, non-blocking, asynchronous, and vectored I/O. Other issues: \ncaching, scheduling, spooling, performance. [1 lecture] \nFile management. File concept. Directory and storage services. File names and metadata. \nDirectory name-space: hierarchies, DAGs, hard and soft links. File operations. Access control. \nExistence and concurrency control. [1 lecture] \nUnix case study. History. General structure. Unix file system: file abstraction, directories, mount \npoints, implementation details. Processes: memory image, lifecycle, start of day. The shell: \nbasic operation, commands, standard I/O, redirection, pipes, signals. Character and block I/O. \nProcess scheduling. [2 lectures] \nObjectives \nAt the end of the course students should be able to: \n \ndescribe the general structure and purpose of an operating system; \nexplain the concepts of process, address space, and file; \ncompare and contrast various CPU scheduling algorithms; \ncompare and contrast mechanisms involved in memory and storage management; \ncompare and contrast polled, interrupt-driven, and DMA-based access to I/O devices. \nRecommended reading \n",
        "* Silberschatz, A., Galvin, P.C., and Gagne, G. (2018). Operating systems concepts. Wiley (10th \ned.). Prior editions, at least back to 8th ed., should also be acceptable. \nAnderson, T. and Dahlin, M. (2014). Operating Systems: Principles and Practice. Recursive \nBooks (2nd ed.). \nBacon, J. and Harris, T. (2003). Operating systems. Addison-Wesley (3rd ed.). Currently \nappears out-of-print. \nLeffler, S. (1989). The design and implementation of the 4.3BSD Unix operating system. \nAddison-Wesley. \nMcKusick, M.K., Neville-Neil, G.N. and Watson, R.N.M. (2014) The Design and Implementation \nof the FreeBSD Operating System. Pearson Education. (2nd ed.).\n \n",
        "INTERACTION DESIGN \nAims \nThe aim of this course is to provide an introduction to interaction design, with an emphasis on \nunderstanding and experiencing the user-centred design process, from conducting user \nresearch and requirements development to implementation and evaluation, while understanding \nthe background to human-computer interaction. \n \nLectures \nCourse overview and user research methods. Introduction to the course and practicals. \nUser-Centred Design. User research methods. \nKeeping usera in mind. User research data analysis. Identifying users and stakeholders. \nRepresenting user goals and activities. Identifying and establishing requirements. \nDesign and prototyping. Methods for exploring the design space. Prototyping and different kinds \nof prototypes. \nVisual and interaction design. Memory, perception, attention, and their implications for \ninteraction design. Modalities of interaction, interaction design patterns, information architecture, \nand their implications for interaction design. \nEvaluation. Practical methods for evaluating designs. Evaluation methods without users. \nEvaluation methods with users. \nCase studies from industry and research. Guest lectures (the topics of these lectures are \nsubject to change). \nObjectives \nBy the end of the course students should \n \nhave a thorough understanding of the user-centred design process and be able to apply it to \ninteraction design; \nbe able to design new user interfaces that are informed by principles of good design, and the \nprinciples of human visual perception, cognition and communication; \nbe able to prototype and implement interactive user interfaces with a strong emphasis on users, \nusability and appearance; \nbe able to evaluate existing or new user interfaces using multiple techniques; \nbe able to compare and contrast different design techniques and to critique their applicability to \nnew domains. \nRecommended reading \n* Preece, J., Rogers, Y. and Sharp, H. (2015). Interaction design. Wiley (4th ed.).\n \n",
        "INTRODUCTION TO PROBABILITY \n \nAims \nThis course provides an elementary introduction to probability and statistics with applications. \nProbability theory and the related field of statistical inference provide the foundations for \nanalysing and making sense of data. The focus of this course is to introduce the language and \ncore concepts of probability theory. The course will also cover some applications of statistical \ninference and algorithms in order to equip students with the ability to represent problems using \nthese concepts and analyse the data within these principles. \n \nLectures \nPart 1 - Introduction to Probability \n \nIntroduction. Counting/Combinatorics (revision), Probability Space, Axioms, Union Bound. \nConditional probability. Conditional Probabilities and Independence, Bayes\u2019Theorem, Partition \nTheorem \nPart 2 - Discrete Random Variables \n \nRandom variables. Definition of a Random Variable, Probability Mass Function, Cumulative \nDistribution, Expectation. \nProbability distributions. Definition and Properties of Expectation, Variance, different ways of \ncomputing them, Examples of important Distributions (Bernoulli, Binomial, Geometric, Poisson), \nPrimer on Continuous Distributions including Normal and Exponential Distributions. \nMultivariate distributions. Multiple Random Variables, Joint and Marginal Distributions, \nIndependence of Random Variables, Covariance. \nPart 3 - Moments and Limit Theorems \n \nIntroduction. Law of Average, Useful inequalities (Markov and Chebyshef), Weak Law of Large \nNumbers (including Proof using Chebyshef\u2019s inequality), Examples. \nMoments and Central Limit Theorem. Introduction to Moments of Random Variables, Central \nLimit Theorem (Proof using Moment Generating functions), Example. \nPart 4 - Applications/Statistics \n \nStatistics. Classical Parameter Estimation (Maximum-Likelihood-Estimation), bias, sample \nmean, sample variance), Examples (Collision-Sampling, Estimating Population Size). \nAlgorithms. Online Algorithms (Secretary Problem, Odd\u2019s Algorithm). \nObjectives \nAt the end of the course students should \n \nunderstand the basic principles of probability spaces and random variables \nbe able to formulate problems using concepts from probability theory and compute or estimate \nprobabilities \nbe familiar with more advanced concepts such as moments, limit theorems and applications \nsuch as parameter estimation \n",
        "Recommended reading \n* Ross, S.M. (2014). A First course in probability. Pearson (9th ed.). \n \nBertsekas, D.P. and Tsitsiklis, J.N. (2008). Introduction to probability. Athena Scientific. \n \nGrimmett, G. and Welsh, D. (2014). Probability: an Introduction. Oxford University Press (2nd \ned.). \n \nDekking, F.M., et. al. (2005) A modern introduction to probability and statistics. Springer.\n",
        "SOFTWARE AND SECURITY ENGINEERING \n \nAims \nThis course aims to introduce students to software and security engineering, and in particular to \nthe problems of building large systems, safety-critical systems and systems that must withstand \nattack by capable opponents. Case histories of failure are used to illustrate what can go wrong, \nwhile current software and security engineering practice is studied as a guide to how failures \ncan be avoided. \n \nLectures \n1. What is a security policy or a safety case? Definitions and examples; one-way flows for both \nconfidentiality and safety properties; separation of duties. Top-down and bottom-up analysis \nmethods. What architecture can do, versus benefits of decoupling policy from mechanism. \n \n2. Examples of safety and security policies. Safety and security usability; the pyramid of harms. \nPredicting and mitigating user errors. The prevention of fraud and error in accounting systems; \nthe safety usability of medical devices. \n \n3. Attitudes to risk: expected  utility, prospect theory, framing, status quo bias. Authority, \nconformity and gender; mental models, affordances and defaults. The characteristics of human \nmemory; forgetting passwords versus guessing them. \n \n4. Security protocols; how to enforce policy using  structured human interaction, cryptography or \nboth. Middleperson attacks.The role of verification and its limitations. \n \n5. Attacks on TLS, from rogue CAs through side channels to Heartbleed. Other types of \nsoftware bugs: syntactic, timing, concurrency, code injection, buffer overflows. Defensive \nprogramming: secure coding, contracts. Fuzzing. \n \n6. The software crisis. Examples of large-scale project failure, such as the London Ambulance \nService system and the NHS National Programme for IT. Intrinsic difficulties with complex \nsoftware. \n \n7. Software engineering as the management of complexity. The software life cycle; requirements \nanalysis methods; modular design; the role of prototyping; the waterfall, spiral and agile models. \n \n8. The economics of software as a Service (SaaS); the impact SaaS has on software \nengineering. Continuous integration, release engineering, behavioural analytics and experiment \nframeworks, rearchitecting systems while in operation. \n \n9. Critical systems: safety as an emergent system property. Examples of catastrophic failure: \nfrom Therac-25 to the Boeing 737Max. The problems of managing redundancy. The overall \nprocess of safety engineering. \n \n",
        "10. Managing the development of critical systems: tools and methods, individual versus group \nproductivity, economics of testing and agile development, measuring outcomes versus process, \nthe technical and human aspects of management, post-market surveillance and coordinated \ndisclosure. The sustainability of products with software components. \n \nAt the end of the course students should know how writing programs with tough assurance \ntargets, in large teams, or both, differs from the programming exercises they have engaged in \nso far. They should understand the different models of software development described in the \ncourse as well as the value of various development and management tools. They should \nunderstand the development life cycle and its basic economics. They should understand the \nvarious types of bugs, vulnerabilities and hazards, how to find them, and how to avoid \nintroducing them. Finally, they should be prepared for the organizational aspects of their Part IB \ngroup project. \n \nRecommended reading \nAnderson, R. (Third Edition 2020). Security engineering (Part 1 and Chapters 27-28). Wiley. \nAvailable at: http://www.cl.cam.ac.uk/users/rja14/book.html\n \n",
        "3rd Semester \nCONCURRENT AND DISTRIBUTED SYSTEMS \n \nAims \nThis course considers two closely related topics, Concurrent Systems and Distributed Systems, \nover 16 lectures. The aim of the first half of the course is to introduce concurrency control \nconcepts and their implications for system design and implementation. The aims of the latter \nhalf of the course are to study the fundamental characteristics of distributed systems, including \ntheir models and architectures; the implications for software design; some of the techniques that \nhave been used to build them; and the resulting details of good distributed algorithms and \napplications. \n \nLectures: Concurrency \nIntroduction to concurrency, threads, and mutual exclusion. Introduction to concurrent systems; \nthreads; interleaving; preemption; parallelism; execution orderings; processes and threads; \nkernel vs. user threads; M:N threads; atomicity; mutual exclusion; and mutual exclusion locks \n(mutexes). \nAutomata Composition. Synchronous and asynchronous parallelism; sequential consistency; \nrendezvous. Safety, liveness and deadlock; the Dining Philosophers; Hardware foundations for \natomicity: test-and-set, load-linked/store-conditional and fence instructions. Lamport bakery \nalgorithm. \nCommon design patterns: semaphores, producer-consumer, and MRSW. Locks and invariants; \nsemaphores; condition synchronisation; N-resource allocation; two-party and generalised \nproducer-consumer; Multi-Reader, Single-Write (MRSW) locks. \nCCR, monitors, and concurrency in practice. Conditional critical regions (CCR); monitors; \ncondition variables; signal-wait vs. signal-continue semantics; concurrency in practice (kernels, \npthreads, Java, Cilk, OpenMP). \nDeadlock and liveness guarantees Offline vs. online; model checking; resource allocation \ngraphs; lock order checking; deadlock prevention, avoidance, detection, and recovery; livelock; \npriority inversion; auto parallelisation. \nConcurrency without shared data; transactions. Active objects; message passing; tuple spaces; \nCSP; and actor models. Composite operations; transactions; ACID; isolation; and serialisability. \nFurther transactions History graphs; good and bad schedules; isolation vs. strict isolation; \n2-phase locking; rollback; timestamp ordering (TSO); and optimistic concurrency control (OCC). \nCrash recovery, lock-free programming, and transactional memory. Write-ahead logging, \ncheckpoints, and recovery. Lock-free programming. Hardware and software transactional \nmemories. \n  \n \nLectures: Distributed Systems \nIntroduction to distributed systems; RPC. Avantages and challenges of distributed systems; \nunbounded delay and partial failure; network protocols; transparency; client-server systems; \nremote procedure call (RPC); marshalling; interface definition languages (IDLs). \n",
        "System models and faults. Synchronous, partially synchronous, and asynchronous network \nmodels; crash-stop, crash-recovery, and Byzantine faults; failures, faults, and fault tolerance; \ntwo generals problem. \nTime, clocks, and ordering of events. Physical clocks; leap seconds; UTC; clock synchronisation \nand drift; Network Time Protocol (NTP). Causality; happens-before relation. \nLogical time; Lamport clocks; vector clocks. Broadcast (FIFO, causal, total order); gossip \nprotocols. \nReplication. Quorums; idempotence; replica consistency; read-after-write consistency. State \nmachine replication; leader-based replication. \nConsensus and total order broadcast. FLP result; leader election; the Raft consensus algorithm. \nReplica consistency. Two-phase commit; relationship between 2PC and consensus; \nlinearizability; ABD algorithm; eventual consistency; CAP theorem. \nCase studies. Conflict-free Replicated Data Types (CRDTs); collaborative text editing. Google's \nSpanner; TrueTime. \nObjectives \nAt the end of Concurrent Systems portion of the course, students should: \n \nunderstand the need for concurrency control in operating systems and applications, both mutual \nexclusion and condition synchronisation; \nunderstand how multi-threading can be supported and the implications of different approaches; \nbe familiar with the support offered by various programming languages for concurrency control \nand be able to judge the scope, performance implications and possible applications of the \nvarious approaches; \nbe aware that dynamic resource allocation can lead to deadlock; \nunderstand the concept of transaction; the properties of transactions, how they can be \nimplemented, and how their performance can be optimised based on optimistic assumptions; \nunderstand how the persistence properties of transactions are addressed through logging; and \nhave a high-level understanding of the evolution of software use of concurrency in the \noperating-system kernel case study. \nAt the end of the Distributed Systems portion of the course, students should: \n \nunderstand the difference between shared-memory concurrency and distributed systems; \nunderstand the fundamental properties of distributed systems and their implications for system \ndesign; \nunderstand notions of time, including logical clocks, vector clocks, and physical time \nsynchronisation; \nbe familiar with various approaches to data and service replication, as well as the concept of \nreplica consistency; \nunderstand the effects of large scale on the provision of fundamental services and the tradeoffs \narising from scale; \nappreciate the implications of individual node and network communications failures on \ndistributed computation; \nbe aware of a variety of programming models and abstractions for distributed systems, such as \nRPC, middleware, and total order broadcast; \n",
        "be familiar with a range of distributed algorithms, such as consensus, causal broadcast, and \ntwo-phase commit. \nRecommended reading \nModern Operating Systems (free PDF available online) by Andrew S Tanenbaum, Herbert Bos \nJava Concurrency in Practice' (2006 but still highly relevant) by Brian Goetz \nKleppmann, M. (2017). Designing data-intensive applications. O\u2019Reilly. \nTanenbaum, A.S. and van Steen, M. (2017). Distributed systems, 3rd edition. available online. \nCachin, C., Guerraoui, R. and Rodrigues, L. (2011) Introduction to Reliable and Secure \nDistributed Programming. Springer (2nd edition).\n \n",
        "DATA SCIENCE \n \nAims \nThis course introduces fundamental tools for describing and reasoning about data. There are \ntwo themes: designing probability models to describe systems; and drawing conclusions based \non data generated by such systems. \n \nLectures \nSpecifying and fitting probability models. Random variables. Maximum likelihood estimation. \nGenerative and supervised models. Goodness of fit. \nFeature spaces. Vector spaces, bases, inner products, projection. Linear models. Model fitting \nas projection. Design of features. \nHandling probability models. Handling pdf and cdf. Bayes\u2019s rule. Monte Carlo estimation. \nEmpirical distribution. \nInference. Bayesianism. Frequentist confidence intervals, hypothesis testing. Bootstrap \nresampling. \nRandom processes. Markov chains. Stationarity, and drift analysis. Processes with memory. \nLearning a random process. \nObjectives \nAt the end of the course students should \n \nbe able to formulate basic probabilistic models, including discrete time Markov chains and linear \nmodels \nbe familiar with common random variables and their uses, and with the use of empirical \ndistributions rather than formulae \nunderstand different types of inference about noisy data, including model fitting, hypothesis \ntesting, and making predictions \nunderstand the fundamental properties of inner product spaces and orthonormal systems, and \ntheir application to modelling \nRecommended reading \n* F.M. Dekking, C. Kraaikamp, H.P. Lopuha\u00e4, L.E. Meester (2005). A modern introduction to \nprobability and statistics: understanding why and how. Springer. \n \nS.M. Ross (2002). Probability models for computer science. Harcourt / Academic Press. \n \nM. Mitzenmacher and E. Upfal (2005). Probability and computing: randomized algorithms and \nprobabilistic analysis. Cambridge University Press.\n \n",
        "ECAD AND ARCHITECTURE PRACTICAL CLASSES \nAims \nThe aims of this course are to enable students to apply the concepts learned in the Computer \nDesign course. In particular a web based tutor is used to introduce the SystemVerilog hardware \ndescription language, while the remaining practical classes will then allow students to implement \nthe design of components in this language. \n \nPractical Classes \nWeb tutor The first class uses a web based tutor to rapidly teach the SystemVerilog language. \nFPGA design flow Test driven hardware development for FPGA including an embedded \nprocessor and peripherals [3 classes] \nEmbedded system implementation Embedded system implementation on FPGA [3-4 classes] \nObjectives \nGain experience in electronic computer aided design (ECAD) through learning a design-flow for \nfield programmable gate arrays (FPGAs). \nLearn how to interface to peripherals like a touch screen. \nLearn how to debug hardware and software systems in simulation. \nUnderstand how to construct and program a heterogeneous embedded system. \nRecommended reading \n* Harris, D.M. and Harris, S.L. (2007). Digital design and computer architecture: from gates to \nprocessors. Morgan Kaufmann. \n \nPointers to sources of more specialist information are included on the associated course web \npage. \n \nIntroduction \nThe ECAD and Architecture Laboratory sessions are a companion to the Introduction to \nComputer Architecture course. The objective is to provide experience of hardware design for \nFPGA including use of a small embedded processor. It covers hardware design in \nSystemVerilog, embedded software design in RISC-V assembly and C, and use of FPGA tools. \n \nPrerequisites \nThis course assumes familiarity with the material from Part IA Digital Electronics, although we \nwill use automated tools to perform many of the steps you might have done there by hand (for \nexample, logic minimisation). \n \nWe will be using a Linux command-line environment. We provide scripts and guidance on what \nyou need, however you may find it useful to review Unix Tools, in particular basic navigation \nsuch as ls, cd, mkdir, cp, mv. We'll be using the GCC compiler and Makefiles, described in parts \n31-32. We will also be using git for revision control and submission of work for ticking - you \nshould review at least parts 23-25 and 29. \n \nPracticalities \n",
        "The course runs over the 8 weeks of Michaelmas term. The course has been designed so you \ncan do most of the work at home or at any time you wish. You should be able to run all the \nnecessary tools on your own machine, through the use of virtual machines and Docker \ncontainers. We have a number of routes to do this in case some of them aren't suitable for the \nmachine you have. \n \nWe're running the course in a hybrid mode this year. We will provide online help sessions via \nMicrosoft Teams as well as in-person lab sessions on Fridays and Tuesdays 2-5pm during term. \n \nThe core components of the course, being the RISC-V architecture and software and ticked \nexercise, can be done entirely online, without needing access to hardware. They can be \nundertaken with any of the platforms we support, including MCS Linux (on the Intel Lab \nworkstations and remotely) if your own machine isn't suitable. We expect everyone to complete \nthese parts. \n \nThe rest of the course is optional. The hardware simulation and FPGA compilation can be done \non your machine without access to hardware, if you are able to run the virtual machine. \n \nThe parts that require access to an FPGA board will need the ability to run the virtual machine \nand you be able to collect and return an FPGA board from the Department. That process will be \ndescribed when you come to those parts. There is an optional starred tick available for \ncompletion of this part, which will need assessment by a demonstrator in person. \n \nWe'll do our best to support all the different platforms and variations, but please bear with us - \nit's quite possible we'll come across a problem we haven't seen before! \n \nWe have provided four routes you can use different tools to complete the course. Some routes \nnecessarily exclude some material but this material is optional. \n \nAssessment \nIn a similar way to other practical courses, this course carries one 'tick'. Everyone should expect \nto receive this tick, and those who do not should expect to lose marks on their final exams. \n \nThis year we have adopted the 'chime' autoticker used by Further Java. You will check out the \ninitial files as a git repository from the chime server, commit your work, and push the repository \nback to submit it. You can then press a button to run tests against your code and the autoticker \nwill tell you whether you have passed. Additionally you may be selected for a (real/video) \ninterview with a ticker to discuss your code and understanding of the material. \n \nTicking procedures will only be available during weeks 1-8 of Michaelmas term. The deadline for \nsubmitting Tick 1 is 5pm on the Tuesday of week 6 (19 November 2024). After passing the \nautoticker you may be asked for a tick interview with a demonstrator (online or in person). You \ncan gain Tick 1* during the timetabled lab sessions, assuming demonstrators are available. \n \n",
        "If you do not submit a successful Tick 1 to the autoticker by 5pm on the Tuesday of week 6 you \ncan self-certify an extension. If you need more time than this you will need to ask your Directory \nof Studies to organise a further extension. The hard deadline by which all ticks must be \ncompleted is noon on Friday 31 January 2025 (the Head of Department Notices is the definitive \ndocument). Extensions beyond this date due to exceptional circumstances require a formal \napplication from your college to the Examiners. \n \nScheduling \nWhile the full course contains 8 exercises, there is not a tight binding to doing one exercise per \nweek. Students should expect to spend about three hours per week on the course, however it is \nrecommended that you make a start on the next exercise if you have time in hand. \n \nDemonstrator help will be focused on the Tuesday and Friday 2-5pm slots during term the lab \nwould normally operate in, so you may find it helpful to work in these times as that will provide \nthe quickest response to questions. \n \nCollaboration \nWhen we have done these labs in person, we have often found they worked well when doing \nthem collaboratively. While your work must be your own, students can work together and help \neach other, and demonstrators contribute advice and experience with the tools. \n \nFor the timetabled sessions, which are Tuesdays and Fridays 2-5pm UK time during term, there \nwill be demonstrators available in the lab and at the same time we'll use the ECAD group on \nMicrosoft Teams. With this we can provide online audio/video help, screensharing and (for the \nWindows and possibly Mac Teams apps) optional remote control of your development \nenvironment. You will be added to the Team in week 1 - if you believe you have been missed \nplease post in the Moodle forum. In Teams there are a number of Helpdesk Channels (A-C) - if \nyou need help during lab times, post in Help Centre and a demonstrator can ask you to go to a \nspecific channel. There are also Student Breakout Channels (D-F) which are available for \nstudents to chat amongst themselves. \n \nFor help outside of timetabled sessions we've set up a Moodle forum where you can post \nquestions - we'll keep an eye on this at other times, but students are encouraged to use it to \nsupport each other. \n \nStudents are of course free to use other platforms to help each other. If you find anything useful \nthat we might use in future, please let us know! \n \nIf you are having difficulty accessing both Moodle and Teams, please email theo.markettos at \ncl.cam.ac.uk. \n \nFeedback \nWe revise the course each year, which may cause new bugs in the code or the notes. The \ncreators (Theo Markettos and Simon Moore) appreciate constructive feedback. \n",
        " \nCredits \nThis course was written by Theo Markettos and Simon Moore, with portions developed by \nRobert Eady, Jonas Fiala, Paul Fox, Vlad Gavrila, Brian Jones and Roy Spliet. We would like to \nthank Altera, Intel and Terasic for funding and other contributions.\n \n",
        "ECONOMICS, LAW AND ETHICS \n \nAims \nThis course aims to give students an introduction to some basic concepts in economics, law and \nethics. \n \nLectures \nClassical economics and consumer theory. Prices and markets; Pareto efficiency; preferences; \nutility; supply and demand; the marginalist revolution; elasticity; the welfare theorems; \ntransaction costs. \nInformation economics. The discriminating monopolist; marginal costs; effects of technology on \nsupply and demand; competition and information; lock in; real and virtual networks; Metcalfe\u2019s \nlaw; the dominant firm model; price discrimination; bundling; income distribution. \nMarket failure and behavioural economics. Market failure: the business cycle; recession and \ntechnology; tragedy of the commons; externalities; monopoly rents; asymmetric information: the \nmarket for lemons; adverse selection; moral hazard; signalling. Behavioural economics: \nbounded rationality, heuristics and biases; nudge theory; the power of defaults; agency effects. \nAuction theory and game theory. Auction theory: types of auctions; strategic equivalence; the \nrevenue equivalence theorem; the winner\u2019s curse; problems with real auctions; mechanism \ndesign and the combinatorial auction; applicability of auction mechanisms in computer science; \nadvertising auctions. Game theory: the choice between cooperation and conflict; strategic \nforms; dominant strategy equilibrium; Nash equilibrium; the prisoners\u2019 dilemma; evolution of \nstrategies; stag hunt; volunteer\u2019s dilemma; chicken; iterated games; hawk-dove; application to \ncomputer science. \nPrinciples of law. Criminal and civil law; contract law; choice of law and jurisdiction; arbitration; \ntort; negligence; defamation; intellectual property rights. \nLaw and the Internet. Computer evidence; the General Data Protection Regulation; UK laws that \nspecifically affect the Internet; e-commerce regulations; privacy and electronic communications. \nPhilosophies of ethics. Authority, intuitionist, egoist and deontological theories; utilitarian and \nRawlsian models; morality; insights from evolutionary psychology, neurology, and experimental \nethics; professional codes of ethics; research ethics. \nContemporary ethical issues. The Internet and social policy; current debates on privacy, \nsurveillance, and censorship; responsible vulnerability disclosure; algorithmic bias; predictive \npolicing; gamification and engagement; targeted political advertising; environmental impacts. \nObjectives \nOn completion of this course, students should be able to: \n \nReflect on and discuss professional, economic, social, environmental, moral and ethical issues \nrelating to computer science \nDefine and explain economic and legal terminology and arguments \nApply the philosophies and theories covered to computer science problems and scenarios \nReflect on the main constraints that market, legislation and ethics place on firms dealing in \ninformation goods and services \nRecommended reading \n",
        "* Shapiro, C. & Varian, H. (1998). Information rules. Harvard Business School Press. \nHare, S. (2022). Technology is not neutral: A short guide to technology ethics. London \nPublishing Partnership. \n \nFurther reading: \nSmith, A. (1776). An inquiry into the nature and causes of the wealth of nations, available at    \nhttp://www.econlib.org/library/Smith/smWN.html \nThaler, R.H. (2016). Misbehaving. Penguin. \nGalbraith, J.K. (1991). A history of economics. Penguin. \nPoundstone, W. (1992). Prisoner\u2019s dilemma. Anchor Books. \nPinker, S (2011). The Better Angels of our Nature. Penguin. \nAnderson, R. (2008). Security engineering (Chapter 7). Wiley. \nVarian, H. (1999). Intermediate microeconomics - a modern approach. Norton. \nNuffield Council on Bioethics (2015) The collection, linking and use of data in biomedical \nresearch and health care.\n \n",
        "FURTHER GRAPHICS \n \nAims \nThe course introduces fundamental concepts of modern graphics pipelines. \n \nLectures \nThe order and content of lectures is provisional and subject to change. \n \nGeometry Representations. Parametric surfaces, implicit surfaces, meshes, point-set surfaces, \ngeometry processing pipeline. [1 lecture] \nDiscrete Differential Geometry. Surface normal and curvature, Laplace-Beltrami operator, heat \ndiffusion. [1 lecture] \nGeometry Processing.  Parametrization, filtering, 3D capture. [1 lecture] \nAnimation I. Animation types, animation pipeline, rigging/skinning, character animation. [1 \nlecture] \nAnimation II. Blending transformations, pose-space animation, controllers. [1 lecture] \nThe Rendering Equation. Radiosity, reflection models, BRDFs, local vs. global illumination. [1 \nlecture] \nDistributed Ray Tracing. Quadrature, importance sampling, recursive ray tracing. [1 lecture] \nInverse Rendering. Differentiable rendering, inverse problems. [1 lecture] \nObjectives \nOn completing the course, students should be able to \n \nunderstand and use fundamental 3D geometry/scene representations and operations; \nlearn animation techniques and controls; \nlearn how light transport is modeled and simulated in rendering; \nunderstand how graphics and rendering can be used to solve computer perception problems. \nRecommended reading \nStudents should expect to refer to one or more of these books, but should not find it necessary \nto purchase any of them. \n \nShirley, P. and Marschner, S. (2009). Fundamentals of Computer Graphics. CRC Press (3rd \ned.). \nWatt, A. (2000). 3D Computer Graphics. Addison-Wesley (3rd ed). \nHughes, van Dam, McGuire, Skalar, Foley, Feiner and Akeley (2013). Computer Graphics: \nPrinciples and Practice. Addison-Wesley (3rd edition) \nAkenine-M\u00f6ller, et. al. (2018). Real-time rendering. CRC Press (4th ed.).\n \n",
        "FURTHER JAVA \n \nAims \nThe goal of this course is to provide students with the ability to understand the advanced \nprogramming features available in the Java programming language, completing the coverage of \nthe language started in the Programming in Java course. The course is designed to \naccommodate students with diverse programming backgrounds; consequently Java is taught \nfrom first principles in a practical class setting where students can work at their own pace from a \ncourse handbook. \n \nObjectives \n \nAt the end of the course students should \n \nunderstand different mechanisms for communication between distributed applications and be \nable to evaluate their trade-offs; \nbe able to use Java generics and annotations to improve software usability, readability and \nsafety; \nunderstand and be able to exploit the Java class-loading mechansim; \nunderstand and be able to use concurrency control correctly; \nbe able to implement a vector clock algorithm and the happens-before relation. \nRecommended reading \n* Goetz, B. (2006). Java concurrency in practice. Addison-Wesley. \nGosling, J., Joy, B., Steele, G., Bracha, G. and Buckley, A. (2014). The Java language \nspecification, Java SE 8 Edition. Addison-Wesley. \nhttp://docs.oracle.com/javase/specs/jls/se8/html/\n \n",
        "GROUP PROJECTS \nExample Risk Report \nThe risk report should focus on the risks to your project succeeding on time (and not, say, on \ngeneral health and safety points). It is only 50 words. Example: \n \n\"Identified risks: \n * Training data requires access authorisation that has not been approved, and we don\u2019t know if \nclient can do this \n * Two group members have unreliable network connections and may miss meetings \n * One group member is in timezone UTC+8, and will have to attend meetings late at night \n * Bug reports on Github for a library we plan to use suggest maintenance updates will be \nnecessary for recent Android release\"\n \n",
        "INTRODUCTION TO COMPUTER ARCHITECTURE \n \nAims \nThe aims of this course are to introduce a hardware description language (SystemVerilog) and \ncomputer architecture concepts in order to design computer systems. The parallel ECAD+Arch \npractical classes will allow students to apply the concepts taught in lectures. \n \nLectures \nPart 1 - Gates to processors  \n \nTechnology trends and design challenges. Current technology, technology trends, ECAD trends, \nchallenges. [1 lecture] \nDigital system design. Practicalities of mapping SystemVerilog descriptions of hardware \n(including a processor) onto an FPGA board. Tips and pitfalls when generating larger modular \ndesigns. [1 lecture] \nEight great ideas in computer architecture. [1 lecture] \nReduced instruction set computers and RISC-V. Introduction to the RISC-V processor design. [1 \nlecture] \nExecutable and synthesisable models. [1 lecture] \nPipelining. [2 lectures] \nMemory hierarchy and caching. Caching, etc. [1 lecture] \nSupport for operating systems. Memory protection, exceptions, interrupts, etc. [1 lecture] \nOther instruction set architectures. CISC, stack, accumulator. [1 lecture] \nPart 2  \n \nOverview of Systems-on-Chip (SoCs) and DRAM. [1 lecture] High-level SoCs, DRAM storage \nand accessing. \nMulticore Processors. [2 lectures] Communication, cache coherence, barriers and \nsynchronisation primitives. \nGraphics processing units (GPUs) [2 lectures] Basic GPU architecture and programming. \nFuture Directions [1 lecture] Where is computer architecture heading? \nObjectives \nAt the end of the course students should \n \nbe able to read assembler given a guide to the instruction set and be able to write short pieces \nof assembler if given an instruction set or asked to invent an instruction set; \nunderstand the differences between RISC and CISC assembler; \nunderstand what facilities a processor provides to support operating systems, from memory \nmanagement to software interrupts; \nunderstand memory hierarchy including different cache structures and coherency needed for \nmulticore systems; \nunderstand how to implement a processor in SystemVerilog; \nappreciate the use of pipelining in processor design; \nhave an appreciation of control structures used in processor design; \n",
        "have an appreciation of system-on-chips and their components; \nunderstand how DRAM stores data; \nunderstand how multicore processors communicate; \nunderstand how GPUs work and have an appreciation of how to program them. \nRecommended reading \n* Patterson, D.A. and Hennessy, J.L. (2017). Computer organization and design: The \nhardware/software interface RISC-V edition. Morgan Kaufmann. ISBN 978-0-12-812275-4. \n \nRecommended further reading: \n \nHarris, D.M. and Harris, S.L. (2012). Digital design and computer architecture. Morgan \nKaufmann. ISBN 978-0-12-394424-5. \nHennessy, J. and Patterson, D. (2006). Computer architecture: a quantitative approach. Elsevier \n(4th ed.). ISBN 978-0-12-370490-0. (Older versions of the book are also still generally relevant.) \n \nPointers to sources of more specialist information are included in the lecture notes and on the \nassociated course web page\n \n",
        "PROGRAMMING IN C AND C++ \n \nAims \nThis course aims \n \nto provide a solid introduction to programming in C and to provide an overview of the principles \nand constraints that affect the way in which the C programming language has been designed \nand is used; \nto introduce the key additional features of C++. \n  \n \nLectures \nIntroduction to the C language. Background and goals of C. Types, expressions, control flow \nand strings. \n(continued) Functions. Multiple compilation units. Scope. Segments. Incremental compilation. \nPreprocessor. \n(continued) Pointers and pointer arithmetic. Function pointers. Structures and Unions. \n(continued) Const and volatile qualifiers. Typedefs. Standard input/output. Heap allocation. \nMiscellaneous Features, Hints and Tips. \nC semantics and tools. Undefined vs implementation-defined behaviour. Buffer and integer \noverflows. ASan, MSan, UBsan, Valgrind checkers. \nMemory allocation, data structures and aliasing. Malloc/free, tree and DAG examples and their \ndeallocation using a memory arena. \nFurther memory management. Reference Counting and Garbage Collection \nMemory hierarchy and cache optimization. Data structure layouts. Intrusive lists. Array-of-structs \nvs struct-of-arrays representations. Loop blocking. \nDebugging. Using print statements, assertions and an interactive debugger. Unit testing and \nregression. \nIntroduction to C++. Goals of C++. Differences between C and C++. References versus \npointers. Overloaded functions. \nObjects in C++. Classes and structs. Destructors. Resource Acquisition is Initialisation. Operator \noverloading. Virtual functions. Casts. Multiple inheritance. Virtual base classes. \nOther C++ concepts. Templates and meta-programming. \nObjectives \nAt the end of the course students should \n \nbe able to read and write C programs; \nunderstand the interaction between C programs and the host operating system; \nbe familiar with the structure of C program execution in machine memory; \nunderstand the potential dangers of writing programs in C; \nunderstand the object model and main additional features of C++. \nRecommended reading \n* Kernighan, B.W. and Ritchie, D.M. (1988). The C programming language. Prentice Hall (2nd \ned.).\n \n",
        "SEMANTICS OF PROGRAMMING LANGUAGES \n \nAims \nThe aim of this course is to introduce the structural, operational approach to programming \nlanguage semantics. It will show how to specify the meaning of typical programming language \nconstructs, in the context of language design, and how to reason formally about semantic \nproperties of programs. \n \nLectures \nIntroduction. Transition systems. The idea of structural operational semantics. Transition \nsemantics of a simple imperative language. Language design options. [2 lectures] \nTypes. Introduction to formal type systems. Typing for the simple imperative language. \nStatements of desirable properties. [2 lectures] \nInduction. Review of mathematical induction. Abstract syntax trees and structural induction. \nRule-based inductive definitions and proofs. Proofs of type safety properties. [2 lectures] \nFunctions. Call-by-name and call-by-value function application, semantics and typing. Local \nrecursive definitions. [2 lectures] \nData. Semantics and typing for products, sums, records, references. [1 lecture] \nSubtyping. Record subtyping and simple object encoding. [1 lecture] \nSemantic equivalence. Semantic equivalence of phrases in a simple imperative language, \nincluding the congruence property. Examples of equivalence and non-equivalence. [1 lecture] \nConcurrency. Shared variable interleaving. Semantics for simple mutexes; a serializability \nproperty. [1 lecture] \nObjectives \nAt the end of the course students should \n \nbe familiar with rule-based presentations of the operational semantics and type systems for \nsome simple imperative, functional and interactive program constructs; \nbe able to prove properties of an operational semantics using various forms of induction \n(mathematical, structural, and rule-based); \nbe familiar with some operationally-based notions of semantic equivalence of program phrases \nand their basic properties. \nRecommended reading \n* Pierce, B.C. (2002). Types and programming languages. MIT Press. \nHennessy, M. (1990). The semantics of programming languages. Wiley. Out of print, but \navailable on the web at \nhttp://www.cs.tcd.ie/matthew.hennessy/splexternal2015/resources/sembookWiley.pdf \nWinskel, G. (1993). The formal semantics of programming languages. MIT Press.\n \n",
        "UNIX TOOLS \n \nAims \nThis video-lecture course gives students who have already basic Unix/Linux experience some \nadditional practical software-engineering knowledge: how to use the shell and related tools as \nan efficient working environment, how to automate routine tasks, and how version-control and \nautomated-build tools can help to avoid confusion and accidents, especially when working in \nteams. These are essential skills, both in industrial software development and student projects. \n \nLectures \nUnix concepts. Brief review of Unix history and design philosophy, documentation, terminals, \ninter-process communication mechanisms and conventions, shell, command-line arguments, \nenvironment variables, file descriptors. \nShell concepts. Program invocation, redirection, pipes, file-system navigation, argument \nexpansion, quoting, job control, signals, process groups, variables, locale, history and alias \nfunctions, security considerations. \nScripting. Plain-text formats, executables, #!, shell control structures and functions. Startup \nscripts. \nText, file and networking tools. sed, grep, chmod, find, ssh, rsync, tar, zip, etc. \nVersion control. diff, patch, RCS, Subversion, git. \nSoftware development tools. C compiler, linker, debugger, make. \nPerl. Introduction to a powerful scripting and text-manipulation language. [2 lectures] \nObjectives \nAt the end of the course students should \n \nbe confident in performing routine user tasks on a POSIX system, understand command-line \nuser-interface conventions and know how to find more detailed documentation; \nappreciate how simple tools can be combined to perform a large variety of tasks; \nbe familiar with the most common tools, file formats and configuration practices; \nbe able to understand, write, and maintain shell scripts and makefiles; \nappreciate how using a version-control system and fully automated build processes help to \nmaintain reproducibility and audit trails during software development; \nknow enough about basic development tools to be able to install, modify and debug C source \ncode; \nhave understood the main concepts of, and gained initial experience in, writing Perl scripts \n(excluding the facilities for object-oriented programming). \nRecommended reading \nRobbins, A. (2005). Unix in a nutshell. O\u2019Reilly (4th ed.). \nSchwartz, R.L., Foy, B.D. and Phoenix, T. (2011). Learning Perl. O\u2019Reilly (6th ed.).\n \n",
        "4th Semester \n \nCOMPILER CONSTRUCTION \n \nAims \nThis course aims to cover the main concepts associated with implementing compilers for \nprogramming languages. We use a running example called SLANG (a Small LANGuage) \ninspired by the languages described in 1B Semantics. A toy compiler (written in ML) is provided, \nand students are encouraged to extend it in various ways. \n \nLectures \nOverview of compiler structure The spectrum of interpreters and compilers; compile-time and \nrun-time. Compilation as a sequence of translations from higher-level to lower-level intermediate \nlanguages, where each translation preserves semantics. The structure of a simple compiler: \nlexical analysis and syntax analysis, type checking, intermediate representations, optimisations, \ncode generation. Overview of run-time data structures: stack and heap. Virtual machines. [1 \nlecture] \nLexical analysis and syntax analysis. Lexical analysis based on regular expressions and finite \nstate automata. Using LEX-tools. How does LEX work? Parsing based on context-free \ngrammars and push-down automata. Grammar ambiguity, left- and right-associativity and \noperator precedence. Using YACC-like tools. How does YACC work? LL(k) and LR(k) parsing \ntheory. [3 lectures] \nCompiler Correctness Recursive functions can be transformed into iterative functions using the \nContinuation-Passing Style (CPS) transformation. CPS applied to a (recursive) SLANG \ninterpreter to derive, in a step-by-step manner, a correct stack-based compiler. [3 lectures] \nData structures, procedures/functions Representing tuples, arrays, references. Procedures and \nfunctions: calling conventions, nested structure, non-local variables. Functions as first-class \nvalues represented as closures. Simple optimisations: inline expansion, constant folding, \nelimination of tail recursion, peephole optimisation. [5 lectures] \nAdvanced topics Run-time memory management (garbage collection). Static and dynamic \nlinking. Objects and inheritance; implementation of method dispatch. Try-catch exception \nmechanisms. Compiling a compiler via bootstrapping. [4 lectures] \nObjectives \nAt the end of the course students should understand the overall structure of a compiler, and will \nknow significant details of a number of important techniques commonly used. They will be \naware of the way in which language features raise challenges for compiler builders. \n \nRecommended reading \n* Aho, A.V., Sethi, R. and Ullman, J.D. (2007). Compilers: principles, techniques and tools. \nAddison-Wesley (2nd ed.). \nMogensen, T. \u00c6. (2011). Introduction to compiler design. Springer. http://www.diku.dk/ \ntorbenm/Basics.\n \n",
        "COMPUTATION THEORY \n \nAims \nThe aim of this course is to introduce several apparently different formalisations of the informal \nnotion of algorithm; to show that they are equivalent; and to use them to demonstrate that there \nare uncomputable functions and algorithmically undecidable problems. \n \nLectures \nIntroduction: algorithmically undecidable problems. Decision problems. The informal notion of \nalgorithm, or effective procedure. Examples of algorithmically undecidable problems. [1 lecture] \nRegister machines. Definition and examples; graphical notation. Register machine computable \nfunctions. Doing arithmetic with register machines. [1 lecture] \nUniversal register machine. Natural number encoding of pairs and lists. Coding register machine \nprograms as numbers. Specification and implementation of a universal register machine. [2 \nlectures] \nUndecidability of the halting problem. Statement and proof. Example of an uncomputable partial \nfunction. Decidable sets of numbers; examples of undecidable sets of numbers. [1 lecture] \nTuring machines. Informal description. Definition and examples. Turing computable functions. \nEquivalence of register machine computability and Turing computability. The Church-Turing \nThesis. [2 lectures] \nPrimitive and partial recursive functions. Definition and examples. Existence of a recursive, but \nnot primitive recursive function. A partial function is partial recursive if and only if it is \ncomputable. [2 lectures] \nLambda-Calculus. Alpha and beta conversion. Normalization. Encoding data. Writing recursive \nfunctions in the lambda-calculus. The relationship between computable functions and \nlambda-definable functions. [3 lectures] \nObjectives \nAt the end of the course students should \n \nbe familiar with the register machine, Turing machine and lambda-calculus models of \ncomputability; \nunderstand the notion of coding programs as data, and of a universal machine; \nbe able to use diagonalisation to prove the undecidability of the Halting Problem; \nunderstand the mathematical notion of partial recursive function and its relationship to \ncomputability. \nRecommended reading \n* Hopcroft, J.E., Motwani, R. and Ullman, J.D. (2001). Introduction to automata theory, \nlanguages, and computation. Addison-Wesley (2nd ed.). \n* Hindley, J.R. and Seldin, J.P. (2008). Lambda-calculus and combinators, an introduction. \nCambridge University Press (2nd ed.). \nCutland, N.J. (1980). Computability: an introduction to recursive function theory. Cambridge \nUniversity Press. \nDavis, M.D., Sigal, R. and Weyuker, E.J. (1994). Computability, complexity and languages. \nAcademic Press (2nd ed.). \n",
        "Sudkamp, T.A. (2005). Languages and machines. Addison-Wesley (3rd ed.).\n \n",
        "COMPUTER NETWORKING \n \nAims \nThe aim of this course is to introduce key concepts and principles of computer networks. The \ncourse will use a top-down approach to study the Internet and its protocol stack. Instances of \narchitecture, protocol, application-examples will include email, web and media-streaming. We \nwill cover communications services (e.g., TCP/IP) required to support such network \napplications. The implementation and deployment of communications services in practical \nnetworks: including wired and wireless LAN environments, will be followed by a discussion of \nissues of network-management. Throughout the course, the Internet\u2019s architecture and \nprotocols will be used as the primary examples to illustrate the fundamental principles of \ncomputer networking. \n \nLectures \nIntroduction. Overview of networking using the Internet as an example. LANs and WANs. OSI \nreference model, Internet TCP/IP Protocol Stack. Circuit-switching, packet-switching, Internet \nstructure, networking delays and packet loss. [3 lectures] \nLink layer and local area networks. Link layer services, error detection and correction, Multiple \nAccess Protocols, link layer addressing, Ethernet, hubs and switches, Point-to-Point Protocol. [2 \nlectures] \nWireless and mobile networks. Wireless links and network characteristics, Wi-Fi: IEEE 802.11 \nwireless LANs. [1 lecture] \nNetwork layer addressing. Network layer services, IP, IP addressing, IPv4, DHCP, NAT, ICMP, \nIPv6. [3 lectures] \nNetwork layer routing. Routing and forwarding, routing algorithms, routing in the Internet, \nmulticast. [3 lectures] \nTransport layer. Service models, multiplexing/demultiplexing, connection-less transport (UDP), \nprinciples of reliable data transfer, connection-oriented transport (TCP), TCP congestion control, \nTCP variants. [6 lectures] \nApplication layer. Client/server paradigm, WWW, HTTP, Domain Name System, P2P. [1.5 \nlectures] \nMultimedia networking. Networked multimedia applications, multimedia delivery requirements, \nmultimedia protocols (SIP), content distribution networks. [0.5 lecture] \nObjectives \nAt the end of the course students should \n \nbe able to analyse a communication system by separating out the different functions provided \nby the network; \nunderstand that there are fundamental limits to any communications system; \nunderstand the general principles behind multiplexing, addressing, routing, reliable transmission \nand other stateful protocols as well as specific examples of each; \nunderstand what FEC is; \nbe able to compare communications systems in how they solve similar problems; \n",
        "have an informed view of both the internal workings of the Internet and of a number of common \nInternet applications and protocols. \nRecommended reading \n* Peterson, L.L. and Davie, B.S. (2011). Computer networks: a systems approach. Morgan \nKaufmann (5th ed.). ISBN 9780123850591 \nKurose, J.F. and Ross, K.W. (2009). Computer networking: a top-down approach. \nAddison-Wesley (5th ed.). \nComer, D. and Stevens, D. (2005). Internetworking with TCP-IP, vol. 1 and 2. Prentice Hall (5th \ned.). \nStevens, W.R., Fenner, B. and Rudoff, A.M. (2003). UNIX network programming, Vol.I: The \nsockets networking API. Prentice Hall (3rd ed.).\n \n",
        "FURTHER HUMAN\u2013COMPUTER INTERACTION \n \nAims \nThis aim of this course is to provide an introduction to the theoretical foundations of Human \nComputer Interaction, and an understanding of how these can be applied to the design of \ncomplex technologies. \n \nLectures \nTheory driven approaches to HCI. What is a theory in HCI? Why take a theory driven approach \nto HCI? \nDesign of visual displays. Segmentation and variables of the display plane. Modes of \ncorrespondence. \nGoal-oriented interaction. Using cognitive theories of planning, learning and understanding to \nunderstand user behaviour, and what they find hard. \nDesigning smart systems. Using statistical methods to anticipate user needs and actions with \nBayesian strategies. \nDesigning efficient systems. Measuring and optimising human performance through quantitative \nexperimental methods. \nDesigning meaningful systems. Qualitative research methods to understand social context and \nrequirements of user experience. \nEvaluating interactive system designs. Approaches to evaluation in systems research and \nengineering, including Part II Projects. \nDesigning complex systems. Worked case studies of applying the theories to a hard HCI \nproblem. Research directions in HCI. \nObjectives \nAt the end of the course students should be able to apply theories of human performance and \ncognition to system design, including selection of appropriate techniques to analyse, observe \nand improve the usability of a wide range of technologies. \n \nRecommended reading \n* Preece, J., Sharp, H. and Rogers, Y. (2015). Interaction design: beyond human-computer \ninteraction. Wiley (Currently in 4th edition, but earlier editions will suffice). \n \nFurther reading: \n \nCarroll, J.M. (ed.) (2003). HCI models, theories and frameworks: toward a multi-disciplinary \nscience. Morgan Kaufmann.\n \n",
        "LOGIC AND PROOF \n \nAims \nThis course will teach logic, especially the predicate calculus. It will present the basic principles \nand definitions, then describe a variety of different formalisms and algorithms that can be used \nto solve problems in logic. Putting logic into the context of Computer Science, the course will \nshow how the programming language Prolog arises from the automatic proof method known as \nresolution. It will introduce topics that are important in mechanical verification, such as binary \ndecision diagrams (BDDs), SAT solvers and modal logic. \n \nLectures \nIntroduction to logic. Schematic statements. Interpretations and validity. Logical consequence. \nInference. \nPropositional logic. Basic syntax and semantics. Equivalences. Normal forms. Tautology \nchecking using CNF. \nThe sequent calculus. A simple (Hilbert-style) proof system. Natural deduction systems. \nSequent calculus rules. Sample proofs. \nFirst order logic. Basic syntax. Quantifiers. Semantics (truth definition). \nFormal reasoning in FOL. Free versus bound variables. Substitution. Equivalences for \nquantifiers. Sequent calculus rules. Examples. \nClausal proof methods. Clause form. A SAT-solving procedure. The resolution rule. Examples. \nRefinements. \nSkolem functions, Unification and Herbrand\u2019s theorem. Prenex normal form. Skolemisation. \nMost general unifiers. A unification algorithm. Herbrand models and their properties. \nResolution theorem-proving and Prolog. Binary resolution. Factorisation. Example of Prolog \nexecution. Proof by model elimination. \nSatisfiability Modulo Theories. Decision problems and procedures. How SMT solvers work. \nBinary decision diagrams. General concepts. Fast canonical form algorithm. Optimisations. \nApplications. \nModal logics. Possible worlds semantics. Truth and validity. A Hilbert-style proof system. \nSequent calculus rules. \nTableaux methods. Simplifying the sequent calculus. Examples. Adding unification. \nSkolemisation. The world\u2019s smallest theorem prover? \nObjectives \nAt the end of the course students should \n \nbe able to manipulate logical formulas accurately; \nbe able to perform proofs using the presented formal calculi; \nbe able to construct a small BDD; \nunderstand the relationships among the various calculi, e.g. SAT solving, resolution and Prolog; \nunderstand the concept of a decision procedure and the basic principles of \u201csatisfiability modulo \ntheories\u201d. \nbe able to apply the unification algorithm and to describe its uses. \nRecommended reading \n",
        "* Huth, M. and Ryan, M. (2004). Logic in computer science: modelling and reasoning about \nsystems. Cambridge University Press (2nd ed.). \nBen-Ari, M. (2001). Mathematical logic for computer science. Springer (2nd ed.).\n \n",
        "PROLOG \n \nAims \nThe aim of this course is to introduce programming in the Prolog language. Prolog encourages \na different programming style to Java or ML and particular focus is placed on programming to \nsolve real problems that are suited to this style. Practical experimentation with the language is \nstrongly encouraged. \n \nLectures \nIntroduction to Prolog. The structure of a Prolog program and how to use the Prolog interpreter. \nUnification. Some simple programs. \nArithmetic and lists. Prolog\u2019s support for evaluating arithmetic expressions and lists. The space \ncomplexity of program evaluation discussed with reference to last-call optimisation. \nBacktracking, cut, and negation. The cut operator for controlling backtracking. Negation as \nfailure and its uses. \nSearch and cut. Prolog\u2019s search method for solving problems. Graph searching exploiting \nProlog\u2019s built-in search mechanisms. \nDifference structures. Difference lists: introduction and application to example programs. \nBuilding on Prolog. How particular limitations of Prolog programs can be addressed by \ntechniques such as Constraint Logic Programming (CLP) and tabled resolution. \nObjectives \nAt the end of the course students should \n \nbe able to write programs in Prolog using techniques such as accumulators and difference \nstructures; \nknow how to model the backtracking behaviour of program execution; \nappreciate the unique perspective Prolog gives to problem solving and algorithm design; \nunderstand how larger programs can be created using the basic programming techniques used \nin this course. \nRecommended reading \n* Bratko, I. (2001). PROLOG programming for artificial intelligence. Addison-Wesley (3rd or 4th \ned.). \nSterling, L. and Shapiro, E. (1994). The art of Prolog. MIT Press (2nd ed.). \n \nFurther reading: \n \nO\u2019Keefe, R. (1990). The craft of Prolog. MIT Press. [This book is beyond the scope of this \ncourse, but it is very instructive. If you understand its contents, you\u2019re more than prepared for \nthe examination.]\n \n",
        "ARTIFICIAL INTELLIGENCE \n \nPrerequisites: Algorithms 1, Algorithms 2. In addition the course requires some mathematics, in \nparticular some use of vectors and some calculus. Part IA Natural Sciences Mathematics or \nequivalent and Discrete Mathematics are likely to be helpful although not essential. Similarly, \nelements of Machine Learning and Real World Data, Foundations of Data Science, Logic and \nProof, Prolog and Complexity Theory are likely to be useful. This course is a prerequisite for the \nPart II courses Machine Learning and Bayesian Inference and Natural Language Processing. \nThis course is a prerequisite for: Natural Language Processing \nExam: Paper 7 Question 1, 2 \nPast exam questions, Moodle, timetable \n \nAims \nThe aim of this course is to provide an introduction to some fundamental issues and algorithms \nin artificial intelligence (AI). The course approaches AI from an algorithmic, computer \nscience-centric perspective; relatively little reference is made to the complementary \nperspectives developed within psychology, neuroscience or elsewhere. The course aims to \nprovide some fundamental tools and algorithms required to produce AI systems able to exhibit \nlimited human-like abilities, particularly in the form of problem solving by search, game-playing, \nrepresenting and reasoning with knowledge, planning, and learning. \n \nLectures \nIntroduction. Alternate ways of thinking about AI. Agents as a unifying view of AI systems. [1 \nlecture] \nSearch I. Search as a fundamental paradigm for intelligent problem-solving. Simple, uninformed \nsearch algorithms. Tree search and graph search. [1 lecture] \nSearch II. More sophisticated heuristic search algorithms. The A* algorithm and its properties. \nImproving memory efficiency: the IDA* and recursive best first search algorithms. Local search \nand gradient descent. [1 lecture] \nGame-playing. Search in an adversarial environment. The minimax algorithm and its \nshortcomings. Improving minimax using alpha-beta pruning. [1 lecture] \nConstraint satisfaction problems (CSPs). Standardising search problems to a common format. \nThe backtracking algorithm for CSPs. Heuristics for improving the search for a solution. Forward \nchecking, constraint propagation and arc consistency. [1 lecture] \nBackjumping in CSPs. Backtracking, backjumping using Gaschnig\u2019s algorithm, graph-based \nbackjumping. [1 lecture] \nKnowledge representation and reasoning I. How can we represent and deal with commonsense \nknowledge and other forms of knowledge? Semantic networks, frames and rules. How can we \nuse inference in conjunction with a knowledge representation scheme to perform reasoning \nabout the world and thereby to solve problems? Inheritance, forward and backward chaining. [1 \nlecture] \nKnowledge representation and reasoning II. Knowledge representation and reasoning using first \norder logic. The frame, qualification and ramification problems. The situation calculus. [1 lecture] \n",
        "Planning I. Methods for planning in advance how to solve a problem. The STRIPS language. \nAchieving preconditions, backtracking and fixing threats by promotion or demotion: the \npartial-order planning algorithm. [1 lecture] \nPlanning II. Incorporating heuristics into partial-order planning. Planning graphs. The \nGRAPHPLAN algorithm. Planning using propositional logic. Planning as a constraint satisfaction \nproblem. [1 lecture] \nNeural Networks I. A brief introduction to supervised learning from examples. Learning as fitting \na curve to data. The perceptron. Learning by gradient descent. [1 lecture] \nNeural Networks II. Multilayer perceptrons and the backpropagation algorithm. [1 lecture] \nObjectives \nAt the end of the course students should: \n \nappreciate the distinction between the popular view of the field and the actual research results; \nappreciate the fact that the computational complexity of most AI problems requires us regularly \nto deal with approximate techniques; \nbe able to design basic problem solving methods based on AI-based search, knowledge \nrepresentation, reasoning, planning, and learning algorithms. \nRecommended reading \nThe recommended text is: \n \n* Russell, S. and Norvig, P. (2010). Artificial intelligence: a modern approach. Prentice Hall (3rd \ned.). \n \nThere are many good books available on artificial intelligence; one alternative is: \n \nPoole, D. L. and Mackworth, A. K. (2017). Artificial intelligence: foundations of computational \nagents. Cambridge University Press (2nd ed.). \n \nFor some of the material you might find it useful to consult more specialised texts, in particular: \n \nDechter, R. (2003). Constraint processing. Morgan Kaufmann. \n \nCawsey, A. (1998). The essence of artificial intelligence. Prentice Hall. \n \nGhallab, M., Nau, D. and Traverso, P. (2004). Automated planning: theory and practice. Morgan \nKaufmann. \n \nBishop, C.M. (2006). Pattern recognition and machine learning. Springer. \n \nBrachman, R.J and Levesque, H.J. (2004). Knowledge Representation and Reasoning. Morgan \nKaufmann.\n \n",
        "COMPLEXITY THEORY \n \nAims \nThe aim of the course is to introduce the theory of computational complexity. The course will \nexplain measures of the complexity of problems and of algorithms, based on time and space \nused on abstract models. Important complexity classes will be defined, and the notion of \ncompleteness established through a thorough study of NP-completeness. Applications to \ncryptography will be considered. \n \nSyllabus \nIntroduction to Complexity Theory. Decidability and complexity; lower and upper bounds; the \nChurch-Turing hypothesis. \nTime complexity. Time complexity classes; polynomial time computation and the class P. \nNon-determinism. Non-deterministic machines; the class NP; the problem 3SAT. \nNP-completeness. Reductions and completeness; the Cook-Levin theorem; graph-theoretic \nproblems. \nMore NP-complete problems. 3-colourability; scheduling, matching, set covering, and knapsack.  \ncoNP. Complement classes. NP \u2229 coNP; primality and factorisation. \nCryptography. One-way functions; the class UP. \nSpace complexity. Space complexity classes; Savitch\u2019s theorem. \nHierarchy theorems. Time and space hierarchy theorems. \nRandomised algorithms. The class BPP; derandomisation; error-correcting codes, \ncommunication complexity. \nQuantum Complexity. The classes BQP and QMA. \nInteractive Proofs. IP=PSPACE; Zero Knowledge Proofs; Probabilistically Checkable Proofs \n(PCPs).  \n  \nObjectives \nAt the end of the course students should \n \nbe able to analyse practical problems and classify them according to their complexity; \nbe familiar with the phenomenon of NP-completeness, and be able to identify problems that are \nNP-complete; \nbe aware of a variety of complexity classes and their interrelationships; \nunderstand the role of complexity analysis in cryptography. \nRecommended reading \nA Survey of P vs. NP by Scott Aaronson. \nComputational Complexity: A Modern Approach by Arora and Barak \nComputational Complexity: A Conceptual Perspective by Oded Goldreich \nMathematics and Computation by Avi Wigderson\n \n",
        "CYBERSECURITY \n \nAims \nIn today\u2019s digital society, computer security is vital for commercial competitiveness, national \nsecurity and privacy of individuals. Protection against both criminal and state-sponsored attacks \nwill need a large cohort of skilled individuals with an understanding of the principles of security \nand with practical experience of the application of these principles. We want you to become one \nof them. In this adversarial game, to defeat the bad guys, the good guys have to be at least as \nskilled at them. Therefore this course has a strong foundation of becoming proficient at actual \nattacks. A theoretical understanding is of course necessary, but without some practice the bad \nguys will run circles around the good guys. In 12 hours I can\u2019t bring you from zero to the stage \nwhere you can invent new attacks and countermeasures, but at least by practicing and \ndissecting common attacks (akin to \u201cstudying the classics\u201d) I\u2019ll give you a feeling for the kind of \nadversarial thinking that is required in this field. Large portions of this course are hands-on: you \nwill need to acquire the relevant skills rather than just reading about it. The recommended \ncourse textbook will be especially helpful to those with no prior low-level hacking experience. \n \nLectures \nIntroduction. \nThe adversarial nature of security; thinking like the attacker; confidentiality, integrity and \navailability; systems-level thinking; Trusted Computing Base; role of cryptography. \n \nFundamentals of access control (chapter 1). \nDiscretionary vs mandatory access control; discretionary access control in POSIX; file \npermissions, file ownership, groups, permission bits. \n \nSoftware security (spanning 4 book chapters) \nSetuid (chapter 2): privileged programs, attack surfaces, exploitable vulnerabilities, privilege \nescalation from regular user to root. \nBuffer overflow (chapter 4): stack memory layout, exploiting a buffer overflow vulnerability, \nguessing unknown addresses, payload, countermeasures and counter-countermeasures. \nReturn to libc and return-oriented programming (chapter 5): exploiting a non-executable stack, \nchaining function calls, chaining ROP gadgets. \nSQL injection (chapter 14): vulnerability and its exploitation, countermeasures, input filtering, \nprepared statements. \n \nAuthentication. \nSomething you know, have, are; passwords, their dominance, their shortcomings and the many \nattempts at replacing them; password cracking; tokens; biometrics; taxonomy of Single Sign-On \nsystems; password managers. \n \nWeb and internet security (spanning 3 book chapters). \nCross-Site Request Forgery (chapter 12): why cross-site requests, CSRF attacks on HTTP GET \nand HTTP POST, countermeasures. \n",
        "Cross-Site Scripting (chapter 13): non-persistent vs persistent XSS, javascript, self-propagating \nXSS worm, countermeasures; \n \nHuman factors. \nUsers are not the enemy; Compliance budget; Prospect theory; 7 principles for systems \nsecurity. \n \nAdditional topics. \nViruses, worms and quines; lockpicking; privilege escalation in physical locks; conclusions. \n \nTo complete the course successfully, students must acquire the practical ability to carry out (as \nopposed to just describing) the exploits mentioned in the syllabus, given a vulnerable system. \nThe low-level hands-on portions of the course are supported by the very helpful course \ntextbook. Those who choose not to study on the recommended textbook will be severely \ndisadvantaged. \n \nRecommended textbook: Wenliang Du. Computer Security: A Hands-on Approach. 3rd Edition. \nISBN: 978-17330039-5-7. Published: 1 May 2022. \n \nhttps://www.handsonsecurity.net. Some chapters freely available online. \n \n \nOptional additional books (not substitutes): \n \nRoss Anderson, Security Engineering 3rd Ed, Wiley, 2020. ISBN: 978-1-119-64278-7. \nhttps://www.cl.cam.ac.uk/~rja14/book.html. Some chapters freely available online. \n \nPaul van Oorschot, Computer Security and the Internet, Springer, 2020. ISBN: \n978-3-030-33648-6 (hardcopy), 978-3-030-33649-3 (eBook). \nhttps://people.scs.carleton.ca/~paulv/toolsjewels.html. All chapters freely available online.\n",
        "FORMAL MODELS OF LANGUAGE \n \nAims \nThis course studies formal models of language and considers how they might be relevant to the \nprocessing and acquisition of natural languages. The course will extend knowledge of formal \nlanguage theory; introduce several new grammars; and use concepts from information theory to \ndescribe natural language. \n \nLectures \nNatural language and the Chomsky hierarchy 1. Recap classes of language. Closure properties \nof language classes. Recap pumping lemma for regular languages. Discussion of relevance (or \nnot) to natural languages (example embedded clauses in English). \nNatural language and the Chomsky hierarchy 2. Pumping lemma for context free languages. \nDiscussion of relevance (or not) to natural languages (example Swiss-German cross serial \ndependancies). Properties of minimally context sensitive languages. Introduction to tree \nadjoining grammars. \nLanguage processing and context free grammar parsing 1. Recap of context free grammar \nparsing. Language processing predictions based on top down parsing models (example Yngve\u2019s \nlanguage processing predictions). Language processing predictions based on probabilistic \nparsing (example Halle\u2019s language processing predictions). \nLanguage processing and context free grammar parsing 2. Introduction to context free grammar \nequivalent dependancy grammars. Language processing predictions based on Shift-Reduce \nparsing (examples prosodic look-ahead parsers, Parsey McParseface). \nGrammar induction of language classes. Introduction to grammar induction. Discussion of \nrelevance (or not) to natural language acquisition. Gold\u2019s theorem. Introduction to context free \ngrammar equivalent categorial grammars and their learnable classes. \nNatural language and information theory 1. Entropy and natural language typology. Uniform \ninformation density as a predictor for language processing. \nNatural language and information theory 2. Noisy channel encoding as a model for spelling \nerror, translation and language processing. \nVector space models and word vectors. Introduction to word vectors (example Word2Vec). Word \nvectors as predictors for semantic language processing. \nObjectives \nAt the end of the course students should \n \nunderstand how known natural languages relate to formal languages in the Chomsky hierarchy; \nhave knowledge of several context free grammars equivalents; \nunderstand how we might make predictions about language processing and language \nacquisition from formal models; \nknow how to use information theoretic concepts to describe aspects of natural language. \nRecommended reading \n* Jurafsky, D. and Martin, J. (2008). Speech and language processing. Prentice Hall. \nManning, C.D. and Schutze, H. (1999) Foundations of statistical natural language processing. \nMIT Press. \n",
        "Ruslan, M. (2003) The Oxford handbook of computational linguistics. Oxford University Press. \nClark, A., Fox, C. and Lappin, S. (2010) The handbook of computational linguistics and natural \nlanguage processing. Wiley-Blackwell. \nKozen, D. (1997) Automata and computibility. Springer.\n \n",
        "5th Semester \nBIOINFORMATICS \nAims \nThis course focuses on algorithms used in Bioinformatics and System Biology. Most of the \nalgorithms are general and can be applied in other fields on multidimensional and noisy data. All \nthe necessary biological terms and concepts useful for the course and the examination will be \ngiven in the lectures. The most important software implementing the described algorithms will be \ndemonstrated. \n \nLectures \nIntroduction to biological data: Bioinformatics as an interesting field in computer science. \nComputing and storing information with DNA (including Adleman\u2019s experiment). \nDynamic programming. Longest common subsequence, DNA global and local alignment, linear \nspace alignment, Nussinov algorithm for RNA, heuristics for multiple alignment. (Vol. 1, chapter \n5) \nSequence database search. Blast. (see notes and textbooks) \nGenome sequencing. De Bruijn graph. (Vol. 1, chapter 3) \nPhylogeny. Distance based algorithms (UPGMA, Neighbour-Joining). Parsimony-based \nalgorithms. Examples in Computer Science. (Vol. 2, chapter 7) \nClustering. Hard and soft K-means clustering, use of Expectation Maximization in clustering, \nHierarchical clustering, Markov clustering algorithm. (Vol. 2, chapter 8) \nGenomics Pattern Matching. Suffix Tree String Compression and the Burrows-Wheeler \nTransform. (Vol. 2, chapter 9) \nHidden Markov Models. The Viterbi algorithm, profile HMMs for sequence alignment, classifying \nproteins with profile HMMs, soft decoding problem, Baum-Welch learning. (Vol. 2, chapter 10) \nObjectives \nAt the end of this course students should \n \nunderstand Bioinformatics terminology; \nhave mastered the most important algorithms in the field; \nbe able to work with bioinformaticians and biologists; \nbe able to find data and literature in repositories. \nRecommended reading \n* Compeau, P. and Pevzner, P.A. (2015). Bioinformatics algorithms: an active learning approach. \nActive Learning Publishers. \nDurbin, R., Eddy, S., Krough, A. and Mitchison, G. (1998). Biological sequence analysis: \nprobabilistic models of proteins and nucleic acids. Cambridge University Press. \nJones, N.C. and Pevzner, P.A. (2004). An introduction to bioinformatics algorithms. MIT Press. \nFelsenstein, J. (2003). Inferring phylogenies. Sinauer Associates.\n \n",
        "BUSINESS STUDIES \n \nAims \nHow to start and run a computer company; the aims of this course are to introduce students to \nall the things that go to making a successful project or product other than just the programming. \nThe course will survey some of the issues that students are likely to encounter in the world of \ncommerce and that need to be considered when setting up a new computer company. \n \nSee also Business Seminars in the Easter Term. \n \nLectures \nSo you\u2019ve got an idea? Introduction. Why are you doing it and what is it? Types of company. \nMarket analysis. The business plan. \nMoney and tools for its management. Introduction to accounting: profit and loss, cash flow, \nbalance sheet, budgets. Sources of finance. Stocks and shares. Options and futures. \nSetting up: legal aspects. Company formation. Brief introduction to business law; duties of \ndirectors. Shares, stock options, profit share schemes and the like. Intellectual Property Rights, \npatents, trademarks and copyright. Company culture and management theory. \nPeople. Motivating factors. Groups and teams. Ego. Hiring and firing: employment law. \nInterviews. Meeting techniques. \nProject planning and management. Role of a manager. PERT and GANTT charts, and critical \npath analysis. Estimation techniques. Monitoring. \nQuality, maintenance and documentation. Development cycle. Productization. Plan for quality. \nPlan for maintenance. Plan for documentation. \nMarketing and selling. Sales and marketing are different. Marketing; channels; marketing \ncommunications. Stages in selling. Control and commissions. \nGrowth and exit routes. New markets: horizontal and vertical expansion. Problems of growth; \nsecond system effects. Management structures. Communication. Exit routes: acquisition, \nfloatation, MBO or liquidation. Futures: some emerging ideas for new computer businesses. \nSummary. Conclusion: now you do it! \nObjectives \nAt the end of the course students should \n \nbe able to write and analyse a business plan; \nknow how to construct PERT and GANTT diagrams and perform critical path analysis; \nappreciate the differences between profitability and cash flow, and have some notion of budget \nestimation; \nhave an outline view of company formation, share structure, capital raising, growth and exit \nroutes; \nhave been introduced to concepts of team formation and management; \nknow about quality documentation and productization processes; \nunderstand the rudiments of marketing and the sales process. \nRecommended reading \n",
        "Lang, J. (2001). The high-tech entrepreneur\u2019s handbook: how to start and run a high-tech \ncompany. FT.COM/Prentice Hall. \n \nStudents will be expected to be able to use Microsoft Excel and Microsoft Project. \n \nFor additional reading on a lecture-by-lecture basis, please see the course website. \n \nStudents are strongly recommended to enter the CU Entrepreneurs Business Ideas Competition \nhttp://www.cue.org.uk/\n \n",
        "DENOTATIONAL SEMANTICS \n \nAims \nThe aims of this course are to introduce domain theory and denotational semantics, and to \nshow how they provide a mathematical basis for reasoning about the behaviour of programming \nlanguages. \n \nLectures \nIntroduction.The denotational approach to the semantics of programming \nlanguages.Recursively defined objects as limits of successive approximations. \nLeast fixed points.Complete partial orders (cpos) and least elements.Continuous functions and \nleast fixed points. \nConstructions on domains.Flat domains.Product domains. Function domains. \nScott induction.Chain-closed and admissible subsets of cpos and domains.Scott\u2019s fixed-point \ninduction principle. \nPCF.The Scott-Plotkin language PCF.Evaluation. Contextual equivalence. \nDenotational semantics of PCF.Denotation of types and terms. Compositionality. Soundness \nwith respect to evaluation. [2 lectures]. \nRelating denotational and operational semantics.Formal approximation relation and its \nfundamental property.Computational adequacy of the PCF denotational semantics with respect \nto evaluation. Extensionality properties of contextual equivalence. [2 lectures]. \nFull abstraction.Failure of full abstraction for the domain model. PCF with parallel or. \nObjectives \nAt the end of the course students should \n \nbe familiar with basic domain theory: cpos, continuous functions, admissible subsets, least fixed \npoints, basic constructions on domains; \nbe able to give denotational semantics to simple programming languages with simple types; \nbe able to apply denotational semantics; in particular, to understand the use of least fixed points \nto model recursive programs and be able to reason about least fixed points and simple \nrecursive programs using fixed point induction; \nunderstand the issues concerning the relation between denotational and operational semantics, \nadequacy and full abstraction, especially with respect to the language PCF. \nRecommended reading \nWinskel, G. (1993). The formal semantics of programming languages: an introduction. MIT \nPress. \nGunter, C. (1992). Semantics of programming languages: structures and techniques. MIT Press. \nTennent, R. (1991). Semantics of programming languages. Prentice Hall.\n \n",
        "INFORMATION THEORY \n \nAims \nThis course introduces the principles and applications of information theory: how information is \nmeasured in terms of probability and various entropies, how these are used to calculate the \ncapacity of communication channels, with or without noise, and to measure how much random \nvariables reveal about each other. Coding schemes including error correcting codes are studied \nalong with data compression, spectral analysis, transforms, and wavelet coding. Applications of \ninformation theory are reviewed, from astrophysics to pattern recognition. \n \nLectures \nInformation, probability, uncertainty, and surprise. How concepts of randomness and uncertainty \nare related to information. How the metrics of information are grounded in the rules of \nprobability. Shannon Information. Weighing problems and other examples. \nEntropy of discrete variables. Definition and link to Shannon information. Joint entropy, Mutual \ninformation. Visual depictions of the relationships between entropy types. Why entropy gives \nfundamental measures of information content. \nSource coding theorem and data compression; prefix, variable-, and fixed-length codes. \nInformation rates; Asymptotic equipartition principle; Symbol codes; Huffman codes and the \nprefix property. Binary symmetric channels. Capacity of a noiseless discrete channel. Stream \ncodes. \nNoisy discrete channel coding. Joint distributions; mutual information; Conditional Entropy; \nError-correcting codes; Capacity of a discrete channel. Noisy channel coding theorem. \nEntropy of continuous variables. Differential entropy; Mutual information; Channel Capacity; \nGaussian channels. \nEntropy for comparing probability distributions and for machine learning. Relative entropy/KL \ndivergence; cross-entropy; use as loss function. \nApplications of information theory in other sciences.  \nObjectives \nAt the end of the course students should be able to \n \ncalculate the information content of a random variable from its probability distribution; \nrelate the joint, conditional, and marginal entropies of variables in terms of their coupled \nprobabilities; \ndefine channel capacities and properties using Shannon\u2019s Theorems; \nconstruct efficient codes for data on imperfect communication channels; \ngeneralize the discrete concepts to continuous signals on continuous channels; \nunderstand encoding and communication schemes in terms of the spectral properties of signals \nand channels; \ndescribe compression schemes, and efficient coding using wavelets and other representations \nfor data. \nRecommended reading \n  Mackay's Information Theory, inference and Learning Algorithms \n \n",
        " Stone's Information Theory: A Tutorial Introduction.\n \n",
        "LATEX AND JULIA \n \nAims \nIntroduction to two widely-used languages for typesetting dissertations and scientific \npublications, for prototyping numerical algorithms and to visualize results. \n \nLectures \nLATEX. Workflow example, syntax, typesetting conventions, non-ASCII characters, document \nstructure, packages, mathematical typesetting, graphics and figures, cross references, build \ntools. \nJulia. Tools for technical computing and visualization. The Array type and its operators, 2D/3D \nplotting, functions and methods, notebooks, packages, vectorized audio demonstration. \nObjectives \nStudents should be able to avoid the most common LATEX mistakes, to prototype simple image \nand signal-processing algorithms in Julia, and to visualize the results. \n \nRecommended reading \n* Lamport, L. (1994). LATEX \u2013 a documentation preparation system user\u2019s guide and reference \nmanual. Addison-Wesley (2nd ed.). \nMittelbach, F., et al. (2023). The LATEX companion. Addison-Wesley (3rd ed.).\n \n",
        "PRINCIPLES OF COMMUNICATIONS \n \nAims \nThis course aims to provide a detailed understanding of the underlying principles for how \ncommunications systems operate. Practical examples (from wired and wireless \ncommunications, the Internet, and other communications systems) are used to illustrate the \nprinciples. \n \nLectures \nIntroduction. Course overview. Abstraction, layering. Review of structure of real networks, links, \nend systems and switching systems. [1 lecture] \nRouting. Central versus Distributed Routing Policy Routing. Multicast Routing Circuit Routing [6 \nlectures] \nFlow control and resource optimisation. 1[Control theory] is a branch of engineering familiar to \npeople building dynamic machines. It can be applied to network traffic. Stemming the flood, at \nsource, sink, or in between? Optimisation as a model of networkand user. TCP in the wild. [3 \nlectures] \nPacket Scheduling. Design choices for scheduling and queue management algorithms for \npacket forwarding, and fairness. [2 lectures] \nThe big picture for managing traffic. Economics and policy are relevant to networks in many \nways. Optimisation and game theory are both relevant topics discussed here. [2 lectures] \nSystem Structures and Summary. Abstraction, layering. The structure of real networks, links, \nend systems and switching. [2 lectures] \n1 Control theory was not taught and will not be the subject of any exam question. \n \nObjectives \nAt the end of the course students should be able to explain the underlying design and behaviour \nof protocols and networks, including capacity, topology, control and use. Several specific \nmathematical approaches are covered (control theory, optimisation). \n \nRecommended reading \n* Keshav, S. (2012). Mathematical Foundations of Computer Networking. Addison Wesley. ISBN \n9780321792105 \nBackground reading: \nKeshav, S. (1997). An engineering approach to computer networking. Addison-Wesley (1st ed.). \nISBN 0201634422 \nStevens, W.R. (1994). TCP/IP illustrated, vol. 1: the protocols. Addison-Wesley (1st ed.). ISBN \n0201633469\n \n",
        "TYPES \n \nAims \nThe aim of this course is to show by example how type systems for programming languages can \nbe defined and their properties developed, using techniques that were introduced in the Part IB \ncourse on Semantics of Programming Languages. The emphasis is on type systems for \nfunctional languages and their connection to constructive logic. \n \nLectures \nIntroduction. The role of type systems in programming languages. Review of rule-based \nformalisation of type systems. [1 lecture] \nPropositions as types. The Curry-Howard correspondence between intuitionistic propositional \ncalculus and simply-typed lambda calculus. Inductive types and iteration. Consistency and \ntermination. [2 lectures] \nPolymorphic lambda calculus (PLC). PLC syntax and reduction semantics. Examples of \ndatatypes definable in the polymorphic lambda calculus. Type inference. [3 lectures] \nMonads and effects. Explicit versus implicit effects. Using monadic types to control effects. \nReferences and polymorphism. Recursion and looping. [2 lectures] \nContinuations and classical logic. First-class continuations and control operators. Continuations \nas Curry-Howard for classical logic. Continuation-passing style. [2 lectures] \nDependent types. Dependent function types. Indexed datatypes. Equality types and combining \nproofs with programming. [2 lectures] \nObjectives \nAt the end of the course students should \n \nbe able to use a rule-based specification of a type system to carry out type checking and type \ninference; \nunderstand by example the Curry-Howard correspondence between type systems and logics; \nunderstand how types can be used to control side-effects in programming; \nappreciate the expressive power of parametric polymorphism and dependent types. \nRecommended reading \n* Pierce, B.C. (2002). Types and programming languages. MIT Press. \nPierce, B. C. (Ed.) (2005). Advanced Topics in Types and Programming Languages. MIT Press. \nGirard, J-Y. (tr. Taylor, P. and Lafont, Y.) (1989). Proofs and types. Cambridge University Press.\n",
        "ADVANCED DATA SCIENCE \n \nPracticals \nThe module will have a practical session for each of the three stages of the pipeline. Each of the \nparts will be tied into an aspect of the final project. \n \nObjectives \nAt the end of the course students will be familiar with the purpose of data science, how it differs \nfrom the closely related fields of machine learning, statistics and artificial intelligence and what a \ntypical data analysis pipeline looks like in practice. As well as emphasising the importance of \nanalysis methods we will introduce a formalism for organising how data science is done in \npractice and what the different aspects the data scientist faces when giving data-driven answers \nto questions of interest. \n \nRecommended reading \nSimon Rogers et al. (2016). A First Course in Machine Learning, Second Edition. [] Chapman \nand Hall/CRC, nil \nShai Shalev-Shwartz et al. (2014). Understanding Machine Learning: From Theory to \nAlgorithms. New York, NY, USA: Cambridge  University Press \nChristopher M. Bishop (2006). Pattern Recognition and Machine Learning (Information Science \nand Statistics). Secaucus, NJ, USA: Springer-Verlag New York, Inc. \nAssessment \nPractical There will be four practicals contributing 20% to the final module mark. Data science is \na topic where there is rarely a single \ncorrect answer therefore the important thing is that an informed attempt has been made to \naddress the questions that can be supported and motivated. \nReport The course will be concluded by an individual report covering the material in the course \nthrough \npractical exercise working with real data. This report will make up 80% of the mark for the \ncourse. \n \n \n",
        "AFFECTIVE ARTIFICIAL INTELLIGENCE \n \nSynopsis \nAffective Artificial Intelligence (Affective AI) aims to imbue machines with social and emotional \nintelligence (EQ). More specifically, Affective AI aims to create artificially intelligent systems and \nmachines that can recognize, interpret, process, and simulate human social signals and \nbehaviours, expressions, and emotions, to enhance human-AI interaction and communication. \n\\r\\n\\r\\nTo achieve this goal, Affective AI draws upon various scientific disciplines, including \nmachine learning, computer vision, speech / natural language / signal processing, psychology \nand cognitive science, and ethics and social sciences. \n \n  \nBackground & Aims \nAffective Artificial Intelligence (Affective AI) aims to imbue machines with social and emotional \nintelligence (EQ). More specifically, Affective AI aims to create artificially intelligent systems and \nmachines that can recognize, interpret, process, and simulate human social signals and \nbehaviours, expressions, and emotions, to enhance human-AI interaction and communication.  \n \nTo achieve this goal, Affective AI draws upon various scientific disciplines, including machine \nlearning, computer vision, speech / natural language / signal processing, psychology and \ncognitive science, and ethics and social sciences. \n \nAffective AI has direct applications in and implications for the design of innovative interactive \ntechnology (e.g., interaction with chat bots, virtual agents, robots), single and multi-user smart \nenvironments ( e.g., in-car/ virtual / augmented / mixed reality, serious games), public speaking \nand cognitive training, and clinical and biomedical studies (e.g., autism, depression, pain). \n \nThe aim of this module is to impart knowledge and ability needed to make informed choices of \nmodels, data, and machine learning techniques for sensing, recognition, and generation of \naffective and social behaviour (e.g., smile, frown, head nodding/shaking, \nagreement/disagreement) in order to create Affectively intelligent AI systems, with a \nconsideration for various ethical issues (e.g., privacy, bias) arising from the real-world \ndeployment of these systems. \n \n  \n \nSyllabus \nThe following list provides a representative list of topics: \n \nIntroduction, definitions, and overview \nTheories from various disciplines \nSensing from multiple modalities (e.g., vision, audio, bio signals, text) \nData acquisition and annotation \nSignal processing / feature extraction \n",
        "Learning / prediction / recognition and evaluation \nBehaviour synthesis / generation (e.g., for embodied agents / robots) \nAdvanced topics and ethical considerations (e.g., bias and fairness) \nCross-disciplinary applications (via seminar presentations and discussions) \nGuest lectures (diverse topics \u2013 changes each year) \nHands-on research and programming work (i.e., mini project & report)  \n \n  \n \nObjectives \nOn completion of this module, students will: \n \nunderstand and demonstrate knowledge in key characteristics of affectively intelligent AI, which \ninclude: \nRecognition: How to equip Affective AI systems with capabilities of analysing facial expressions, \nvocal intonations, gestures, and other physiological signals to infer human affective states? \nGeneration: How to enable affectively intelligent AI simulate expressions of emotions in \nmachines, allowing them to respond in a more human-like manner, such as virtual agents and \nhumanoid robots, showing empathy or sympathy? \nAdaptation and Personalization: How to enable affectively intelligent AI adapt system responses \nbased on users' affective states and/or needs, or tailor interactions and experiences to individual \nusers based on their expressivity / emotional profiles, personalities, and past interactions? \nEmpathetic Communication: How to design Affective AI systems to communicate with users in a \nway that demonstrates empathy, understanding, and sensitivity to their affective states and \nneeds? \nEthical and societal considerations: What are the various human differences, ethical guidelines, \nand societal impacts that need to be considered when designing and deploying Affective AI to \nensure that these systems respect users' privacy, autonomy, and well-being? \ncomprehend and apply (appropriate) methods for collection, analysis, representation, and \nevaluation of human affective and communicative behavioural data. \nenhance programming skills for creating and implementing (components of) Affective AI \nsystems. \ndemonstrate critical thinking, analysis and synthesis while deciding on 'when' and 'how' to \nincorporate human affect and social signals in a specific AI system context and gain practical \nexperience in proposing and justifying computational solution(s) of suitable nature and scope. \n  \n \nAssessment - Part II Students \nSeminar presentation: 20% \nParticipating in Q&A and discussions: 10% \nMid-term mini project report and presentation (as a team of 2): 10%  \nFinal mini project report & code (as a team of 2): 60% \n \n",
        "CATEGORY THEORY \n \nAims \nCategory theory provides a unified treatment of mathematical properties and constructions that \ncan be expressed in terms of 'morphisms' between structures. It gives a precise framework for \ncomparing one branch of mathematics (organized as a category) with another and for the \ntransfer of problems in one area to another. Since its origins in the 1940s motivated by \nconnections between algebra and geometry, category theory has been applied to diverse fields, \nincluding computer science, logic and linguistics. This course introduces the basic notions of \ncategory theory: adjunction, natural transformation, functor and category. We will use category \ntheory to organize and develop the kinds of structure that arise in models and semantics for \nlogics and programming languages. \n \nSyllabus \nIntroduction; some history. Definition of category. The category of sets and functions. \nCommutative diagrams. Examples of categories: preorders and monotone functions; monoids \nand monoid homomorphisms; a preorder as a category; a monoid as a category. Definition of \nisomorphism. Informal notion of a 'category-theoretic' property. \nTerminal objects. The opposite of a category and the duality principle. Initial objects. Free \nmonoids as initial objects. \nBinary products and coproducts. Cartesian categories. \nExponential objects: in the category of sets and in general. Cartesian closed categories: \ndefinition and examples. \nIntuitionistic Propositional Logic (IPL) in Natural Deduction style. Semantics of IPL in a cartesian \nclosed preorder. \nSimply Typed Lambda Calculus (STLC). The typing relation. Semantics of STLC types and \nterms in a cartesian closed category (ccc). The internal language of a ccc. The \nCurry-Howard-Lawvere correspondence. \nFunctors. Contravariance. Identity and composition for functors. \nSize: small categories and locally small categories. The category of small categories. Finite \nproducts of categories. \nNatural transformations. Functor categories. The category of small categories is cartesian \nclosed. \nHom functors. Natural isomorphisms. Adjunctions. Examples of adjoint functors. Theorem \ncharacterising the existence of right (respectively left) adjoints in terms of a universal property. \nDependent types. Dependent product sets and dependent function sets as adjoint functors. \nEquivalence of categories. Example: the category of I-indexed sets and functions is equivalent \nto the slice category Set/I. \nPresheaves. The Yoneda Lemma. Categories of presheaves are cartesian closed. \nMonads. Modelling notions of computation as monads. Moggi's computational lambda calculus. \nObjectives \nOn completion of this module, students should: \n \n",
        "be familiar with some of the basic notions of category theory and its connections with logic and \nprogramming language semantics \nAssessment \na graded exercise sheet (25% of the final mark), and \na take-home test (75%) \nRecommended reading \nAwodey, S. (2010). Category theory. Oxford University Press (2nd ed.). \n \nCrole, R. L. (1994). Categories for types. Cambridge University Press. \n \nLambek, J. and Scott, P. J. (1986). Introduction to higher order categorical logic. Cambridge \nUniversity Press. \n \nPitts, A. M. (2000). Categorical Logic. Chapter 2 of S. Abramsky, D. M. Gabbay and T. S. E. \nMaibaum (Eds) Handbook of Logic in Computer Science, Volume 5. Oxford University Press. \n(Draft copy available here.) \n \nClass Size \nThis module can accommodate upto 15 Part II students plus 15 MPhil / Part III students.\n",
        "DIGITAL SIGNAL PROCESSING \n \nAims \nThis course teaches basic signal-processing principles necessary to understand many modern \nhigh-tech systems, with examples from audio processing, image coding, radio communication, \nradar, and software-defined radio. Students will gain practical experience from numerical \nexperiments in programming assignments (in Julia, MATLAB or NumPy). \n \nLectures \nSignals and systems. Discrete sequences and systems: types and properties. Amplitude, \nphase, frequency, modulation, decibels, root-mean square. Linear time-invariant systems, \nconvolution. Some examples from electronics, optics and acoustics. \nPhasors. Eigen functions of linear time-invariant systems. Review of complex arithmetic. \nPhasors as orthogonal base functions. \nFourier transform. Forms and properties of the Fourier transform. Convolution theorem. Rect \nand sinc. \nDirac\u2019s delta function. Fourier representation of sine waves, impulse combs in the time and \nfrequency domain. Amplitude-modulation in the frequency domain. \nDiscrete sequences and spectra. Sampling of continuous signals, periodic signals, aliasing, \ninterpolation, sampling and reconstruction, sample-rate conversion, oversampling, spectral \ninversion. \nDiscrete Fourier transform. Continuous versus discrete Fourier transform, symmetry, linearity, \nFFT, real-valued FFT, FFT-based convolution, zero padding, FFT-based resampling, \ndeconvolution exercise. \nSpectral estimation. Short-time Fourier transform, leakage and scalloping phenomena, \nwindowing, zero padding. Audio and voice examples. DTFM exercise. \nFinite impulse-response filters. Properties of filters, implementation forms, window-based FIR \ndesign, use of frequency-inversion to obtain high-pass filters, use of modulation to obtain \nband-pass filters. \nInfinite impulse-response filters. Sequences as polynomials, z-transform, zeros and poles, some \nanalog IIR design techniques (Butterworth, Chebyshev I/II, elliptic filters, second-order cascade \nform). \nBand-pass signals. Band-pass sampling and reconstruction, IQ up and down conversion, \nsuperheterodyne receivers, software-defined radio front-ends, IQ representation of AM and FM \nsignals and their demodulation. \nDigital communication. Pulse-amplitude modulation. Matched-filter detector. Pulse shapes, \ninter-symbol interference, equalization. IQ representation of ASK, BSK, PSK, QAM and FSK \nsignals. [2 hours] \nRandom sequences and noise. Random variables, stationary and ergodic processes, \nautocorrelation, cross-correlation, deterministic cross-correlation sequences, filtered random \nsequences, white noise, periodic averaging. \nCorrelation coding. Entropy, delta coding, linear prediction, dependence versus correlation, \nrandom vectors, covariance, decorrelation, matrix diagonalization, eigen decomposition, \n",
        "Karhunen-Lo\u00e8ve transform, principal component analysis. Relation to orthogonal transform \ncoding using fixed basis vectors, such as DCT. \nLossy versus lossless compression. What information is discarded by human senses and can \nbe eliminated by encoders? Perceptual scales, audio masking, spatial resolution, colour \ncoordinates, some demonstration experiments. \nQuantization, image coding standards. Uniform and logarithmic quantization, A/\u00b5-law coding, \ndithering, JPEG. \nObjectives \napply basic properties of time-invariant linear systems; \nunderstand sampling, aliasing, convolution, filtering, the pitfalls of spectral estimation; \nexplain the above in time and frequency domain representations; \nuse filter-design software; \nvisualize and discuss digital filters in the z-domain; \nuse the FFT for convolution, deconvolution, filtering; \nimplement, apply and evaluate simple DSP applications; \nfamiliarity with a number of signal-processing concepts used in digital communication systems \nRecommended reading \nLyons, R.G. (2010). Understanding digital signal processing. Prentice Hall (3rd ed.). \nOppenheim, A.V. and Schafer, R.W. (2007). Discrete-time digital signal processing. Prentice \nHall (3rd ed.). \nStein, J. (2000). Digital signal processing \u2013 a computer science perspective. Wiley. \n \nClass size \nThis module can accommodate a maximum of 24 students (16 Part II students and 8 MPhil \nstudents) \n \nAssessment - Part II students \nThree homework programming assignments, each comprising 20% of the mark \nWritten test, comprising 40% of the total mark.\n \n",
        "MACHINE VISUAL PERCEPTION \n \nAims \nThis course aims at introducing the theoretical foundations and practical techniques for machine \nperception, the capability of computers to interpret data resulting from sensor measurements. \nThe course will teach the fundamentals and modern techniques of machine perception, i.e. \nreconstructing the real world starting from sensor measurements with a focus on machine \nperception for visual data. The topics covered will be image/geometry representations for \nmachine perception, semantic segmentation, object detection and recognition, geometry \ncapture, appearance modeling and acquisition, motion detection and estimation, \nhuman-in-the-loop machine perception, select topics in applied machine perception. \n \nMachine perception/computer vision is a rapidly expanding area of research with real-world \napplications. An understanding of machine perception is also important for robotics, interactive \ngraphics (especially AR/VR), applied machine learning, and several other fields and industries. \nThis course will provide a fundamental understanding of and practical experience with the \nrelevant techniques. \n \nLearning outcomes \nStudents will understand the theoretical underpinnings of the modern machine perception \ntechniques for reconstructing models of reality starting from an incomplete and imperfect view of \nreality. \nStudents will be able to apply machine perception theory to solve practical problems, e.g. \nclassification of images, geometry capture. \nStudents will gain an understanding of which machine perception techniques are appropriate for \ndifferent tasks and scenarios. \nStudents will have hands-on experience with some of these techniques via developing a \nfunctional machine perception system in their projects. \nStudents will have practical experience with the current prominent machine perception \nframeworks. \nSyllabus \nThe fundamentals of machine learning for machine perception \nDeep neural networks and frameworks for machine perception \nSemantic segmentation of objects and humans \nObject detection and recognition \nMotion estimation, tracking and recognition \n3D geometry capture \nAppearance modeling and acquisition \nSelect topics in applied machine perception \nAssessment \nA practical exercise, worth 20% of the mark. This will cover the basics and theory of machine \nperception and some of the practical techniques the students will likely use in their projects. This \nis individual work. No GPU hours will be needed for the practical work. \nA machine perception project worth 80% of the marks: \n",
        "Course projects will be selected by the students following the possible project themes proposed \nby the lecturer, and will be checked by the lecturer for appropriateness.  \nThe students will form groups of 2-3 to design, implement, report, and present a project to tackle \na given task in machine perception. \nThe final mark will be composed of an implementation/report mark (60%) and a presentation \nmark (20%). Each team member will be evaluated based on her/his contribution. \nEach project will have extensions to be completed only by the ACS students. Each student will \nwrite a different part of the report, whose author will be clearly marked. Each student will further \nsummarise her/his contributions to the project in the same report. \nRecommended Reading List \nComputer Vision: Algorithms and Applications, Richard Szeliski, Springer, 2010. \nDeep Learning, Ian Goodfellow, Yoshua Bengio, and Aaron Courville, MIT Press, 2016. \nMachine Learning and Visual Perception, Baochang Zhang, De Gruyter, 2020.\n \n",
        "NATURAL LANGUAGE PROCESSING \n \nAims \nThis course introduces the fundamental techniques of natural language processing. It aims to \nexplain the potential and the main limitations of these techniques. Some current research issues \nare introduced and some current and potential applications discussed and evaluated. Students \nwill also be introduced to practical experimentation in natural language processing. \n \nLectures \nOverview. Brief history of NLP research, some current applications, components of NLP \nsystems. \nMorphology and Finite State Techniques. Morphology in different languages, importance of \nmorphological analysis in NLP, finite-state techniques in NLP. \nPart-of-Speech Tagging and Log-Linear Models. Lexical categories, word tagging, corpora and \nannotations, empirical evaluation. \nPhrase Structure and Structure Prediction. Phrase structures, structured prediction, context-free \ngrammars, weights and probabilities. Some limitations of context-free grammars. \nDependency Parsing. Dependency structure, grammar-free parsing, incremental processing.  \nGradient Descent and Neural Nets. Parameter optimisation by gradient descent. Non-linear \nfunctions with neural network layers. Log-linear model as softmax layer. Current findings of \nNeural NLP. \nWord representations. Representing words with vectors, count-based and prediction-based \napproaches, similarity metrics. \nRecurrent Neural Networks. Modelling sequences, parameter sharing in recurrent neural \nnetworks, neural language models, word prediction. \nCompositional Semantics. Logical representations, compositional semantics, lambda calculus, \ninference and robust entailment. \nLexical Semantics. Semantic relations, WordNet, word senses. \nDiscourse. Discourse relations, anaphora resolution, summarization. \nNatural Language Generation. Challenges of natural language generation (NLG), tasks in NLG, \nsurface realisation. \nPractical and assignments. Students will build a natural language processing system which will \nbe trained and evaluated on supplied data. The system will be built from existing components, \nbut students will be expected to compare approaches and some programming will be required \nfor this. Several assignments will be set during the practicals for assessment. \nObjectives \nBy the end of the course students should: \n \nbe able to discuss the current and likely future performance of several NLP applications; \nbe able to describe briefly a fundamental technique for processing language for several \nsubtasks, such as morphological processing, parsing, word sense disambiguation etc.; \nunderstand how these techniques draw on and relate to other areas of computer science. \nRecommended reading \n",
        "Jurafsky, D. & Martin, J. (2023). Speech and language processing. Prentice Hall (3rd ed. draft, \nonline). \n \nAssessment - Part II Students \nAssignment 1 - 10% of marks \nAssignment 2 - 60% of marks \nAssignment 3 - 30% of marks\n \n",
        "PRACTICAL RESEARCH IN HUMAN-CENTRED AI \n \nAims \nThis is an advanced course in human-computer interaction, with a specialist focus on intelligent \nuser interfaces and interaction with machine-learning and artificial intelligence technologies. The \nformat will be largely Practical, with students carrying out a mini-project involving empirical \nresearch investigation. These studies will investigate human interaction with some kind of \nmodel-based system for planning, decision-making, automation etc. Possible study formats \nmight include: System evaluation, Field observation, Hypothesis testing experiment, Design \nintervention or Corpus analysis, following set examples from recent research publications. \nProject work will be formally evaluated through a report and presentation. \n \nLectures \n(note that Lectures 2-7 also include one hour class discussion of practical work) \n        \u2022 Current research themes in intelligent user interfaces \n        \u2022 Program synthesis \n        \u2022 Mixed initiative interaction \n        \u2022 Interpretability / explainable AI \n        \u2022 Labelling as a fundamental problem \n        \u2022 Machine learning risks and bias \n        \u2022 Visualisation and visual analytics \n        \u2022 Student research presentations \n \nObjectives \nBy the end of the course students should: \n        \u2022 be familiar with current state of the art in intelligent interactive systems \n        \u2022 understand the human factors that are most critical in the design of such systems \n        \u2022 be able to evaluate evidence for and against the utility of novel systems \n        \u2022 have experience of conducting user studies meeting the quality criteria of this field \n        \u2022 be able to write up and present user research in a professional manner \n \nClass Size \nThis module can accommodate upto 20 Part II, Part III and MPhil students. \n \nRecommended reading \nBrad A. Myers and Richard McDaniel (2000). Demonstrational Interfaces: Sometimes You Need \na Little Intelligence, Sometimes You Need a Lot. \n \nAlan Blackwell (2024). Moral Codes: Designing alternatives to AI \n \nAssessment - Part II Students \nThe format will be largely practical, with students carrying out an individual mini-project involving \nempirical research investigation. \n \n",
        "Assignment 1: six incremental submissions which together contribute 20% to the final module \nmark. \n \nAssignment 2: Final report - 80% of the final module mark \n \n",
        "6th Semester \nADVANCED COMPUTER ARCHITECTURE \nAims \nThis course examines the techniques and underlying principles that are used to design \nhigh-performance computers and processors. Particular emphasis is placed on understanding \nthe trade-offs involved when making design decisions at the architectural level. A range of \nprocessor architectures are explored and contrasted. In each case we examine their merits and \nlimitations and how ultimately the ability to scale performance is restricted. \n \nLectures \nIntroduction. The impact of technology scaling and market trends. \nFundamentals of Computer Design. Amdahl\u2019s law, energy/performance trade-offs, ISA design. \nAdvanced pipelining. Pipeline hazards; exceptions; optimal pipeline depth; branch prediction; \nthe branch target buffer [2 lectures] \nSuperscalar techniques. Instruction-Level Parallelism (ILP); superscalar processor architecture \n[2 lectures] \nSoftware approaches to exploiting ILP. VLIW architectures; local and global instruction \nscheduling techniques; predicated instructions and support for speculative compiler \noptimisations. \nMultithreaded processors. Coarse-grained, fine-grained, simultaneous multithreading \nThe memory hierarchy. Caches; programming for caches; prefetching [2 lectures] \nVector processors. Vector machines; short vector/SIMD instruction set extensions; stream \nprocessing \nChip multiprocessors. The communication model; memory consistency models; false sharing; \nmultiprocessor memory hierarchies; cache coherence protocols; synchronization [2 lectures] \nOn-chip interconnection networks. Bus-based interconnects; on-chip packet switched networks \nSpecial-purpose architectures. Converging approaches to computer design \nObjectives \nAt the end of the course students should \n \nunderstand what determines processor design goals; \nappreciate what constrains the design process and how architectural trade-offs are made within \nthese constraints; \nbe able to describe the architecture and operation of pipelined and superscalar processors, \nincluding techniques such as branch prediction, register renaming and out-of-order execution; \nhave an understanding of vector, multithreaded and multi-core processor architectures; \nfor the architectures discussed, understand what ultimately limits their performance and \napplication domain. \nRecommended reading \n* Hennessy, J. and Patterson, D. (2012). Computer architecture: a quantitative approach. \nElsevier (5th ed.) ISBN 9780123838728. (the 3rd and 4th editions are also good)\n \n",
        "CRYPTOGRAPHY \n \nAims \nThis course provides an overview of basic modern cryptographic techniques and covers \nessential concepts that users of cryptographic standards need to understand to achieve their \nintended security goals. \n \nLectures \nCryptography. Overview, private vs. public-key ciphers, MACs vs. signatures, certificates, \ncapabilities of adversary, Kerckhoffs\u2019 principle. \nClassic ciphers. Attacks on substitution and transposition ciphers, Vigen\u00e9re. Perfect secrecy: \none-time pads. \nPrivate-key encryption. Stream ciphers, pseudo-random generators, attacking \nlinear-congruential RNGs and LFSRs. Semantic security definitions, oracle queries, advantage, \ncomputational security, concrete-security proofs. \nBlock ciphers. Pseudo-random functions and permutations. Birthday problem, random \nmappings. Feistel/Luby-Rackoff structure, DES, TDES, AES. \nChosen-plaintext attack security. Security with multiple encryptions, randomized encryption. \nModes of operation: ECB, CBC, OFB, CNT. \nMessage authenticity. Malleability, MACs, existential unforgeability, CBC-MAC, ECBC-MAC, \nCMAC, birthday attacks, Carter-Wegman one-time MAC. \nAuthenticated encryption. Chosen-ciphertext attack security, ciphertext integrity, \nencrypt-and-authenticate, authenticate-then-encrypt, encrypt-then-authenticate, padding oracle \nexample, GCM. \nSecure hash functions. One-way functions, collision resistance, padding, Merkle-Damg\u00e5rd \nconstruction, sponge function, duplex construct, entropy pool, SHA standards. \nApplications of secure hash functions. HMAC, stream authentication, Merkle tree, commitment \nprotocols, block chains, Bitcoin. \nKey distribution problem. Needham-Schroeder protocol, Kerberos, hardware-security modules, \npublic-key encryption schemes, CPA and CCA security for asymmetric encryption. \nNumber theory, finite groups and fields. Modular arithmetic, Euclid\u2019s algorithm, inversion, \ngroups, rings, fields, GF(2n), subgroup order, cyclic groups, Euler\u2019s theorem, Chinese \nremainder theorem, modular roots, quadratic residues, modular exponentiation, easy and \ndifficult problems. [2 lectures] \nDiscrete logarithm problem. Baby-step-giant-step algorithm, computational and decision \nDiffie-Hellman problem, DH key exchange, ElGamal encryption, hybrid cryptography, Schnorr \ngroups, elliptic-curve systems, key sizes. [2 lectures] \nTrapdoor permutations. Security definition, turning one into a public-key encryption scheme, \nRSA, attacks on \u201ctextbook\u201d RSA, RSA as a trapdoor permutation, optimal asymmetric \nencryption padding, common factor attacks. \nDigital signatures. One-time signatures, RSA signatures, Schnorr identification scheme, \nElGamal signatures, DSA, PS3 hack, certificates, PKI. \nObjectives \nBy the end of the course students should \n",
        " \nbe familiar with commonly used standardized cryptographic building blocks; \nbe able to match application requirements with concrete security definitions and identify their \nabsence in naive schemes; \nunderstand various adversarial capabilities and basic attack algorithms and how they affect key \nsizes; \nunderstand and compare the finite groups most commonly used with discrete-logarithm \nschemes; \nunderstand the basic number theory underlying the most common public-key schemes, and \nsome efficient implementation techniques. \nRecommended reading \nKatz, J., Lindell, Y. (2015). Introduction to modern cryptography. Chapman and Hall/CRC (2nd \ned.).\n \n",
        "E-COMMERCE \n \nAims \nThis course aims to give students an outline of the issues involved in setting up an e-commerce \nsite. \n \nLectures \nThe history of electronic commerce. Mail order; EDI; web-based businesses, credit card \nprocessing, PKI, identity and other hot topics. \nNetwork economics. Real and virtual networks, supply-side versus demand-side scale \neconomies, Metcalfe\u2019s law, the dominant firm model, the differentiated pricing model Data \nProtection Act, Distance Selling regulations, business models. \nWeb site design. Stock and price control; domain names, common mistakes, dynamic pages, \ntransition diagrams, content management systems, multiple targets. \nWeb site implementation. Merchant systems, system design and sizing, enterprise integration, \npayment mechanisms, CRM and help desks. Personalisation and internationalisation. \nThe law and electronic commerce. Contract and tort; copyright; binding actions; liabilities and \nremedies. Legislation: RIP; Data Protection; EU Directives on Distance Selling and Electronic \nSignatures. \nPutting it into practice. Search engine interaction, driving and analysing traffic; dynamic pricing \nmodels. Integration with traditional media. Logs and audit, data mining modelling the user. \ncollaborative filtering and affinity marketing brand value, building communities, typical behaviour. \nFinance. How business plans are put together. Funding Internet ventures; the recent hysteria; \nmaximising shareholder value. Future trends. \nUK and International Internet Regulation. Data Protection Act and US Privacy laws; HIPAA, \nSarbanes-Oxley, Security Breach Disclosure, RIP Act 2000, Electronic Communications Act \n2000, Patriot Act, Privacy Directives, data retention; specific issues: deep linking, Inlining, brand \nmisuse, phishing. \nObjectives \nAt the end of the course students should know how to apply their computer science skills to the \nconduct of e-commerce with some understanding of the legal, security, commercial, economic, \nmarketing and infrastructure issues involved. \n \nRecommended reading \nShapiro, C. and Varian, H. (1998). Information rules. Harvard Business School Press. \n \nAdditional reading: \n \nStandage, T. (1999). The Victorian Internet. Phoenix Press. Klemperer, P. (2004). Auctions: \ntheory and practice. Princeton Paperback ISBN 0-691-11925-2.\n \n",
        "MACHINE LEARNING AND BAYESIAN INFERENCE \nAims \nThe Part 1B course Artificial Intelligence introduced simple neural networks for supervised \nlearning, and logic-based methods for knowledge representation and reasoning. This course \nhas two aims. First, to provide a rigorous introduction to machine learning, moving beyond the \nsupervised case and ultimately presenting state-of-the-art methods. Second, to provide an \nintroduction to the wider area of probabilistic methods for representing and reasoning with \nknowledge. \n \nLectures \nIntroduction to learning and inference. Supervised, unsupervised, semi-supervised and \nreinforcement learning. Bayesian inference in general. What the naive Bayes method actually \ndoes. Review of backpropagation. Other kinds of learning and inference. [1 lecture] \nHow to classify optimally. Treating learning probabilistically. Bayesian decision theory and Bayes \noptimal classification. Likelihood functions and priors. Bayes theorem as applied to supervised \nlearning. The maximum likelihood and maximum a posteriori hypotheses. What does this teach \nus about the backpropagation algorithm? [2 lectures] \nLinear classifiers I. Supervised learning via error minimization. Iterative reweighted least \nsquares. The maximum margin classifier. [2 lectures] \nGaussian processes. Learning and inference for regression using Gaussian process models. [2 \nlectures] \nSupport vector machines (SVMs). The kernel trick. Problem formulation. Constrained \noptimization and the dual problem. SVM algorithm. [2 lectures] \nPractical issues. Hyperparameters. Measuring performance. Cross-validation. Experimental \nmethods. [1 lecture] \nLinear classifiers II. The Bayesian approach to neural networks. [1 lecture] \nUnsupervised learning I. The k-means algorithm. Clustering as a maximum likelihood problem. \n[1 lecture] \nUnsupervised learning II. The EM algorithm and its application to clustering. [1 lecture] \nBayesian networks I. Representing uncertain knowledge using Bayesian networks. Conditional \nindependence. Exact inference in Bayesian networks. [2 lectures] \nBayesian networks II. Markov random fields. Approximate inference. Markov chain Monte Carlo \nmethods. [1 lecture] \nObjectives \nAt the end of this course students should: \n \nUnderstand how learning and inference can be captured within a probabilistic framework, and \nknow how probability theory can be applied in practice as a means of handling uncertainty in AI \nsystems. \nUnderstand several algorithms for machine learning and apply those methods in practice with \nproper regard for good experimental practice. \nRecommended reading \nIf you are going to buy a single book for this course I recommend: \n \n",
        "* Bishop, C.M. (2006). Pattern recognition and machine learning. Springer. \n \nThe course text for Artificial Intelligence I: \n \nRussell, S. and Norvig, P. (2010). Artificial intelligence: a modern approach. Prentice Hall (3rd \ned.). \n \ncovers some relevant material but often in insufficient detail. Similarly: \n \nMitchell, T.M. (1997). Machine Learning. McGraw-Hill. \n \ngives a gentle introduction to some of the course material, but only an introduction. \n \nRecently a few new books have appeared that cover a lot of relevant ground well. For example: \n \nBarber, D. (2012). Bayesian Reasoning and Machine Learning. Cambridge University Press. \nFlach, P. (2012). Machine Learning: The Art and Science of Algorithms that Make Sense of \nData. Cambridge University Press. \nMurphy, K.P. (2012). Machine Learning: A Probabilistic Perspective. MIT Press.\n \n",
        "OPTIMISING COMPILERS \n \nAims \nThe aims of this course are to introduce the principles of program optimisation and related \nissues in decompilation. The course will cover optimisations of programs at the abstract syntax, \nflowgraph and target-code level. It will also examine how related techniques can be used in the \nprocess of decompilation. \n \nLectures \nIntroduction and motivation. Outline of an optimising compiler. Optimisation partitioned: analysis \nshows a property holds which enables a transformation. The flow graph; representation of \nprogramming concepts including argument and result passing. The phase-order problem. \nKinds of optimisation. Local optimisation: peephole optimisation, instruction scheduling. Global \noptimisation: common sub-expressions, code motion. Interprocedural optimisation. The call \ngraph. \nClassical dataflow analysis. Graph algorithms, live and avail sets. Register allocation by register \ncolouring. Common sub-expression elimination. Spilling to memory; treatment of \nCSE-introduced temporaries. Data flow anomalies. Static Single Assignment (SSA) form. \nHigher-level optimisations. Abstract interpretation, Strictness analysis. Constraint-based \nanalysis, Control flow analysis for lambda-calculus. Rule-based inference of program properties, \nTypes and effect systems. Points-to and alias analysis. \nTarget-dependent optimisations. Instruction selection. Instruction scheduling and its phase-order \nproblem. \nDecompilation. Legal/ethical issues. Some basic ideas, control flow and type reconstruction. \nObjectives \nAt the end of the course students should \n \nbe able to explain program analyses as dataflow equations on a flowgraph; \nknow various techniques for high-level optimisation of programs at the abstract syntax level; \nunderstand how code may be re-scheduled to improve execution speed; \nknow the basic ideas of decompilation. \nRecommended reading \n* Nielson, F., Nielson, H.R. and Hankin, C.L. (1999). Principles of program analysis. Springer. \nGood on part A and part B. \nAppel, A. (1997). Modern compiler implementation in Java/C/ML (3 editions). \nMuchnick, S. (1997). Advanced compiler design and implementation. Morgan Kaufmann. \nWilhelm, R. (1995). Compiler design. Addison-Wesley. \nAho, A.V., Sethi, R. and Ullman, J.D. (2007). Compilers: principles, techniques and tools. \nAddison-Wesley (2nd ed.).\n \n",
        "QUANTUM COMPUTING \n \nAims \nThe principal aim of the course is to introduce students to the basics of the quantum model of \ncomputation. The model will be used to study algorithms for searching, factorisation and \nquantum chemistry as well as other important topics in quantum information such as \ncryptography and super-dense coding. Issues in the complexity of computation will also be \nexplored. A second aim of the course is to introduce student to near-term quantum computing. \nTo this end, error-correction and adiabatic quantum computing are studied. \n \nLectures \nBits and qubits. Introduction to quantum states and measurements with motivating examples. \nComparison with discrete classical states. \nLinear algebra. Review of linear algebra: vector spaces, linear operators, Dirac notation, the \ntensor product. \nThe postulates of quantum mechanics. Postulates of quantum mechanics, incl. evolution and \nmeasurement. \nImportant concepts in quantum mechanics. Entanglement, distinguishing orthogonal and \nnon-orthogonal quantum states, no-cloning and no signalling. \nThe quantum circuit model. The circuit model of quantum computation. Quantum gates and \ncircuits. Universality of the quantum circuit model, and efficient simulation of arbitrary two-qubit \ngates with a standard universal set of gates. \nSome applications of quantum information. Applications of quantum information (other than \nquantum computation): quantum key distribution, superdense coding and quantum teleportation. \nDeutsch-Jozsa algorithm. Introducing Deutsch\u2019s problem and Deutsch\u2019s algorithm leading onto \nits generalisation, the Deutsch-Jozsa algorithm. \nQuantum search. Grover\u2019s search algorithm: analysis and lower bounds. \nQuantum Fourier Transform and Quantum Phase Estimation. Definition of the Quantum Fourier \nTransform (QFT), and efficient representation thereof as a quantum circuit. Application of the \nQFT to enable Quantum Phase Estimation (QPE). \nApplication 1 of QFT / QPE: Factoring. Shor\u2019s algorithm: reduction of factoring to period finding \nand then using the QFT for period finding. \nApplication 2 of QFT / QPE: Quantum Chemistry. Efficient simulation of quantum systems, and \napplications to real-world problems in quantum chemistry. \nQuantum complexity. Quantum complexity classes and their relationship to classical complexity. \nComparison with probabilistic computation. \nQuantum error correction. Introducing the concept of quantum error correction required for the \nfollowing lecture on fault-tolerance. \nFault tolerant quantum computing. Elements of fault tolerant computing; the threshold theorem \nfor efficient suppression of errors. \nAdiabatic quantum computing and quantum optimisation. The quantum adiabatic theorem, and \nadiabatic optimisation. Quantum annealing and D-Wave. \nCase studies in near-term quantum computation. Examples of state-of-the-art quantum \nalgorithms and computers, including superconducting and networked quantum computers. \n",
        "Objectives \nAt the end of the course students should: \n \nunderstand the quantum model of computation and the basic principles of quantum mechanics; \nbe familiar with basic quantum algorithms and their analysis; \nbe familiar with basic quantum protocols such as teleportation and superdense coding; \nsee how the quantum model relates to classical models of deterministic and probabilistic \ncomputation. \nappreciate the importance of efficient error-suppression if quantum computation is to yield an \nadvantage over classical computation. \ngain a general understanding of the important topics in near-term quantum computing, including \nadiabatic quantum computing. \nRecommended reading \nBooks: \nNielsen M.A., Chuang I.L. (2010). Quantum Computation and Quantum Information. Cambridge \nUniversity Press. \nMermin N.D. (2007). Quantum Computer Science: An Introduction. Cambridge University Press. \nMcGeoch, C. (2014). Adiabatic Quantum Computation and Quantum Annealing Theory and \nPractice. Morgan and Claypool. https://ieeexplore.ieee.org/document/7055969 \n \nPapers: \n \nBraunstein S.L. (2003). Quantum computation tutorial. Available at: \nhttps://www-users.cs.york.ac.uk/~schmuel/comp/comp_best.pdf \nAharonov D., Quantum computation [arXiv:quant-ph/9812037] \nAlbash T., Adiabatic Quantum Computing https://arxiv.org/pdf/1611.04471.pdf \nMcCardle S. et al, Quantum computational chemistry https://arxiv.org/abs/1808.10402 \n \nOther lecture notes: \n \nAndrew Childs (University of Maryland): http://cs.umd.edu/~amchilds/qa/ \n \n",
        "RANDOMISED ALGORITHMS \n \nAims: \nThe aim of this course is to introduce advanced techniques in the design and analysis \nalgorithms, with a strong focus on randomised algorithms. It covers essential tools and ideas \nfrom probability, optimisation and graph theory, and develops this knowledge through a variety \nof examples of algorithms and processes. This course will demonstrate that randomness is not \nonly an important design technique which often leads to simpler and more elegant algorithms, \nbut may also be essential in settings where time and space are restricted.   \n \nLectures: \nIntroduction. Course Overview. Why Randomised Algorithms? A first Randomised Algorithm for \nthe MAX-CUT problem. [1 Lecture]  \n \nConcentration Inequalities. Moment-Generating Functions and Chernoff Bounds. Extension: \nMethod of Bounded Independent Differences. Applications: Balls-into-Bins, Quick-Sort and Load \nBalancing. [approx. 2 Lectures] \n \nMarkov Chains and Mixing Times. Random Walks on Graphs. Application: Randomised \nAlgorithm for the 2-SAT problem. Mixing Times of Markov Chains. Application: Load Balancing \non Networks. [approx. 2 Lectures] \n \nLinear Programming and Applications. Definitions and Applications. Formulating Linear \nPrograms. The Simplex Algorithm. Finding Initial Solutions. How to use Linear Programs and \nBranch & Bound to Solve a Classical TSP instance. [approx. 3 Lectures] \n \nRandomised Approximation Algorithms. Randomised Approximation Schemes. Linearity of \nExpectations, Derandomisation. Deterministic and Randomised Rounding of Linear Programs. \nApplications: MAX3-SAT problem, Weighted Vertex Cover, Weighted Set Cover, MAX-SAT. \n[approx. 2 Lectures] \n \nSpectral Graph Theory and Clustering. Eigenvalues of Graphs and Matrices: Relations between \nEigenvalues and Graph Properties, Spectral Graph Drawing. Spectral Clustering: Conductance, \nCheeger's Inequality. Spectral Partitioning Algorithm [approx. 2 Lectures] \n \nObjectives: \nBy the end of the course students should be able to: \n \nlearn how to use randomness in the design of algorithms, in particular, approximation \nalgorithms; \nlearn the basics of linear programming, integer programming and randomised rounding; \napply randomisation to various problems coming from optimisation, machine learning and data \nscience; \nuse results from probability theory to analyse the performance of randomised algorithms. \n",
        "Recommended reading \n* Michael Mitzenmacher and Eli Upfal. Probability and Computing: Randomized Algorithms and \nProbabilistic Analysis., Cambridge University Press, 2nd edition. \n \n* David P. Williamson and David B. Shmoys. The Design of Approximation Algorithms, \nCambridge University Press, 2011 \n \n* Cormen, T.H., Leiserson, C.D., Rivest, R.L. and Stein, C. (2009). Introduction to Algorithms. \nMIT Press (3rd ed.). ISBN 978-0-262-53305-8 \n \n \n",
        "CLOUD COMPUTING \nAims \nThis module aims to teach students the fundamentals of Cloud Computing covering topics such \nas virtualization, data centres, cloud resource management, cloud storage and popular cloud \napplications including batch and data stream processing. Emphasis is given on the different \nbackend technologies to build and run efficient clouds and the way clouds are used by \napplications to realise computing on demand. The course will include practical tutorials on cloud \ninfrastructure technologies. Students will be assessed via a Cloud-based coursework project. \n \nLectures \nIntroduction to Cloud Computing \nData centres \nVirtualization I \nVirtualization II \nMapReduce \nMapReduce advanced \nResource management for virtualized data centres \nCloud storage \nCloud-based data stream processing \nObjectives \nBy the end of the course students should: \n \nunderstand how modern clouds operate and provide computing on demand; \nunderstand about cloud availability, performance, scalability and cost; \nknow about cloud infrastructure technologies including virtualization, data centres, resource \nmanagement and storage; \nknow how popular applications such as batch and data stream processing run efficiently on \nclouds; \nAssessment \nAssignment 1, worth 55% of the final mark. Develop batch processing applications running on a \ncluster assessed through automatic testing, code inspection and a report. \nAssignment 2, worth 30% of the final mark: Design and develop a framework for running the \nbatch processing applications from Assignment 1 on a cluster according to certain resource \nallocation policies. This assigment will be assessed by an expert, testing, code inspection and a \nreport. \nAssignment 3, worth 15% of the final mark: Write two individual project reports describing your \nwork for assignments 1 and 2. Write two sets of scripts to manage and run your code. Reports \nand scripts will be submitted with each assignment. \nRecommended reading \nMarinescu, D.C. Cloud Computing, Theory and Practice. Morgan Kaufmann. \nBarham, P., et. al. (2003). \u201cXen and the Art of Virtualization\u201d. In Proceedings of SOSP 2003. \nCharkasova, L., Gupta, D. and Vahdat, A. (2007). \u201cComparison of the Three CPU Schedulers in \nXen\u201d. In SIGMETRICS 2007. \n",
        "Dean, J. and Ghemawat, S. (2004). \u201cMapReduce: Simplified Data Processing on Large \nClusters\u201d. In Proceedings of OSDI 2004. \nZaharia, M, et al. (2008). \u201cImproving MapReduce Performance in Heterogeneous \nEnvironments\u201d. In Proceedings of OSDI 2008. \nHindman, A., et al. (2011). \u201cMesos: A Platform for Fine-Grained Resource Sharing in Data \nCenter\u201d. In Proceedings of NSDI 2011. \nSchwarzkopf, M., et al. (2013). \u201cOmega: Flexible, Scalable Schedulers for Large Compute \nClusters\u201d. In EuroSys 2013. \nGhemawat, S. (2003). \u201cThe Google File System\u201d. In Proceedings of SOSP 2003. \nChang, F. (2006). \u201cBigtable: A Distributed Storage System for Structured Data\u201d. In Proceedings \nof OSDI 2006. \nFernandez, R.C., et al. (2013). \u201cIntegrating Scale Out and Fault Tolerance in Stream Processing \nusing Operator State Management\u201d. In SIGMOD 2013.\n \n",
        "COMPUTER SYSTEMS MODELLING \n \nAims \n \nThe aims of this course are to introduce the concepts and principles of mathematical modelling \n \nand simulation, with particular emphasis on using queuing theory and control theory for \nunderstanding the behaviour of computer and communications systems. \n \nSyllabus \n \n1.     Overview of computer systems modeling using both analytic techniques and simulation. \n \n2.     Introduction to discrete event simulation. \n \na.     Simulation techniques \n \nb.     Random number generation methods \n \nc.     Statistical aspects: confidence intervals, stopping criteria \n \nd.     Variance reduction techniques. \n \n3.     Stochastic processes (builds on starred material in Foundations of Data Science) \n \na.     Discrete and continuous stochastic processes. \n \nb.     Markov processes and Chapman-Kolmogorov equations. \n \nc.     Discrete time Markov chains. \n \nd.     Ergodicity and the stationary distribution. \n \ne.     Continuous time Markov chains. \n \nf.      Birth-death processes, flow balance equations. \n \ng.     The Poisson process. \n \n4.     Queueing theory \n \na.     The M/M/1 queue in detail. \n \n",
        "b.     The equilibrium distribution with conditions for existence and common performance \nmetrics. \n \nc.     Extensions of the M/M/1 queue: the M/M/k queue, the M/M/infinity queue. \n \nd.     Queueing networks. Jacksonian networks. \n \ne.     The M/G/1 queue. \n \n5.     Signals, systems, and transforms \n \na.     Discrete- and continuous-time convolution. \n \nb.     Signals. The complex exponential signal. \n \nc.     Linear Time-Invariant Systems. Modeling practical systems as an LTI system. \n \nd.     Fourier and Laplace transforms. \n \n6.     Control theory. \n \na.     Controlled systems. Modeling controlled systems. \n \nb.     State variables. The transfer function model. \n \nc.     First-order and second-order systems. \n \nd.     Feedback control. PID control. \n \ne.     Stability. BIBO stability. Lyapunov stability. \n \nf.      Introduction to Model Predictive Control. \n \nObjectives \n \nAt the end of the course students should \n \n\u00b7       Be aware of different approaches to modeling a computer system; their pros and cons \n \n\u00b7       Be aware of the issues in building a simulation of a computer system and analysing the \nresults obtained \n \n\u00b7       Understand the concept of a stochastic process and how they arise in practice \n \n",
        "\u00b7       Be able to build simple Markov models and understand the critical modelling assumptions \n \n\u00b7       Be able to solve simple birth-death processes \n \n\u00b7       Understand and use M/M/1 queues to model computer systems \n \n\u00b7       Be able to model a computer system as a linear time-invariant system \n \n\u00b7       Understand the dynamics of a second-order controlled system \n \n\u00b7       Design a PID control for an LTI system \n \n\u00b7       Understand what is meant by BIBO and Lyapunov stability \n \nAssessment \n \nThere will be one programming assignment to build a discrete event simulator and using it to \nsimulate an M/M/1 and an M/D/1 queue (worth 15% of the final marks). There will also be two \none-hour in-person assessments for the course on: Stochastic processes and Queueing theory \n(40%) and Signals, Systems, Transforms, and Control Theory (45%). \n \nReference books \n \nKeshav, S. (2012)*. Mathematical Foundations of Computer Networking. Addison-Wesley. A \ncustomized PDF will be made available to class participants. \n \nKleinrock, L. (1975). Queueing systems, vol. 1. Theory. Wiley. \n \nKraniauskas, Peter. Transforms in signals and systems. Addison-Wesley Longman, 1992. \n \nJain, R. (1991). The art of computer systems performance analysis. Wiley. \n \n \n",
        "COMPUTING EDUCATION \n \nAims \nThis module considers various aspects of the teaching and learning of computer science in \nschool, including practical experience, and provides an introduction to the field of computing \neducation research. It is offered by the Raspberry Pi Computing Education Research Centre, \npart of the Department of Computer Science and Technology, in conjunction with local schools \nin the Cambridge area. Students will have the opportunity to observe, plan and deliver a lesson, \nand explore the teaching of computing in a secondary education setting. The lesson taught will \nbe on a topic within the computing/computer science curriculum in England but will be \nnegotiated with the teacher mentoring the school placement. In the sessions in the Department, \nstudents will be introduced to elements of pedagogy and assessment alongside current issues \nin computing education. The course can also serve as a useful introduction to research in \ncomputing education for those interested in this area. Assessment will include lesson planning, \npresentations, an in-person viva and a 3000-word report which will include evidence of \nengagement with relevant research underpinning the lessons and activities undertaken. An \ninformation session relating to the module will be available in the preceding Easter Term. The \narrangement of school placements and DBS checks will be carried out in Michaelmas Term, so \nstudents should be prepared to engage in these preparatory activities. \n \nLectures \nThis is not a traditional lecture course and the term will be structured as follows: \n \nWeek 1: Introduction to computing education & visit to school placement \nWeek 2: In school (observation, supporting teacher) \nWeek 3: In school (observation, supporting teacher) \nWeek 4: In school (co-teach / small group teaching) \nWeek 5: Computing pedagogy and assessment (school half-term) \nWeek 6: In school (co-teach / small group teaching) \nWeek 7: In school (teach part/whole lesson) \nWeek 8: Presentations \nObjectives \nBy the end of the course students should: \n \ngain experience of computing education in practice \ndemonstrate an ability to communicate an aspect of computer science clearly and effectively \nbe able to plan and design a teaching activity/lesson with consideration of the audience \nbe able to thoroughly evaluate the design and implementation of a teaching/activity lesson \nbe able to demonstrate an understanding of relevant theories of learning and pedagogy from the \nliterature and how they apply to the learning/teaching of the topic under consideration \ngain an understanding of the breadth and depth of the field of computing education research \nClass Size \nThis module can accommodate up to 10 Part II students. \n \n",
        "Recommended reading \nBrown, N. C., Sentance, S., Crick, T., & Humphreys, S. (2014). Restart: The resurgence of \ncomputer science in UK schools. ACM Transactions on Computing Education (TOCE), 14(2), \n1-22. https://doi.org/10.1145/2602484 \n \nSentance, S., Barendsen, E., Howard, N. R., & Schulte, C. (Eds.). (2023). Computer science \neducation: Perspectives on teaching and learning in school. Bloomsbury Publishing. \n \nGrover, S. (Ed). 2020. Computer Science in K-12: An A-to-Z Handbook on Teaching \nProgramming. Edfinity. https://www.shuchigrover.com/atozk12cs/ \n \nRaspberry Pi Foundation (2022). Hello World Big Book of Pedagogy. \nhttps://www.raspberrypi.org/hello-world/issues/the-big-book-of-computing-pedagogy \n \nAssessment \nWeek 2: Reflections on an observed lesson (10%, graded out of 20) \nWeek 4: Submission of a lesson plan outline (10% graded out of 20) \nWeek 8: Delivery of an oral presentation relating to the lesson delivery (10% graded out of 20) \nAfter course completion: 3000 word report (60%), graded out of 20 \nViva with assessor (10%) (pass/fail) \nFurther Information \nWe will find placements for students in secondary schools within cycling or bus distance from \nthe Department/centre of Cambridge. There will be a timetabled slot for the course that students \ncan use to go to their school placement, but students may need to be flexible and liaise with the \nschool to find the best time for working with a specific class. \n \n \n",
        "DEEP NEURAL NETWORKS \n \nObjectives \nYou will gain detailed knowledge of \n \nCurrent understanding of generalization in neural networks vs classical statistical models. \nOptimization procedures for neural network models such as stochastic gradient descent and \nADAM. \nAutomatic differentiation and at least one software framework (PyTorch, TensorFolow) as well as \nan overview of other software approaches. \nArchitectures that are deployed to deal with different data types such as images or sequences \nincluding a. convolutional networks b. recurrent networks and c. transformers. \nIn addition you will gain knowledge of more advanced topics reflecting recent research in \nmachine learning. These change from year to year, examples include: \n \nApproaches to unsupervised learning including autoencoders and self-supervised learning.. \nTechniques for deploying models in low data regimes such as transfer learning and \nmeta-learning. \nTechniques for propagating uncertainty such as Bayesian neural networks. \nReinforcement learning and its applications to robotics or games. \nGraph Neural Networks and Geometric Deep Learning. \nTeaching Style \nThe start of the course will focus on the latest undertanding of current theory of neural networks, \ncontrasting with previous classical understandings of generalization performance. Then we will \nmove to practical examples of network architectures and deployment. We will end with more \nadvanced topics reflecting current research, mostly delivered by guest lecturers from the \nDepartment and beyond. \n \nSchedule \nWeek 1 \nTwo lectures: Generalization and Approximation with Neural Networks \n \nWeek 2 \nTwo lectures: Optimization: Stochastic Gradient Descent and ADAM \n \nWeek 3 \nTwo lectures: Hardware Acceleration and Convolutional Neural Networks \n \nWeek 4 \nTwo lectures: Vanishing Gradients, Recurrent and Residual Networks, Sequence Modeling. \n \nWeek 5 \nTwo lectures: Attention, The Transformer Architecture, Large Language Models \n \n",
        "Week 6-8 \nLectures and guest lectures from the special topics below. \n \nSpecial topics \nSelf-Supervised Learning, AutoEncoders, Generative Modeling of Images \nHardware Implementations \nReinforcement learning \nTransfer learning and meta-learning \nUncertainty and Bayesian Neural Networks \nGraph Neural Networks \nAssessment \nAssessment involves solving a number of exercises in a jupyter notebook environment. \nFamiliarity with the python programming language is assumed. The exercises rely on the \npytorch deep learning framework, the syntax of which we don\u2019t cover in detail in the lectures, \nstudents are referred to documentation and tutorials to learn this component of the assessment. \n \nAssignment 1 \n \n30% of the total marks, with guided exercises. \n \nAssignment 2 \n \n70% of the total marks, a combination of guided exercises and a more exploratory mini-project.\n",
        "EXTENDED REALITY \n \nSyllabus & Schedule \nWeek 1 \nIntroduction \nCourse schedule and concept \nThe history of AR/VR/XR \nApplications of AR/VR/XR \nOverview of the XR pipeline \u2013 3 main components of XR \nDisplay technologies and rendering \nMachine perception and input interfaces \nContent generation \u2013 geometry, appearance, motion \nOverview of the AR/VR/XR frameworks \nPractical 1 \nProject structure \nProject themes \nIntroduction to the XR programming platform \nWeek 2 \n3D scene representations and rendering \nObject representations with polygonal meshes \nEfficient rendering via rasterization \nPractical XR rendering of textured geometries \nTracking and pose estimation for XR \nRigid transforms \nCamera pose estimation \nBody/ hand tracking \nPractical 2 \nProject draft proposal due \nProject feedback \nRendering and displaying a first object \nWeek 3 \n3D scene capture and understanding \nDepth estimation \n3D geometry capture \nAppearance acquisition \nLighting estimation and relighting \nLighting models for 3D scenes \nEstimating lighting in a scene \nRelighting rendered models \nPractical 3 \nAcquiring and utilizing depth from sensors \nRelighting a displayed object \nWeek 4 \nPhysically-based simulation of rigid objects \n",
        "Rigid-body dynamics \nCollision detection \nTime integration \nPhysically-based simulation of non-rigid volumetric phenomena \nParticle systems \nBasic fluid dynamics \nPractical 4 \nPractical due \nProject prototype due \nProject check-in \nSimulating and rendering a simple object \nWeek 5 \nAdvanced topics in XR I: AR/VR displays technologies \nMicro LED-based displays \nWaveguide-based displays \nNext generation displays \nWeek 6 \nAdvanced topics in XR II \u2013 Mobile AR \nComputer vision on limited hardware \nUX/UI challenges \nContent challenges \nProject due \n  \n \nAssessment \nA practical exercise, worth 20% of the mark. This will cover the basics of AR/VR development \nand some of the practical techniques the students will use in their projects. This is individual \nwork. No mobile device or GPU hours are needed. \nAn AR/VR project worth 80% of the marks: \nCourse projects will be designed by the students following the possible project themes proposed \nby the lecturer and will be checked by the lecturer for appropriateness. \nThe students will form groups of 2-3 to design, implement, report, and present the course \nproject. \nThe final mark will be an implementation mark (40%), a report mark (20%), and a \npresentation/demo mark (20%). Each team member will be evaluated based on their \ncontribution. \nEach project will have extensions to be completed only by the ACS students. Each student will \nwrite a different part of the report, whose author will be clearly marked. Each student will further \nsummarise her/his contributions to the project in the same report. \nNote: Submission is by one submission per group but work by each individual must be clearly \nlabelled.\n \n",
        "FEDERATED LEARNING: THEORY AND PRACTICE \n \nObjectives \nThis course aims to extend the machine learning knowledge available to students in Part I (or \npresent in typical undergraduate degrees in other universities), and allow them to understand \nhow these concepts can manifest in a decentralized setting. The course will consider both \ntheoretical (e.g., decentralized optimization) and practical (e.g., networking efficiency) aspects \nthat combine to define this growing area of machine learning.  \n \nAt the end of the course students should: \n \nUnderstand popular methods used in federated learning \nBe able to construct and scale a simple federated system \nHave gained an appreciation of the core limitations to existing methods, and the approaches \navailable to cope with these issues \nDeveloped an intuition for related technologies like differential privacy and secure aggregation, \nand are able to use them within typical federated settings  \nCan reason about the privacy and security issues with federated systems \nLectures \nCourse Overview. Introduction to Federated Learning. \nDecentralized Optimization. \nStatistical and Systems Heterogeneity. \nVariations of Federated Aggregation. \nSecure Aggregation. \nDifferential Privacy within Federated Systems. \nExtensions to Federated Analytics. \nApplications to Speech, Video, Images and Robotics. \nLab sessions \nFederating a Centralized ML Classifier. \nBehaviour under Heterogeneity. \nScaling a Federated Implementation. \nExploring Privacy with Federated Settings \nRecommended Reading \nReadings will be assigned for each lecture. Readings will be taken from either research papers, \ntutorials, source code or blogs that provide more comprehensive treatment of taught concepts. \n \nAssessment - Part II Students \nFour labs are performed during the course, and students receive 10% of their total grade for \nwork done as part of each lab. (For a total of 40% of the total grade from lab work alone). Labs \nwill primarily provide hands-on teaching opportunities, that are then utilized within the lab \nassignment which is completed outside of the lab contact time. MPhil and Part III students will \nbe given additional questions to answer within their version of the lab assignment which will \ndiffer from the assignment given to Part II CST students. \n \n",
        "The remainder of the course grade (60%) will be given based on a hands-on project that applies \nthe concepts taught in lectures and labs. This hands-on project will be assessed based on upon \na combination of source code, related documentation and brief 8-minute pre-recorded talk that \nsummarizes key project elements (any slides used are also submitted as part of the project). \nPlease note, that in the case of Part II CST students, the talk itself is not examinable -- as such \nwill be made optional to those students. \n \nA range of possible practical projects will be described and offered to students to select from, or \nalternatively students may propose their own. MPhil and Part III students will select from a \nproject pool that is separate from those offered to Part II CST students. MPhil and Part III \nprojects will contain a greater emphasis on a research element, and the pre-recorded talks \nprovided by this student group will focus on this research contribution. The project will be \nassessed on the level of student understanding demonstrated, the degree of difficulty, \ncorrectness of implementation -- and for Part III/MPhil students the additional criteria of the \nquality and execution of the research methodology, and depth and quality of results analysis. \n \nThis project can be done individually or in groups -- although individual projects will be strongly \nencouraged. It will be required the project is performed using a code repository that also will \ncontain all documentation -- access to this repository will be shared with course staff (e.g., \nlecturer and TAs). Where needed, marks assigned to students within a group will be \ndifferentiated using this repository as an input. Furthermore if groups are formed, members must \nbe either entirely from Part III/MPhil students or Part II CST, i.e., these two student groups \nshould not mix to form a project group. \n \nProjects will be made available publicly. A maximum word count for written contributions for the \nproject will be enforced.\n \n",
        "MOBILE HEALTH \n \nAims \nThe course aims to explore how wearable and mobile systems sensors can be used to gather \ndata relevant to understand health, how the data can be analysed with advanced signal \nprocessing and machine learning and the performance of these systems in terms of diagnostics \nand disease progression detection. \n \nSyllabus \nCourse Overview. Introduction to Mobile Health. Evaluation metrics and methodology. Basics of \nSignal Processing. \nInertial Measurement Units, Human Activity Recognition (HAR) and Gait Analysis and Machine \nLearning for IMU data. \nRadios, Bluetooth, GPS and Cellular. Epidemiology and contact tracing, Social interaction \nsensing and applications. Location tracking in health monitoring and public health. \nAudio Signal Processing. Voice and Speech Analysis: concepts and data analysis. Body \nSounds analysis. \nPhotoplethysmogram and Light sensing for health (heart and sleep) \nContactless and wireless behaviour and physiological monitoring \nGenerative Models for Wearable Health Data \nTopical Guest Lectures \nObjectives \nThe course aims to explore how wearable and mobile systems sensors can be used to gather \ndata relevant to understand health, how the data can be analysed with advanced signal \nprocessing and machine learning and the performance of these systems in terms of diagnostics \nand disease progression detection. \n \nRoughly, each lecture contains a theory part about the working of \u201csensor signals\u201d or \u201cdata \nanalysis methods\u201d and an application part which contextualises the concepts. \n \nAt the end of the course students should: Understand how mobile/wearable sensors capture \ndata and their working. Understand different approaches to acquiring and analysing sensor data \nfrom different types of sensors. Understand the concept of signal processing applied to time \nseries data and their practical application in health. Be able to extract sensor data and analyse it \nwith basic signal processing and machine learning techniques. Be aware of the different health \napplications of the various sensor techniques. The course will also touch on privacy and ethics \nimplications of the approaches developed in an orthogonal fashion. \n \nRecommended Reading \nPlease see Course Materials for recommended reading for each session. \n \nAssessment - Part II students \nTwo assignments will be based on two datasets which will be provided to the students: \n \n",
        "Assignment 1 (shared for Part II and Part III/MPhil): this will be based on a dataset and will be \nworth 40% of the final mark. The task of the assessment will be to perform pre-processing and \nbasic data analysis in a \"colab\" and an answer sheet of no more than 1000 words. \n \nAssignment 2 (Part II): This assignment (worth 60% of the final mark) will be a fuller analysis of \na dataset focusing on machine learning algorithms and metrics. Discussion and interpretation of \nthe findings will be reported in a colab and a report of no more than 1200 words.\n \n",
        "MULTICORE SEMANTICS AND PROGRAMMING \n \nAims \nIn recent years multiprocessors have become ubiquitous, but building reliable concurrent \nsystems with good performance remains very challenging. The aim of this module is to \nintroduce some of the theory and the practice of concurrent programming, from hardware \nmemory models and the design of high-level programming languages to the correctness and \nperformance properties of concurrent algorithms. \n \nLectures \nPart 1: Introduction and relaxed-memory concurrency [Professor P. Sewell] \n \nIntroduction. Sequential consistency, atomicity, basic concurrent problems. [1 block] \nConcurrency on real multiprocessors: the relaxed memory model(s) for x86, ARM, and IBM \nPower, and theoretical tools for reasoning about x86-TSO programs. [2 blocks] \nHigh-level languages. An introduction to C/C++11 and Java shared-memory concurrency. [1 \nblock] \nPart 2: Concurrent algorithms [Dr T. Harris] \n \nConcurrent programming. Simple algorithms (readers/writers, stacks, queues) and correctness \ncriteria (linearisability and progress properties). Advanced synchronisation patterns (e.g. some \nof the following: optimistic and lazy list algorithms, hash tables, double-checked locking, RCU, \nhazard pointers), with discussion of performance and on the interaction between algorithm \ndesign and the underlying relaxed memory models. [3 blocks] \nResearch topics, likely to include one hour on transactional memory and one guest lecture. [1 \nblock] \nObjectives \nBy the end of the course students should: \n \nhave a good understanding of the semantics of concurrent programs, both at the multprocessor \nlevel and the C/Java programming language level; \nhave a good understanding of some key concurrent algorithms, with practical experience. \nAssessment - Part II Students \nTwo assignments each worth 50% \n \nRecommended reading \nHerlihy, M. and Shavit, N. (2008). The art of multiprocessor programming. Morgan Kaufmann.\n",
        "BUSINESS STUDIES SEMINARS \nAims \nThis course is a series of seminars by former members and friends of the Laboratory about their \nreal-world experiences of starting and running high technology companies. It is a follow on to \nthe Business Studies course in the Michaelmas Term. It provides practical examples and case \nstudies, and the opportunity to network with and learn from actual entrepreneurs. \n \nLectures \nEight lectures by eight different entrepreneurs. \n \nObjectives \nAt the end of the course students should have a better knowledge of the pleasures and pitfalls \nof starting a high tech company. \n \nRecommended reading \nLang, J. (2001). The high-tech entrepreneur\u2019s handbook: how to start and run a high-tech \ncompany. FT.COM/Prentice Hall. \nMaurya, A. (2012). Running Lean: Iterate from Plan A to a Plan That Works. O\u2019Reilly. \nOsterwalder, A. and Pigneur, Y. (2010). Business Model Generation: A Handbook for \nVisionaires, Game Changers, and Challengers. Wiley. \nKim, W. and Mauborgne, R. (2005). Blue Ocean Strategy. Harvard Business School Press. \n \nSee also the additional reading list on the Business Studies web page.\n \n",
        "HOARE LOGIC AND MODEL CHECKING \n \nAims \nThe course introduces two verification methods, Hoare Logic and Temporal Logic, and uses \nthem to formally specify and verify imperative programs and systems. \n \nThe first aim is to introduce Hoare logic for a simple imperative language and then to show how \nit can be used to formally specify programs (along with discussion of soundness and \ncompleteness), and also how to use it in a mechanised program verifier. \n \nThe second aim is to introduce model checking: to show how temporal models can be used to \nrepresent systems, how temporal logic can describe the behaviour of systems, and finally to \nintroduce model-checking algorithms to determine whether properties hold, and to find \ncounter-examples. \n \nCurrent research trends also will be outlined. \n \nLectures \nPart 1: Hoare logic. Formal versus informal methods. Specification using preconditions and \npostconditions. \nAxioms and rules of inference. Hoare logic for a simple language with assignments, sequences, \nconditionals and while-loops. Syntax-directedness. \nLoops and invariants. Various examples illustrating loop invariants and how they can be found. \nPartial and total correctness. Hoare logic for proving termination. Variants. \nSemantics and metatheory Mathematical interpretation of Hoare logic. Semantics and \nsoundness of Hoare logic. \nSeparation logic Separation logic as a resource-aware reinterpretation of Hoare logic to deal \nwith aliasing in programs with pointers. \nPart 2: Model checking. Models. Representation of state spaces. Reachable states. \nTemporal logic. Linear and branching time. Temporal operators. Path quantifiers. CTL, LTL, and \nCTL*. \nModel checking. Simple algorithms for verifying that temporal properties hold. \nApplications and more recent developments Simple software and hardware examples. CEGAR \n(counter-example guided abstraction refinement). \nObjectives \nAt the end of the course students should \n \nbe able to prove simple programs correct by hand and implement a simple program verifier; \nbe familiar with the theory and use of separation logic; \nbe able to write simple models and specify them using temporal logic; \nbe familiar with the core ideas of model checking, and be able to implement a simple model \nchecker. \nRecommended reading \n",
        "Huth, M. and Ryan M. (2004). Logic in Computer Science: Modelling and Reasoning about \nSystems. Cambridge University Press (2nd ed.).\n \n",
        "7th Semester \nADVANCED TOPICS IN COMPUTER ARCHITECTURE \nAims \nThis course aims to provide students with an introduction to a range of advanced topics in \ncomputer architecture. It will explore the current and future challenges facing the architects of \nmodern computers. These will also be used to illustrate the many different influences and \ntrade-offs involved in computer architecture. \n \nObjectives \nOn completion of this module students should: \n \nunderstand the challenges of designing and verifying modern microprocessors \nbe familiar with recent research themes and emerging challenges \nappreciate the complex trade-offs at the heart of computer architecture \nSyllabus \nEach seminar will focus on a different topic. The proposed topics are listed below but there may \nbe some minor changes to this: \n \nTrends in computer architecture \nState-of-the-art microprocessor design \nMemory system design \nHardware reliability \nSpecification, verification and test (may be be replaced with a different topic) \nHardware security (2) \nHW accelerators and accelerators for machine learning \nEach two hour seminar will include three student presentations (15mins) questions (5mins) and \na broader discussion of the topics (around 30mins). The last part of the seminar will include a \nshort scene setting lecture (around 20mins) to introduce the following week's topic. \n \nAssessment \nEach week students will compare and contrast two of the main papers and submit a written \nsummary and review in advance of each seminar (except when presenting). \n \nStudents will be expected to give a number of 15 minute presentations. \n \nEssays and presentations will be marked out of 10. After dropping the lowest mark, the \nremaining marks will be scaled to give a final score out of 100. \n \nStudents will give at least one presentation during the course. They will not be required to \nsubmit an essay during the weeks they are presenting. \n \nEach presentation will focus on a single paper from the reading list. Marks will be awarded for \nclarity and the communication of the paper's key ideas, an analysis of the work's strengths and \nweaknesses and the work\u2019s relationship to related work and broader trends and constraints. \n",
        " \nRecommended prerequisite reading \nPatterson, D. A., Hennessy, J. L. (2017). Computer organization and design: The \nHardware/software interface RISC-V edition Morgan Kaufmann. ISBN 978-0-12-812275-4. \n \nHennessy, J. and Patterson, D. (2012). Computer architecture: a quantitative approach. Elsevier \n(5th ed.) ISBN 9780123838728. (the 3rd and 4th editions are also relevant)\n \n",
        "ADVANCED TOPICS IN PROGRAMMING LANGUAGES \nAims \nThis module explores various topics in programming languages beyond the scope of \nundergraduate courses. It aims to introduce students to ideas, results and techniques found in \nthe literature and prepare them for research in the field. \n \nSyllabus and structure \nThe module consists of eight two-hour seminars, each on a particular topic. Topics will vary from \nyear to year, but may include, for example, \n \nAbstract interpretation \nVerified software \nMetaprogramming \nBehavioural types \nProgram synthesis \nVerified compilation \nPartial evaluation \nGarbage collection \nDependent types \nAutomatic differentiation \nDelimited continuations \nModule systems \nThere will be three papers assigned for each topic, which students are expected to read before \nthe seminar. \nEach seminar will include three 20 minute student presentations (15 minutes + 5 minutes \nquestions), time for general discussion of the topic, and a brief overview lecture for the following \nweek\u2019s topic. \nBefore each seminar, except in weeks in which they give presentations, students will submit a \nshort essay about two of the papers. \n \n \nObjectives \nOn completion of this module, students should \n \nbe able to identify some major themes in programming language research \nbe familiar with some classic papers and recent advances \nhave an understanding of techniques used in the field \n \nAssessment \nAssessment consists of: \n \nPresentation of one of the papers from the reading list (typically once or twice in total for each \nstudent, depending on class numbers) \nOne essay per week (except on the first week and on presentation weeks) \n",
        "All essays and presentations carry equal numbers of marks. \n \nEssay marks are awarded for understanding, for insight and analysis, and for writing quality. \nEssays should be around 1500 words (with a lower limit of 1450 and upper limit of 1650). \nPresentation marks are awarded for clarity, for effective communication, and for selection and \norganisation of topics. \n \nThere will be seven submissions (essays or presentations) in total and, as in other courses, the \nlowest mark for each student will be disregarded when computing the final mark. \n \nMarking, deadlines and extensions will be handled in accordance with the MPhil Assessment \nGuidelines. \n \nRecommended reading material and resources \nResearch and survey papers from programming language conferences and journals (e.g. \nPOPL, PLDI, TOPLAS, FTPL) will be assigned each week. General background material may \nbe found in: \n \n\u2022 Types and Programming Languages (Benjamin C. Pierce) \n   The MIT Press \n   ISBN 0-262-16209-1 \n \n\u2022 Advanced Topics in Types and Programming Languages (ed. Benjamin C. Pierce) \n   The MIT Press \n   ISBN 0-262-16228-8 \n \n\u2022 Practical Foundations for Programming Languages (Robert Harper) \n   Cambridge University Press \n   9781107150300\n \n",
        "AFFECTIVE ARTIFICIAL INTELLIGENCE \n \nSynopsis \nAffective Artificial Intelligence (Affective AI) aims to imbue machines with social and emotional \nintelligence (EQ). More specifically, Affective AI aims to create artificially intelligent systems and \nmachines that can recognize, interpret, process, and simulate human social signals and \nbehaviours, expressions, and emotions, to enhance human-AI interaction and communication. \n\\r\\n\\r\\nTo achieve this goal, Affective AI draws upon various scientific disciplines, including \nmachine learning, computer vision, speech / natural language / signal processing, psychology \nand cognitive science, and ethics and social sciences. \n \n  \nBackground & Aims \nAffective Artificial Intelligence (Affective AI) aims to imbue machines with social and emotional \nintelligence (EQ). More specifically, Affective AI aims to create artificially intelligent systems and \nmachines that can recognize, interpret, process, and simulate human social signals and \nbehaviours, expressions, and emotions, to enhance human-AI interaction and communication.  \n \nTo achieve this goal, Affective AI draws upon various scientific disciplines, including machine \nlearning, computer vision, speech / natural language / signal processing, psychology and \ncognitive science, and ethics and social sciences. \n \nAffective AI has direct applications in and implications for the design of innovative interactive \ntechnology (e.g., interaction with chat bots, virtual agents, robots), single and multi-user smart \nenvironments ( e.g., in-car/ virtual / augmented / mixed reality, serious games), public speaking \nand cognitive training, and clinical and biomedical studies (e.g., autism, depression, pain). \n \nThe aim of this module is to impart knowledge and ability needed to make informed choices of \nmodels, data, and machine learning techniques for sensing, recognition, and generation of \naffective and social behaviour (e.g., smile, frown, head nodding/shaking, \nagreement/disagreement) in order to create Affectively intelligent AI systems, with a \nconsideration for various ethical issues (e.g., privacy, bias) arising from the real-world \ndeployment of these systems. \n \n  \n \nSyllabus \nThe following list provides a representative list of topics: \n \nIntroduction, definitions, and overview \nTheories from various disciplines \nSensing from multiple modalities (e.g., vision, audio, bio signals, text) \nData acquisition and annotation \nSignal processing / feature extraction \n",
        "Learning / prediction / recognition and evaluation \nBehaviour synthesis / generation (e.g., for embodied agents / robots) \nAdvanced topics and ethical considerations (e.g., bias and fairness) \nCross-disciplinary applications (via seminar presentations and discussions) \nGuest lectures (diverse topics \u2013 changes each year) \nHands-on research and programming work (i.e., mini project & report)  \n \n  \n \nObjectives \nOn completion of this module, students will: \n \nunderstand and demonstrate knowledge in key characteristics of affectively intelligent AI, which \ninclude: \nRecognition: How to equip Affective AI systems with capabilities of analysing facial expressions, \nvocal intonations, gestures, and other physiological signals to infer human affective states? \nGeneration: How to enable affectively intelligent AI simulate expressions of emotions in \nmachines, allowing them to respond in a more human-like manner, such as virtual agents and \nhumanoid robots, showing empathy or sympathy? \nAdaptation and Personalization: How to enable affectively intelligent AI adapt system responses \nbased on users' affective states and/or needs, or tailor interactions and experiences to individual \nusers based on their expressivity / emotional profiles, personalities, and past interactions? \nEmpathetic Communication: How to design Affective AI systems to communicate with users in a \nway that demonstrates empathy, understanding, and sensitivity to their affective states and \nneeds? \nEthical and societal considerations: What are the various human differences, ethical guidelines, \nand societal impacts that need to be considered when designing and deploying Affective AI to \nensure that these systems respect users' privacy, autonomy, and well-being? \ncomprehend and apply (appropriate) methods for collection, analysis, representation, and \nevaluation of human affective and communicative behavioural data. \nenhance programming skills for creating and implementing (components of) Affective AI \nsystems. \ndemonstrate critical thinking, analysis and synthesis while deciding on 'when' and 'how' to \nincorporate human affect and social signals in a specific AI system context and gain practical \nexperience in proposing and justifying computational solution(s) of suitable nature and scope. \n  \n \nAssessment - Part II Students \nSeminar presentation: 20% \nParticipating in Q&A and discussions: 10% \nMid-term mini project report and presentation (as a team of 2): 10%  \nFinal mini project report & code (as a team of 2): 60% \n  \n \n",
        "Assessment - MPhil / Part III Students \nSeminar presentation: 20% \nParticipating in Q&A and discussions: 10% \nMid-term mini project report and presentation: 10%  \nFinal mini project report & code: 60% \nFurther Information \nCurrent Cambridge undergraduate students who are continuing onto Part III or the MPhil in \nAdvanced Computer Science may only take this module if they did NOT take it as a Unit of \nAssessment in Part II. \n \nThis module is shared with ACS. Assessment will be adjusted for the two groups of students to \nbe at an appropriate level for whichever course the student is enrolled on. More information will \nfollow at the first lecture. \n \nThis module is shared with Part II of the Computer Science Tripos. Assessment will be adjusted \nfor the two groups of students to be at an appropriate level for whichever course the student is \nenrolled on. Further information about assessment and practicals will follow at the first lecture.\n",
        "CATEGORY THEORY \n \nAims \nCategory theory provides a unified treatment of mathematical properties and constructions that \ncan be expressed in terms of 'morphisms' between structures. It gives a precise framework for \ncomparing one branch of mathematics (organized as a category) with another and for the \ntransfer of problems in one area to another. Since its origins in the 1940s motivated by \nconnections between algebra and geometry, category theory has been applied to diverse fields, \nincluding computer science, logic and linguistics. This course introduces the basic notions of \ncategory theory: adjunction, natural transformation, functor and category. We will use category \ntheory to organize and develop the kinds of structure that arise in models and semantics for \nlogics and programming languages. \n \nSyllabus \nIntroduction; some history. Definition of category. The category of sets and functions. \nCommutative diagrams. Examples of categories: preorders and monotone functions; monoids \nand monoid homomorphisms; a preorder as a category; a monoid as a category. Definition of \nisomorphism. Informal notion of a 'category-theoretic' property. \nTerminal objects. The opposite of a category and the duality principle. Initial objects. Free \nmonoids as initial objects. \nBinary products and coproducts. Cartesian categories. \nExponential objects: in the category of sets and in general. Cartesian closed categories: \ndefinition and examples. \nIntuitionistic Propositional Logic (IPL) in Natural Deduction style. Semantics of IPL in a cartesian \nclosed preorder. \nSimply Typed Lambda Calculus (STLC). The typing relation. Semantics of STLC types and \nterms in a cartesian closed category (ccc). The internal language of a ccc. The \nCurry-Howard-Lawvere correspondence. \nFunctors. Contravariance. Identity and composition for functors. \nSize: small categories and locally small categories. The category of small categories. Finite \nproducts of categories. \nNatural transformations. Functor categories. The category of small categories is cartesian \nclosed. \nHom functors. Natural isomorphisms. Adjunctions. Examples of adjoint functors. Theorem \ncharacterising the existence of right (respectively left) adjoints in terms of a universal property. \nDependent types. Dependent product sets and dependent function sets as adjoint functors. \nEquivalence of categories. Example: the category of I-indexed sets and functions is equivalent \nto the slice category Set/I. \nPresheaves. The Yoneda Lemma. Categories of presheaves are cartesian closed. \nMonads. Modelling notions of computation as monads. Moggi's computational lambda calculus. \nObjectives \nOn completion of this module, students should: \n \n",
        "be familiar with some of the basic notions of category theory and its connections with logic and \nprogramming language semantics \nAssessment \na graded exercise sheet (25% of the final mark), and \na take-home test (75%) \nRecommended reading \nAwodey, S. (2010). Category theory. Oxford University Press (2nd ed.). \n \nCrole, R. L. (1994). Categories for types. Cambridge University Press. \n \nLambek, J. and Scott, P. J. (1986). Introduction to higher order categorical logic. Cambridge \nUniversity Press. \n \nPitts, A. M. (2000). Categorical Logic. Chapter 2 of S. Abramsky, D. M. Gabbay and T. S. E. \nMaibaum (Eds) Handbook of Logic in Computer Science, Volume 5. Oxford University Press. \n(Draft copy available here.) \n \nClass Size \nThis module can accommodate upto 15 Part II students plus 15 MPhil / Part III students. \n \nPre-registration evaluation form  \nStudents who are interested in taking this module will be asked to complete a pre-registration \nevaluation form to determine whether they have enough mathematical background to take the \nmodule.  \n \nThis will take place during the week Monday 12 August - Friday 16 August.  \n \nCoursework \nThe coursework in this module will consist of a graded exercise sheet. \n \nFurther Information \nCurrent Cambridge undergraduate students who are continuing onto Part III or the MPhil in \nAdvanced Computer Science may only take this module if they did NOT take 'Catgeory Theory' \nas a Unit of Assessment in Part II. \n \nThis module is shared with Part II of the Computer Science Tripos. Assessment will be adjusted \nfor the two groups of students to be at an appropriate level for whichever course the student is \nenrolled on. Further information about assessment and practicals will follow at the first lecture.\n",
        "DIGITAL MONEY AND DECENTRALISED FINANCE \n \nAims \nOur society is evolving towards digital payments and the elimination of physical cash. Digital \nalternatives to cash require trade-offs between various properties including unforgeability, \ntraceability, divisibility, transferability without intermediaries, privacy, redeemability, control over \nthe money supply and many more, often in conflict with each other. Since the 1980s, \ncryptographers have proposed a variety of clever technical solutions addressing some of these \nissues but it is only with the appearance of Bitcoin, blockchain and a plethora of copycat \ncryptocurrencies that a new asset class has now emerged, worth in excess of two trillion dollars \n(although plagued by extreme volatility). Meanwhile, most major countries have been planning \nthe introduction of Central Bank Digital Currencies in an attempt to retain control. This \nseminar-style interactive module encourages the students to understand what's happening, \nwhat the underlying problems and opportunities are (technical, social and economic) and where \nwe should go from here. We are at a historical turning point where an enterprising candidate \nmight disrupt the status quo and truly make a difference. \n \nSyllabus \nUnderstanding money: from gold standard to digital cash \nIntro to the Decentralised Finance (DeFi) ecosystem: Bitcoin, blockchain, wallets, smart \ncontracts \nStablecoins and Central Bank Digital Currencies \nDecentralised Autonomous Organisations and Automated Market Makers \nYield Farming, Perpetual Futures and Maximal Extractable Value \nWeb3, Non Fungible Tokens \nCryptocurrency crashes; crimes facilitated by cryptocurrencies \nNew areas of development. Where from here? \nThe module is structured as a reading club with a high degree of interactivity. Aside from the first \nand last session, which are treated differently, the regular two-hour sessions have the following \nstructure (not necessarily in this order). \n \nTwo half-hour debates on each of two papers \nTwo rapid-fire presentations on open questions previously assigned by the lecturers. \nHalf an hour of presentation by the lecturers. \nAbout ten minutes of buffer that can account for switchover time or be absorbed into the \nlecturers' presentation. \nThe first session is scene-setting by the lecturers, with no student presentations. The last \nsession has no paper debates, only one rapid-fire presentation on an open question from each \nof the students. \n \nObjectives \nOn completion of this module, students will gain an in-depth understanding of digital money \nalternatives as well as many modern applications and components of Decentralised Finance. By \ncomparing DeFi with current processes in traditional banking, students will be able to discuss \n",
        "the merits and limitations of these new technologies as well as to identify potential new areas of \ndevelopment. \n \nThrough having to write, present and defend very brief extended abstracts, the students will \nimprove their communication skills and in particular the ability to distil and convey the most \nimportant points forcefully and concisely. \n \n \nAssessment \nThe module is an interactive reading club. Each student produces four output triplets throughout \nthe course. Each output triplet consists of three parts: a 1200-word essay, a 200-word extended \nabstract and a 7-minute presentation. \n \nThe essay and abstract must be submitted in advance, using the LaTeX templates provided. \nThe abstract must consist of full sentences, not bullet points, and must fit on a single slide. \nDuring the presentation, the slide with the abstract will be projected on screen as the only visual \naid; the essay will not be accessible to either presenter or audience. Reading out the slide \nverbatim will obviously not count as a great presentation. Presenters must be able to expand \nand defend their ideas live, and answer questions from lecturers and audience. \n \nThe essays, the abstracts and the presentations are graded separately, each out of 10, and \nassigned weights of 60%, 20%, 20%, respectively, within the triplet. \n \nIn a regular 2-hour session (all but the first and last) there will be two paper slots and two \ninvestigation slots, plus a lecturer slot, described next. \n \nEach paper slot (30 min) involves a \u201csupporter\u201d and a \u201cdissenter\u201d for the paper, each \ncontributing one output triplet, and then a panel Q&A where all the other participants quiz the \nsupporter and dissenter. \n \nEach investigation slot (10 min) involves a single student contributing an output triplet on a \ngiven theme. \n \nRoles, papers and themes are assigned by the lecturers the previous week. \n \nIn the first session there will be no student presentations, only lecturer-led exploration. In the \nlast session there will be 12 investigation slots and no paper slots. \n \nOverall, each student will be supporter once, dissenter once and investigator twice, for a total of \n4 output triplets. The output triplets of the last session will be weighed twice as much as the \nothers, i.e. 40% each, while the others 20% each. \n \nAttendance is required at all sessions. Missing a session in which the candidate was due to \npresent will result in the loss of the corresponding presentation marks, while the written work will \n",
        "still have to be submitted and will be marked as usual. Missing a session in which the candidate \nwas not due to present will result in the loss of 3%. \n \nRecommended reading \nThere isn't a textbook covering all the material in this course. The following textbook, also \nadopted by R47 Distributed Ledger Technologies (also freely available online), is a thorough \nintroduction to Bitcoin and Blockchain, but we will complement it with research papers and \nindustry white papers for each lecture. \n \nNarayanan, A. , Bonneau, J., Felten, E., Miller, A. and Goldfeder, S. (2016). Bitcoin and \nCryptocurrency Technologies: A Comprehensive Introduction. Princeton University Press. \n(2016 Draft available here: \nhttps://d28rh4a8wq0iu5.cloudfront.net/bitcointech/readings/princeton_bitcoin_book.pdf)\n \n",
        "DIGITAL SIGNAL PROCESSING \n \nAims \nThis course teaches basic signal-processing principles necessary to understand many modern \nhigh-tech systems, with examples from audio processing, image coding, radio communication, \nradar, and software-defined radio. Students will gain practical experience from numerical \nexperiments in programming assignments (in Julia, MATLAB or NumPy). \n \nLectures \nSignals and systems. Discrete sequences and systems: types and properties. Amplitude, \nphase, frequency, modulation, decibels, root-mean square. Linear time-invariant systems, \nconvolution. Some examples from electronics, optics and acoustics. \nPhasors. Eigen functions of linear time-invariant systems. Review of complex arithmetic. \nPhasors as orthogonal base functions. \nFourier transform. Forms and properties of the Fourier transform. Convolution theorem. Rect \nand sinc. \nDirac\u2019s delta function. Fourier representation of sine waves, impulse combs in the time and \nfrequency domain. Amplitude-modulation in the frequency domain. \nDiscrete sequences and spectra. Sampling of continuous signals, periodic signals, aliasing, \ninterpolation, sampling and reconstruction, sample-rate conversion, oversampling, spectral \ninversion. \nDiscrete Fourier transform. Continuous versus discrete Fourier transform, symmetry, linearity, \nFFT, real-valued FFT, FFT-based convolution, zero padding, FFT-based resampling, \ndeconvolution exercise. \nSpectral estimation. Short-time Fourier transform, leakage and scalloping phenomena, \nwindowing, zero padding. Audio and voice examples. DTFM exercise. \nFinite impulse-response filters. Properties of filters, implementation forms, window-based FIR \ndesign, use of frequency-inversion to obtain high-pass filters, use of modulation to obtain \nband-pass filters. \nInfinite impulse-response filters. Sequences as polynomials, z-transform, zeros and poles, some \nanalog IIR design techniques (Butterworth, Chebyshev I/II, elliptic filters, second-order cascade \nform). \nBand-pass signals. Band-pass sampling and reconstruction, IQ up and down conversion, \nsuperheterodyne receivers, software-defined radio front-ends, IQ representation of AM and FM \nsignals and their demodulation. \nDigital communication. Pulse-amplitude modulation. Matched-filter detector. Pulse shapes, \ninter-symbol interference, equalization. IQ representation of ASK, BSK, PSK, QAM and FSK \nsignals. [2 hours] \nRandom sequences and noise. Random variables, stationary and ergodic processes, \nautocorrelation, cross-correlation, deterministic cross-correlation sequences, filtered random \nsequences, white noise, periodic averaging. \nCorrelation coding. Entropy, delta coding, linear prediction, dependence versus correlation, \nrandom vectors, covariance, decorrelation, matrix diagonalization, eigen decomposition, \n",
        "Karhunen-Lo\u00e8ve transform, principal component analysis. Relation to orthogonal transform \ncoding using fixed basis vectors, such as DCT. \nLossy versus lossless compression. What information is discarded by human senses and can \nbe eliminated by encoders? Perceptual scales, audio masking, spatial resolution, colour \ncoordinates, some demonstration experiments. \nQuantization, image coding standards. Uniform and logarithmic quantization, A/\u00b5-law coding, \ndithering, JPEG. \nObjectives \napply basic properties of time-invariant linear systems; \nunderstand sampling, aliasing, convolution, filtering, the pitfalls of spectral estimation; \nexplain the above in time and frequency domain representations; \nuse filter-design software; \nvisualize and discuss digital filters in the z-domain; \nuse the FFT for convolution, deconvolution, filtering; \nimplement, apply and evaluate simple DSP applications; \nfamiliarity with a number of signal-processing concepts used in digital communication systems \nRecommended reading \nLyons, R.G. (2010). Understanding digital signal processing. Prentice Hall (3rd ed.). \nOppenheim, A.V. and Schafer, R.W. (2007). Discrete-time digital signal processing. Prentice \nHall (3rd ed.). \nStein, J. (2000). Digital signal processing \u2013 a computer science perspective. Wiley. \n \nClass size \nThis module can accommodate a maximum of 24 students (16 Part II students and 8 MPhil \nstudents) \n \nAssessment - Part II students \nThree homework programming assignments, each comprising 20% of the mark \nWritten test, comprising 40% of the total mark. \nAssessment - Part III and MPhil students \nThree homework programming exercises, each comprising 10% of the mark. \nWritten test, comprising 20% of the total mark. \nIndividual implementation project with 8-page written report, topic agreed with lecturer, \ncomprising 50% of the mark. \nFurther Information \nCurrent Cambridge undergraduate students who are continuing onto Part III or the MPhil in \nAdvanced Computer Science may only take this module if they did NOT take it as a Unit of \nAssessment in Part II. \n \nThis module is shared with Part II of the Computer Science Tripos. Assessment will be adjusted \nfor the two groups of students to be at an appropriate level for whichever course the student is \nenrolled on. Further information about assessment and practicals will follow at the first lecture.\n",
        "INTRODUCTION TO COMPUTATIONAL SEMANTICS \n \nAims \nThis is a lecture-style course that introduces students to various aspects of the semantics of \nNatural Languages (mainly English): \n \nLexical Semantics, with an emphasis on theory and phenomenology  \nCompositional Semantics \nDiscourse and pragmatics-related aspects of semantics \nObjectives \nGive an operational definition of what is meant by \u201cmeaning\u201d (for instance, above and beyond \nsyntax); \nName the types of phenomena in language that require semantic consideration, in terms of \nlexical, compositional and discourse/pragmatic aspects, in other words, argue why semantics is \nimportant; \nDemonstrate an understanding of the basics of various semantic representations, including \nlogic-based and graph-based semantic representations, their properties, how they are used and \nwhy they are important, and how they are different from syntactic representations; \nKnow how such semantic representations are derived during or after parsing, and how they can \nbe analysed and mapped to surface strings; \nUnderstand applications of semantic representations e.g. reasoning, validation, and methods \nhow these are approached. \nWhen designing NL tasks that clearly require semantic processing (e.g. knowledge-based QA), \nto be aware of and reuse semantic representations and algorithms when designing the task, \nrather than reinventing the wheel. \nPractical advantages of this course for NLP students \nKnowledge of underlying semantic effects helps improve NLP evaluation, for instance by \nproviding more meaningful error analysis. You will be able to link particular errors to design \ndecisions inside your system. \nYou will learn methods for better benchmarking of your system, whatever the task may be. \nSupervised ML systems (in particular black-box systems such as Deep Learning) are only as \nclever as the datasets they are based on. In this course, you will learn to design datasets so that \nthey are harder to trick without real understanding, or critique existing datasets. \nYou will be able to design tests for ML systems that better pinpoint which aspects of language \nan end-to-end system has \u201cunderstood\u201d. \nYou will learn to detect ambiguity and ill-formed semantics in human-human communication. \nThis can serve to write more clearly and logically. \nYou will learn about decomposing complex semantics-reliant tasks sensibly so that you can \nreuse the techniques underlying semantic analyzers in a modular way. In this way, rather than \nbeing forced to treat complex tasks in an end-to-end manner, you will be able to profit from \npartial explanations and a better error analysis already built into the system. \nSyllabus \nOverview of the course \nEvents and semantic role labelling \n",
        "Referentiality and coreference resolution \nTruth-conditional semantics \nGraph-based meaning representation \nCompositional semantics \nContext-free Graph Rewriting \nSurface realisation \nNegation and psychological approach to semantics \nDynamic semantics \nGricean pragmatics \nVector space models \nCross-modality \nAcquisition of semantics \nDiachronic change of semantics \nSummary of the course \n  \n \nAssessment \n5 take-home exercises worth 20% each: \n \nTake-home assignment 1 is given in Lecture 4 and due is Lecture 6 \n \nStudents are given 10 English sentences and asked to provide their semantic analysis \naccording to truth-conditions. Students will be expected to return 10 logical expressions as their \nanswers. No word limit. Assessment criteria: correctness of the 10 logical expressions; 2 points \nfor each logical expression. \n \nTake-home assignment 2 is given in Lecture 7 and due is Lecture 9  \n \nStudents are given 10 English sentences and asked to provide their syntactico-semantic \nderivations according to the compositionality principle. Students will be expected to return \nderivation graphs as their answers. No word limit. Assessment criteria: correctness of the 10 \nderivation graphs; 2 points for each derivation. \n \nTake-home assignment 3 is given in Lecture 11 and due is Lecture 13  \n \nAll students are assigned with a paper on modelling common ground in dialogue system. \nStudents will receive related but different papers. Each student will write a review of their \nassigned paper, including a comprehensive summary and their own thoughts. Word limit: 1000 \nwords. Assessment criteria: 15 points on whether a student understands the paper correctly; 5 \npoints on whether a student is able to think critically. \n \nTake-home assignment 4 is given in Lecture 13 and due is Lecture 15 \n \n",
        "All students are assigned with a paper on language-vision interaction. Students will receive \nrelated but different papers. Each student will write a review of their assigned paper, including a \ncomprehensive summary and their own thoughts. Word limit: 1000 words. Assessment criteria: \n15 points on whether a student understands the paper correctly; 5 points on whether a student \nis able to think critically. \n \nTake-home assignment 5 is given in Lecture 14 and due is two weeks later. \n \nContent: All students are assigned with a paper on bootstrapping language acquisition. All \nstudents will receive the same paper. Each student will write a review of the paper, including a \ncomprehensive summary and their own thoughts. Word limit: 1000 words. Assessment criteria: \n15 points on whether a student understands the paper correctly; 5 points on whether a student \nis able to think critically.\n \n",
        "INTRODUCTION TO NATURAL LANGUAGE SYNTAX AND PARSING \n \nAims \nThis module aims to provide a brief introduction to linguistics for computer scientists and then \ngoes on to cover some of the core tasks in natural language processing (NLP), focussing on \nstatistical parsing of sentences to yield syntactic representations. We will look at how to \nevaluate parsers and see how well state-of-the-art tools perform given current techniques. \n \nSyllabus \nLinguistics for NLP focusing on morphology and syntax \nGrammars and representations \nStatistical and neural parsing \nTreebanks and evaluation \nObjectives \nOn completion of this module, students should: \n \nunderstand the basic properties of human languages and be familiar with descriptive and \ntheoretical frameworks for handling these properties; \nunderstand the design of tools for NLP tasks such as parsing and be able to apply them to text \nand evaluate their performance; \nPractical work \nWeek 1-7: There will be non-assessed practical exercises between sessions and during \nsessions. \nWeek 8: Download and apply two parsers to a designated text. Evaluate the performance of the \ntools quantitatively and qualitatively. \nAssessment \nThere will be one presentation worth 10% of the final mark; presentation topics will be allocated \nat the start of term. \nAn assessed practical report of not more 8 pages in ACL conference format. It will contribute \n90% of the final mark. \nRecommended reading \nJurafsky, D. and Martin, J. (2008). Speech and Language Processing. Prentice-Hall (2nd ed.). \n(See also 3rd ed. available online.)\n \n",
        "INTRODUCTION TO NETWORKING AND SYSTEMS MEASUREMENTS \n \nAims \nSystems research refers to the study of a broad range of behaviours arising from complex \nsystem design, including: resource sharing and scheduling; interactions between hardware and \nsoftware; network topology, protocol and device design and implementation; low-level operating \nsystems; Interconnect, storage and more. This module will: \n \nTeach performance characteristics and performance measurement methodology and practice \nthrough profiling experiments; \nExpose students to real-world systems artefacts evident through different measurement tools; \nDevelop scientific writing skills through a series of laboratory reports; \nProvide research skills for characterization and modelling of systems and networks using \nmeasurements. \nPrerequisites \nIt is strongly recommended that students have previously (and successfully) completed an \nundergraduate networking course -- or have equivalent experience through project or \nopen-source work. \n \nSyllabus \nIntroduction to performance measurements, performance characteristics [1 lecture] \nPerformance measurements tools and techniques [2 lectures + 2 lab sessions] \nReproducible experiments [1 lecture + 1 lab session] \nCommon pitfalls in measurements [1 lecture] \nDevice and system characterisation [1 lecture + 2 lab sessions] \nObjectives \nOn completion of this module, students should: \n \nDescribe the objectives of measurements, and what they can achieve; \nCharacterise and model a system using measurements; \nPerform reproducible measurements experiments; \nEvaluate the performance of a system using measurements; \nOperate measurements tools and be aware of their limitations; \nDetect anomalies in the network and avoid common measurements pitfalls; \nWrite system-style performance evaluations. \nPractical work \nFive 2-hour in-classroom labs will ask students to develop and use skills in performance \nmeasurements as applied to real-world systems and networking artefacts. Results from these \nlabs (and follow-up work by students outside of the classroom) will by the primary input to lab \nreports. \n \nThe first three labs will provide an introduction and hands on experience with measurement \ntools and measurements methodologies, while the last two labs will focus on practical \nmeasurements and evaluation of specific platforms. Students may find it useful to work in pairs \n",
        "within the lab, but must prepare lab reports independently. The module lecturer will give a short \nintroductory at the start of each lab, and instructors will be on-hand throughout labs to provide \nassistance. \n \nLab participation is not directly included in the final mark, but lab work is a key input to lab \nreports that are assessed. Guided lab experiments resulting in practical write ups. \n \nAssessment \nEach student will write two lab reports. The first lab report will summarise the experiments done \nin the first three labs (20%). The second will be a lab report (5000 words) summarising the \nevaluation of a device or a system (80%). \n \nRecommended reading \nThe following list provides some background to the course materials, but is not mandatory. A \nreading list, including research papers, will be provided in the course materials. \n \nGeorge Varghese. Network algorithmics. Chapman and Hall/CRC, 2010. \nMark Crovella and Balachander Krishnamurthy. Internet measurement: infrastructure, traffic and \napplications. John Wiley and Sons, Inc., 2006. \nBrendan Gregg. Systems Performance: Enterprise and the Cloud, Prentice Hall Press, Upper \nSaddle River, NJ, USA, October 2013. \nRaj Jain, The Art of Computer Systems Performance Analysis: Techniques for Experimental \nDesign, Measurement, Simulation, and Modeling, Wiley - Interscience, New York, NY, USA, \nApril 1991.\n \n",
        "LARGE-SCALE DATA PROCESSING AND OPTIMISATION \n \nAims \nThis module provides an introduction to large-scale data processing, optimisation, and the \nimpact on computer system's architecture. Large-scale distributed applications with high volume \ndata processing such as training of machine learning will grow ever more in importance. \nSupporting the design and implementation of robust, secure, and heterogeneous large-scale \ndistributed systems is essential. To deal with distributed systems with a large and complex \nparameter space, tuning and optimising computer systems is becoming an important and \ncomplex task, which also deals with the characteristics of input data and algorithms used in the \napplications. Algorithm designers are often unaware of the constraints imposed by systems and \nthe best way to consider these when designing algorithms with massive volume of data. On the \nother hand, computer systems often miss advances in algorithm design that can be used to cut \ndown processing time and scale up systems in terms of the size of the problem they can \naddress. Integrating machine learning approaches (e.g. Bayesian Optimisation, Reinforcement \nLearning) for system optimisation will be explored in this course. \n \nSyllabus \nThis course provides perspectives on large-scale data processing, including data-flow \nprogramming, graph data processing, probabilistic programming and computer system \noptimisation, especially using machine learning approaches, thus providing a solid basis to work \non the next generation of distributed systems. \n \nThe module consists of 8 sessions, with 5 sessions on specific aspects of large-scale data \nprocessing research. Each session discusses 3-4 papers, led by the assigned students. One \nsession is a hands-on tutorial on on dataflow programming fundamentals. The first session \nadvises on how to read/review a paper together with a brief introduction on different \nperspectives in large-scale data \nprocessing and optimisation. The last session is dedicated to the student presentation of \nopensource project studies. \n \nIntroduction to large-scale data processing and optimisation \nData flow programming: Map/Reduce to TensorFlow \nLarge-scale graph data processing: storage, processing model and parallel processing \nDataflow programming hands-on tutorial \nProbabilistic Programming \nOptimisations in ML Compiler \nOptimisations of Computer systems using ML \nPresentation of Open Source Project Study \nObjectives \nOn completion of this module, students should: \n \nUnderstand key concepts of scalable data processing approaches in future computer systems. \n",
        "Obtain a clear understanding of building distributed systems using data centric programming \nand large-scale data processing. \nUnderstand a large and complex parameter space in computer system's optimisation and \napplicability of Machine Learning approach. \nCoursework \nReading Club: \nThe preparation for the reading club will involve 1-3 papers every week. At each session, \naround 3-4 papers are selected under the given topic, and the students present their review \nwork. \nHands-on tutorial session of data flow programming including writing an application of \nprocessing streaming in Twitter data and/or Deep Neural Networks using Google TensorFlow \nusing cluster computing. \nReports \nThe following three reports are required, which could be extended from the assignment of the \nreading club, within the scope of data centric systems. \n \nReview report on a full length paper (max 1800 words) \nDescribe the contribution of the paper in depth with criticisms \nCrystallise the significant novelty in contrast to other related work \nSuggestions for future work \nSurvey report on sub-topic in large-scale data processing and optimisation (max 2000 words) \nPick up to 5 papers as core papers in the survey scope \nRead the above and expand reading through related work \nComprehend the view and finish an own survey paper \nProject study and exploration of a prototype (max 2500 words) \nWhat is the significance of the project in the research domain? \nCompare with similar and succeeding projects \nDemonstrate the project by exploring its prototype \nReports 1 should be handed in by the end of 5th week; Report 2 in the 8th week and Report 3 \non the first day of Lent Term. \n \nAssessment \nThe final grade for the course will be provided as a percentage, and the assessment will consist \nof two parts: \n \n25%: for reading club (participation, presentation) \n75%: for the three reports: \n15%: Intensive review report \n25%: Survey report \n35%: Project study \nRecommended reading \nM. Abadi et al. TensorFlow: A System for Large-Scale Machine Learning, OSDI, 2016. \nD. Aken et al.: Automatic Database Management System Tuning Through Large-scale Machine \nLearning, SIGMOD, 2017. \n",
        "J. Ansel et al. Opentuner: an extensible framework for program autotuning. PACT, 2014. \nV. Dalibard, M. Schaarschmidt, E. Yoneki. BOAT: Building Auto-Tuners with Structured Bayesian \nOptimization, WWW, 2017. \nJ. Dean et al. Large scale distributed deep networks. NIPS, 2012. \nG. Malewicz, M. Austern, A. Bik, J. Dehnert, I. Horn, N. Leiser, G. Czajkowski. Pregel: A System \nfor Large-Scale Graph Processing, SIGMOD, 2010. \nA. Mirhoseini et al. Device Placement Optimization with Reinforcement Learning, ICML, 2017. \nD. Murray, F. McSherry, R. Isaacs, M. Isard, P. Barham, M. Abadi. Naiad: A Timely Dataflow \nSystem, SOSP, 2013. \nM. Schaarschmidt, S. Mika, K. Fricke and E. Yoneki: RLgraph: Modular Computation Graphs for \nDeep Reinforcement Learning, SysML, 2019. \nZ. Jia, O. Padon, J. Thomas, T. Warszawski, M. Zaharia,  A. Aiken: TASO: Optimizing Deep \nLearning Computation with Automated Generation of Graph Substitutions: SOSP, 2019. \nH. Mao et al.: Park: An Open Platform for Learning-Augmented Computer Systems, \nOpenReview, 2019. \nJ. Shao, et al.: Tensor Program Optimization with Probabilistic Programs, NeurIPS, 2022.  \nA complete list can be found on the course material web page. See also 2023-2024 course \nmaterial on the previous course Large-Scale Data Processing and Optimisation.\n \n",
        "MACHINE LEARNING AND THE PHYSICAL WORLD \nAims \nThe module \u201cMachine Learning and the Physical World\u201d is focused on machine learning \nsystems that interact directly with the real world. Building artificial systems that interact with the \nphysical world have significantly different challenges compared to the purely digital domain. In \nthe real world data is scares, often uncertain and decisions can have costly and irreversible \nconsequences. However, we also have the benefit of centuries of scientific knowledge that we \ncan draw from. This module will provide the methodological background to machine learning \napplied in this scenario. We will study how we can build models with a principled treatment of \nuncertainty, allowing us to leverage prior knowledge and provide decisions that can be \ninterrogated. \n \nThere are three principle points about machine learning in the real world that will concern us. \n \nWe often have a mechanistic understanding of the real world which we should be able to \nbootstrap to make decisions. For example, equations from physics or an understanding of \neconomics. \nReal world decisions have consequences which may have costs, and often these cost functions \nneed to be assimilated into our machine learning system. \nThe real world is surprising, it does things that you do not expect and accounting for these \nchallenges requires us to build more robust and or interpretable systems. \nDecision making in the real world hasn\u2019t begun only with the advent of machine learning \ntechnologies. There are other domains which take these areas seriously, physics, environmental \nscientists, econometricians, statisticians, operational researchers. This course identifies how \nmachine learning can contribute and become a tool within these fields. It will equip you with an \nunderstanding of methodologies based on uncertainty and decision making functions for \ndelivering on these challenges. \n \nObjectives \nYou will gain detailed knowledge of \n \nsurrogate models and uncertainty \nsurrogate-based optimization \nsensitivity analysis \nexperimental design \nYou will gain knowledge of \n \ncounterfactual analysis \nsurrogate-based quadrature \nSchedule \nWeek 1: Introduction to the unit and foundation of probabilistic modelling \n \nWeek 2: Gaussian processes and probablistic inference \n \n",
        "Week 3: Simulation and Sequential decision making under uncertainty \n \nWeek 4: Emulation and Experimental Design \n \nWeek 5: Sensitivity Analysis and Multifidelity Modelling \n \nWeek 6-8: Case studies of applications and Projects \n \nPractical work \nDuring the first five weeks of the unit we will provide a weekly worksheet that will focus on \nimplementation and practical exploration of the material covered in the lectures. The worksheets \nwill allow you to build up a these methods without relying on extensive external libraries. You are \nfree to use any programming language of choice however we highly recommended the use of \n=Python=. \n \nAssessment \nTwo individual tasks (15% each) \nGroup Project (70%). Each group will work on an application of uncertainty that covers the \nmaterial of the first 5 weeks of lectures in the unit. Each group will submit a report which will \nform the basis of the assessment. In addition to the report each group will also attend a short \noral examination based on the material covered both in the report and the taught material. \nRecommended Reading \nRasmussen, C. E. and Williams, C. K. I. (2006). Gaussian Processes for Machine Learning. MIT \nPress \n \nBishop, C. (2006). Pattern recognition and machine learning. Springer. \nhttps://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-an\nd-Machine-Learning-2006.pdf \n \nLaplace, P. S. (1902). A Philosophical Essay on Probabilities. John Wiley & Sons. \nhttps://archive.org/details/philosophicaless00lapliala\n \n",
        "MACHINE VISUAL PERCEPTION \nAims \nThis course aims at introducing the theoretical foundations and practical techniques for machine \nperception, the capability of computers to interpret data resulting from sensor measurements. \nThe course will teach the fundamentals and modern techniques of machine perception, i.e. \nreconstructing the real world starting from sensor measurements with a focus on machine \nperception for visual data. The topics covered will be image/geometry representations for \nmachine perception, semantic segmentation, object detection and recognition, geometry \ncapture, appearance modeling and acquisition, motion detection and estimation, \nhuman-in-the-loop machine perception, select topics in applied machine perception. \n \nMachine perception/computer vision is a rapidly expanding area of research with real-world \napplications. An understanding of machine perception is also important for robotics, interactive \ngraphics (especially AR/VR), applied machine learning, and several other fields and industries. \nThis course will provide a fundamental understanding of and practical experience with the \nrelevant techniques. \n \nLearning outcomes \nStudents will understand the theoretical underpinnings of the modern machine perception \ntechniques for reconstructing models of reality starting from an incomplete and imperfect view of \nreality. \nStudents will be able to apply machine perception theory to solve practical problems, e.g. \nclassification of images, geometry capture. \nStudents will gain an understanding of which machine perception techniques are appropriate for \ndifferent tasks and scenarios. \nStudents will have hands-on experience with some of these techniques via developing a \nfunctional machine perception system in their projects. \nStudents will have practical experience with the current prominent machine perception \nframeworks. \nSyllabus \nThe fundamentals of machine learning for machine perception \nDeep neural networks and frameworks for machine perception \nSemantic segmentation of objects and humans \nObject detection and recognition \nMotion estimation, tracking and recognition \n3D geometry capture \nAppearance modeling and acquisition \nSelect topics in applied machine perception \nAssessment \nA practical exercise, worth 20% of the mark. This will cover the basics and theory of machine \nperception and some of the practical techniques the students will likely use in their projects. This \nis individual work. No GPU hours will be needed for the practical work. \nA machine perception project worth 80% of the marks: \n",
        "Course projects will be selected by the students following the possible project themes proposed \nby the lecturer, and will be checked by the lecturer for appropriateness.  \nThe students will form groups of 2-3 to design, implement, report, and present a project to tackle \na given task in machine perception. \nThe final mark will be composed of an implementation/report mark (60%) and a presentation \nmark (20%). Each team member will be evaluated based on her/his contribution. \nEach project will have extensions to be completed only by the ACS students. Each student will \nwrite a different part of the report, whose author will be clearly marked. Each student will further \nsummarise her/his contributions to the project in the same report. \nRecommended Reading List \nComputer Vision: Algorithms and Applications, Richard Szeliski, Springer, 2010. \nDeep Learning, Ian Goodfellow, Yoshua Bengio, and Aaron Courville, MIT Press, 2016. \nMachine Learning and Visual Perception, Baochang Zhang, De Gruyter, 2020. \nFurther Information \nCurrent Cambridge undergraduate students who are continuing onto Part III or the MPhil in \nAdvanced Computer Science may only take this module if they did NOT take it as a Unit of \nAssessment in Part II. \n \nThis module is shared with Part II of the Computer Science Tripos. Assessment will be adjusted \nfor the two groups of students to be at an appropriate level for whichever course the student is \nenrolled on. Further information about assessment and practicals will follow at the first lecture.\n",
        "MOBILE, WEARABLE SYSTEMS AND MACHINE LEARNING \n \nAims \nThis module aims to introduce the latest research advancements in mobile systems and mobile \ndata machine learning, spanning a range of domains including systems, data gathering, \nanalytics and machine learning, on device machine learning and applications such as health, \ntransportation, behavior monitoring, cyber-physical systems, autonomous vehicles, drones. The \ncourse will cover current and seminal research papers in the area of research. \n \nSyllabus \nThe course will consist of one introductory lecture, seven two-hour, and one three-hour, \nsessions covering a variety of topics roughly including the following material (some variation in \nthe topics might happen from year to year): \n \nSystem, Energy and Security \nBackscatter Communication, Battery Free and Energy Harvesting Devices \nNew Sensing Modalities \nMachine Learning on Wearable Data \nOn Device Machine Learning \nMobile and Wearable Health \nMobile and Wearable Systems of Sustainability \nEach week, three class participants will be assigned to introduce assigned three papers via \n20-minute presentations, conference-style and highlighting critically its features. Each \npresentation will be followed by 10 minutes of questions. This will be followed by 10 minutes of \ngeneral discussion. Slides will be used for presentation. \n \nStudents will give one or more presentations each term. Each student will submit a paper review \neach week for one of the three papers presented except for the week they will be presenting \nslides. Each review will follow a template and be up to 1,000 words. Each review will receive a \nmaximum of 10 points. As a result, each student will produce 6-7 reviews and at least one \npresentation, probably two. All participants are expected to attend and participate in every class; \nthe instructor must be notified of any absences in advance. \n \nObjectives \nOn completion of this module students should have an understanding of the recent key research \nin mobile and sensor systems and mobile analytics as well as an improved critical thinking over \nresearch papers. \n \nAssessment \nAggregate mark for 7 assignments from 6-7 reports and 1-2 presentations. Each report or \npresentation will contribute one seventh of the course mark. \nA tick for presence and participation to each class will also be awarded. \nRecommended reading \n",
        "Readings from the most recent conferences such as AAAI, ACM KDD, ACM MobiCom, ACM \nMobiSys, ACM SenSys, ACM UbiComp, ICLR, ICML Neurips, and WWW pertinent to mobile \nsystems and data.\n \n",
        "NETWORK ARCHITECTURES \nAims \nThis module aims to provide the world with more network architects. The 2011-2012 version \nwas oriented around the evolution of IP to support new services like multicast, mobility, \nmultihoming, pub/sub and, in general, data oriented networking. The course is a paper reading \nwhich puts the onus on the student to do the work. \n \nSyllabus \nIPng [2 lectures, Jon Crowcroft] \nNew Architectures [2 lectures, Jon Crowcroft] \nMulticast [2 lectures, Jon Crowcroft] \nContent Distribution and Content Centric Networks [2 lectures, Jon Crowcroft] \nResource Pooling [2 lectures, Jon Crowcroft] \nGreen Networking [2 lectures, Jon Crowcroft] \nAlternative Router Implementions [2 lectures, Jon Crowcroft] \nData Center Networks [2 Lectures, Jon Crowcroft] \nObjectives \nOn completion of this module, students should be able to: \n \ncontribute to new network system designs; \nengineer evolutionary changes in network systems; \nidentify and repair architectural design flaws in networked systems; \nsee that there are no perfect solutions (aside from academic ones) for routing, addressing, \nnaming; \nunderstand tradeoffs in modularisation and other pressures on clean software systems \nimplementation, and see how the world is changing the proper choices in protocol layering, or \nnon layered or cross-layered. \nCoursework \nAssessment is through three graded essays (each chosen individually from a number of \nsuggested or student-chosen topics), as follows: \n \nAnalysis of two different architectures for a particular scenario in terms of cost/performance \ntradeoffs for some functionality and design dimension, for example: \nATM \u2013 e.g. for hardware versus software tradeoff \nIP \u2013 e.g. for mobility, multi-homing, multicast, multipath \n3GPP \u2013 e.g. for plain complexity versus complicatedness \nA discursive essay on a specific communications systems component, in a particular context, \nsuch as ad hoc routing, or wireless sensor networks. \nA bespoke network design for a narrow, well specified specialised target scenario, for example: \nA customer baggage tracking network for an airport. \nin-flight entertainment system. \nin-car network for monitoring and control. \ninter-car sensor/control network for automatic highways. \nPractical work \n",
        "This course does not feature any implementation work due to time constraints. \n \nAssessment \nThree 1,200-word essays (worth 25% each), and \nan annotated bibliography (25%). \nRecommended reading \nPre-course reading: \n \nKeshav, S. (1997). An engineering approach to computer networking. Addison-Wesley (1st ed.). \nISBN 0201634422 \nPeterson, L.L. and Davie, B.S. (2007). Computer networks: a systems approach. Morgan \nKaufmann (4th ed.). \n \nDesign patterns: \n \nDay, John (2007). Patterns in network architecture: a return to fundamentals. Prentice Hall. \n \nExample systems: \n \nKrishnamurthy, B. and Rexford, J. (2001). Web protocols and practice: HTTP/1.1, Networking \nprotocols, caching, and traffic measurement. Addison-Wesley. \n \nEconomics and networks: \n \nFrank, Robert H. (2008). The economic naturalist: why economics explains almost everything. \n \nPapers: \n \nCertainly, a collection of papers (see ACM CCR which publishes notable network researchers' \nfavourite ten papers every 6 months or so).\n \n",
        "OVERVIEW OF NATURAL LANGUAGE PROCESSING \n \nAims \nThis course introduces the fundamental techniques of natural language processing. It aims to \nexplain the potential and the main limitations of these techniques. Some current research issues \nare introduced and some current and potential applications discussed and evaluated. Students \nwill also be introduced to practical experimentation in natural language processing. \n \nLectures \nOverview. Brief history of NLP research, some current applications, components of NLP \nsystems. \nMorphology and Finite State Techniques. Morphology in different languages, importance of \nmorphological analysis in NLP, finite-state techniques in NLP. \nPart-of-Speech Tagging and Log-Linear Models. Lexical categories, word tagging, corpora and \nannotations, empirical evaluation. \nPhrase Structure and Structure Prediction. Phrase structures, structured prediction, context-free \ngrammars, weights and probabilities. Some limitations of context-free grammars. \nDependency Parsing. Dependency structure, grammar-free parsing, incremental processing.  \nGradient Descent and Neural Nets. Parameter optimisation by gradient descent. Non-linear \nfunctions with neural network layers. Log-linear model as softmax layer. Current findings of \nNeural NLP. \nWord representations. Representing words with vectors, count-based and prediction-based \napproaches, similarity metrics. \nRecurrent Neural Networks. Modelling sequences, parameter sharing in recurrent neural \nnetworks, neural language models, word prediction. \nCompositional Semantics. Logical representations, compositional semantics, lambda calculus, \ninference and robust entailment. \nLexical Semantics. Semantic relations, WordNet, word senses. \nDiscourse. Discourse relations, anaphora resolution, summarization. \nNatural Language Generation. Challenges of natural language generation (NLG), tasks in NLG, \nsurface realisation. \nPractical and assignments. Students will build a natural language processing system which will \nbe trained and evaluated on supplied data. The system will be built from existing components, \nbut students will be expected to compare approaches and some programming will be required \nfor this. Several assignments will be set during the practicals for assessment. \nObjectives \nBy the end of the course students should: \n \nbe able to discuss the current and likely future performance of several NLP applications; \nbe able to describe briefly a fundamental technique for processing language for several \nsubtasks, such as morphological processing, parsing, word sense disambiguation etc.; \nunderstand how these techniques draw on and relate to other areas of computer science. \nRecommended reading \n",
        "Jurafsky, D. & Martin, J. (2023). Speech and language processing. Prentice Hall (3rd ed. draft, \nonline). \n \nAssessment - Part II Students \nAssignment 1 - 10% of marks \nAssignment 2 - 60% of marks \nAssignment 3 - 30% of marks \nCoursework \nSubmit work for 3 assignments as part of practical sessions on information extraction. \n \nThese include an annotation exercise, a feature-based classifier along with code repository and \ndocumentation, and a 4,000-word report on results and analysis from an extended information \nextraction experiment. \n \nPractical work \nStudents will build a natural language processing system which will be trained and evaluated on \nsupplied data. The system will be built from existing components, but students will be expected \nto compare approaches and some programming will be required for this. \n \nAssessment - Part III and MPhil Students \nAssessment will be based on the practicals: \n \nFirst practical exercise (10%, ticked) \nSecond practical exercise (25%, code repository or notebook with documentation) \nFinal report (65%, 4,000 words, excluding references) \nFurther Information \nAlthough the lectures don't assume any exposure to linguistics, the course will be easier to \nfollow if students have some understanding of basic linguistic concepts. The following may be \nuseful for this: The Internet Grammar of English \n \nCurrent Cambridge undergraduate students who are continuing onto Part III or the MPhil in \nAdvanced Computer Science may only take this module if they did NOT take it as a Unit of \nAssessment in Part II. \n \nStudents from other departments may attend the lectures for this module if space allows. \nHowever students wanting to take it for credit will need to make arrangements for assessment \nwithin their own department. \n \nThis module is shared with Part II of the Computer Science Tripos. Assessment will be adjusted \nfor the two groups of students to be at an appropriate level for whichever course the student is \nenrolled on. Further information about assessment and practicals will follow at the first lecture.\n",
        "PRACTICAL RESEARCH IN HUMAN-CENTRED AI \n \nAims \nThis is an advanced course in human-computer interaction, with a specialist focus on intelligent \nuser interfaces and interaction with machine-learning and artificial intelligence technologies. The \nformat will be largely Practical, with students carrying out a mini-project involving empirical \nresearch investigation. These studies will investigate human interaction with some kind of \nmodel-based system for planning, decision-making, automation etc. Possible study formats \nmight include: System evaluation, Field observation, Hypothesis testing experiment, Design \nintervention or Corpus analysis, following set examples from recent research publications. \nProject work will be formally evaluated through a report and presentation. \n \nLectures \n(note that Lectures 2-7 also include one hour class discussion of practical work) \n        \u2022 Current research themes in intelligent user interfaces \n        \u2022 Program synthesis \n        \u2022 Mixed initiative interaction \n        \u2022 Interpretability / explainable AI \n        \u2022 Labelling as a fundamental problem \n        \u2022 Machine learning risks and bias \n        \u2022 Visualisation and visual analytics \n        \u2022 Student research presentations \n \nObjectives \nBy the end of the course students should: \n        \u2022 be familiar with current state of the art in intelligent interactive systems \n        \u2022 understand the human factors that are most critical in the design of such systems \n        \u2022 be able to evaluate evidence for and against the utility of novel systems \n        \u2022 have experience of conducting user studies meeting the quality criteria of this field \n        \u2022 be able to write up and present user research in a professional manner \n \nClass Size \nThis module can accommodate upto 20 Part II, Part III and MPhil students. \n \nRecommended reading \nBrad A. Myers and Richard McDaniel (2000). Demonstrational Interfaces: Sometimes You Need \na Little Intelligence, Sometimes You Need a Lot. \n \nAlan Blackwell (2024). Moral Codes: Designing alternatives to AI \n \nAssessment - Part II Students \nThe format will be largely practical, with students carrying out an individual mini-project involving \nempirical research investigation. \n \n",
        "Assignment 1: six incremental submissions which together contribute 20% to the final module \nmark. \n \nAssignment 2: Final report - 80% of the final module mark \n  \n \nAssessment - MPhil / Part III Students \nThere will be a minor assessment component (20%) in which students compile a reflective diary \nthroughout the term, reporting on the weekly sessions. Diary entries should include citations to \nany key references, notes of possible further reading, summary of key points, questions relevant \nto the personal project, and points of interest noted in relation to the work of other students. \n \nThe major assessment component (80%) will involve a report on research findings, in the style \nof a submission to the ACM CHI or IUI conferences. This work will be submitted incrementally \nthrough the term, in order that feedback can be provided before final assessment of the full \nreport. Phased submissions will cover the following aspects of the empirical study: \n \nResearch question \nMethod \nLiterature Review \nIntroduction \nResults \nDiscussion / Conclusion \nFeedback on each phased submission will include an indicative mark for guidance, but with the \nunderstanding that the final grade will be based on the final delivered report, and that this may \ngo up (or possibly down), depending on how well the student responds to earlier feedback. \n \nReflective diary entries will also be assessed, but graded at a relatively coarse granularity \ncorresponding to the ACS grading bands, and with minimal written feedback. Informal and \ngeneric feedback will be offered verbally in class, and also potentially supplemented with peer \nassessment. \n \nFurther Information \nCurrent Cambridge undergraduate students who are continuing onto Part III or the MPhil in \nAdvanced Computer Science may only take this module if they did NOT take it as a Unit of \nAssessment in Part II. \n \nThis module is shared with Part II of the Computer Science Tripos. Assessment will be adjusted \nfor the two groups of students to be at an appropriate level for whichever course the student is \nenrolled on. Further information about assessment and practicals will follow at the first lecture.\n",
        "PRINCIPLES OF MACHINE LEARNING SYSTEMS \n \nPrerequisites: It is recommended that students have successfully completed introductory \ncourses, at an undergraduate level, in: 1) operating systems, 2) computer architecture, and 3) \nmachine learning. In addition, this course will heavily focus on deep neural network \nmethodologies and assume familiarity with common neural architectures and related algorithms. \nIf these topics were not covered within the machine learning course taken, students should \nsupplement by reviewing material in a book like: \"Dive into Deep Learning\". Finally, students are \nassumed to be comfortable with programming in Python. \nMoodle, timetable \n \nAims \nThis course will examine the emerging principles and methodologies that underpin scalable and \nefficient machine learning systems. Primarily, the course will focus on an exciting cross-section \nof algorithms and system techniques that are used to support the training and inference of \nmachine learning models under a spectrum of computing systems that range from constrained \nembedded systems up to large-scale distributed systems. It will also touch upon the new \nengineering practices that are developing in support of such systems at scale. When needed to \nappreciate issues of scalability and efficiency, the course will drill down to certain aspects of \ncomputer architecture, systems software and distributed systems and explore how these \ninteract with the usage and deployment of state-of-the-art machine learning. \n \nSyllabus \nTopics covered may include the following, with confirmation a month before the course begins: \n \nSystem Performance Trade-offs \nDistributed Learning Algorithms  \nModel Compression  \nDeep Learning Compilers  \nFrameworks and Run-times  \nScalable Inference Serving  \nDevelopment Practices  \nAutomated Machine Learning  \nFederated Learning  \nPrimarily, topics are covered with conventional lectures. However, where appropriate, material \nwill be delivered through hands-on lab tutorials. Lab tutorials will make use of hardware \nincluding ARM microcontrollers and multi-GPU machines to explore forms of efficient machine \nlearning (any necessary equipment will be provided to students) \n \nAssessment \nEach student will be assessed on 3 labs which will be worth 30% of their grade. They will also \nundertake a written project report which will be worth 70% of the grade. This report will detail an \ninvestigation into a particular aspect of machine learning systems, this report will be made \navailable publicly.\n \n",
        "PROOF ASSISTANTS \nAims \nThis module introduces students to interactive theorem proving using Isabelle and Coq. It \nincludes techniques for specifying formal models of software and hardware systems and for \nderiving properties of these models. \n \n  \n \nSyllabus \nIntroduction to proof assistants and logic. \nReasoning in predicate logic and typed set theory. \nReasoning in dependent type theory. \nInductive definitions and recursive functions: modelling them in logic, reasoning about them. \nModelling operational semantics definitions and proving properties. \n  \n \nObjectives \nOn completion of this module, students should: \n \npossess good skills in the use of Isabelle and Coq; \nbe able to specify inductive definitions and perform proofs by induction; \nbe able to formalise and reason about a variety of specifications in a proof assistant. \nCoursework \nPractical sessions will allow students to develop skills. Some of the exercises will serve as the \nbasis for assessment. \n \nAssessment \nAssessment will be based on two marked assignments (due in weeks 5 and 8) with students \nperforming small formalisation and verification projects in a proof assistant together with a \nwrite-up explaining their work (word limit 2,500 per project for the write-up). Each of the \nassignments will be worth 100 marks, distributed as follows: \n \n50 marks for completing basic formalisation and verification tasks assessing grasp of the \nmaterial taught in the lecture. Students will submit their work as theory files for either the \nIsabelle or Coq proof assistant, and they will be assessed for correctness and completeness of \nthe specifications and proofs. \n20 marks for completing designated more challenging tasks, requiring the use of advanced \ntechniques or creative proof strategies. \n30 marks for a clear write-up explaining the design decisions made during the formalisation and \nthe strategy used for the proofs, where 10 of these marks will be reserved for write-ups of \nexceptional quality, e.g. demonstrating particular insight. \nThe main tasks in the assignments will be designed to assess the student's proficiency with the \nbasic material and techniques taught in the lectures and the practical sessions, while the more \nchallenging tasks will give exceptional students the opportunity to earn distinction marks \n",
        " \nRecommended reading \nNipkow, T., Klein, G. (2014). Concrete Semantics with Isabelle/HOL. (The first part of this book, \n\u201cProgramming and Proving in Isabelle/HOL\u201d, comes with the Isabelle distribution.)  \n \nNipkow, T., Paulson, L.C. and Wenzel, M. (2002). A proof assistant for higher-order logic. \nSpringer LNCS 2283.  \n \nThe Software Foundations series of books, in particular the first two: \n \nPierce, B., Azevedo de Amorim, A., Casinghino, C., Gaboardi, M., Greenberg, M., Hri\u0163cu, C., \nSj\u00f6berg, V., Yorgey, B. (2023). Logical Foundations  \nPierce, B., Azevedo de Amorim, A., Casinghino, C., Gaboardi, M., Greenberg, M., Hri\u0163cu, C., \nSj\u00f6berg, V., Tolmach, A., Yorgey, B. (2023). Programming Language Foundations  \nChlipala, A. (2022). Certified programming with dependent types: a pragmatic introduction to the \nCoq proof assistant. MIT Press.  \n \nSergey, I. (2014). Programs and Proofs: Mechanizing Mathematics with Dependent Types.  \n \nAll of these are freely available online.\n \n",
        "QUANTUM ALGORITHMS AND COMPLEXITY \n \nAims \nThis module is a research-focused introduction to the theory of quantum computing. The aim is \nto prepare the students to conduct research in quantum algorithms and quantum complexity \ntheory. \n \nSyllabus \nTopics will vary from year to year, based on developments in cutting edge research. \nRepresentative topics include: \n \nQuantum algorithms: \n \nQuantum learning theory \nQuantum property testing \nShadow tomography \nQuantum walks \nQuantum state/unitary synthesis. \nStructure vs randomness in quantum algorithms \nQuantum complexity theory: \n \nThe quantum PCP conjecture \nEntanglement and MIP \nState complexity, AdS/CFT, and quantum gravity \nQuantum locally testable codes \nPseudorandom states and unitaries \nQuantum zero-knowledge proofs \nObjectives \nOn completion of this module, students should: \n \nbe familiar with contemporary quantum algorithms \ndevelop an understanding of the power and limitations of quantum computation \nconduct research in quantum algorithms and complexity theory \nAssessment \nMini research project (70%) \nPresentation (20%) \nAttendance and participation (10%) \nTimelines for the assignment submissions:  \n \nsubmit questions/observations on a weekly basis (i.e., Weeks 2-8) \ndeliver talks in Weeks 5-8 \nsubmit their mini-project at the end of term \nRecommended reading material and resources \n\u201cQuantum Computing Since Democritus\u201d by Scott Aaronson \n",
        " \n\u201cIntroduction to Quantum Information Science\u201d by Scott Aaronson \n \n\u201cQuantum Computing: Lecture Notes\u201d by Ronald de Wolf \n \n\u201cQuantum Computation and Quantum Information\u201d by Nielsen and Chuang\n \n",
        "UNDERSTANDING QUANTUM ARCHITECTURE \nPrerequisites: Requires introductory linear algebra - concepts such as eigenvalues, Hermitian \nmatrices, unitary matrices. Taking the Part II Quantum Computing course (or similar) is helpful \nbut not required. Familiarity with computer architecture and compilers is helpful. \nMoodle, timetable \n \nAims \nThis course covers the architecture of a practical-scale quantum computer. We will examine the \nresource requirements of practical quantum applications, understand the different layers of the \nquantum stack, the techniques used in these layers and examine how these layers come \ntogether to enable practical quantum advantage over classical computing. \n \nSyllabus \nThe course has two parts: a series of lectures to cover important aspects of the quantum stack \nand a set of student presentations. The following are a list of representative topics: \n \n- Basics of quantum computing \n- The fault-tolerant quantum stack \n- Compilation \n- Instruction sets \n- Implementations of quantum error correction \n- Implementation of magic state distillation \n- Resource estimation \n \nStudent presentations will be based on a reading list of important papers in quantum \narchitecture.  \n  \n \nObjectives \nAt the end of the course, students will have a broad understanding of the quantum computing \nstack. They will understand how major qubit technologies work, design challenges in real \nquantum hardware, how quantum applications are mapped to a system and the importance of \nquantum error correction for scalability.  \n \nAssessment \nSeminar presentation: 20% \nRead one paper from a provided reading list \nPrepare a 20 minute presentation on it + 10 minutes of answering questions. \nThe student presentation should be similar to a conference presentation - convey the problem, \nwhat are prior solutions, what did the paper do, what are the results, what are future directions \nFor the 20% of the score, the score will be split as 15% and 5%. 15% based on how well the \nstudent understands and explains the paper to the rest of the audience. 5% based on the Q&A \nsession. \n \n",
        "  \n \nCourse project: 80% (split across a proposal, mid-term report, final report) \nA. Research proposal - at least 500 words (10% of total) \nB. Mid-term report - at least 1000 words (including aspects like the research problem, literature \nreview, what questions will be evaluated, any early ideas or methods (20% of total)  \nC. Final report - up to 4 pages double column in a conference paper style with 3000-4000 \nwords. In addition to the mid term report, should include aspects like results and future \ndirections. (50% of the total) \nD. Report on contributions (in case of group work by 2 students) - 1 paragraph on individual \ncontribution of the student. 1 paragraph on teammate's contribution. \nStudents may work alone in the project, in which case they need only components A-C. \nStudents may work in pairs of two, but they should in addition individually submit D. The \ninstructor may conduct a short viva in case contributions from both team members are not clear \nor imbalanced. \nRecommended reading \nNielsen M.A., Chuang I.L. (2010). Quantum Computation and Quantum Information. Cambridge \nUniversity Press. \n \nMermin N.D. (2007). Quantum Computer Science: An Introduction. Cambridge University Press. \n \nAssessing requirements to scale to practical quantum advantage (2022) Beverland et al.\n",
        "8th Semester \nADVANCED TOPICS IN CATEGORY THEORY \n \n \nTeaching \nThe teaching style will be lecture-based, but supported by a practical component where \nstudents will learn to use a proof assistant for higher category theory, and build a small portfolio \nof proofs. Towards the end of the course we will explore some of the exciting computer science \nresearch literature on monoidal and higher categories, and students will choose a paper and \npresent it to the class. \n \nAims \nThe module will introduce advanced topics in category theory. The aim is to train students to \nengage and start modern research on the mathematical foundations of higher categories, the \ngraphical calculus, monoids and representations, type theories, and their applications in \ntheoretical computer science, both classical and quantum. \n \nObjectives \nOn completion of this module, students should: \n \nBe familiar with the techniques of compositional category theory. \nHave a strong understanding of basic categorical semantic models. \nBegun exploring current research in monoidal categories and higher structures. \nSyllabus \nPart 1, lecture course: \nThe first part of the course introduces concepts from monoidal categories and higher categories, \nand explores their application in computer science. \n\u2010 Monoidal categories and the graphical calculus \n\u2010 The proof assistant homotopy.io \n\u2010 Coherence theorems and higher category theory \n\u2010 Linearity, superposition, duality, quantum entanglement \n\u2010 Monoids, Frobenius algebras and bialgebras \n\u2010 Type theory for higher category theory \n \nPart 2, exploring the research frontier: \nIn the second part of the course, students choose a research paper to study, and give a \npresentation to the class. \nThere is a nice varied literature related to the topics of the course, and the lecturer will supply a \nlist of suggested papers.  \n \nClasses \nThere will be three exercise sheets for homework, with accompanying classes by a teaching \nassistant to go over them. \n \n",
        "Assessment \nProblem sheets (50%) \nClass presentation (20%) \nPractical portfolio (30%) \nReading List \nChris Heunen and Jamie Vicary, \u201cCategory for Quantum Theory: An Introduction\u201d, Oxford \nUniversity Press\n \n",
        "ADVANCED TOPICS IN COMPUTER SYSTEMS \n \nAims \nThis module will attempt to provide an overview of \u201csystems\u201d research. This is a very broad field \nwhich has existed for over 50 years and which has historically included areas such as operating \nsystems, database systems, file systems, distributed systems and networking, to name but a \nfew. The course will thus necessarily cover only a tiny subset of the field. \n \nMany good ideas in systems research are the result of discussing and debating previous work. \nA primary aim of this course therefore will be to educate students in the art of critical thinking: \nthe ability to argue for and/or against a particular approach or idea. This will be done by having \nstudents read and critique a set of papers each week. In addition, each week will include \npresentations from a number of participants which aim to advocate or criticise each piece of \nwork. \n \nSyllabus \nThe syllabus for this course will vary from year to year so as to cover a mixture of older and \nmore contemporary systems papers. Contemporary papers will be generally selected from the \npast 5 years, primarily drawn from high quality conferences such as SOSP, OSDI, ASPLOS, \nFAST, NSDI and EuroSys. Example topics might include: \n \nSystems Research and System Design \nOS Structure and Virtual Memory \nVirtualisation \nConsensus \nScheduling \nPrivacy \nData Intensive Computing \nBugs \nThe reading each week will involve a load equivalent to 3 full length papers. Students will be \nexpected to read these in detail and prepare a written summary and review. In addition, each \nweek will contain one or more short presentations by students for each paper. The types of \npresentation will include: \n \nOverview: a balanced presentation of the paper, covering both positive and negative aspects. \nAdvocacy: a positive spin on the paper, aiming to convince others of its value. \nCriticism: a negative take on the paper, focusing on its weak spots and omissions. \nThese presentation roles will be assigned in advance, regardless of the soi disant absolute merit \nof the paper or the preference of the student. Furthermore, all students \u2013 regardless of any \nassigned presentation role in a given week \u2013 will be expected to participate in the class by \nasking questions and generally entering into the debate. \n \nObjectives \n",
        "On completion of this module students should have a broad understanding of some key papers \nand concepts in computer systems research, as well as an appreciation of how to argue for or \nagainst any particular idea. \n \nCoursework and practical work \nCoursework will be the production of the weekly paper reviews. Practical work will be presenting \npapers as appropriate, as well as ongoing participation in the class. \n \nAssessment \nAssessment consists of: \n \nOne essay per week for 7 weeks (10% each) \nPresentation (20%) \nParticipation in class over the term (10%) \nRecommended reading \nMost of the reading for this course will be in the form of the selected papers each week. \nHowever, the following may be useful background reading to refresh your knowledge from \nundergraduate courses: \n \nSilberschatz, A., Peterson, J.L. and Galvin, P.C. (2005). Operating systems concepts. \nAddison-Wesley (7th ed.). \n \nTanenbaum, A.S. (2008). Modern Operating Systems. Prentice-Hall (3rd ed.). \n \nBacon, J. and Harris, T. (2003). Operating systems. Addison-Wesley (3rd ed.). \n \nAnderson, T. and Dahlin, M. (2014). Operating Systems: Principles and Practice. Recursive \nBooks (2nd ed.). \n \nHennessy, J. and Patterson, D. (2006). Computer architecture: a quantitative approach. Elsevier \n(4th ed.). ISBN 978-0-12-370490-0. \n \nKleppmann M (2016) Designing Data-Intensive Applications, O'Reilly (1st ed.)\n \n",
        "ADVANCED TOPICS IN MACHINE LEARNING \nAims \nThis course explores current research topics in machine learning in sufficient depth that, at the \nend of the course, participants will be in a position to contribute to research on their chosen \ntopics. Each topic will be introduced with a lecture which, building on the material covered in the \nprerequisite courses, will make the current research literature accessible. Each lecture will be \nfollowed by up to three sessions which will typically be run as a reading group with student \npresentations on recent papers from the literature followed by a discussion, or a practical, or \nsimilar. \n \nStructure \nEach student will attend 3 topics and each topic's sessions will be spread over 5 contact hours. \nStudents will be expected to undertake readings for their selected topics. There will be some \ngroup work. \n \nThere will be a briefing session in Michaelmas term. \n \nSyllabus \nStudents choose five topics in preferential order from a list to be published in Michaelmas term. \nThey will be assigned to three topics out of their list. Students are assessed on one of these \ntopics which may not necessarily be their first choice topic. \n \nThe topics to be offered in 2024-25 are yet to be decided but to give an indicative idea of the \ntypes of topics, the ones offered in 2023-24 were: \n \nImitation learning  Prof A. Vlachos \nThe Future of Large Language Models: data architectures ethics Dr M. Tomalin \nPhysics and Geometry in Machine Learning Dr C. Mishra \nDiffusion Models and SDEs Francisco Vargas, Dr C. H. Ek \nExplainable Artificial Intelligence M. Espinosa, Z. Shams, Prof M. Jamnik \nUnconventional approaches to AI Dr S. Banerjee \nNarratives in Artificial Intelligence and Machine Learning Prof N. Lawrence \nMultimodal Machine Learning K. Hemker, N. Simidjievski, Prof M. Jamnik \nDeep Reinforcement Learning S. Morad, Dr C. H. Ek \nAutomation in Proof Assistants A. Jiang, W. Li, Prof M. Jamnik \nOn completion of this module, students should: \n \nbe in a strong position to contribute to the research topics covered; \nunderstand the fundamental methods (algorithms, data analysis, specific tasks) underlying each \ntopic; \nand be familiar with recent research papers and advances in the field. \nCoursework \nStudents will typically work in groups to give a presentation on assigned papers. Alternatively, a \ntopic may include practical sessions. Each topic will typically consist of one preliminary lecture \n",
        "followed by 3 reading and discussion sessions, or several lectures followed by a practical \nsession. A typical topic can accommodate up to 9 students presenting papers. There will be at \nleast 10 minutes general discussion per session. \n \nFull coursework details will be published by October. \n \nAssessment \nCoursework will be marked by the topic leaders and second marked by the module conveners. \n \nParticipation in all assigned topics, 10% \nPresentation or practical work or similar (for one of the chosen topics), 20% \nTopic coursework (for one of the chosen topics), 70% \nIndividual topic coursework will be published late Michaelmas term. \n \nAssessment criteria for topic coursework will follow project assessment criteria here: \nhttps://www.cl.cam.ac.uk/teaching/exams/acs_project_marking.pdf \n \nPlease note that students will be assessed on one of their three chosen topics but this may not \nbe their first choice. \n \nRecommended reading \nTo be confirmed by each topic convenor.\n \n",
        "COMPUTING FOR COLLECTIVE INTELLIGENCE \nPrerequisites: Familiarity with core machine learning paradigms - Basic calculus (analytical \nskills) - Basic discretre optimization (combinatorics) \nMoodle, timetable \n \nObjectives \nThere is a substantial body of academic work demonstrating that complex life is built by \ncooperation across scales: collections of genes cooperate to produce organisms, cells \ncooperate to produce multi-cellular organisms and multicellular animals cooperate to form \ncomplex social groups. Arguably, all intelligence is collective intelligence. Yet, to-date, many \nartificially intelligent agents (both embodied and virtual), are generally not conceived from the \nground up to interact with other intelligent agents (be it machines or humans). The canonical AI \nproblem is that of a monolithic and solitary machine confronting a non-social environment. This \ncourse aims to balance this trend by (i) equipping students with conceptual and practical \nknowledge on collective intelligence from a computational standpoint, and (ii) by conveying \nvarious computational paradigms by which collective intelligence can be modelled as well as \nsynthesized. \n \nAssessment \nThe assessment will be based on a reading-group paper presentation (individual or in pairs), \naccompanied by a 2-page summary, and a technical position paper (individual work) handed in \nafter the course. The position paper will be assessed based on its technical correctness, \nstrength or arguments, \nand clarity of research vision, and will be accompanied by a brief 5-minute pre-recorded talk that \nsummarizes key arguments (any slides used are also submitted as part of the project). \n \nPaper presentation and 1-page summary: 25% \nTechnical position paper: 75% (4000 word limit) \nRecommended reading \nSwarm Intelligence : From Natural to Artificial Systems, Bonabeau et al (1999) \nJoined-Up Thinking, Hannah Critchlow, (2024) \nSupercooperators, Martin Nowak (2011) \nA Course on Cooperative Game Theory, Chakravarty et al., (2015) \nGoverning the Commons: The Evolution of Institutions for Collective Action. Ostrom (1990).  \n \n",
        "CRYPTOGRAPHY AND PROTOCOL ENGINEERING \nAims \nWe all use cryptographic protocols every day: whenever we access an https:// website or send a \nmessage via WhatsApp or Signal, for example. In this module, students will get hands-on \nexperience of how those protocols are implemented. Students start by writing their own secure \nmessaging protocol from scratch, and then move on to more advanced examples of \ncryptographic protocols for security and privacy, such as private information retrieval (searching \nfor data without revealing what you\u2019re searching for). \n \nSyllabus \nThis is a practical course in which the first half of each of the 2-hour weekly sessions is a lecture \nthat introduces a topic, while the second half is lab time during which the students can work on \ntheir implementations, discuss the topics in small groups, and get help from the lecturers. The \nmodule is structured into three blocks as follows:  \n \nCryptographic primitives (2 weeks). Using common building blocks such as symmetric ciphers \nand hash functions. Implementing selected aspects of elliptic curve Diffie-Hellman and digital \nsignatures. Protocol security, Dolev-Yao model, side-channel attacks.  \nSecure communication (3 weeks). Authenticated key exchange (e.g. SIGMA, TLS), \npassword-authenticated key exchange (e.g. SPAKE2), forward secrecy, post-compromise \nsecurity, double ratchet, the Signal protocol.  \nPrivate database lookups (3 weeks). Lattice-based post-quantum cryptography, learning with \nerrors, homomorphic encryption, private information retrieval.  \nIn each block, students will be given a description of the algorithms and protocols covered (e.g. \na research paper or an RFC standards document), and a code template that the students \nshould use for their implementation. The implementation language will probably be Python (to \nbe confirmed after the practical materials have been developed ). Students will also be given a \nset of test cases to check their code, and will have the opportunity to test their implementation \ncommunicating with other students\u2019 implementations. Finally, for each block, students will write \nand submit a lab report explaining their approach and the findings from their implementation. \n \nObjectives \nOn completion of this module, students should: \n \nUnderstand that cryptography is not magic! The mathematics can look intimidating, but this \nmodule will show students that they needn\u2019t be afraid of cryptography. \nGain appreciation for the complexity and careful implementation of real-world cryptography \nlibraries ,and understand why one should prefer vetted implementations. \nBe familiar with the mathematical notation and concepts commonly used in descriptions of \ncryptographic protocols, and be able to understand and implement research papers and RFCs \nin this field. \nBe comfortable with cryptographic primitives commonly used in protocols, and able to correctly \nuse software libraries that implement them. \n",
        "Have gained hands-on experience of attacks on cryptographic protocols, and an appreciation \nfor the difficulty of making these protocols correct. \nHave been exposed to ideas and techniques from recent research, which may be useful for their \nown future research. \nHave practised technical writing through lab reports. \nAssessment \nStudents will be provided with code skeletons in one or two different programming languages \n(probably Python, maybe Rust) to avoid them struggling with setup issues. For each \nassignment, students are required to produce a working implementation of the protocols \ndiscussed in the course, using the programming language and skeleton provided. Here, \n\u201cworking\u201d means that the algorithms are functionally correct, but they are not production-quality \n(in particular, no side-channel countermeasures such as constant-time algorithms are required). \nFor some primitives (e.g. symmetric ciphers and hash functions) existing libraries should be \nused, whereas other cryptographic algorithms will be implemented from scratch. Students \nshould submit their code as a Docker container that can be run against specified test cases.  \n \nAfter each of the three blocks, students will be asked to submit a lab report of approximately \n2,000 words along with their code for that block. In the lab report, the students should explain \nhow their implementation works and why it is correct, as well as any findings from the work (e.g. \nany limitations or trade-offs they found with the protocols). The report structure and marking \nscheme will be specified in advance. The lab reports provide an opportunity for students to \nprovide critical insights and creativity. For instance, they might compare their implementation \nwith existing real-world implementations which provide additional features and guarantees.  \n \nEach of the three lab reports (along with the associated code) will be marked, forming the basis \nof assessment, and feedback on the reports will be given to the students before the next \nsubmission is due, so that students can take the feedback on board for their next submission. \nOf the three reports, the worst mark will be discarded, and the other two reports each contribute \n50% of the final mark. As such, students might decide to submit only two reports.  \n \nStudents may discuss their work with others, but the implementations and lab reports must be \nindividual work. \n \nRecommended reading \nTextbook: Jonathan Katz and Yehuda Lindell. Introduction to Modern Cryptography (3rd edition). \nCRC Press, 2020. \n \nThe textbook covers prerequisite knowledge on cryptographic primitives, such as ciphers, \nMACs, hash functions, and signatures. This book is also used in the Part II Cryptography \ncourse. For the protocols we cover in this course, we are not aware of a suitable textbook; \ninstead we will use research papers, lecture slides, and RFCs as resources, which will be \nprovided at the start of the module.\n \n",
        "DISTRIBUTED LEDGER TECHNOLOGIES: FOUNDATIONS AND APPLICATIONS \n \nAims \nThis reading group course examines foundations and current research into distributed ledger \n(blockchain) technologies and their applications. Students will read, review, and present seminal \nresearch papers in this area. Once completed, students should be able to integrate blockchain \ntechnologies into their own research and gain familiarity with a range of research skills. \n \nLectures \nIntroduction \nConsensus protocols \nBitcoin and its variants \nEthereum, smart contracts, and other permissionless DLTs \nHybrid and permissioned DLTs \nApplications \nLearning objectives \nThere are two broad objectives: to acquire familiarity with a body of work in the area of \ndistributed ledgers and to learn some specific research skills: \n \nHow to read a paper \nHow to review a paper \nHow to analyze a paper\u2019s strengths and weaknesses \nWritten and oral presentation skills \nAssessment \nYou are expected to read all assigned papers and submit paper reviews each week. Each \nreview must either follow the provided review form [PDF] [Latex source]. Each \u201creview\u201d is worth \n5% of your total mark, and is marked out of 100 with 60 a passing grade. Marks will be awarded \nand penalties for late submission applied according to ACS Assessment Guidelines.  \n \nOne paper review for the first week, then two paper reviews each week for 6 weeks (13 reviews, \n5% each) 65% (approx 600 words per review) \nSummative essay 25% (max 3000 words) \nPresentation 5% \n100% attendance in class 5% (2 marks deducted per missed class) \nRecommended Reading \nNarayanan, A. , Bonneau, J., Felten, E., Miller, A. and Goldfeder, S. (2016). Bitcoin and \nCryptocurrency Technologies: A Comprehensive Introduction. Princeton University Press. \n(2016 Draft available here: \nhttps://d28rh4a8wq0iu5.cloudfront.net/bitcointech/readings/princeton_bitcoin_book.pdf)\n \n",
        "EXPLAINABLE ARTIFICIAL INTELLIGENCE \nPrerequisites: A solid background in statistics, calculus and linear algebra. We strongly \nrecommend some experience with machine learning and deep neural networks (to the level of \nthe first chapters of Goodfellow et al.\u2019s \u201cDeep Learning\u201d). Students are expected to be \ncomfortable reading and writing Python code for the module\u2019s practical sessions. \nMoodle, timetable \n \nAims \nThe recent palpable introduction of Artificial Intelligence (AI) models to everyday \nconsumer-facing products, services, and tools brings forth several new technical challenges and \nethical considerations. Amongst these is the fact that most of these models are driven by Deep \nNeural Networks (DNNs), models that, although extremely expressive and useful, are \nnotoriously complex and opaque. This \u201cblack-box\u201d nature of DNNs limits their ability to be \nsuccessfully deployed in critical scenarios such as those in healthcare and law. Explainable \nArtificial Intelligence (XAI) is a fast-moving subfield of AI that aims to circumvent this crucial \nlimitation of DNNs by either  \n \n(i) constructing human-understandable explanations for their predictions,  \n \nor (ii) designing novel neural architectures that are interpretable by construction.  \n \nIn this module, we will introduce the key ideas behind XAI methods and discuss some of the \nimportant application areas of these methods (e.g., healthcare, scientific discovery, debugging, \nmodel auditing, etc.). We will approach this by focusing on the nature of what constitutes an \nexplanation, and discussing different ways in which explanations can be constructed or learnt to \nbe generated as a by-product of a model. The main aim of this module is to introduce students \nto several commonly used approaches in XAI, both theoretically in lectures and through \nhands-on exercises in practicals, while also bringing recent promising directions within this field \nto their attention. We hope that, by the end of this module, students will be able to directly \ncontribute to XAI research and will understand how methods discussed in this module may be \npowerful tools for their own work and research. \n \nSyllabus \nOverview and taxonomy of XAI (why is explainability needed, definition of terms, taxonomy of \nthe XAI space, etc.)  \nPerturbation-based feature attribution (e.g., LIME, Anchors, SHAP, etc.) \nPropagation-based feature attribution methods (e.g., Relevance Propagation, Saliency \nmethods, etc.) \nConcept-based explainability (Net2Vec, T-CAV, ACE, etc.) \nInterpretable architectures (CBMs and variants, Concept Whitening, SENNs, etc.) \nNeurosymbolic methods (DeepProbLog, Neural Reasoners, etc.) \nSample-based Explanations (Influence functions, ProtoPNets, etc.) \nCounterfactual explanations \nProposed Schedule \n",
        "The 16 hours of lectures across 8 weeks will be divided as follows:  \nWeek 1: 1h lecture + 1h lecture  \nWeek 2: 1h lecture + 1h reading group & presentations  \nWeek 3: 1h lecture + 1h reading group & presentations  \nWeek 4: 2h practical  \nWeek 5: 1h lecture + 1h reading group & presentations  \nWeek 6: 2h practical  \nWeek 7: 1h lecture + 1h reading group & presentations  \nWeek 8: 1h reading group & presentation + 1h reading group & presentations \n \nIn weeks where lectures are planned, one-hour lecture slots will intercalate with one hour of \nstudent paper presentations. During each paper presentation session, three students will \npresent a paper related to the topic covered in the earlier lecture that week for about 10 minutes \neach + 5 minutes of questions. At the end of all paper presentations, there will be a discussion \non all the papers. The order of student presentations will be allocated randomly during the first \nweek so that students know in advance when they are expected to present. For the sake of \nfairness, we will release the paper to be presented by each student a week before their \npresentation slot. \n \nObjectives \nBy the end of this module, students should be able to: \n \nRecognise and identify key concepts in XAI together with their connection to related subfields in \nAI such as fairness, accountability, and trustworthy AI. \nUnderstand how to use, design, and deploy model-agnostic perturbation methods such as \nLIME, Anchors, and RISE. In particular, students should understand the connection between \nfeature importance and cooperative game theory, and its uses in methods such as SHAP. \nIdentify the uses and limitations of propagation-based feature importance methods such as \nSaliency, SmoothGrad, GradCAM, and Integrated Gradients. Students should be able to \nimplement each of these methods on their own and connect the theoretical ideas behind them \nto practical code, exploiting modern frameworks\u2019 auto-differentiation. \nUnderstand what concept learning is and what limitations it overcomes compared to traditional \nfeature-based methods. Specifically, students should understand how probing a DNN\u2019s latent \nspace may be exploited for learning useful concepts for explainability. \nReason about the key components of inherently interpretable architectures and neuro-symbolic \nmethods and understand how interpretable neural networks can be designed from first \nprinciples. \nElaborate on what sample-based explanations are and how influence functions and prototypical \narchitectures such as ProtoPNet can be used to construct such explanations. \nExplain what counterfactual explanations are, how they are related to causality, and under which \nconditions they may be useful. \nUpon completion of this module, students will have the technical background and tools to use \nXAI as part of their own research or partake in XAI research itself. Moreover, we hope that by \ndetailing a clear timeline of how this relatively young subfield has developed, students may be \n",
        "able to better understand what are some fundamental open questions in this area and what are \nsome promising directions that are currently actively being explored. \n  \n \nAssessment \n(10%) student presentation: each student will be randomly assigned a presentation slot at the \nbeginning of the course. We will then distribute a paper for them to present in their slot a week \nbefore the presentation\u2019s scheduled time. Students are expected to prepare a 10-minute \npresentation of their assigned paper where they will present the motivation of their assigned \nwork and discuss the main methodology and findings reported in that paper. We will encourage \nstudents to focus on fully communicating the intuition of the work in their assigned papers and \ntry and connect it with ideas that we have previously discussed in previous lectures. All students \nnot presenting each week will be asked to submit one question pertaining to each paper \npresented that week before the paper presentations. \n \n(20%) practical exercises: We will run two practical sessions where students will be asked to \nperform a series of exercises that require them to use concepts we have introduced in lecture \nup to that point. For each practical session, we will prepare a colab notebook to guide the \nstudent through exercises and we will ask the students to submit their solutions through this \ncolab notebook. We expect students to complete about \u2154 of the exercises in the practical class \nand complete the rest at home as homework. Each practical will be worth 10%. \n \n(70%) mini-project: At the end of week 3, we will hand out a list of papers for students to select \ntheir mini-projects from. Each mini-project will consist of a student selecting a paper from our list \nand reimplementing and expanding the key idea in the paper. We encourage students to be as \ncreative as they want with how they drive their mini-project once the paper has been selected. \nFor example, they can reimplement the technique in the paper and combine it with \nmethodologies from other works we discussed in lecture, or they can apply their paper\u2019s \nmethodology to a new domain, datasets, or setup, where the technique may offer interesting \nand potentially novel insights. We will ask all students to submit a report in a workshop format of \nup to 4,000 words. This report, due roughly a week after Lent term ends, should describe their \nmethodology, experiments, and results. A crucial aspect of this report involves explaining the \nrationale behind different choices in methodology and experiments, as well as elaborating on \nthe choices made and hypotheses tested throughout their mini-project (potentially showing a \ndeep understanding of the work they are basing their mini-project on). To aid students with \nselecting their projects and making progress on them, we will hold regular office hours when \nstudents can come to discuss their progress and questions with us. \n \nRecommended reading \nTextbooks \n \n* Christoph Molnar, \u201cInterpretable Machine Learning\u201d. (2022): \nhttps://christophm.github.io/interpretable-ml-book/ \n \n",
        "Online Courses and Tutorial \n \n* Su-in Lee and Ian Cover, \u201cCSEP 590B Explainable AI\u201d University of Washington* Explaining \nMachine Learning Predictions: State-of-the-art, Challenges, and Opportunities \n \n* On Explainable AI: From Theory to Motivation, Industrial Applications, XAI Coding & \nEngineering Practices - AAAI 2022 Turorial \n \nSurvey papers \n \n* Arrieta, Alejandro Barredo, et al. \"Explainable Artificial Intelligence (XAI): Concepts, \ntaxonomies, opportunities and challenges toward responsible AI.\" Information fusion 58 (2020): \n82-115. \n \n* Rudin, Cynthia, et al. \"Interpretable machine learning: Fundamental principles and 10 grand \nchallenges.\" Statistics Surveys 16 (2022): 1-85.\n \n",
        "FEDERATED LEARNING: THEORY AND PRACTICE \nPrerequisites: It is strongly recommended that students have previously (and successfully) \ncompleted an undergraduate machine learning course - or have equivalent experience through \nopen-source material (e.g., lectures or similar). An example course would be: Part1B \"Artificial \nIntelligence\". Students should feel comfortable with SGD and optimization methods used to train \ncurrent popular neural networks and simple forms of neural network architectures. \nMoodle, timetable \n \nObjectives \nThis course aims to extend the machine learning knowledge available to students in Part I (or \npresent in typical undergraduate degrees in other universities), and allow them to understand \nhow these concepts can manifest in a decentralized setting. The course will consider both \ntheoretical (e.g., decentralized optimization) and practical (e.g., networking efficiency) aspects \nthat combine to define this growing area of machine learning.  \n \nAt the end of the course students should: \n \nUnderstand popular methods used in federated learning \nBe able to construct and scale a simple federated system \nHave gained an appreciation of the core limitations to existing methods, and the approaches \navailable to cope with these issues \nDeveloped an intuition for related technologies like differential privacy and secure aggregation, \nand are able to use them within typical federated settings  \nCan reason about the privacy and security issues with federated systems \nLectures \nCourse Overview. Introduction to Federated Learning. \nDecentralized Optimization. \nStatistical and Systems Heterogeneity. \nVariations of Federated Aggregation. \nSecure Aggregation. \nDifferential Privacy within Federated Systems. \nExtensions to Federated Analytics. \nApplications to Speech, Video, Images and Robotics. \nLab sessions \nFederating a Centralized ML Classifier. \nBehaviour under Heterogeneity. \nScaling a Federated Implementation. \nExploring Privacy with Federated Settings \nRecommended Reading \nReadings will be assigned for each lecture. Readings will be taken from either research papers, \ntutorials, source code or blogs that provide more comprehensive treatment of taught concepts. \n \nAssessment - Part II Students \n",
        "Four labs are performed during the course, and students receive 10% of their total grade for \nwork done as part of each lab. (For a total of 40% of the total grade from lab work alone). Labs \nwill primarily provide hands-on teaching opportunities, that are then utilized within the lab \nassignment which is completed outside of the lab contact time. MPhil and Part III students will \nbe given additional questions to answer within their version of the lab assignment which will \ndiffer from the assignment given to Part II CST students. \n \nThe remainder of the course grade (60%) will be given based on a hands-on project that applies \nthe concepts taught in lectures and labs. This hands-on project will be assessed based on upon \na combination of source code, related documentation and brief 8-minute pre-recorded talk that \nsummarizes key project elements (any slides used are also submitted as part of the project). \nPlease note, that in the case of Part II CST students, the talk itself is not examinable -- as such \nwill be made optional to those students. \n \nA range of possible practical projects will be described and offered to students to select from, or \nalternatively students may propose their own. MPhil and Part III students will select from a \nproject pool that is separate from those offered to Part II CST students. MPhil and Part III \nprojects will contain a greater emphasis on a research element, and the pre-recorded talks \nprovided by this student group will focus on this research contribution. The project will be \nassessed on the level of student understanding demonstrated, the degree of difficulty, \ncorrectness of implementation -- and for Part III/MPhil students the additional criteria of the \nquality and execution of the research methodology, and depth and quality of results analysis. \n \nThis project can be done individually or in groups -- although individual projects will be strongly \nencouraged. It will be required the project is performed using a code repository that also will \ncontain all documentation -- access to this repository will be shared with course staff (e.g., \nlecturer and TAs). Where needed, marks assigned to students within a group will be \ndifferentiated using this repository as an input. Furthermore if groups are formed, members must \nbe either entirely from Part III/MPhil students or Part II CST, i.e., these two student groups \nshould not mix to form a project group. \n \nProjects will be made available publicly. A maximum word count for written contributions for the \nproject will be enforced.  \n \n  \nAssessment - MPhil / Part III Students \n50% final mini-project. This hands-on project will be assessed based on upon a combination of \nsource code, related documentation and brief 8-minute pre-recorded talk that summarizes key \nproject elements (any slides used are also submitted as part of the project). \n \n30% midterm lab report (a written report of a bit more depth of thought than required in a lab, \nthe report provides the results of experiments -- with the skills to perform these experiments \ncoming from lab sessions),  \n \n",
        "20% for two individual hands-on labs. Labs will primarily provide hands-on teaching \nopportunities, that are then utilized within the lab assignment which is completed outside of the \nlab contact time. There will also be additional questions to answer within their version of the lab \nassignment. \n \nFor the mini-project, a range of possible practical projects will be described and offered to \nstudents to select from, or alternatively students may propose their own. The project will be \nassessed on the level of student understanding demonstrated, the degree of difficulty, \ncorrectness of implementation, the quality and execution of the research methodology, and \ndepth and quality of results analysis. \n \nThe project can be done individually or in groups -- although individual projects will be strongly \nencouraged. It will be required the project is performed using a code repository that also will \ncontain all documentation -- access to this repository will be shared with course staff (e.g., \nlecturer and TAs). Where needed, marks assigned to students within a group will be \ndifferentiated using this repository as an input. Furthermore if groups are formed, members must \nbe either entirely from Part III or entirely from MPhil students, i.e., these two student groups \nshould not mix to form a project group. \n \nProjects will be made available publicly. A maximum word count for written contributions for the \nproject will be enforced.   \n \nAdvanced Material sessions - MPhil / Part III Students \nThese sessions are mandatory for the MPhil and Part III students. Part II CST students may \nattend if they wish \n \nTopics and announced the first week of class. Selected to extend beyond topics covered in \nlectures and labs. Content is a mixture of sessions run in the lecture room the are either (1) \npresentations by guest lectures by an invited domain expert or (2) class-wide discussions \nregarding one or more related academic papers. Paper discussions will require students to read \nthe paper ahead of the lecture, and a brief discussion primer presentation will be given students \nbefore discussions begin.  \n \nThis module is shared with Part II of the Computer Science Tripos. Assessment will be adjusted \nfor the two groups of students to be at an appropriate level for whichever course the student is \nenrolled on. Further information about assessment and practicals will follow at the first lecture.\n",
        "GEOMETRIC DEEP LEARNING \n \nPrerequisites: Experience with machine learning and deep neural networks is recommended. \nKnowledge of concepts from graph theory and group theory is useful, although the relevant \nparts will be explicitly retaught. \nMoodle, timetable \n \nAims and Objectives \nMost of the patterns we see in nature can be elegantly reasoned about using spatial \nsymmetries\u2014transformations that leave underlying objects unchanged. This observation has \nhad direct implications for the development of modern deep learning architectures that are \nseemingly able to escape the \u201ccurse of dimensionality\u201d and fit complex high-dimensional tasks \nfrom noisy real-world data. Beyond this, one of the most generic symmetries\u2014the \npermutation\u2014will prove remarkably powerful in building models that reason over graph \nstructured-data, which is an excellent abstraction to reason about naturally-occurring, \nirregularly-structured data. Prominent examples include molecules (represented as graphs of \natoms and bonds, with three-dimensional coordinates provided), social networks and \ntransportation networks. Several already-impacted application areas include traffic forecasting, \ndrug discovery, social network analysis and recommender systems. The module will provide the \nstudents the capability to analyse irregularly- and nontrivially-structured data in an effective way, \nand position geometric deep learning in a proper context with related fields. The main aim of the \ncourse is to enable students to make direct contributions to the field, thoroughly assimilate the \nkey concepts in the area, and draw relevant connections to various other fields (such as NLP, \nFourier Analysis and Probabilistic Graphical Models). We assume only a basic background in \nmachine learning with deep neural networks. \n \nLearning outcomes \nThe framework of geometric deep learning, and its key building blocks: symmetries, \nrepresentations, invariance and equivariance \nFundamentals of processing data on graphs, as well as impactful application areas for graph \nrepresentation learning \nTheoretical principles of graph machine learning: permutation invariance and equivariance \nThe three \"flavours\" of spatial graph neural networks (GNNs) (convolutional, attentional, \nmessage passing) and their relative merits. The Transformer architecture as a special case. \nAttaching symmetries to graphs: CNNs on images, spheres and manifolds, Geometric Graphs \nand E(n)-equivariant GNNs \nRelevant connections of geometric deep learning to various other fields (such as NLP, Fourier \nAnalysis and Probabilistic Graphical Models) \nLectures \nThe lectures will cover the following topics: \n \nLearning with invariances and symmetries: geometric deep learning. Foundations of group \ntheory and representation theory. \n",
        "Why study data on graphs? Success stories: drug screening, travel time estimation, \nrecommender systems. Fundamentals of graph data processing: network science, spectral \nclustering, node embeddings. \nPermutation invariance and equivariance on sets and graphs. The principal tasks of node, edge \nand graph classification. Neural networks for point clouds: Deep Sets, PointNet; universal \napproximation properties. \nThe three flavours of spatial GNNs: convolutional, attentional, message passing. Prominent \nexamples: GCN, SGC, ChebyNets, MoNet, GAT, GATv2, IN, MPNN, GraphNets. Tradeoffs of \nusing different GNN variants. \nGraph Rewiring: how to apply GNNs when there is no graph? Links to natural language \nprocessing---Transformers as a special case of attentional GNNs. Representative \nmethodologies for graph rewiring: GDC, SDRF, EGP, DGCNN. \nExpressive power of graph neural networks: the Weisfeiler-Lehman hierarchy. GINs as a \nmaximally expressive GNN. Links between GNNs and graph algorithms: neural algorithmic \nreasoning. \nCombining spatial symmetries with GNNs: E(n)-equivariant GNNs, TFNs, SE(3)-Transformer. A \ndeep dive into AlphaFold 2. \nWorked examples: Circulant matrices on grids, the discrete Fourier transform, and convolutional \nnetworks on spheres. Graph Fourier transform and the Laplacian eigenbasis. \nPracticals \nThe practical is designed to complement the knowledge learnt in lectures and teach students to \nderive additional important results and architectures not directly shown in lectures. The practical \nwill be given as a series of individual exercises (each either code implementation or \nproof/derivation). Each of these exercises can be individually assessed based on a specified \nmark budget. \n \nPossible practical topics include the study of higher-order GNNs and equivariant message \npassing. \n \nAssessment \n(60%) Group Mini-project (writeup) at the end of the course. The mini projects can either be \nself-proposed, or the students can express their preference for one of the provided topics, the \nlist of which will be announced at the start of term. The projects will consist of implementing \nand/or extending graph representation learning models in the literature, applying them to \npublicly available datasets. Students will undertake the project in pairs and submit a joint \nwriteup limited to 4,000 words (in line with other modules); appendix of work logs to be included \nbut ungraded; \n \n(10%) Short presentation and viva: students will give a short presentation to explain their \nindividual contribution to the mini-project and there will be a short viva following. \n \n(30%) Practical work completion. Completing the exercises specified in the practical to a \nsatisfactory standard. The practical assessor should be satisfied that the student derived their \nanswers using insight gained from the course; coupled with original thought, not by simple \n",
        "copy-pasting of relevant related work. The students would submit code and a short report, which \nwould then be marked in line with the predetermined mark budget for each practical item. \nThe students will learn how to run advanced architectures on GPU but no specific need for \ndedicated GPU resources. Practicals will be made possible to do on CPU; if required, students \ncan use GPUs on publicly available free services (such as Colab) for their mini-project work. \n \nReferences \nThe course will be based on the following literature: \n \n\"Geometric Deep Learning: Grids, Graphs, Groups, Geodesics, and Gauges\", by Michael \nBronstein, Joan Bruna, Taco Cohen and Petar Veli\u010dkovi\u0107 \n\"Graph Representation Learning\", by Will Hamilton \n\"Deep Learning\", by Ian Goodfellow, Yoshua Bengio and Aaron Courville.\n \n",
        "MOBILE HEALTH \nAims \nThe course aims to explore how wearable and mobile systems sensors can be used to gather \ndata relevant to understand health, how the data can be analysed with advanced signal \nprocessing and machine learning and the performance of these systems in terms of diagnostics \nand disease progression detection. \n \nSyllabus \nCourse Overview. Introduction to Mobile Health. Evaluation metrics and methodology. Basics of \nSignal Processing. \nInertial Measurement Units, Human Activity Recognition (HAR) and Gait Analysis and Machine \nLearning for IMU data. \nRadios, Bluetooth, GPS and Cellular. Epidemiology and contact tracing, Social interaction \nsensing and applications. Location tracking in health monitoring and public health. \nAudio Signal Processing. Voice and Speech Analysis: concepts and data analysis. Body \nSounds analysis. \nPhotoplethysmogram and Light sensing for health (heart and sleep) \nContactless and wireless behaviour and physiological monitoring \nGenerative Models for Wearable Health Data \nTopical Guest Lectures \nObjectives \nThe course aims to explore how wearable and mobile systems sensors can be used to gather \ndata relevant to understand health, how the data can be analysed with advanced signal \nprocessing and machine learning and the performance of these systems in terms of diagnostics \nand disease progression detection. \n \nRoughly, each lecture contains a theory part about the working of \u201csensor signals\u201d or \u201cdata \nanalysis methods\u201d and an application part which contextualises the concepts. \n \nAt the end of the course students should: Understand how mobile/wearable sensors capture \ndata and their working. Understand different approaches to acquiring and analysing sensor data \nfrom different types of sensors. Understand the concept of signal processing applied to time \nseries data and their practical application in health. Be able to extract sensor data and analyse it \nwith basic signal processing and machine learning techniques. Be aware of the different health \napplications of the various sensor techniques. The course will also touch on privacy and ethics \nimplications of the approaches developed in an orthogonal fashion. \n \nRecommended Reading \nPlease see Course Materials for recommended reading for each session. \n \nAssessment - Part II students \nTwo assignments will be based on two datasets which will be provided to the students: \n \n",
        "Assignment 1 (shared for Part II and Part III/MPhil): this will be based on a dataset and will be \nworth 40% of the final mark. The task of the assessment will be to perform pre-processing and \nbasic data analysis in a \"colab\" and an answer sheet of no more than 1000 words. \n \nAssignment 2 (Part II): This assignment (worth 60% of the final mark) will be a fuller analysis of \na dataset focusing on machine learning algorithms and metrics. Discussion and interpretation of \nthe findings will be reported in a colab and a report of no more than 1200 words. \n \nAssessment  - Part III and MPhil students \nTwo assignments will be based on datasets which will be provided to the students: \n \nAssignment 1: this will be based on a dataset and will be worth 40% of the final mark. The task \nof the assessment will be to perform pre-processing and basic data analysis in a \"colab\" and an \nanswer sheet of no more than 1000 words. \n \nAssignment 2: The second assignment (worth 60% of the final mark) will be based on multiple \ndatasets. The students will be asked to compare and contract ML algorithms/solutions (trained \nand tested on the different data) and discuss the findings and interpretation in terms of health \ncontext. This will be in the form of a colab and a report of 1800 words. \n \nFurther Information \nCurrent Cambridge undergraduate students who are continuing onto Part III or the MPhil in \nAdvanced Computer Science may only take this module if they did NOT take it as a Unit of \nAssessment in Part II. \n \nThis module is shared with Part II of the Computer Science Tripos. Assessment will be adjusted \nfor the two groups of students to be at an appropriate level for whichever course the student is \nenrolled on. Further information about assessment and practicals will follow at the first lecture.\n",
        "MULTICORE SEMANTICS AND PROGRAMMING \nPrerequisites: Some familiarity with discrete mathematics (sets, partial orders, etc.) and with \nsequential Java programming will be assumed. Experience with operational semantics and with \nsome concurrent programming would be helpful. \nMoodle, timetable \n \nAims \nIn recent years multiprocessors have become ubiquitous, but building reliable concurrent \nsystems with good performance remains very challenging. The aim of this module is to \nintroduce some of the theory and the practice of concurrent programming, from hardware \nmemory models and the design of high-level programming languages to the correctness and \nperformance properties of concurrent algorithms. \n \nLectures \nPart 1: Introduction and relaxed-memory concurrency [Professor P. Sewell] \n \nIntroduction. Sequential consistency, atomicity, basic concurrent problems. [1 block] \nConcurrency on real multiprocessors: the relaxed memory model(s) for x86, ARM, and IBM \nPower, and theoretical tools for reasoning about x86-TSO programs. [2 blocks] \nHigh-level languages. An introduction to C/C++11 and Java shared-memory concurrency. [1 \nblock] \nPart 2: Concurrent algorithms [Dr T. Harris] \n \nConcurrent programming. Simple algorithms (readers/writers, stacks, queues) and correctness \ncriteria (linearisability and progress properties). Advanced synchronisation patterns (e.g. some \nof the following: optimistic and lazy list algorithms, hash tables, double-checked locking, RCU, \nhazard pointers), with discussion of performance and on the interaction between algorithm \ndesign and the underlying relaxed memory models. [3 blocks] \nResearch topics, likely to include one hour on transactional memory and one guest lecture. [1 \nblock] \nObjectives \nBy the end of the course students should: \n \nhave a good understanding of the semantics of concurrent programs, both at the multprocessor \nlevel and the C/Java programming language level; \nhave a good understanding of some key concurrent algorithms, with practical experience. \nAssessment - Part II Students \nTwo assignments each worth 50% \n \nRecommended reading \nHerlihy, M. and Shavit, N. (2008). The art of multiprocessor programming. Morgan Kaufmann. \n \nCoursework \nCoursework will consist of assessed exercises. \n",
        " \nPractical work \nPart 2 of the course will include a practical exercise sheet.  The practical exercises involve \nbuilding concurrent data structures and measuring their performance.  The work can be \ncompleted in C, Java, or similar languages. \n \nAssessment - Part III and MPhil Students \nAssignment 1, for 50% of the overall grade. Written coursework comprising a series of questions \non hardware and software relaxed-memory concurrency semantics. \nAssignment 2, for 50% of the overall grade. Written coursework comprising a series of questions \non the design of mutual exclusion locks and shared-memory data structures. \nFurther Information \nCurrent Cambridge undergraduate students who are continuing onto Part III or the MPhil in \nAdvanced Computer Science may only take this module if they did NOT take it as a Unit of \nAssessment in Part II. \n \nThis module is shared with Part II of the Computer Science Tripos. Assessment will be adjusted \nfor the two groups of students to be at an appropriate level for whichever course the student is \nenrolled on. Further information about assessment and practicals will follow at the first lecture.\n",
        "REINFORCEMENT LEARNING \n \nAims \nThe aim of this module is to introduce students to the foundations of the field of reinforcement \nlearning (RL), discuss state-of-the-art RL methods, incentivise students to produce critical \nanalysis of recent methods, and prompt students to propose novel solutions that address \nshortcomings of existing methods. If judged by the headlines, RL has seen an unprecedented \nsuccess in recent years. However, the vast majority of RL methods still have shortcomings that \nmight not be apparent at first glance. The aim of the course is to inspire students by \ncommunicating the promising aspects of RL, but ensure that students develop the ability to \nproduce a critical analysis of current RL limitations. \n \nObjectives \nStudents will learn the following technical concepts: \n \nfundamental RL terminology and mathematical formalism; a brief history of RL and its \nconnection to neuroscience and biological systems \nRL methods for discrete action spaces, e.g. deep Q-learning and large-scale Monte Carlo Tree \nSearch \nmethods for exploration, modelling uncertainty, and partial observability for RL \nmodern policy gradient and actor-critic methods \nconcepts needed to construct model-based RL and Model Predictive Control methods \napproaches to make RL data-efficient and ways to enable simulation-to-reality transfer \nexamples of fine-tuning foundation models and large language models (LLMs) with human \nfeedback; safe RL concepts; examples of using RL for safety validation \nexamples of using RL for scientific discovery \nStudents will also gain experience with analysing RL methods to uncover their strengths and \nshortcomings, as well as proposing extensions to improve performance. \n \nFinally, students will gain skills needed to create and deliver a successful presentation in a \nformat similar to that of conference presentations. \n \nWith all of the above, students who take part in this module will be well-prepared to start \nconducting research in the field of reinforcement learning. \n  \n \nSyllabus \nTopic 1: Introduction and Fundamentals \n \nOverview of RL: foundational ideas, history, and books; connection to neuroscience and \nbiological systems, recent industrial applications and research demonstrations \nMathematical fundamentals: Markov decision processes, Bellman equations, policy and value \niteration, temporal difference learning \nTopic 2: RL in Discrete Action Spaces \n",
        " \nQ-learning, function approximation and deep Q-learning; nonstationarity in RL and its \nimplications for deep learning; example applications (video games; initial example: Atari) \nMonte Carlo Tree Search; example applications (AlphaGo) \nTopic 3: Exploration, Uncertainty, Partial Observability \n \nMulti-armed bandits, Bayesian optimisation, regret analysis \nPartially observable Markov decision process; belief, memory, and sequence modelling \n(probabilistic methods, recurrent networks, transformers) \nTopic 4: Policy Gradient and Actor-critic Methods for Continuous Action Spaces \n \nImportance sampling, policy gradient theorem, actor-critic methods (SPG, DDPG) \nProximal policy optimisation; example applications \nTopic 5: Model-based RL and Model Predictive Control \n \nLearning dynamics models (graph networks, stochastic processes, diffusion models, \nphysics-based models, ensembles); planning with learned models \nModel predictive control; example applications (real-time control) \nTopic 6: Data-efficient RL and Simulation-to-reality Transfer \n \nData-efficient learning with probabilistic methods from real data (e.g. policy search in robotics), \nreal-to-sim inference and differentiable simulation, data-efficient simulation-to-reality transfer \nRL for physical systems (successful examples in locomotion, open problems in contact-rich \nmanipulation, applications to logistics, energy, and transport systems); examples of RL for \nhealthcare. \nTopic 7: RL with Human Feedback ; Safe RL and RL for Validation \n \nFine-tuning large language models (LLMs) and other foundation models with human feedback \n(TRLX,RL4LMs, a light-weight overview of RLHF) \nA review of SafeRL, example: optimising commercial HVAC systems using policy improvement \nwith constraints; improving safety using RL for validation: examples in autonomous driving and \nautonomous flying and aircraft collision avoidance \nTopic 8: RL for Scientific Discovery; Student Presentations \n \nExamples of RL for molecular design and drug discovery, active learning for synthesising new \nmaterials, RL for nuclear fusion experiments \nStudent presentations (based on essays and mini-projects) for other topics in RL, e.g. \nmulti-agent RL, hierarchical RL, RL for hyperparameter optimisation and NN architecture \nsearch, RL for multi-task transfer, lifelong RL, RL in biological systems, etc. \nAssessment \nThe assessment for this module consists of: \n \nEssay and mini-project (60%)  \n",
        "Students will choose a concrete RL method from the literature, then complete a 2-part essay \ndescribed below: \nEssay Part 1 (1500 words, 20%): Introduce a formal description of the method and explain how \nthe method extended the state-of-the-art at the time of its publication \nEssay Part 2 or a mini-project (2500 words, 40%): Provide a critical analysis of the chosen RL \nmethod. \nPresentation of the essay and mini-project results (15%)  \nstudents will be expected to create and record a 15-minute video presentation of the analysis \ndescribed in their essay and mini-project. \nShort test of RL theory fundamentals (15%)  \nto test the understanding of RL fundamentals (~15 minutes, in-class, closed book) \nParticipation in seminar discussions (10%)  \ntake an active part in the seminars by asking clarifying questions and mentioning related works. \nRecommended Reading \nBooks \n \n[S&B] Reinforcement Learning: An Introduction (second print edition). Richard S. Sutton, \nAndrew G. Barto. [Available from the book\u2019s website as a free PDF updated in 2022] \n \n[CZ] Algorithms for Reinforcement Learning. Csaba Szepesvari. [Available from the book\u2019s \nwebsite as a free PDF updated in 2019] \n \n[MK] Algorithms for Decision Making. Mykel J. Kochenderfer Tim A. WheelerKyle H. Wray. \n[Available from the book\u2019s website as a free PDF updated in 2023] \n \n[DB] Reinforcement Learning and Optimal Control. Dimitri Bertsekas. [Available from the book\u2019s \nwebsite as a free PDF updated in 2023] \n \nPresentation guidelines \n \n[KN] Ten simple rules for effective presentation slides. Kristen M.Naegle. PLoS computational \nbiology 17, no. 12 (2021).\n \n",
        "THEORIES OF SOCIO-DIGITAL DESIGN FOR HUMAN-CENTRED AI \n \nPrerequisites: This course is appropriate to any ACS student, and will assist in the development \nof critical thinking, argument and long-form writing skills. Prior experience of essay-based \nhumanities subjects at university or secondary school will be beneficial, but not required. \nMoodle, timetable \n \nAims \nThis module is a theoretically-oriented advanced introduction to the broad field of \nhuman-computer interaction, extending to consider topics such as intelligent user interfaces, \ncritical and speculative design, participatory design, cognitive models of users, human-centred, \nas well as more-than-human-centred design, and others. The course will not address purely \nengineering approaches to the development of user interfaces (unless there is a clear \ntheoretical question being addressed), but allow students to effectively link the political, social, \nand ethical considerations in HCI to specific technical and conceptual design challenges. The \nmodule builds on the Practical Research in Human-Centred AI course offered in Michaelmas \n(developed with researchers at Microsoft Research Cambridge) and a collaboration with the \nLeverhulme Centre for the Future of Intelligence at Cambridge. Participants may include visitors \nfrom these groups and/or interdisciplinary research students and academic guests from other \nUniversity departments, including Cambridge Digital Humanities. \n \nSyllabus \nThe syllabus will remain broadly within the area of human-computer interaction, including \ntheories of design practice and the social contexts of technology use. Individual seminar topics \nwill be selected in response to contemporary and recent research developments, in consultation \nwith members of the class and visiting contributors. \n \nRepresentative topics in the next year are likely to include: \n \nResponsible AI and HCI \nIntersectional design for social justice \nParticipatory design and co-design \nSustainable AI and more-than-human-centred design \nUsefulness and usability of ethical design toolkits \nThe EU AI Act in design practice \nAI and interface design for transparency \nDesigning otherwise: on anarchist HCI \n  \nObjectives \nOn completion of this module, students should have developed facility in discussing and \ncritiquing the aims of their research, especially for an audience drawn from other academic \ndisciplines, including the following skills: \n \nTheoretical motivation and defence of a research question. \n",
        "Consideration of a research proposal from one or more alternative theoretical perspectives. \nPotential critique of the theoretical basis for a programme of research. \nCoursework \nIn advance of each seminar, all participants must read the essential reading - generally one \nlong, plus one or two short papers. Further reading is suggested for some seminars. Some \nweeks, instead of shorter papers, students will be asked to explore specific design tools or \nresources. \n \nBefore seminars 2-8, each student will submit a written commentary on the paper, either \ngenerated using a large language model (LLM), or written in the style of an LLM, supplemented \nby a short original observation. \n \nEach member of the class must select one of the 8 seminar themes as the subject for a more \nextended critical review essay. The critical review should be written as an academic research \nessay, not in LLM style. \n \nEach seminar will include an informal design exercise to be carried out in small groups. The \npurpose of these exercises is to reflect on a variety of practical design resources, by applying \nthose resources to concrete case studies. \n \nThroughout the course, students will be expected to keep a \"reflective diary\", briefly reporting \nthemes that arise in discussion, reflections on the design exercises, and ways in which the \nreadings relate to their own research interests. \n  \n \nPractical work \nThere is no practical work element in this course. \n \nAssessment \nWritten critical review of a single discussion text (20%) \nReflective diary submitted at the end of the module (60%) \nIn advance of the session, each student will submit a written commentary on the paper, either \ngenerated using a large language model (LLM), or written in the style of an LLM. A short original \nobservation should be added (20%) \nRecommended reading \nTo be assigned during the module, as discussed above. Most set readings will be available \neither from the ACM Digital Library, or from institutional repositories of the authors. \n \nFurther Information \nStudents from the MPhil in the Ethics of AI, Data and Algorithms, MSt in AI, and MPhil in Digital \nHumanities courses also attend the lectures for this module.\n \n",
        "THEORY OF DEEP LEARNING \nPrerequisites: A strong background in calculus, probability theory and linear algebra, familiarity \nwith differential equations, optimization and information theory is required. Students need to \nhave studied Deep Neural Networks, familiarity with deep learning terminology (e.g. \narchitectures, benchmark problems) as well as deep learning frameworks (pytorch, jax or \nsimilar) is assumed \nMoodle, timetable \n \nAims \nThe objectives of this course is to expose you to one of the most active contemporary research \ndirections within machine learning: the theory of deep learning (DL). While the first wave of \nmodern DL has focussed on empirical breakthroughs and ever more complex techniques, the \nattention is now shifting to building a solid mathematical understanding of why these techniques \nwork so well in the first place. The purpose of this course is to review this recent progress \nthrough a mixture of reading group sessions and invited talks by leading researchers in the \ntopic, and prepare you to embark on a PhD in modern deep learning research. Compared to \ntypical, non-mathematical courses on deep learning, this advanced module will appeal to those \nwho have strong foundations in mathematics and theoretical computer science. In a way, this \ncourse is our answer to the question \u201cWhat should the world\u2019s best computer science students \nknow about deep learning in 2023?\u201d \n \nLearning Outcomes \nThis course should prepare the best students to start a PhD in the theory and mathematics of \ndeep learning, and to start formulating their own hypotheses in this space. You will be \nintroduced to a range of empirical and mathematical tools developed in recent years for the \nstudy of deep learning behaviour, and you will build an awareness of the main open questions \nand current lines of attack. At the end of the course you will: \n \nbe able to explain why classical learning theory is insufficient to describe the phenomenon of \ngeneralization in DL \nbe able to design and interpret empirical studies aimed at understanding generalization \nbe able to explain the role of overparameterization: be able to use deep linear models as a \nmodel to study implicit regularisation of gradient-based learning \nbe able to state PAC-Bayes and Information-theoretic bounds, and apply them to DL \nbe able to explain the connection between Gaussian processes and neural networks, and will \nbe able to study learning dynamics in the neural tangent kernel (NTK) regime. \nbe able to formulate your own hypotheses about DL and choose tools to prove/test them \nleverage your deeper theoretical understanding to produce more robust, rigorous and \nreproducible solutions to practical machine learning problems. \nSyllabus \nEach week we'll have two to four student-lead presentations about a research paper chosen \nfrom a reading list. The reading list loosely follows the weekly breakdown below (but we adapt it \neach year based as this is an active research area): \n \n",
        "Week 1: Introduction to the topic \nWeek 2: Empirical Studies of Deep Learning Phenomena \nWeek 3: Interpolation Regime and \u201cDouble Descent\u201d Phenomena \nWeek 4: Implicit Regularization in Deep Linear Models \nWeek 5: Approximation Theory \nWeek 6: Networks in the Infinite Width Limit \nWeek 7: PAC-Bayes and Information Theoretic Bounds for SGD \nWeek 8: Discussion and Coursework Spotlight Session \n \nAssessment \nStudents will be assessed on the following basis: \n \n20% for presentation/content contributed to the module: Each student will have an opportunity \nto present one of the recommended papers during Weeks 1-7 (30 minute slot including Q&A). \nFor the presentation, students should aim to communicate the core ideas behind the paper, and \nclearly present the results, conclusions, and future directions. Where possible, students are \nencouraged to comment on how the work itself fits into broader research goals. \n10% for active participation (regular attendance and contribution to discussions during the Q&A \nsessions). \n70% for a group project report, with a word limit of 4000. Either (1) an original research \nproposal/report with a hypothesis, review of related literature, and ideally preliminary findings, \nor, (2) reproduction and ideally extension of an existing relevant paper. \nCoursework reports are marked in line with general ACS guidelines, reports receiving top marks \nwill have have demonstrable research value (contain an original research idea, extension of \nexisting work, or a thorough reproduction effort which is valuable to the research community). \nAdditionally, some projects will be suggested during the first weeks of the course, although \nstudents are encouraged to come up with their own ideas. Students may be required to \nparticipate in group projects, with groups of size 2-3 (the class groups will be separated). For \nany given project, individual contributions would be noted for assessment though a viva \ncomponent. \nRelationship with related modules \nThis course can be considered as an advanced follow-up to the Part IIB course on Deep Neural \nNetworks. That course introduces some high level concepts that this course significantly \nexpands on. \n \nThis module complements L48: Machine Learning in the Physical World and L46: Principles of \nMachine Learning Systems, which focus on applications and hardware/systems aspects of ML \nrespectively. \n \nRecommended reading \nPNAS Colloquium on the Science of Deep Learning \nMathematics of Machine Learning book by Marc Deisenroth, Aldo Faisal and Cheng Soon Ong. \nProbabilistic Machine Learning: An Introduction book by Kevin Murphy \nMatus Telgarsky's lecture notes on deep learning theory  \n",
        "These are in addition to the papers which will be discussed in the lectures.\n \n",
        "UNDERSTANDING NETWORKED-SYSTEMS PERFORMANCE \nPrerequisites: A student must possess knowledge comparable to the undergraduate subjects on \nUnix Tools, C/C++ programming and Computer Networks. Optionally; a student will be \nadvantaged by doing an introduction to Computer Systems Modelling; such as the Part 2 \nComputer Systems Modelling subject as well as having a background in measurement \nmethodologies such as that of L50: Introduction to networking and systems measurements \nprovided in the ACS/Part III curriculum. \nMoodle, timetable \n \nAims \nThis is a practical course, actively building and extending software tools to observe the detailed \nbehaviour of transaction-oriented datacenter-like software. Students will observe and \nunderstand sources of user-facing tail latency, including that stemming \nfrom resource contention, cross-program interference, bad software locking, and simple design \nerrors. \nA 2-hour weekly hybrid, lecture/lab format permits students continuous monitored progress with \ncomplexity of tasks building naturally upon the previous weeks learning. \n \nObjectives \nUpon successful completion of the course, students will be able to: \n \nMake order-of-magnitude estimates of software, hardware, and I/O speeds \nMake valid measurements of actual software, hardware, and I/O speeds \nCreate observation facilities, including logs and dashboards, as part of a software system \ndesign \nCreate tracing facilities to fully observe the execution of complex software \nTime-align traces across multiple computers \nDisplay dense tracing information in meaningful ways \nReason about the sources of real-time and transaction delays, including cross-program \nresource interference, remote procedure call delays, and software locking surprises \nFix programs based on the above reasoning, making their response times faster and more \nrobust \n \nSyllabus \nMeasurement \n   Week 1 Intro, Measuring CPU time, rdtsc, Measuring memory access times, Measuring disks, \n   Week 2 gettimeofday, logs Measuring networks, Remote Procedure Calls, Multi-threads, locks \nObservation \n   Week 3 RPC, logs, displaying traces, interference \n   Week 4 Antagonist programs, Logging, dashboards, profiling. \nKUtrace \n   Week 5 Kernel patches, hello world, post-processing \n   Week 6 KUtrace multi-CPU time display \n   Week 7 Client-server KUtrace, with antagonists and interference \n",
        "   Week 8 Other trace mysteries \n \n \nAssessment \nThis is a Lab-based module; assessment is based upon reports covering guided laboratory work \nperformed each week. Assessment will be via two submissions: \n \n20% assignment deadline week 4 based upon Lab work from Weeks 1-4; word target 1000, limit \n2000 \n80% assignment deadline week 8 based upon lab work from weeks 5-8; word target 2000, limit \n4000 \nThe intent of the first assessment point is to provide rich feedback to students based upon a \n20% assignment permitting focussed and improved work to be executedfor the final assignment. \n \nAll work in this module is expected to be the effort of the student. Enough equipment is \nprovisioned to allow each student an independent set of apparatus. While classmembers may \nfind sharing operational experience valuable all assessment is based upon a students sole \nsubmission based upon. their own experiments and findings. \n \nReading Material \nCore text \n \nRichard L. Sites, Understanding Software Dynamics, Addison-Wesley Professional Computing \nSeries, 2022 \nFurther reading: papers and presentations that you might find interesting. None are required \nreading \u2013 they are well-written and/or informative. \n \nJohn K. Ousterhout et al., A trace-driven analysis of the UNIX 4.2 BSD file system ACM \nSIGOPS Operating Systems Review, Vol. 19, No. 5, Dec, 1985 \nhttps://dl.acm.org/doi/pdf/10.1145/323627.323631 \nLuiz Andr\u00b4e Barroso, Jimmy Clidaras, Urs H\u00a8olzle, The Datacenter as a Computer: An \nIntroduction to the Design of Warehouse-Scale Machines, Second Edition \nJohn L. Hennessy, David A. Patterson, Computer Architecture, A Quantitative Approach 5th \nEdition \nGeorge Varghese, Network Algorithmics. Morgan Kaufmann \nActually keeping datacenter software up and running \u2013 how Google runs production systems \nSite Reliability Engineering Edited by Betsy Beyer, Chris Jones, Jennifer Petoff and Niall \nRichard Murphy free PDF: https://landing.google.com/sre/book.html \nHow NOT to run datacenters (27 minute video, funny/sad) dotScale 2014 - Robert Kennedy - \nLife in the Trenches of healthcare.gov https://www.youtube.com/watch?v=GLQyj-kBRdo \nAn extended (optional) reading list will also be provided. \n \n"
    ],
    "semesters_3966951222928821661": [
        "1st Semester \n \nDATABASES \n \nAims \nThis course introduces basic concepts for database systems as seen from the perspective of \napplication designers. That is, the focus is on the abstractions supported by database \nmanagement systems and not on how those abstractions are implemented. \n \nThe database world is currently undergoing swift and dramatic transformations largely driven by \nInternet-oriented applications and services. Today many more options are available to database \napplication developers than in the past and so it is becoming increasingly difficult to sort fact \nfrom fiction. The course attempts to cut through the fog with a practical approach that \nemphasises engineering tradeoffs that underpin these recent developments and also guide our \nselection of \u201cthe right tool for the job.\u201d \n \nThis course covers three approaches. First, the traditional mainstay of the database industry -- \nthe relational approach -- is described with emphasis on eliminating logical redundancy in data. \nThen two representatives of recent trends are presented -- graph-oriented and \ndocument-oriented databases. The lectures are supported with two Help and Tick sessions, \nwhere students gain hands-on experience and guidance with the Assessed Exercises (ticks). \n \nLectures \nL1 Introduction. What is a database system? What is a data model? Typical DBMS \nconfigurations and variations (fast queries or reliable update). In-core, in secondary store or \ndistributed. \nL2 Conceptual modelling. Entities, relations, E/R diagrams and implementation-independent \nmodelling, weak entities, cardinality. \nL3+4 The relational database model. Implementing E/R models with relational tables. Relational \nalgebra and SQL. Basic query primitives. Update anomalies caused by redundancy. Minimising \nredundancy with normalised schemas. \nL5 Transactions. On-Line Transaction Processing. On-line Analytical Processing. Reliability, \nthroughput, normal forms, ACID, BASE, eventual consistency. \nL6 Documents and semi-structured data. The NoSQL, schema-free movement. XML/JSON. \nKey/value stores. Embracing data redundancy: representing data for fast, application-specific \naccess. Path queries (if time permits). \nL7 Further SQL. Multisets, NULL values, aggregates, transitive closure, expressibility (nested \nqueries and recursive SQL). \nL8 Graph databases. Optimised for processing enormous numbers of nodes and edges. \nImplementing E/R models in a graph-oriented database. Comparison of the presented models. \nObjectives \nAt the end of the course students should \n\n \nbe able to design entity-relationship diagrams to represent simple database application \nscenarios \nknow how to convert entity-relationship diagrams to relational- and graph-oriented \nimplementations \nunderstand the fundamental tradeoff between the ease of updating data and the response time \nof complex queries \nunderstand that no single data architecture can be used to meet all data management \nrequirements \nbe familiar with recent trends in the database area. \nRecommended reading \nLemahieu, W., Broucke, S. van den and Baesens, B. (2018) Principles of database \nmanagement. Cambridge University Press.\n \n\nDIGITAL ELECTRONICS \n \nAims \nThe aims of this course are to present the principles of combinational and sequential digital logic \ndesign and optimisation at a gate level. The use of n and p channel MOSFETs for building logic \ngates is also introduced. \n \nTopics \nIntroduction. Semiconductors to computers. Logic variables. Examples of simple logic. Logic \ngates. Boolean algebra. De Morgan\u2019s theorem. \nLogic minimisation. Truth tables and normal forms. Karnaugh maps. Quine-McCluskey method. \nBinary adders. Half adder, full adder, ripple carry adder, fast carry generation. \nCombinational logic design: further considerations. Multilevel logic. Gate propagation delay. An \nintroduction to timing diagrams. Hazards and hazard elimination. Other ways to implement \ncombinational logic. \nIntroduction to practical classes. Prototyping box. Breadboard and Dual in line (DIL) packages. \nWiring. \nSequential logic. Memory elements. RS latch. Transparent D latch. Master-slave D flip-flop. T \nand JK flip-flops. Setup and hold times. \nSequential logic. Counters: Ripple and synchronous. Shift registers. System timing - setup time \nconstraint, clock skew, metastability. \nSynchronous State Machines. Moore and Mealy finite state machines (FSMs). Reset and self \nstarting. State transition diagrams. Elimination of redundant states - row matching and state \nequivalence/implication table. \nFurther state machines. State assignment: sequential, sliding, shift register, one hot. \nImplementation of FSMs. \nIntroduction to microprocessors. Microarchitecture, fetch, register access, memory access, \nbranching, execution time. \nElectronics, Devices and Circuits. Current and voltage, conductors/insulators/semiconductors, \nresistance, basic circuit theory, the potential divider. Solving non-linear circuits. P-N junction \n(forward and reverse bias), N and p channel MOSFETs (operation and characteristics) and \nn-MOSFET logic, e.g., n-MOSFET inverter. Power consumption and switching time problems \nproblems in n-MOSFET logic. CMOS logic (NOT, NAND and NOR gates), logic families, noise \nmargin. \nObjectives \nAt the end of the course students should \n \nunderstand the relationships between combination logic and boolean algebra, and between \nsequential logic and finite state machines; \nbe able to design and minimise combinational logic; \nappreciate tradeoffs in complexity and speed of combinational designs; \nunderstand how state can be stored in a digital logic circuit; \nknow how to design a simple finite state machine from a specification and be able to implement \nthis in gates and edge triggered flip-flops; \n\nunderstand how to use MOSFETs to build digital logic circuits. \nRecommended reading \n* Harris, D.M. and Harris, S.L. (2013). Digital design and computer architecture. Morgan \nKaufmann (2nd ed.). The first edition is still relevant. \nKatz, R.H. (2004). Contemporary logic design. Benjamin/Cummings. The 1994 edition is more \nthan sufficient. \nHayes, J.P. (1993). Introduction to digital logic design. Addison-Wesley. \n \nBooks for reference: \n \nHorowitz, P. and Hill, W. (1989). The art of electronics. Cambridge University Press (2nd ed.) \n(more analog). \nWeste, N.H.E. and Harris, D. (2005). CMOS VLSI Design - a circuits and systems perspective. \nAddison-Wesley (3rd ed.). \nMead, C. and Conway, L. (1980). Introduction to VLSI systems. Addison-Wesley. \nCrowe, J. and Hayes-Gill, B. (1998). Introduction to digital electronics. Butterworth-Heinemann. \nGibson, J.R. (1992). Electronic logic circuits. Butterworth-Heinemann.\n \n\nDISCRETE MATHEMATICS \n \nAims \nThe course aims to introduce the mathematics of discrete structures, showing it as an essential \ntool for computer science that can be clever and beautiful. \n \nLectures \nProof [5 lectures]. \nProofs in practice and mathematical jargon. Mathematical statements: implication, bi-implication, \nuniversal quantification, conjunction, existential quantification, disjunction, negation. Logical \ndeduction: proof strategies and patterns, scratch work, logical equivalences. Proof by \ncontradiction. Divisibility and congruences. Fermat\u2019s Little Theorem. \n \nNumbers [5 lectures]. \nNumber systems: natural numbers, integers, rationals, modular integers. The Division Theorem \nand Algorithm. Modular arithmetic. Sets: membership and comprehension. The greatest \ncommon divisor, and Euclid\u2019s Algorithm and Theorem. The Extended Euclid\u2019s Algorithm and \nmultiplicative inverses in modular arithmetic. The Diffie-Hellman cryptographic method. \nMathematical induction: Binomial Theorem, Pascal\u2019s Triangle, Fundamental Theorem of \nArithmetic, Euclid\u2019s infinity of primes. \n \nSets [9 lectures]. \nExtensionality Axiom: subsets and supersets. Separation Principle: Russell\u2019s Paradox, the \nempty set. Powerset Axiom: the powerset Boolean algebra, Venn and Hasse diagrams. Pairing \nAxiom: singletons, ordered pairs, products. Union axiom: big unions, big intersections, disjoint \nunions. Relations: composition, matrices, directed graphs, preorders and partial orders. Partial \nand (total) functions. Bijections: sections and retractions. Equivalence relations and set \npartitions. Calculus of bijections: characteristic (or indicator) functions. Finite cardinality and \ncounting. Infinity axiom. Surjections. Enumerable and countable sets. Axiom of choice. \nInjections. Images: direct and inverse images. Replacement Axiom: set-indexed constructions. \nSet cardinality: Cantor-Schoeder-Bernstein Theorem, unbounded cardinality, diagonalisation, \nfixed-points. Foundation Axiom. \n \nFormal languages and automata [5 lectures]. \nIntroduction to inductive definitions using rules and proof by rule induction. Abstract syntax \ntrees. Regular expressions and their algebra. Finite automata and regular languages: Kleene\u2019s \ntheorem and the Pumping Lemma. \n \nObjectives \nOn completing the course, students should be able to \n \nprove and disprove mathematical statements using a variety of techniques; \napply the mathematical principle of induction; \nknow the basics of modular arithmetic and appreciate its role in cryptography; \n\nunderstand and use the language of set theory in applications to computer science; \ndefine sets inductively using rules and prove properties about them; \nconvert between regular expressions and finite automata; \nuse the Pumping Lemma to prove that a language is not regular. \nRecommended reading \nBiggs, N.L. (2002). Discrete mathematics. Oxford University Press (Second Edition). \nDavenport, H. (2008). The higher arithmetic: an introduction to the theory of numbers. \nCambridge University Press. \nHammack, R. (2013). Book of proof. Privately published (Second edition). Available at: \nhttp://www.people.vcu.edu/ rhammack/BookOfProof/index.html \nHouston, K. (2009). How to think like a mathematician: a companion to undergraduate \nmathematics. Cambridge University Press. \nKozen, D.C. (1997). Automata and computability. Springer. \nLehman, E.; Leighton, F.T.; Meyer, A.R. (2014). Mathematics for computer science. Available \non-line. \nVelleman, D.J. (2006). How to prove it: a structured approach. Cambridge University Press \n(Second Edition).\n \n\nFOUNDATIONS OF COMPUTER SCIENCE \nAims \nThe main aim of this course is to present the basic principles of programming. As the \nintroductory course of the Computer Science Tripos, it caters for students from all backgrounds. \nTo those who have had no programming experience, it will be comprehensible; to those \nexperienced in languages such as C, it will attempt to correct any bad habits that they have \nlearnt. \n \nA further aim is to introduce the principles of data structures and algorithms. The course will \nemphasise the algorithmic side of programming, focusing on problem-solving rather than on \nhardware-level bits and bytes. Accordingly it will present basic algorithms for sorting, searching, \netc., and discuss their efficiency using O-notation. Worked examples (such as polynomial \narithmetic) will demonstrate how algorithmic ideas can be used to build efficient applications. \n \nThe course will use a functional language (OCaml). OCaml is particularly appropriate for \ninexperienced programmers, since a faulty program cannot crash and OCaml\u2019s unobtrusive type \nsystem captures many program faults before execution. The course will present the elements of \nfunctional programming, such as curried and higher-order functions. But it will also introduce \ntraditional (procedural) programming, such as assignments, arrays and references. \n \nLectures \nIntroduction to Programming. The role of abstraction and representation. Introduction to integer \nand floating-point arithmetic. Declaring functions. Decisions and booleans. Example: integer \nexponentiation. \nRecursion and Efficiency. Examples: Exponentiation and summing integers. Iteration versus \nrecursion. Examples of growth rates. Dominance and O-Notation. The costs of some \nrepresentative functions. Cost estimation. \nLists. Basic list operations. Append. Na\u00efve versus efficient functions for length and reverse. \nStrings. \nMore on lists. The utilities take and drop. Pattern-matching: zip, unzip. A word on polymorphism. \nThe \u201cmaking change\u201d example. \nSorting. A random number generator. Insertion sort, mergesort, quicksort. Their efficiency. \nDatatypes and trees. Pattern-matching and case expressions. Exceptions. Binary tree traversal \n(conversion to lists): preorder, inorder, postorder. \nDictionaries and functional arrays. Functional arrays. Dictionaries: association lists (slow) versus \nbinary search trees. Problems with unbalanced trees. \nFunctions as values. Nameless functions. Currying. The \u201capply to all\u201d functional, map. \nExamples: The predicate functionals filter and exists. \nSequences, or lazy lists. Non-strict functions such as IF. Call-by-need versus call-by-name. Lazy \nlists. Their implementation in OCaml. Applications, for example Newton-Raphson square roots. \nQueues and search strategies. Depth-first search and its limitations. Breadth-first search (BFS). \nImplementing BFS using lists. An efficient representation of queues. Importance of efficient data \nrepresentation. \n\nElements of procedural programming. Address versus contents. Assignment versus binding. \nOwn variables. Arrays, mutable or not. Introduction to linked lists. \nObjectives \nAt the end of the course, students should \n \nbe able to write simple OCaml programs; \nunderstand the fundamentals of using a data structure to represent some mathematical \nabstraction; \nbe able to estimate the efficiency of simple algorithms, using the notions of average-case, \nworse-case and amortised costs; \nknow the comparative advantages of insertion sort, quick sort and merge sort; \nunderstand binary search and binary search trees; \nknow how to use currying and higher-order functions; \nunderstand how OCaml combines imperative and functional programming in a single language. \nRecommended reading \nJohn Whitington. OCaml from the Very Beginning. (http://ocaml-book.com). \n \nOkasaki, C. (1998). Purely functional data structures. Cambridge University Press.\n \n\nHARDWARE PRACTICAL CLASSES \nThe Hardware Practical Classes accompany the Digital Electronics series of lectures. The aim \nof the Practical Classes is to enable students to get hands-on experience of designing, building, \nand testing and debugging of digital electronic circuits. The labs will take place in the Intel Lab. \nlocated in the William Gates Building (WGB). \n \nThe Digital Electronics lecture series occupies 12 lectures in the first 4 weeks of the Michaelmas \nTerm, while the Practical Classes occupy the latter 6 weeks of the Michaelmas Term and the \nfirst 6 weeks of the Lent Term. If required, extra sessions will be available for any students \nneeding to 'catch-up' on missed sessions or to complete any remaining practical work. \n \nThe Practical Classes take the form of 4 workshops, specifically: \n \nWorkshop 1 \u2013 Electronic Die; \nWorkshop 2 \u2013 Shaft Position Encoder; \nWorkshop 3 \u2013 Debouncing a Switch; \nWorkshop 4 \u2013 Framestore for an LED Array. \nIn general, the workshops require some preparatory work to be done prior to the practical \nsession. These tasks are highlighted at the beginning of each Worksheet. Typically this involves \npreparing a design that you will then build, test and modify during the practical class. Note that \ninsufficient preparation prior to the practical classes may compromise effective use of your time \nin the Intel lab. \n \nIn the Practical Classes you will usually work on your own, and you are expected to complete \none Workshop in each of your scheduled sessions. Demonstrators are available during the \nsessions to assist you with any queries or problems you may have. \n \nImportant: Remember to get your work ticked.\n \n\nINTRODUCTION TO GRAPHICS \nAims \nTo introduce the necessary background, the basic algorithms, and the applications of computer \ngraphics and graphics hardware. \n \nLectures \nBackground. What is an image? Resolution and quantisation. Storage of images in memory. [1 \nlecture] \nRendering. Perspective. Reflection of light from surfaces and shading. Geometric models. Ray \ntracing. [2 lectures] \nGraphics pipeline. Polygonal mesh models. Transformations using matrices in 2D and 3D. \nHomogeneous coordinates. Projection: orthographic and perspective. [1 lecture] \nGraphics hardware and modern OpenGL. GPU rendering. GPU frameworks and APIs. Vertex \nprocessing. Rasterisation. Fragment processing. Working with meshes and textures. Z-buffer. \nDouble-buffering and frame synchronization. [2 lectures] \n  \nHuman vision, colour and tone mapping. Perception of colour. Tone mapping. Colour spaces. [2 \nlectures] \nObjectives \nBy the end of the course students should be able to: \n \nunderstand and apply in practice basic concepts of ray-tracing: ray-object intersection, \nreflections, refraction, shadow rays, distributed ray-tracing, direct and indirect illumination; \ndescribe and explain the following algorithms: z-buffer, texture mapping, double buffering, \nmip-map, and normal-mapping; \nuse matrices and homogeneous coordinates to represent and perform 2D and 3D \ntransformations; understand and use 3D to 2D projection, the viewing volume, and 3D clipping; \nimplement OpenGL code for rendering of polygonal objects, control camera and lighting, work \nwith vertex and fragment shaders; \ndescribe a number of colour spaces and their relative merits. \nexplain the need for tone mapping and colour processing in rendering pipeline. \nRecommended reading \n* Shirley, P. and Marschner, S. (2009). Fundamentals of Computer Graphics. CRC Press (3rd \ned.). \n \nFoley, J.D., van Dam, A., Feiner, S.K. and Hughes, J.F. (1990). Computer graphics: principles \nand practice. Addison-Wesley (2nd ed.). \n \nKessenich, J.M., Sellers, G. and Shreiner, D (2016). OpenGL Programming Guide: The Official \nGuide to Learning OpenGL, Version 4.5 with SPIR-V, [seventh edition and later]\n \n\nOBJECT-ORIENTED PROGRAMMING \n \nAims \nThe goal of this course is to provide students with an understanding of Object-Oriented \nProgramming. Concepts are demonstrated in multiple languages, but the primary language is \nJava. \n \nLecture syllabus \nTypes, Objects and Classes Moving from functional to imperative. Functions, methods. Control \nflow. values, variables and types. Primitive Types. Classes as custom types. Objects vs \nClasses. Class definition, constructors. Static data and methods. \nDesigning Classes Identifying classes. UML class diagrams. Modularity. Encapsulation/data \nhiding. Immutability. Access modifiers. Parameterised types (Generics). \nPointers, References and Memory Pointers and references. Reference types in Java. The call \nstack. The heap. Iteration and recursion. Pass-by-value and pass-by-reference. \nInheritance Inheritance. Casting. Shadowing. Overloading. Overriding. Abstract Methods and \nClasses. \nPolymorphism and Multiple Inheritance Polymorphism in ML and Java. Multiple inheritance. \nInterfaces in Java. \nLifecycle of an Object Constructors and chaining. Destructors. Finalizers. Garbage Collection: \nreference counting, tracing. \nJava Collections and Object Comparison Java Collection interface. Key classes. Collections \nclass. Iteration options and the use of Iterator. Comparing primitives and objects. Operator \noverloading. \nError Handling Types of errors. Limitations of return values. Deferred error handling. Exceptions. \nCustom exceptions. Checked vs unchecked. Inappropriate use of exceptions. Assertions. \nDesignLanguage evolution Need for languages to evolve. Generics in Java. Type erasure. \nIntroduction to Java 8: Lambda functions, functions as values, method references, streams. \nDesign Patterns Introduction to design patterns. Open-closed principle. Examples of Singleton, \nDecorator, State, Composite, Strategy, Observer. [2 lectures] \nObjectives \nAt the end of the course students should \n \nProvide an overview of key OOP concepts that are transferable in mainstream programming \nlanguages; \nGain an understanding of software development approaches adopted in the industry including \nmaintainability, testing, and software design patterns; \nIncrease programming familiarity with Java; \nRecommended reading \n1. Real-World Software Development: A Project-Driven Guide to Fundamentals in Java [Urma et \nal.] \n2. Modern Java in Action (2nd edition) [Urma et al.]. \n3. Java How to Program: early objects [Deitel et al.]\n \n\nOCAML PRACTICAL CLASSES \n \nThe Role of Tickers \nA ticking session is a short (5 minute) one-to-one session with a ticker conducted on Thursday \nafternoons. The role of the ticker is twofold: \n \nto check you understand your solution (and didn\u2019t just have some lucky guesses or copy code \nfrom elsewhere) \nto give you general feedback on ways to improve your code. \nTo those ends a Ticker will typically ask you questions related to the core material of the tick and \ndiscuss your code directly. \n \nDeadline Extensions \nDeadline extensions can be granted for illness or similar reasons. To obtain an extension please \nemail Jon Ludlam in the first instance, clearly stating your justification for an extension and \nCCing your Director of Studies, who will need to support your request.\n \n\nSCIENTIFIC COMPUTING \n \nAims \nThis course is a hands-on introduction to using computers to investigate scientific models and \ndata. \n \nSyllabus \nPython notebooks. Overview of the Python programming language. Use of notebooks for \nscientific computing. \nNumerical computation. Writing fast vectorized code in numpy. Optimization and fitting. \nSimulation. \nWorking with data. Data import. Common ways to summarize and plot data, for univariate and \nmultivariate analysis. \nObjectives \nAt the end of the course students should \n \nbe able to import data, plot it, and summarize it appropriately \nbe able to write fast vectorized code for scientific / data work\n \n\n",
        "2nd Semester \nALGORITHMS 1 \nAims \nThe aim of this course is to provide an introduction to computer algorithms and data structures, \nwith an emphasis on foundational material. \n \nLectures \nSorting. Review of complexity and O-notation. Trivial sorting algorithms of quadratic complexity. \nReview of merge sort and quicksort, understanding their memory behaviour on statically \nallocated arrays. Heapsort. Stability. Other sorting methods including sorting in linear time. \nMedian and order statistics. [Ref: CLRS3 chapters 1, 2, 3, 6, 7, 8, 9] [about 4 lectures] \nStrategies for algorithm design. Dynamic programming, divide and conquer, greedy algorithms \nand other useful paradigms. [Ref: CLRS3 chapters 4, 15, 16] [about 3 lectures] \nData structures. Elementary data structures: pointers, objects, stacks, queues, lists, trees. \nBinary search trees. Red-black trees. B-trees. Hash tables. Priority queues and heaps. [Ref: \nCLRS3 chapters 6, 10, 11, 12, 13, 18] [about 5 lectures]. \nObjectives \nAt the end of the course students should: \n \nhave a thorough understanding of several classical algorithms and data structures; \nbe able to analyse the space and time efficiency of most algorithms; \nhave a good understanding of how a smart choice of data structures may be used to increase \nthe efficiency of particular algorithms; \nbe able to design new algorithms or modify existing ones for new applications and reason about \nthe efficiency of the result. \nRecommended reading \n* Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest and Clifford Stein Introduction to \nAlgorithms, Fourth Edition ISBN 9780262046305 Published: April 5, 2022. \n \nSedgewick, R., Wayne, K. (2011). Algorithms. Addison-Wesley. ISBN 978-0-321-57351-3. \n \nKleinberg, J. and Tardos, \u00c9. (2006). Algorithm design. Addison-Wesley. ISBN \n978-0-321-29535-4. \n \nKnuth, D.A. (2011). The Art of Computer Programming. Addison-Wesley. ISBN \n978-0-321-75104-1.\n \n\nALGORITHMS 2 \n \nAims \nThe aim of this course is to provide an introduction to computer algorithms and data structures, \nwith an emphasis on foundational material. \n \nLectures \nGraphs and path-finding algorithms. Graph representations. Breadth-first and depth-first search. \nSingle-source shortest paths: Bellman-Ford and Dijkstra algorithms. All-pairs shortest paths: \ndynamic programming and Johnson\u2019s algorithms. [About 4 lectures] \nGraphs and subgraphs. Maximum flow: Ford-Fulkerson method, Max-flow min-cut theorem. \nMatchings in bipartite graphs. Minimum spanning tree: Kruskal and Prim algorithms. Topological \nsort. [About 4 lectures] \nAdvanced data structures. Binomial heap. Amortized analysis: aggregate analysis, potential \nmethod. Fibonacci heaps. Disjoint sets. [About 4 lectures] \nObjectives \nAt the end of the course students should: \n \nhave a thorough understanding of several classical algorithms and data structures; \nbe able to analyse the space and time efficiency of most algorithms; \nhave a good understanding of how a smart choice of data structures may be used to increase \nthe efficiency of particular algorithms; \nbe able to design new algorithms or modify existing ones for new applications and reason about \nthe efficiency of the result. \nRecommended reading \n* Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest and Clifford Stein Introduction to \nAlgorithms, Fourth Edition ISBN 9780262046305 Published: April 5, 2022. \n \nSedgewick, R., Wayne, K. (2011). Algorithms. Addison-Wesley. ISBN 978-0-321-57351-3. \n \nKleinberg, J. and Tardos, \u00c9. (2006). Algorithm design. Addison-Wesley. ISBN \n978-0-321-29535-4. \n \nKnuth, D.A. (2011). The Art of Computer Programming. Addison-Wesley. ISBN \n978-0-321-75104-1.\n \n\nMACHINE LEARNING AND REAL-WORLD DATA \n \nAims \nThis course introduces students to machine learning algorithms as used in real-world \napplications, and to the experimental methodology necessary to perform statistical analysis of \nlarge-scale data from unpredictable processes. Students will perform 3 extended practicals, as \nfollows: \n \nStatistical classification: Determining movie review sentiment using Naive Bayes; \nSequence Analysis: Hidden Markov Modelling and its application to a task from biology \n(predicting protein interactions with a cell membrane); \nAnalysis of social networks, including detection of cliques and central nodes. \nSyllabus \nTopic One: Statistical Classification \nIntroduction to sentiment classification. \nNaive Bayes parameter estimation. \nStatistical laws of language. \nStatistical tests for classification tasks. \nCross-validation and test sets. \nUncertainty and human agreement. \nTopic Two: Sequence Analysis  \nHidden Markov Models (HMM) and HMM training. \nThe Viterbi algorithm. \nUsing an HMM in a biological application. \nTopic Three: Social Networks  \nProperties of networks: Degree, Diameter. \nBetweenness Centrality. \nClustering using betweenness centrality. \nObjectives \nBy the end of the course students should be able to: \n \nunderstand and program two simple supervised machine learning algorithms; \nuse these algorithms in statistically valid experiments, including the design of baselines, \nevaluation metrics, statistical testing of results, and provision against overtraining; \nvisualise the connectivity and centrality in large networks; \nuse clustering (i.e., a type of unsupervised machine learning) for detection of cliques in \nunstructured networks. \nRecommended reading \nJurafsky, D. and Martin, J. (2008). Speech and language processing. Prentice Hall. \n \nEasley, D. and Kleinberg, J. (2010). Networks, crowds, and markets: reasoning about a highly \nconnected world. Cambridge University Press.\n \n\nOPERATING SYSTEMS \n \nAims \nThe overall aim of this course is to provide a general understanding of the structure and key \nfunctions of the operating system. Case studies will be used to illustrate and reinforce \nfundamental concepts. \n \nLectures \nIntroduction to operating systems. Elementary computer organisaton. Abstract view of an \noperating system. Key concepts: layering, multiplexing, performance, caching, buffering, policies \nversus mechanisms. OS evolution: multi-programming, time-sharing. Booting. [1 lecture] \nProtection. Dual-mode operation. Protecting CPU, memory, I/O, storage. Kernels, micro-kernels, \nand modules. System calls. Virtual machines and containers. Subjects and objects. \nAuthentication. Access matrix: ACLs and capabilities. Covert channels. [1 lecture] \nProcesses. Job/process concepts. Process lifecycle. Process management. Context switching. \nInter-process communication. [1 lecture] \nScheduling. CPU-I/O burst cycle. Scheduling basics: preemption and non-preemption. \nScheduling algorithms: FCFS, SJF, SRTF, Priority, and Round Robin. Multilevel scheduling. [2 \nlectures] \nMemory management. Processes in memory. Logical versus physical addresses. Partitions: \nstatic versus dynamic, free space management, external fragmentation. Segmented memory. \nPaged memory: concepts, internal fragmentation, page tables. Demand paging/segmentation. \nPage replacement strategies: FIFO, OPT, and LRU with approximations including NRU, LFU, \nMFU, MRU. Working set schemes. Thrashing. [3 lectures] \nI/O subsystem. General structure. Polled mode versus interrupt-driven I/O. Programmed I/O \n(PIO) versus Direct Memory Access (DMA). Application I/O interface: block and character \ndevices, buffering, blocking, non-blocking, asynchronous, and vectored I/O. Other issues: \ncaching, scheduling, spooling, performance. [1 lecture] \nFile management. File concept. Directory and storage services. File names and metadata. \nDirectory name-space: hierarchies, DAGs, hard and soft links. File operations. Access control. \nExistence and concurrency control. [1 lecture] \nUnix case study. History. General structure. Unix file system: file abstraction, directories, mount \npoints, implementation details. Processes: memory image, lifecycle, start of day. The shell: \nbasic operation, commands, standard I/O, redirection, pipes, signals. Character and block I/O. \nProcess scheduling. [2 lectures] \nObjectives \nAt the end of the course students should be able to: \n \ndescribe the general structure and purpose of an operating system; \nexplain the concepts of process, address space, and file; \ncompare and contrast various CPU scheduling algorithms; \ncompare and contrast mechanisms involved in memory and storage management; \ncompare and contrast polled, interrupt-driven, and DMA-based access to I/O devices. \nRecommended reading \n\n* Silberschatz, A., Galvin, P.C., and Gagne, G. (2018). Operating systems concepts. Wiley (10th \ned.). Prior editions, at least back to 8th ed., should also be acceptable. \nAnderson, T. and Dahlin, M. (2014). Operating Systems: Principles and Practice. Recursive \nBooks (2nd ed.). \nBacon, J. and Harris, T. (2003). Operating systems. Addison-Wesley (3rd ed.). Currently \nappears out-of-print. \nLeffler, S. (1989). The design and implementation of the 4.3BSD Unix operating system. \nAddison-Wesley. \nMcKusick, M.K., Neville-Neil, G.N. and Watson, R.N.M. (2014) The Design and Implementation \nof the FreeBSD Operating System. Pearson Education. (2nd ed.).\n \n\nINTERACTION DESIGN \nAims \nThe aim of this course is to provide an introduction to interaction design, with an emphasis on \nunderstanding and experiencing the user-centred design process, from conducting user \nresearch and requirements development to implementation and evaluation, while understanding \nthe background to human-computer interaction. \n \nLectures \nCourse overview and user research methods. Introduction to the course and practicals. \nUser-Centred Design. User research methods. \nKeeping usera in mind. User research data analysis. Identifying users and stakeholders. \nRepresenting user goals and activities. Identifying and establishing requirements. \nDesign and prototyping. Methods for exploring the design space. Prototyping and different kinds \nof prototypes. \nVisual and interaction design. Memory, perception, attention, and their implications for \ninteraction design. Modalities of interaction, interaction design patterns, information architecture, \nand their implications for interaction design. \nEvaluation. Practical methods for evaluating designs. Evaluation methods without users. \nEvaluation methods with users. \nCase studies from industry and research. Guest lectures (the topics of these lectures are \nsubject to change). \nObjectives \nBy the end of the course students should \n \nhave a thorough understanding of the user-centred design process and be able to apply it to \ninteraction design; \nbe able to design new user interfaces that are informed by principles of good design, and the \nprinciples of human visual perception, cognition and communication; \nbe able to prototype and implement interactive user interfaces with a strong emphasis on users, \nusability and appearance; \nbe able to evaluate existing or new user interfaces using multiple techniques; \nbe able to compare and contrast different design techniques and to critique their applicability to \nnew domains. \nRecommended reading \n* Preece, J., Rogers, Y. and Sharp, H. (2015). Interaction design. Wiley (4th ed.).\n \n\nINTRODUCTION TO PROBABILITY \n \nAims \nThis course provides an elementary introduction to probability and statistics with applications. \nProbability theory and the related field of statistical inference provide the foundations for \nanalysing and making sense of data. The focus of this course is to introduce the language and \ncore concepts of probability theory. The course will also cover some applications of statistical \ninference and algorithms in order to equip students with the ability to represent problems using \nthese concepts and analyse the data within these principles. \n \nLectures \nPart 1 - Introduction to Probability \n \nIntroduction. Counting/Combinatorics (revision), Probability Space, Axioms, Union Bound. \nConditional probability. Conditional Probabilities and Independence, Bayes\u2019Theorem, Partition \nTheorem \nPart 2 - Discrete Random Variables \n \nRandom variables. Definition of a Random Variable, Probability Mass Function, Cumulative \nDistribution, Expectation. \nProbability distributions. Definition and Properties of Expectation, Variance, different ways of \ncomputing them, Examples of important Distributions (Bernoulli, Binomial, Geometric, Poisson), \nPrimer on Continuous Distributions including Normal and Exponential Distributions. \nMultivariate distributions. Multiple Random Variables, Joint and Marginal Distributions, \nIndependence of Random Variables, Covariance. \nPart 3 - Moments and Limit Theorems \n \nIntroduction. Law of Average, Useful inequalities (Markov and Chebyshef), Weak Law of Large \nNumbers (including Proof using Chebyshef\u2019s inequality), Examples. \nMoments and Central Limit Theorem. Introduction to Moments of Random Variables, Central \nLimit Theorem (Proof using Moment Generating functions), Example. \nPart 4 - Applications/Statistics \n \nStatistics. Classical Parameter Estimation (Maximum-Likelihood-Estimation), bias, sample \nmean, sample variance), Examples (Collision-Sampling, Estimating Population Size). \nAlgorithms. Online Algorithms (Secretary Problem, Odd\u2019s Algorithm). \nObjectives \nAt the end of the course students should \n \nunderstand the basic principles of probability spaces and random variables \nbe able to formulate problems using concepts from probability theory and compute or estimate \nprobabilities \nbe familiar with more advanced concepts such as moments, limit theorems and applications \nsuch as parameter estimation \n\nRecommended reading \n* Ross, S.M. (2014). A First course in probability. Pearson (9th ed.). \n \nBertsekas, D.P. and Tsitsiklis, J.N. (2008). Introduction to probability. Athena Scientific. \n \nGrimmett, G. and Welsh, D. (2014). Probability: an Introduction. Oxford University Press (2nd \ned.). \n \nDekking, F.M., et. al. (2005) A modern introduction to probability and statistics. Springer.\n\nSOFTWARE AND SECURITY ENGINEERING \n \nAims \nThis course aims to introduce students to software and security engineering, and in particular to \nthe problems of building large systems, safety-critical systems and systems that must withstand \nattack by capable opponents. Case histories of failure are used to illustrate what can go wrong, \nwhile current software and security engineering practice is studied as a guide to how failures \ncan be avoided. \n \nLectures \n1. What is a security policy or a safety case? Definitions and examples; one-way flows for both \nconfidentiality and safety properties; separation of duties. Top-down and bottom-up analysis \nmethods. What architecture can do, versus benefits of decoupling policy from mechanism. \n \n2. Examples of safety and security policies. Safety and security usability; the pyramid of harms. \nPredicting and mitigating user errors. The prevention of fraud and error in accounting systems; \nthe safety usability of medical devices. \n \n3. Attitudes to risk: expected  utility, prospect theory, framing, status quo bias. Authority, \nconformity and gender; mental models, affordances and defaults. The characteristics of human \nmemory; forgetting passwords versus guessing them. \n \n4. Security protocols; how to enforce policy using  structured human interaction, cryptography or \nboth. Middleperson attacks.The role of verification and its limitations. \n \n5. Attacks on TLS, from rogue CAs through side channels to Heartbleed. Other types of \nsoftware bugs: syntactic, timing, concurrency, code injection, buffer overflows. Defensive \nprogramming: secure coding, contracts. Fuzzing. \n \n6. The software crisis. Examples of large-scale project failure, such as the London Ambulance \nService system and the NHS National Programme for IT. Intrinsic difficulties with complex \nsoftware. \n \n7. Software engineering as the management of complexity. The software life cycle; requirements \nanalysis methods; modular design; the role of prototyping; the waterfall, spiral and agile models. \n \n8. The economics of software as a Service (SaaS); the impact SaaS has on software \nengineering. Continuous integration, release engineering, behavioural analytics and experiment \nframeworks, rearchitecting systems while in operation. \n \n9. Critical systems: safety as an emergent system property. Examples of catastrophic failure: \nfrom Therac-25 to the Boeing 737Max. The problems of managing redundancy. The overall \nprocess of safety engineering. \n \n\n10. Managing the development of critical systems: tools and methods, individual versus group \nproductivity, economics of testing and agile development, measuring outcomes versus process, \nthe technical and human aspects of management, post-market surveillance and coordinated \ndisclosure. The sustainability of products with software components. \n \nAt the end of the course students should know how writing programs with tough assurance \ntargets, in large teams, or both, differs from the programming exercises they have engaged in \nso far. They should understand the different models of software development described in the \ncourse as well as the value of various development and management tools. They should \nunderstand the development life cycle and its basic economics. They should understand the \nvarious types of bugs, vulnerabilities and hazards, how to find them, and how to avoid \nintroducing them. Finally, they should be prepared for the organizational aspects of their Part IB \ngroup project. \n \nRecommended reading \nAnderson, R. (Third Edition 2020). Security engineering (Part 1 and Chapters 27-28). Wiley. \nAvailable at: http://www.cl.cam.ac.uk/users/rja14/book.html\n \n\n",
        "3rd Semester \nCONCURRENT AND DISTRIBUTED SYSTEMS \n \nAims \nThis course considers two closely related topics, Concurrent Systems and Distributed Systems, \nover 16 lectures. The aim of the first half of the course is to introduce concurrency control \nconcepts and their implications for system design and implementation. The aims of the latter \nhalf of the course are to study the fundamental characteristics of distributed systems, including \ntheir models and architectures; the implications for software design; some of the techniques that \nhave been used to build them; and the resulting details of good distributed algorithms and \napplications. \n \nLectures: Concurrency \nIntroduction to concurrency, threads, and mutual exclusion. Introduction to concurrent systems; \nthreads; interleaving; preemption; parallelism; execution orderings; processes and threads; \nkernel vs. user threads; M:N threads; atomicity; mutual exclusion; and mutual exclusion locks \n(mutexes). \nAutomata Composition. Synchronous and asynchronous parallelism; sequential consistency; \nrendezvous. Safety, liveness and deadlock; the Dining Philosophers; Hardware foundations for \natomicity: test-and-set, load-linked/store-conditional and fence instructions. Lamport bakery \nalgorithm. \nCommon design patterns: semaphores, producer-consumer, and MRSW. Locks and invariants; \nsemaphores; condition synchronisation; N-resource allocation; two-party and generalised \nproducer-consumer; Multi-Reader, Single-Write (MRSW) locks. \nCCR, monitors, and concurrency in practice. Conditional critical regions (CCR); monitors; \ncondition variables; signal-wait vs. signal-continue semantics; concurrency in practice (kernels, \npthreads, Java, Cilk, OpenMP). \nDeadlock and liveness guarantees Offline vs. online; model checking; resource allocation \ngraphs; lock order checking; deadlock prevention, avoidance, detection, and recovery; livelock; \npriority inversion; auto parallelisation. \nConcurrency without shared data; transactions. Active objects; message passing; tuple spaces; \nCSP; and actor models. Composite operations; transactions; ACID; isolation; and serialisability. \nFurther transactions History graphs; good and bad schedules; isolation vs. strict isolation; \n2-phase locking; rollback; timestamp ordering (TSO); and optimistic concurrency control (OCC). \nCrash recovery, lock-free programming, and transactional memory. Write-ahead logging, \ncheckpoints, and recovery. Lock-free programming. Hardware and software transactional \nmemories. \n  \n \nLectures: Distributed Systems \nIntroduction to distributed systems; RPC. Avantages and challenges of distributed systems; \nunbounded delay and partial failure; network protocols; transparency; client-server systems; \nremote procedure call (RPC); marshalling; interface definition languages (IDLs). \n\nSystem models and faults. Synchronous, partially synchronous, and asynchronous network \nmodels; crash-stop, crash-recovery, and Byzantine faults; failures, faults, and fault tolerance; \ntwo generals problem. \nTime, clocks, and ordering of events. Physical clocks; leap seconds; UTC; clock synchronisation \nand drift; Network Time Protocol (NTP). Causality; happens-before relation. \nLogical time; Lamport clocks; vector clocks. Broadcast (FIFO, causal, total order); gossip \nprotocols. \nReplication. Quorums; idempotence; replica consistency; read-after-write consistency. State \nmachine replication; leader-based replication. \nConsensus and total order broadcast. FLP result; leader election; the Raft consensus algorithm. \nReplica consistency. Two-phase commit; relationship between 2PC and consensus; \nlinearizability; ABD algorithm; eventual consistency; CAP theorem. \nCase studies. Conflict-free Replicated Data Types (CRDTs); collaborative text editing. Google's \nSpanner; TrueTime. \nObjectives \nAt the end of Concurrent Systems portion of the course, students should: \n \nunderstand the need for concurrency control in operating systems and applications, both mutual \nexclusion and condition synchronisation; \nunderstand how multi-threading can be supported and the implications of different approaches; \nbe familiar with the support offered by various programming languages for concurrency control \nand be able to judge the scope, performance implications and possible applications of the \nvarious approaches; \nbe aware that dynamic resource allocation can lead to deadlock; \nunderstand the concept of transaction; the properties of transactions, how they can be \nimplemented, and how their performance can be optimised based on optimistic assumptions; \nunderstand how the persistence properties of transactions are addressed through logging; and \nhave a high-level understanding of the evolution of software use of concurrency in the \noperating-system kernel case study. \nAt the end of the Distributed Systems portion of the course, students should: \n \nunderstand the difference between shared-memory concurrency and distributed systems; \nunderstand the fundamental properties of distributed systems and their implications for system \ndesign; \nunderstand notions of time, including logical clocks, vector clocks, and physical time \nsynchronisation; \nbe familiar with various approaches to data and service replication, as well as the concept of \nreplica consistency; \nunderstand the effects of large scale on the provision of fundamental services and the tradeoffs \narising from scale; \nappreciate the implications of individual node and network communications failures on \ndistributed computation; \nbe aware of a variety of programming models and abstractions for distributed systems, such as \nRPC, middleware, and total order broadcast; \n\nbe familiar with a range of distributed algorithms, such as consensus, causal broadcast, and \ntwo-phase commit. \nRecommended reading \nModern Operating Systems (free PDF available online) by Andrew S Tanenbaum, Herbert Bos \nJava Concurrency in Practice' (2006 but still highly relevant) by Brian Goetz \nKleppmann, M. (2017). Designing data-intensive applications. O\u2019Reilly. \nTanenbaum, A.S. and van Steen, M. (2017). Distributed systems, 3rd edition. available online. \nCachin, C., Guerraoui, R. and Rodrigues, L. (2011) Introduction to Reliable and Secure \nDistributed Programming. Springer (2nd edition).\n \n\nDATA SCIENCE \n \nAims \nThis course introduces fundamental tools for describing and reasoning about data. There are \ntwo themes: designing probability models to describe systems; and drawing conclusions based \non data generated by such systems. \n \nLectures \nSpecifying and fitting probability models. Random variables. Maximum likelihood estimation. \nGenerative and supervised models. Goodness of fit. \nFeature spaces. Vector spaces, bases, inner products, projection. Linear models. Model fitting \nas projection. Design of features. \nHandling probability models. Handling pdf and cdf. Bayes\u2019s rule. Monte Carlo estimation. \nEmpirical distribution. \nInference. Bayesianism. Frequentist confidence intervals, hypothesis testing. Bootstrap \nresampling. \nRandom processes. Markov chains. Stationarity, and drift analysis. Processes with memory. \nLearning a random process. \nObjectives \nAt the end of the course students should \n \nbe able to formulate basic probabilistic models, including discrete time Markov chains and linear \nmodels \nbe familiar with common random variables and their uses, and with the use of empirical \ndistributions rather than formulae \nunderstand different types of inference about noisy data, including model fitting, hypothesis \ntesting, and making predictions \nunderstand the fundamental properties of inner product spaces and orthonormal systems, and \ntheir application to modelling \nRecommended reading \n* F.M. Dekking, C. Kraaikamp, H.P. Lopuha\u00e4, L.E. Meester (2005). A modern introduction to \nprobability and statistics: understanding why and how. Springer. \n \nS.M. Ross (2002). Probability models for computer science. Harcourt / Academic Press. \n \nM. Mitzenmacher and E. Upfal (2005). Probability and computing: randomized algorithms and \nprobabilistic analysis. Cambridge University Press.\n \n\nECAD AND ARCHITECTURE PRACTICAL CLASSES \nAims \nThe aims of this course are to enable students to apply the concepts learned in the Computer \nDesign course. In particular a web based tutor is used to introduce the SystemVerilog hardware \ndescription language, while the remaining practical classes will then allow students to implement \nthe design of components in this language. \n \nPractical Classes \nWeb tutor The first class uses a web based tutor to rapidly teach the SystemVerilog language. \nFPGA design flow Test driven hardware development for FPGA including an embedded \nprocessor and peripherals [3 classes] \nEmbedded system implementation Embedded system implementation on FPGA [3-4 classes] \nObjectives \nGain experience in electronic computer aided design (ECAD) through learning a design-flow for \nfield programmable gate arrays (FPGAs). \nLearn how to interface to peripherals like a touch screen. \nLearn how to debug hardware and software systems in simulation. \nUnderstand how to construct and program a heterogeneous embedded system. \nRecommended reading \n* Harris, D.M. and Harris, S.L. (2007). Digital design and computer architecture: from gates to \nprocessors. Morgan Kaufmann. \n \nPointers to sources of more specialist information are included on the associated course web \npage. \n \nIntroduction \nThe ECAD and Architecture Laboratory sessions are a companion to the Introduction to \nComputer Architecture course. The objective is to provide experience of hardware design for \nFPGA including use of a small embedded processor. It covers hardware design in \nSystemVerilog, embedded software design in RISC-V assembly and C, and use of FPGA tools. \n \nPrerequisites \nThis course assumes familiarity with the material from Part IA Digital Electronics, although we \nwill use automated tools to perform many of the steps you might have done there by hand (for \nexample, logic minimisation). \n \nWe will be using a Linux command-line environment. We provide scripts and guidance on what \nyou need, however you may find it useful to review Unix Tools, in particular basic navigation \nsuch as ls, cd, mkdir, cp, mv. We'll be using the GCC compiler and Makefiles, described in parts \n31-32. We will also be using git for revision control and submission of work for ticking - you \nshould review at least parts 23-25 and 29. \n \nPracticalities \n\nThe course runs over the 8 weeks of Michaelmas term. The course has been designed so you \ncan do most of the work at home or at any time you wish. You should be able to run all the \nnecessary tools on your own machine, through the use of virtual machines and Docker \ncontainers. We have a number of routes to do this in case some of them aren't suitable for the \nmachine you have. \n \nWe're running the course in a hybrid mode this year. We will provide online help sessions via \nMicrosoft Teams as well as in-person lab sessions on Fridays and Tuesdays 2-5pm during term. \n \nThe core components of the course, being the RISC-V architecture and software and ticked \nexercise, can be done entirely online, without needing access to hardware. They can be \nundertaken with any of the platforms we support, including MCS Linux (on the Intel Lab \nworkstations and remotely) if your own machine isn't suitable. We expect everyone to complete \nthese parts. \n \nThe rest of the course is optional. The hardware simulation and FPGA compilation can be done \non your machine without access to hardware, if you are able to run the virtual machine. \n \nThe parts that require access to an FPGA board will need the ability to run the virtual machine \nand you be able to collect and return an FPGA board from the Department. That process will be \ndescribed when you come to those parts. There is an optional starred tick available for \ncompletion of this part, which will need assessment by a demonstrator in person. \n \nWe'll do our best to support all the different platforms and variations, but please bear with us - \nit's quite possible we'll come across a problem we haven't seen before! \n \nWe have provided four routes you can use different tools to complete the course. Some routes \nnecessarily exclude some material but this material is optional. \n \nAssessment \nIn a similar way to other practical courses, this course carries one 'tick'. Everyone should expect \nto receive this tick, and those who do not should expect to lose marks on their final exams. \n \nThis year we have adopted the 'chime' autoticker used by Further Java. You will check out the \ninitial files as a git repository from the chime server, commit your work, and push the repository \nback to submit it. You can then press a button to run tests against your code and the autoticker \nwill tell you whether you have passed. Additionally you may be selected for a (real/video) \ninterview with a ticker to discuss your code and understanding of the material. \n \nTicking procedures will only be available during weeks 1-8 of Michaelmas term. The deadline for \nsubmitting Tick 1 is 5pm on the Tuesday of week 6 (19 November 2024). After passing the \nautoticker you may be asked for a tick interview with a demonstrator (online or in person). You \ncan gain Tick 1* during the timetabled lab sessions, assuming demonstrators are available. \n \n\nIf you do not submit a successful Tick 1 to the autoticker by 5pm on the Tuesday of week 6 you \ncan self-certify an extension. If you need more time than this you will need to ask your Directory \nof Studies to organise a further extension. The hard deadline by which all ticks must be \ncompleted is noon on Friday 31 January 2025 (the Head of Department Notices is the definitive \ndocument). Extensions beyond this date due to exceptional circumstances require a formal \napplication from your college to the Examiners. \n \nScheduling \nWhile the full course contains 8 exercises, there is not a tight binding to doing one exercise per \nweek. Students should expect to spend about three hours per week on the course, however it is \nrecommended that you make a start on the next exercise if you have time in hand. \n \nDemonstrator help will be focused on the Tuesday and Friday 2-5pm slots during term the lab \nwould normally operate in, so you may find it helpful to work in these times as that will provide \nthe quickest response to questions. \n \nCollaboration \nWhen we have done these labs in person, we have often found they worked well when doing \nthem collaboratively. While your work must be your own, students can work together and help \neach other, and demonstrators contribute advice and experience with the tools. \n \nFor the timetabled sessions, which are Tuesdays and Fridays 2-5pm UK time during term, there \nwill be demonstrators available in the lab and at the same time we'll use the ECAD group on \nMicrosoft Teams. With this we can provide online audio/video help, screensharing and (for the \nWindows and possibly Mac Teams apps) optional remote control of your development \nenvironment. You will be added to the Team in week 1 - if you believe you have been missed \nplease post in the Moodle forum. In Teams there are a number of Helpdesk Channels (A-C) - if \nyou need help during lab times, post in Help Centre and a demonstrator can ask you to go to a \nspecific channel. There are also Student Breakout Channels (D-F) which are available for \nstudents to chat amongst themselves. \n \nFor help outside of timetabled sessions we've set up a Moodle forum where you can post \nquestions - we'll keep an eye on this at other times, but students are encouraged to use it to \nsupport each other. \n \nStudents are of course free to use other platforms to help each other. If you find anything useful \nthat we might use in future, please let us know! \n \nIf you are having difficulty accessing both Moodle and Teams, please email theo.markettos at \ncl.cam.ac.uk. \n \nFeedback \nWe revise the course each year, which may cause new bugs in the code or the notes. The \ncreators (Theo Markettos and Simon Moore) appreciate constructive feedback. \n\n \nCredits \nThis course was written by Theo Markettos and Simon Moore, with portions developed by \nRobert Eady, Jonas Fiala, Paul Fox, Vlad Gavrila, Brian Jones and Roy Spliet. We would like to \nthank Altera, Intel and Terasic for funding and other contributions.\n \n\nECONOMICS, LAW AND ETHICS \n \nAims \nThis course aims to give students an introduction to some basic concepts in economics, law and \nethics. \n \nLectures \nClassical economics and consumer theory. Prices and markets; Pareto efficiency; preferences; \nutility; supply and demand; the marginalist revolution; elasticity; the welfare theorems; \ntransaction costs. \nInformation economics. The discriminating monopolist; marginal costs; effects of technology on \nsupply and demand; competition and information; lock in; real and virtual networks; Metcalfe\u2019s \nlaw; the dominant firm model; price discrimination; bundling; income distribution. \nMarket failure and behavioural economics. Market failure: the business cycle; recession and \ntechnology; tragedy of the commons; externalities; monopoly rents; asymmetric information: the \nmarket for lemons; adverse selection; moral hazard; signalling. Behavioural economics: \nbounded rationality, heuristics and biases; nudge theory; the power of defaults; agency effects. \nAuction theory and game theory. Auction theory: types of auctions; strategic equivalence; the \nrevenue equivalence theorem; the winner\u2019s curse; problems with real auctions; mechanism \ndesign and the combinatorial auction; applicability of auction mechanisms in computer science; \nadvertising auctions. Game theory: the choice between cooperation and conflict; strategic \nforms; dominant strategy equilibrium; Nash equilibrium; the prisoners\u2019 dilemma; evolution of \nstrategies; stag hunt; volunteer\u2019s dilemma; chicken; iterated games; hawk-dove; application to \ncomputer science. \nPrinciples of law. Criminal and civil law; contract law; choice of law and jurisdiction; arbitration; \ntort; negligence; defamation; intellectual property rights. \nLaw and the Internet. Computer evidence; the General Data Protection Regulation; UK laws that \nspecifically affect the Internet; e-commerce regulations; privacy and electronic communications. \nPhilosophies of ethics. Authority, intuitionist, egoist and deontological theories; utilitarian and \nRawlsian models; morality; insights from evolutionary psychology, neurology, and experimental \nethics; professional codes of ethics; research ethics. \nContemporary ethical issues. The Internet and social policy; current debates on privacy, \nsurveillance, and censorship; responsible vulnerability disclosure; algorithmic bias; predictive \npolicing; gamification and engagement; targeted political advertising; environmental impacts. \nObjectives \nOn completion of this course, students should be able to: \n \nReflect on and discuss professional, economic, social, environmental, moral and ethical issues \nrelating to computer science \nDefine and explain economic and legal terminology and arguments \nApply the philosophies and theories covered to computer science problems and scenarios \nReflect on the main constraints that market, legislation and ethics place on firms dealing in \ninformation goods and services \nRecommended reading \n\n* Shapiro, C. & Varian, H. (1998). Information rules. Harvard Business School Press. \nHare, S. (2022). Technology is not neutral: A short guide to technology ethics. London \nPublishing Partnership. \n \nFurther reading: \nSmith, A. (1776). An inquiry into the nature and causes of the wealth of nations, available at    \nhttp://www.econlib.org/library/Smith/smWN.html \nThaler, R.H. (2016). Misbehaving. Penguin. \nGalbraith, J.K. (1991). A history of economics. Penguin. \nPoundstone, W. (1992). Prisoner\u2019s dilemma. Anchor Books. \nPinker, S (2011). The Better Angels of our Nature. Penguin. \nAnderson, R. (2008). Security engineering (Chapter 7). Wiley. \nVarian, H. (1999). Intermediate microeconomics - a modern approach. Norton. \nNuffield Council on Bioethics (2015) The collection, linking and use of data in biomedical \nresearch and health care.\n \n\nFURTHER GRAPHICS \n \nAims \nThe course introduces fundamental concepts of modern graphics pipelines. \n \nLectures \nThe order and content of lectures is provisional and subject to change. \n \nGeometry Representations. Parametric surfaces, implicit surfaces, meshes, point-set surfaces, \ngeometry processing pipeline. [1 lecture] \nDiscrete Differential Geometry. Surface normal and curvature, Laplace-Beltrami operator, heat \ndiffusion. [1 lecture] \nGeometry Processing.  Parametrization, filtering, 3D capture. [1 lecture] \nAnimation I. Animation types, animation pipeline, rigging/skinning, character animation. [1 \nlecture] \nAnimation II. Blending transformations, pose-space animation, controllers. [1 lecture] \nThe Rendering Equation. Radiosity, reflection models, BRDFs, local vs. global illumination. [1 \nlecture] \nDistributed Ray Tracing. Quadrature, importance sampling, recursive ray tracing. [1 lecture] \nInverse Rendering. Differentiable rendering, inverse problems. [1 lecture] \nObjectives \nOn completing the course, students should be able to \n \nunderstand and use fundamental 3D geometry/scene representations and operations; \nlearn animation techniques and controls; \nlearn how light transport is modeled and simulated in rendering; \nunderstand how graphics and rendering can be used to solve computer perception problems. \nRecommended reading \nStudents should expect to refer to one or more of these books, but should not find it necessary \nto purchase any of them. \n \nShirley, P. and Marschner, S. (2009). Fundamentals of Computer Graphics. CRC Press (3rd \ned.). \nWatt, A. (2000). 3D Computer Graphics. Addison-Wesley (3rd ed). \nHughes, van Dam, McGuire, Skalar, Foley, Feiner and Akeley (2013). Computer Graphics: \nPrinciples and Practice. Addison-Wesley (3rd edition) \nAkenine-M\u00f6ller, et. al. (2018). Real-time rendering. CRC Press (4th ed.).\n \n\nFURTHER JAVA \n \nAims \nThe goal of this course is to provide students with the ability to understand the advanced \nprogramming features available in the Java programming language, completing the coverage of \nthe language started in the Programming in Java course. The course is designed to \naccommodate students with diverse programming backgrounds; consequently Java is taught \nfrom first principles in a practical class setting where students can work at their own pace from a \ncourse handbook. \n \nObjectives \n \nAt the end of the course students should \n \nunderstand different mechanisms for communication between distributed applications and be \nable to evaluate their trade-offs; \nbe able to use Java generics and annotations to improve software usability, readability and \nsafety; \nunderstand and be able to exploit the Java class-loading mechansim; \nunderstand and be able to use concurrency control correctly; \nbe able to implement a vector clock algorithm and the happens-before relation. \nRecommended reading \n* Goetz, B. (2006). Java concurrency in practice. Addison-Wesley. \nGosling, J., Joy, B., Steele, G., Bracha, G. and Buckley, A. (2014). The Java language \nspecification, Java SE 8 Edition. Addison-Wesley. \nhttp://docs.oracle.com/javase/specs/jls/se8/html/\n \n\nGROUP PROJECTS \nExample Risk Report \nThe risk report should focus on the risks to your project succeeding on time (and not, say, on \ngeneral health and safety points). It is only 50 words. Example: \n \n\"Identified risks: \n * Training data requires access authorisation that has not been approved, and we don\u2019t know if \nclient can do this \n * Two group members have unreliable network connections and may miss meetings \n * One group member is in timezone UTC+8, and will have to attend meetings late at night \n * Bug reports on Github for a library we plan to use suggest maintenance updates will be \nnecessary for recent Android release\"\n \n\nINTRODUCTION TO COMPUTER ARCHITECTURE \n \nAims \nThe aims of this course are to introduce a hardware description language (SystemVerilog) and \ncomputer architecture concepts in order to design computer systems. The parallel ECAD+Arch \npractical classes will allow students to apply the concepts taught in lectures. \n \nLectures \nPart 1 - Gates to processors  \n \nTechnology trends and design challenges. Current technology, technology trends, ECAD trends, \nchallenges. [1 lecture] \nDigital system design. Practicalities of mapping SystemVerilog descriptions of hardware \n(including a processor) onto an FPGA board. Tips and pitfalls when generating larger modular \ndesigns. [1 lecture] \nEight great ideas in computer architecture. [1 lecture] \nReduced instruction set computers and RISC-V. Introduction to the RISC-V processor design. [1 \nlecture] \nExecutable and synthesisable models. [1 lecture] \nPipelining. [2 lectures] \nMemory hierarchy and caching. Caching, etc. [1 lecture] \nSupport for operating systems. Memory protection, exceptions, interrupts, etc. [1 lecture] \nOther instruction set architectures. CISC, stack, accumulator. [1 lecture] \nPart 2  \n \nOverview of Systems-on-Chip (SoCs) and DRAM. [1 lecture] High-level SoCs, DRAM storage \nand accessing. \nMulticore Processors. [2 lectures] Communication, cache coherence, barriers and \nsynchronisation primitives. \nGraphics processing units (GPUs) [2 lectures] Basic GPU architecture and programming. \nFuture Directions [1 lecture] Where is computer architecture heading? \nObjectives \nAt the end of the course students should \n \nbe able to read assembler given a guide to the instruction set and be able to write short pieces \nof assembler if given an instruction set or asked to invent an instruction set; \nunderstand the differences between RISC and CISC assembler; \nunderstand what facilities a processor provides to support operating systems, from memory \nmanagement to software interrupts; \nunderstand memory hierarchy including different cache structures and coherency needed for \nmulticore systems; \nunderstand how to implement a processor in SystemVerilog; \nappreciate the use of pipelining in processor design; \nhave an appreciation of control structures used in processor design; \n\nhave an appreciation of system-on-chips and their components; \nunderstand how DRAM stores data; \nunderstand how multicore processors communicate; \nunderstand how GPUs work and have an appreciation of how to program them. \nRecommended reading \n* Patterson, D.A. and Hennessy, J.L. (2017). Computer organization and design: The \nhardware/software interface RISC-V edition. Morgan Kaufmann. ISBN 978-0-12-812275-4. \n \nRecommended further reading: \n \nHarris, D.M. and Harris, S.L. (2012). Digital design and computer architecture. Morgan \nKaufmann. ISBN 978-0-12-394424-5. \nHennessy, J. and Patterson, D. (2006). Computer architecture: a quantitative approach. Elsevier \n(4th ed.). ISBN 978-0-12-370490-0. (Older versions of the book are also still generally relevant.) \n \nPointers to sources of more specialist information are included in the lecture notes and on the \nassociated course web page\n \n\nPROGRAMMING IN C AND C++ \n \nAims \nThis course aims \n \nto provide a solid introduction to programming in C and to provide an overview of the principles \nand constraints that affect the way in which the C programming language has been designed \nand is used; \nto introduce the key additional features of C++. \n  \n \nLectures \nIntroduction to the C language. Background and goals of C. Types, expressions, control flow \nand strings. \n(continued) Functions. Multiple compilation units. Scope. Segments. Incremental compilation. \nPreprocessor. \n(continued) Pointers and pointer arithmetic. Function pointers. Structures and Unions. \n(continued) Const and volatile qualifiers. Typedefs. Standard input/output. Heap allocation. \nMiscellaneous Features, Hints and Tips. \nC semantics and tools. Undefined vs implementation-defined behaviour. Buffer and integer \noverflows. ASan, MSan, UBsan, Valgrind checkers. \nMemory allocation, data structures and aliasing. Malloc/free, tree and DAG examples and their \ndeallocation using a memory arena. \nFurther memory management. Reference Counting and Garbage Collection \nMemory hierarchy and cache optimization. Data structure layouts. Intrusive lists. Array-of-structs \nvs struct-of-arrays representations. Loop blocking. \nDebugging. Using print statements, assertions and an interactive debugger. Unit testing and \nregression. \nIntroduction to C++. Goals of C++. Differences between C and C++. References versus \npointers. Overloaded functions. \nObjects in C++. Classes and structs. Destructors. Resource Acquisition is Initialisation. Operator \noverloading. Virtual functions. Casts. Multiple inheritance. Virtual base classes. \nOther C++ concepts. Templates and meta-programming. \nObjectives \nAt the end of the course students should \n \nbe able to read and write C programs; \nunderstand the interaction between C programs and the host operating system; \nbe familiar with the structure of C program execution in machine memory; \nunderstand the potential dangers of writing programs in C; \nunderstand the object model and main additional features of C++. \nRecommended reading \n* Kernighan, B.W. and Ritchie, D.M. (1988). The C programming language. Prentice Hall (2nd \ned.).\n \n\nSEMANTICS OF PROGRAMMING LANGUAGES \n \nAims \nThe aim of this course is to introduce the structural, operational approach to programming \nlanguage semantics. It will show how to specify the meaning of typical programming language \nconstructs, in the context of language design, and how to reason formally about semantic \nproperties of programs. \n \nLectures \nIntroduction. Transition systems. The idea of structural operational semantics. Transition \nsemantics of a simple imperative language. Language design options. [2 lectures] \nTypes. Introduction to formal type systems. Typing for the simple imperative language. \nStatements of desirable properties. [2 lectures] \nInduction. Review of mathematical induction. Abstract syntax trees and structural induction. \nRule-based inductive definitions and proofs. Proofs of type safety properties. [2 lectures] \nFunctions. Call-by-name and call-by-value function application, semantics and typing. Local \nrecursive definitions. [2 lectures] \nData. Semantics and typing for products, sums, records, references. [1 lecture] \nSubtyping. Record subtyping and simple object encoding. [1 lecture] \nSemantic equivalence. Semantic equivalence of phrases in a simple imperative language, \nincluding the congruence property. Examples of equivalence and non-equivalence. [1 lecture] \nConcurrency. Shared variable interleaving. Semantics for simple mutexes; a serializability \nproperty. [1 lecture] \nObjectives \nAt the end of the course students should \n \nbe familiar with rule-based presentations of the operational semantics and type systems for \nsome simple imperative, functional and interactive program constructs; \nbe able to prove properties of an operational semantics using various forms of induction \n(mathematical, structural, and rule-based); \nbe familiar with some operationally-based notions of semantic equivalence of program phrases \nand their basic properties. \nRecommended reading \n* Pierce, B.C. (2002). Types and programming languages. MIT Press. \nHennessy, M. (1990). The semantics of programming languages. Wiley. Out of print, but \navailable on the web at \nhttp://www.cs.tcd.ie/matthew.hennessy/splexternal2015/resources/sembookWiley.pdf \nWinskel, G. (1993). The formal semantics of programming languages. MIT Press.\n \n\nUNIX TOOLS \n \nAims \nThis video-lecture course gives students who have already basic Unix/Linux experience some \nadditional practical software-engineering knowledge: how to use the shell and related tools as \nan efficient working environment, how to automate routine tasks, and how version-control and \nautomated-build tools can help to avoid confusion and accidents, especially when working in \nteams. These are essential skills, both in industrial software development and student projects. \n \nLectures \nUnix concepts. Brief review of Unix history and design philosophy, documentation, terminals, \ninter-process communication mechanisms and conventions, shell, command-line arguments, \nenvironment variables, file descriptors. \nShell concepts. Program invocation, redirection, pipes, file-system navigation, argument \nexpansion, quoting, job control, signals, process groups, variables, locale, history and alias \nfunctions, security considerations. \nScripting. Plain-text formats, executables, #!, shell control structures and functions. Startup \nscripts. \nText, file and networking tools. sed, grep, chmod, find, ssh, rsync, tar, zip, etc. \nVersion control. diff, patch, RCS, Subversion, git. \nSoftware development tools. C compiler, linker, debugger, make. \nPerl. Introduction to a powerful scripting and text-manipulation language. [2 lectures] \nObjectives \nAt the end of the course students should \n \nbe confident in performing routine user tasks on a POSIX system, understand command-line \nuser-interface conventions and know how to find more detailed documentation; \nappreciate how simple tools can be combined to perform a large variety of tasks; \nbe familiar with the most common tools, file formats and configuration practices; \nbe able to understand, write, and maintain shell scripts and makefiles; \nappreciate how using a version-control system and fully automated build processes help to \nmaintain reproducibility and audit trails during software development; \nknow enough about basic development tools to be able to install, modify and debug C source \ncode; \nhave understood the main concepts of, and gained initial experience in, writing Perl scripts \n(excluding the facilities for object-oriented programming). \nRecommended reading \nRobbins, A. (2005). Unix in a nutshell. O\u2019Reilly (4th ed.). \nSchwartz, R.L., Foy, B.D. and Phoenix, T. (2011). Learning Perl. O\u2019Reilly (6th ed.).\n \n\n",
        "4th Semester \n \nCOMPILER CONSTRUCTION \n \nAims \nThis course aims to cover the main concepts associated with implementing compilers for \nprogramming languages. We use a running example called SLANG (a Small LANGuage) \ninspired by the languages described in 1B Semantics. A toy compiler (written in ML) is provided, \nand students are encouraged to extend it in various ways. \n \nLectures \nOverview of compiler structure The spectrum of interpreters and compilers; compile-time and \nrun-time. Compilation as a sequence of translations from higher-level to lower-level intermediate \nlanguages, where each translation preserves semantics. The structure of a simple compiler: \nlexical analysis and syntax analysis, type checking, intermediate representations, optimisations, \ncode generation. Overview of run-time data structures: stack and heap. Virtual machines. [1 \nlecture] \nLexical analysis and syntax analysis. Lexical analysis based on regular expressions and finite \nstate automata. Using LEX-tools. How does LEX work? Parsing based on context-free \ngrammars and push-down automata. Grammar ambiguity, left- and right-associativity and \noperator precedence. Using YACC-like tools. How does YACC work? LL(k) and LR(k) parsing \ntheory. [3 lectures] \nCompiler Correctness Recursive functions can be transformed into iterative functions using the \nContinuation-Passing Style (CPS) transformation. CPS applied to a (recursive) SLANG \ninterpreter to derive, in a step-by-step manner, a correct stack-based compiler. [3 lectures] \nData structures, procedures/functions Representing tuples, arrays, references. Procedures and \nfunctions: calling conventions, nested structure, non-local variables. Functions as first-class \nvalues represented as closures. Simple optimisations: inline expansion, constant folding, \nelimination of tail recursion, peephole optimisation. [5 lectures] \nAdvanced topics Run-time memory management (garbage collection). Static and dynamic \nlinking. Objects and inheritance; implementation of method dispatch. Try-catch exception \nmechanisms. Compiling a compiler via bootstrapping. [4 lectures] \nObjectives \nAt the end of the course students should understand the overall structure of a compiler, and will \nknow significant details of a number of important techniques commonly used. They will be \naware of the way in which language features raise challenges for compiler builders. \n \nRecommended reading \n* Aho, A.V., Sethi, R. and Ullman, J.D. (2007). Compilers: principles, techniques and tools. \nAddison-Wesley (2nd ed.). \nMogensen, T. \u00c6. (2011). Introduction to compiler design. Springer. http://www.diku.dk/ \ntorbenm/Basics.\n \n\nCOMPUTATION THEORY \n \nAims \nThe aim of this course is to introduce several apparently different formalisations of the informal \nnotion of algorithm; to show that they are equivalent; and to use them to demonstrate that there \nare uncomputable functions and algorithmically undecidable problems. \n \nLectures \nIntroduction: algorithmically undecidable problems. Decision problems. The informal notion of \nalgorithm, or effective procedure. Examples of algorithmically undecidable problems. [1 lecture] \nRegister machines. Definition and examples; graphical notation. Register machine computable \nfunctions. Doing arithmetic with register machines. [1 lecture] \nUniversal register machine. Natural number encoding of pairs and lists. Coding register machine \nprograms as numbers. Specification and implementation of a universal register machine. [2 \nlectures] \nUndecidability of the halting problem. Statement and proof. Example of an uncomputable partial \nfunction. Decidable sets of numbers; examples of undecidable sets of numbers. [1 lecture] \nTuring machines. Informal description. Definition and examples. Turing computable functions. \nEquivalence of register machine computability and Turing computability. The Church-Turing \nThesis. [2 lectures] \nPrimitive and partial recursive functions. Definition and examples. Existence of a recursive, but \nnot primitive recursive function. A partial function is partial recursive if and only if it is \ncomputable. [2 lectures] \nLambda-Calculus. Alpha and beta conversion. Normalization. Encoding data. Writing recursive \nfunctions in the lambda-calculus. The relationship between computable functions and \nlambda-definable functions. [3 lectures] \nObjectives \nAt the end of the course students should \n \nbe familiar with the register machine, Turing machine and lambda-calculus models of \ncomputability; \nunderstand the notion of coding programs as data, and of a universal machine; \nbe able to use diagonalisation to prove the undecidability of the Halting Problem; \nunderstand the mathematical notion of partial recursive function and its relationship to \ncomputability. \nRecommended reading \n* Hopcroft, J.E., Motwani, R. and Ullman, J.D. (2001). Introduction to automata theory, \nlanguages, and computation. Addison-Wesley (2nd ed.). \n* Hindley, J.R. and Seldin, J.P. (2008). Lambda-calculus and combinators, an introduction. \nCambridge University Press (2nd ed.). \nCutland, N.J. (1980). Computability: an introduction to recursive function theory. Cambridge \nUniversity Press. \nDavis, M.D., Sigal, R. and Weyuker, E.J. (1994). Computability, complexity and languages. \nAcademic Press (2nd ed.). \n\nSudkamp, T.A. (2005). Languages and machines. Addison-Wesley (3rd ed.).\n \n\nCOMPUTER NETWORKING \n \nAims \nThe aim of this course is to introduce key concepts and principles of computer networks. The \ncourse will use a top-down approach to study the Internet and its protocol stack. Instances of \narchitecture, protocol, application-examples will include email, web and media-streaming. We \nwill cover communications services (e.g., TCP/IP) required to support such network \napplications. The implementation and deployment of communications services in practical \nnetworks: including wired and wireless LAN environments, will be followed by a discussion of \nissues of network-management. Throughout the course, the Internet\u2019s architecture and \nprotocols will be used as the primary examples to illustrate the fundamental principles of \ncomputer networking. \n \nLectures \nIntroduction. Overview of networking using the Internet as an example. LANs and WANs. OSI \nreference model, Internet TCP/IP Protocol Stack. Circuit-switching, packet-switching, Internet \nstructure, networking delays and packet loss. [3 lectures] \nLink layer and local area networks. Link layer services, error detection and correction, Multiple \nAccess Protocols, link layer addressing, Ethernet, hubs and switches, Point-to-Point Protocol. [2 \nlectures] \nWireless and mobile networks. Wireless links and network characteristics, Wi-Fi: IEEE 802.11 \nwireless LANs. [1 lecture] \nNetwork layer addressing. Network layer services, IP, IP addressing, IPv4, DHCP, NAT, ICMP, \nIPv6. [3 lectures] \nNetwork layer routing. Routing and forwarding, routing algorithms, routing in the Internet, \nmulticast. [3 lectures] \nTransport layer. Service models, multiplexing/demultiplexing, connection-less transport (UDP), \nprinciples of reliable data transfer, connection-oriented transport (TCP), TCP congestion control, \nTCP variants. [6 lectures] \nApplication layer. Client/server paradigm, WWW, HTTP, Domain Name System, P2P. [1.5 \nlectures] \nMultimedia networking. Networked multimedia applications, multimedia delivery requirements, \nmultimedia protocols (SIP), content distribution networks. [0.5 lecture] \nObjectives \nAt the end of the course students should \n \nbe able to analyse a communication system by separating out the different functions provided \nby the network; \nunderstand that there are fundamental limits to any communications system; \nunderstand the general principles behind multiplexing, addressing, routing, reliable transmission \nand other stateful protocols as well as specific examples of each; \nunderstand what FEC is; \nbe able to compare communications systems in how they solve similar problems; \n\nhave an informed view of both the internal workings of the Internet and of a number of common \nInternet applications and protocols. \nRecommended reading \n* Peterson, L.L. and Davie, B.S. (2011). Computer networks: a systems approach. Morgan \nKaufmann (5th ed.). ISBN 9780123850591 \nKurose, J.F. and Ross, K.W. (2009). Computer networking: a top-down approach. \nAddison-Wesley (5th ed.). \nComer, D. and Stevens, D. (2005). Internetworking with TCP-IP, vol. 1 and 2. Prentice Hall (5th \ned.). \nStevens, W.R., Fenner, B. and Rudoff, A.M. (2003). UNIX network programming, Vol.I: The \nsockets networking API. Prentice Hall (3rd ed.).\n \n\nFURTHER HUMAN\u2013COMPUTER INTERACTION \n \nAims \nThis aim of this course is to provide an introduction to the theoretical foundations of Human \nComputer Interaction, and an understanding of how these can be applied to the design of \ncomplex technologies. \n \nLectures \nTheory driven approaches to HCI. What is a theory in HCI? Why take a theory driven approach \nto HCI? \nDesign of visual displays. Segmentation and variables of the display plane. Modes of \ncorrespondence. \nGoal-oriented interaction. Using cognitive theories of planning, learning and understanding to \nunderstand user behaviour, and what they find hard. \nDesigning smart systems. Using statistical methods to anticipate user needs and actions with \nBayesian strategies. \nDesigning efficient systems. Measuring and optimising human performance through quantitative \nexperimental methods. \nDesigning meaningful systems. Qualitative research methods to understand social context and \nrequirements of user experience. \nEvaluating interactive system designs. Approaches to evaluation in systems research and \nengineering, including Part II Projects. \nDesigning complex systems. Worked case studies of applying the theories to a hard HCI \nproblem. Research directions in HCI. \nObjectives \nAt the end of the course students should be able to apply theories of human performance and \ncognition to system design, including selection of appropriate techniques to analyse, observe \nand improve the usability of a wide range of technologies. \n \nRecommended reading \n* Preece, J., Sharp, H. and Rogers, Y. (2015). Interaction design: beyond human-computer \ninteraction. Wiley (Currently in 4th edition, but earlier editions will suffice). \n \nFurther reading: \n \nCarroll, J.M. (ed.) (2003). HCI models, theories and frameworks: toward a multi-disciplinary \nscience. Morgan Kaufmann.\n \n\nLOGIC AND PROOF \n \nAims \nThis course will teach logic, especially the predicate calculus. It will present the basic principles \nand definitions, then describe a variety of different formalisms and algorithms that can be used \nto solve problems in logic. Putting logic into the context of Computer Science, the course will \nshow how the programming language Prolog arises from the automatic proof method known as \nresolution. It will introduce topics that are important in mechanical verification, such as binary \ndecision diagrams (BDDs), SAT solvers and modal logic. \n \nLectures \nIntroduction to logic. Schematic statements. Interpretations and validity. Logical consequence. \nInference. \nPropositional logic. Basic syntax and semantics. Equivalences. Normal forms. Tautology \nchecking using CNF. \nThe sequent calculus. A simple (Hilbert-style) proof system. Natural deduction systems. \nSequent calculus rules. Sample proofs. \nFirst order logic. Basic syntax. Quantifiers. Semantics (truth definition). \nFormal reasoning in FOL. Free versus bound variables. Substitution. Equivalences for \nquantifiers. Sequent calculus rules. Examples. \nClausal proof methods. Clause form. A SAT-solving procedure. The resolution rule. Examples. \nRefinements. \nSkolem functions, Unification and Herbrand\u2019s theorem. Prenex normal form. Skolemisation. \nMost general unifiers. A unification algorithm. Herbrand models and their properties. \nResolution theorem-proving and Prolog. Binary resolution. Factorisation. Example of Prolog \nexecution. Proof by model elimination. \nSatisfiability Modulo Theories. Decision problems and procedures. How SMT solvers work. \nBinary decision diagrams. General concepts. Fast canonical form algorithm. Optimisations. \nApplications. \nModal logics. Possible worlds semantics. Truth and validity. A Hilbert-style proof system. \nSequent calculus rules. \nTableaux methods. Simplifying the sequent calculus. Examples. Adding unification. \nSkolemisation. The world\u2019s smallest theorem prover? \nObjectives \nAt the end of the course students should \n \nbe able to manipulate logical formulas accurately; \nbe able to perform proofs using the presented formal calculi; \nbe able to construct a small BDD; \nunderstand the relationships among the various calculi, e.g. SAT solving, resolution and Prolog; \nunderstand the concept of a decision procedure and the basic principles of \u201csatisfiability modulo \ntheories\u201d. \nbe able to apply the unification algorithm and to describe its uses. \nRecommended reading \n\n* Huth, M. and Ryan, M. (2004). Logic in computer science: modelling and reasoning about \nsystems. Cambridge University Press (2nd ed.). \nBen-Ari, M. (2001). Mathematical logic for computer science. Springer (2nd ed.).\n \n\nPROLOG \n \nAims \nThe aim of this course is to introduce programming in the Prolog language. Prolog encourages \na different programming style to Java or ML and particular focus is placed on programming to \nsolve real problems that are suited to this style. Practical experimentation with the language is \nstrongly encouraged. \n \nLectures \nIntroduction to Prolog. The structure of a Prolog program and how to use the Prolog interpreter. \nUnification. Some simple programs. \nArithmetic and lists. Prolog\u2019s support for evaluating arithmetic expressions and lists. The space \ncomplexity of program evaluation discussed with reference to last-call optimisation. \nBacktracking, cut, and negation. The cut operator for controlling backtracking. Negation as \nfailure and its uses. \nSearch and cut. Prolog\u2019s search method for solving problems. Graph searching exploiting \nProlog\u2019s built-in search mechanisms. \nDifference structures. Difference lists: introduction and application to example programs. \nBuilding on Prolog. How particular limitations of Prolog programs can be addressed by \ntechniques such as Constraint Logic Programming (CLP) and tabled resolution. \nObjectives \nAt the end of the course students should \n \nbe able to write programs in Prolog using techniques such as accumulators and difference \nstructures; \nknow how to model the backtracking behaviour of program execution; \nappreciate the unique perspective Prolog gives to problem solving and algorithm design; \nunderstand how larger programs can be created using the basic programming techniques used \nin this course. \nRecommended reading \n* Bratko, I. (2001). PROLOG programming for artificial intelligence. Addison-Wesley (3rd or 4th \ned.). \nSterling, L. and Shapiro, E. (1994). The art of Prolog. MIT Press (2nd ed.). \n \nFurther reading: \n \nO\u2019Keefe, R. (1990). The craft of Prolog. MIT Press. [This book is beyond the scope of this \ncourse, but it is very instructive. If you understand its contents, you\u2019re more than prepared for \nthe examination.]\n \n\nARTIFICIAL INTELLIGENCE \n \nPrerequisites: Algorithms 1, Algorithms 2. In addition the course requires some mathematics, in \nparticular some use of vectors and some calculus. Part IA Natural Sciences Mathematics or \nequivalent and Discrete Mathematics are likely to be helpful although not essential. Similarly, \nelements of Machine Learning and Real World Data, Foundations of Data Science, Logic and \nProof, Prolog and Complexity Theory are likely to be useful. This course is a prerequisite for the \nPart II courses Machine Learning and Bayesian Inference and Natural Language Processing. \nThis course is a prerequisite for: Natural Language Processing \nExam: Paper 7 Question 1, 2 \nPast exam questions, Moodle, timetable \n \nAims \nThe aim of this course is to provide an introduction to some fundamental issues and algorithms \nin artificial intelligence (AI). The course approaches AI from an algorithmic, computer \nscience-centric perspective; relatively little reference is made to the complementary \nperspectives developed within psychology, neuroscience or elsewhere. The course aims to \nprovide some fundamental tools and algorithms required to produce AI systems able to exhibit \nlimited human-like abilities, particularly in the form of problem solving by search, game-playing, \nrepresenting and reasoning with knowledge, planning, and learning. \n \nLectures \nIntroduction. Alternate ways of thinking about AI. Agents as a unifying view of AI systems. [1 \nlecture] \nSearch I. Search as a fundamental paradigm for intelligent problem-solving. Simple, uninformed \nsearch algorithms. Tree search and graph search. [1 lecture] \nSearch II. More sophisticated heuristic search algorithms. The A* algorithm and its properties. \nImproving memory efficiency: the IDA* and recursive best first search algorithms. Local search \nand gradient descent. [1 lecture] \nGame-playing. Search in an adversarial environment. The minimax algorithm and its \nshortcomings. Improving minimax using alpha-beta pruning. [1 lecture] \nConstraint satisfaction problems (CSPs). Standardising search problems to a common format. \nThe backtracking algorithm for CSPs. Heuristics for improving the search for a solution. Forward \nchecking, constraint propagation and arc consistency. [1 lecture] \nBackjumping in CSPs. Backtracking, backjumping using Gaschnig\u2019s algorithm, graph-based \nbackjumping. [1 lecture] \nKnowledge representation and reasoning I. How can we represent and deal with commonsense \nknowledge and other forms of knowledge? Semantic networks, frames and rules. How can we \nuse inference in conjunction with a knowledge representation scheme to perform reasoning \nabout the world and thereby to solve problems? Inheritance, forward and backward chaining. [1 \nlecture] \nKnowledge representation and reasoning II. Knowledge representation and reasoning using first \norder logic. The frame, qualification and ramification problems. The situation calculus. [1 lecture] \n\nPlanning I. Methods for planning in advance how to solve a problem. The STRIPS language. \nAchieving preconditions, backtracking and fixing threats by promotion or demotion: the \npartial-order planning algorithm. [1 lecture] \nPlanning II. Incorporating heuristics into partial-order planning. Planning graphs. The \nGRAPHPLAN algorithm. Planning using propositional logic. Planning as a constraint satisfaction \nproblem. [1 lecture] \nNeural Networks I. A brief introduction to supervised learning from examples. Learning as fitting \na curve to data. The perceptron. Learning by gradient descent. [1 lecture] \nNeural Networks II. Multilayer perceptrons and the backpropagation algorithm. [1 lecture] \nObjectives \nAt the end of the course students should: \n \nappreciate the distinction between the popular view of the field and the actual research results; \nappreciate the fact that the computational complexity of most AI problems requires us regularly \nto deal with approximate techniques; \nbe able to design basic problem solving methods based on AI-based search, knowledge \nrepresentation, reasoning, planning, and learning algorithms. \nRecommended reading \nThe recommended text is: \n \n* Russell, S. and Norvig, P. (2010). Artificial intelligence: a modern approach. Prentice Hall (3rd \ned.). \n \nThere are many good books available on artificial intelligence; one alternative is: \n \nPoole, D. L. and Mackworth, A. K. (2017). Artificial intelligence: foundations of computational \nagents. Cambridge University Press (2nd ed.). \n \nFor some of the material you might find it useful to consult more specialised texts, in particular: \n \nDechter, R. (2003). Constraint processing. Morgan Kaufmann. \n \nCawsey, A. (1998). The essence of artificial intelligence. Prentice Hall. \n \nGhallab, M., Nau, D. and Traverso, P. (2004). Automated planning: theory and practice. Morgan \nKaufmann. \n \nBishop, C.M. (2006). Pattern recognition and machine learning. Springer. \n \nBrachman, R.J and Levesque, H.J. (2004). Knowledge Representation and Reasoning. Morgan \nKaufmann.\n \n\nCOMPLEXITY THEORY \n \nAims \nThe aim of the course is to introduce the theory of computational complexity. The course will \nexplain measures of the complexity of problems and of algorithms, based on time and space \nused on abstract models. Important complexity classes will be defined, and the notion of \ncompleteness established through a thorough study of NP-completeness. Applications to \ncryptography will be considered. \n \nSyllabus \nIntroduction to Complexity Theory. Decidability and complexity; lower and upper bounds; the \nChurch-Turing hypothesis. \nTime complexity. Time complexity classes; polynomial time computation and the class P. \nNon-determinism. Non-deterministic machines; the class NP; the problem 3SAT. \nNP-completeness. Reductions and completeness; the Cook-Levin theorem; graph-theoretic \nproblems. \nMore NP-complete problems. 3-colourability; scheduling, matching, set covering, and knapsack.  \ncoNP. Complement classes. NP \u2229 coNP; primality and factorisation. \nCryptography. One-way functions; the class UP. \nSpace complexity. Space complexity classes; Savitch\u2019s theorem. \nHierarchy theorems. Time and space hierarchy theorems. \nRandomised algorithms. The class BPP; derandomisation; error-correcting codes, \ncommunication complexity. \nQuantum Complexity. The classes BQP and QMA. \nInteractive Proofs. IP=PSPACE; Zero Knowledge Proofs; Probabilistically Checkable Proofs \n(PCPs).  \n  \nObjectives \nAt the end of the course students should \n \nbe able to analyse practical problems and classify them according to their complexity; \nbe familiar with the phenomenon of NP-completeness, and be able to identify problems that are \nNP-complete; \nbe aware of a variety of complexity classes and their interrelationships; \nunderstand the role of complexity analysis in cryptography. \nRecommended reading \nA Survey of P vs. NP by Scott Aaronson. \nComputational Complexity: A Modern Approach by Arora and Barak \nComputational Complexity: A Conceptual Perspective by Oded Goldreich \nMathematics and Computation by Avi Wigderson\n \n\nCYBERSECURITY \n \nAims \nIn today\u2019s digital society, computer security is vital for commercial competitiveness, national \nsecurity and privacy of individuals. Protection against both criminal and state-sponsored attacks \nwill need a large cohort of skilled individuals with an understanding of the principles of security \nand with practical experience of the application of these principles. We want you to become one \nof them. In this adversarial game, to defeat the bad guys, the good guys have to be at least as \nskilled at them. Therefore this course has a strong foundation of becoming proficient at actual \nattacks. A theoretical understanding is of course necessary, but without some practice the bad \nguys will run circles around the good guys. In 12 hours I can\u2019t bring you from zero to the stage \nwhere you can invent new attacks and countermeasures, but at least by practicing and \ndissecting common attacks (akin to \u201cstudying the classics\u201d) I\u2019ll give you a feeling for the kind of \nadversarial thinking that is required in this field. Large portions of this course are hands-on: you \nwill need to acquire the relevant skills rather than just reading about it. The recommended \ncourse textbook will be especially helpful to those with no prior low-level hacking experience. \n \nLectures \nIntroduction. \nThe adversarial nature of security; thinking like the attacker; confidentiality, integrity and \navailability; systems-level thinking; Trusted Computing Base; role of cryptography. \n \nFundamentals of access control (chapter 1). \nDiscretionary vs mandatory access control; discretionary access control in POSIX; file \npermissions, file ownership, groups, permission bits. \n \nSoftware security (spanning 4 book chapters) \nSetuid (chapter 2): privileged programs, attack surfaces, exploitable vulnerabilities, privilege \nescalation from regular user to root. \nBuffer overflow (chapter 4): stack memory layout, exploiting a buffer overflow vulnerability, \nguessing unknown addresses, payload, countermeasures and counter-countermeasures. \nReturn to libc and return-oriented programming (chapter 5): exploiting a non-executable stack, \nchaining function calls, chaining ROP gadgets. \nSQL injection (chapter 14): vulnerability and its exploitation, countermeasures, input filtering, \nprepared statements. \n \nAuthentication. \nSomething you know, have, are; passwords, their dominance, their shortcomings and the many \nattempts at replacing them; password cracking; tokens; biometrics; taxonomy of Single Sign-On \nsystems; password managers. \n \nWeb and internet security (spanning 3 book chapters). \nCross-Site Request Forgery (chapter 12): why cross-site requests, CSRF attacks on HTTP GET \nand HTTP POST, countermeasures. \n\nCross-Site Scripting (chapter 13): non-persistent vs persistent XSS, javascript, self-propagating \nXSS worm, countermeasures; \n \nHuman factors. \nUsers are not the enemy; Compliance budget; Prospect theory; 7 principles for systems \nsecurity. \n \nAdditional topics. \nViruses, worms and quines; lockpicking; privilege escalation in physical locks; conclusions. \n \nTo complete the course successfully, students must acquire the practical ability to carry out (as \nopposed to just describing) the exploits mentioned in the syllabus, given a vulnerable system. \nThe low-level hands-on portions of the course are supported by the very helpful course \ntextbook. Those who choose not to study on the recommended textbook will be severely \ndisadvantaged. \n \nRecommended textbook: Wenliang Du. Computer Security: A Hands-on Approach. 3rd Edition. \nISBN: 978-17330039-5-7. Published: 1 May 2022. \n \nhttps://www.handsonsecurity.net. Some chapters freely available online. \n \n \nOptional additional books (not substitutes): \n \nRoss Anderson, Security Engineering 3rd Ed, Wiley, 2020. ISBN: 978-1-119-64278-7. \nhttps://www.cl.cam.ac.uk/~rja14/book.html. Some chapters freely available online. \n \nPaul van Oorschot, Computer Security and the Internet, Springer, 2020. ISBN: \n978-3-030-33648-6 (hardcopy), 978-3-030-33649-3 (eBook). \nhttps://people.scs.carleton.ca/~paulv/toolsjewels.html. All chapters freely available online.\n\nFORMAL MODELS OF LANGUAGE \n \nAims \nThis course studies formal models of language and considers how they might be relevant to the \nprocessing and acquisition of natural languages. The course will extend knowledge of formal \nlanguage theory; introduce several new grammars; and use concepts from information theory to \ndescribe natural language. \n \nLectures \nNatural language and the Chomsky hierarchy 1. Recap classes of language. Closure properties \nof language classes. Recap pumping lemma for regular languages. Discussion of relevance (or \nnot) to natural languages (example embedded clauses in English). \nNatural language and the Chomsky hierarchy 2. Pumping lemma for context free languages. \nDiscussion of relevance (or not) to natural languages (example Swiss-German cross serial \ndependancies). Properties of minimally context sensitive languages. Introduction to tree \nadjoining grammars. \nLanguage processing and context free grammar parsing 1. Recap of context free grammar \nparsing. Language processing predictions based on top down parsing models (example Yngve\u2019s \nlanguage processing predictions). Language processing predictions based on probabilistic \nparsing (example Halle\u2019s language processing predictions). \nLanguage processing and context free grammar parsing 2. Introduction to context free grammar \nequivalent dependancy grammars. Language processing predictions based on Shift-Reduce \nparsing (examples prosodic look-ahead parsers, Parsey McParseface). \nGrammar induction of language classes. Introduction to grammar induction. Discussion of \nrelevance (or not) to natural language acquisition. Gold\u2019s theorem. Introduction to context free \ngrammar equivalent categorial grammars and their learnable classes. \nNatural language and information theory 1. Entropy and natural language typology. Uniform \ninformation density as a predictor for language processing. \nNatural language and information theory 2. Noisy channel encoding as a model for spelling \nerror, translation and language processing. \nVector space models and word vectors. Introduction to word vectors (example Word2Vec). Word \nvectors as predictors for semantic language processing. \nObjectives \nAt the end of the course students should \n \nunderstand how known natural languages relate to formal languages in the Chomsky hierarchy; \nhave knowledge of several context free grammars equivalents; \nunderstand how we might make predictions about language processing and language \nacquisition from formal models; \nknow how to use information theoretic concepts to describe aspects of natural language. \nRecommended reading \n* Jurafsky, D. and Martin, J. (2008). Speech and language processing. Prentice Hall. \nManning, C.D. and Schutze, H. (1999) Foundations of statistical natural language processing. \nMIT Press. \n\nRuslan, M. (2003) The Oxford handbook of computational linguistics. Oxford University Press. \nClark, A., Fox, C. and Lappin, S. (2010) The handbook of computational linguistics and natural \nlanguage processing. Wiley-Blackwell. \nKozen, D. (1997) Automata and computibility. Springer.\n \n\n",
        "5th Semester \nBIOINFORMATICS \nAims \nThis course focuses on algorithms used in Bioinformatics and System Biology. Most of the \nalgorithms are general and can be applied in other fields on multidimensional and noisy data. All \nthe necessary biological terms and concepts useful for the course and the examination will be \ngiven in the lectures. The most important software implementing the described algorithms will be \ndemonstrated. \n \nLectures \nIntroduction to biological data: Bioinformatics as an interesting field in computer science. \nComputing and storing information with DNA (including Adleman\u2019s experiment). \nDynamic programming. Longest common subsequence, DNA global and local alignment, linear \nspace alignment, Nussinov algorithm for RNA, heuristics for multiple alignment. (Vol. 1, chapter \n5) \nSequence database search. Blast. (see notes and textbooks) \nGenome sequencing. De Bruijn graph. (Vol. 1, chapter 3) \nPhylogeny. Distance based algorithms (UPGMA, Neighbour-Joining). Parsimony-based \nalgorithms. Examples in Computer Science. (Vol. 2, chapter 7) \nClustering. Hard and soft K-means clustering, use of Expectation Maximization in clustering, \nHierarchical clustering, Markov clustering algorithm. (Vol. 2, chapter 8) \nGenomics Pattern Matching. Suffix Tree String Compression and the Burrows-Wheeler \nTransform. (Vol. 2, chapter 9) \nHidden Markov Models. The Viterbi algorithm, profile HMMs for sequence alignment, classifying \nproteins with profile HMMs, soft decoding problem, Baum-Welch learning. (Vol. 2, chapter 10) \nObjectives \nAt the end of this course students should \n \nunderstand Bioinformatics terminology; \nhave mastered the most important algorithms in the field; \nbe able to work with bioinformaticians and biologists; \nbe able to find data and literature in repositories. \nRecommended reading \n* Compeau, P. and Pevzner, P.A. (2015). Bioinformatics algorithms: an active learning approach. \nActive Learning Publishers. \nDurbin, R., Eddy, S., Krough, A. and Mitchison, G. (1998). Biological sequence analysis: \nprobabilistic models of proteins and nucleic acids. Cambridge University Press. \nJones, N.C. and Pevzner, P.A. (2004). An introduction to bioinformatics algorithms. MIT Press. \nFelsenstein, J. (2003). Inferring phylogenies. Sinauer Associates.\n \n\nBUSINESS STUDIES \n \nAims \nHow to start and run a computer company; the aims of this course are to introduce students to \nall the things that go to making a successful project or product other than just the programming. \nThe course will survey some of the issues that students are likely to encounter in the world of \ncommerce and that need to be considered when setting up a new computer company. \n \nSee also Business Seminars in the Easter Term. \n \nLectures \nSo you\u2019ve got an idea? Introduction. Why are you doing it and what is it? Types of company. \nMarket analysis. The business plan. \nMoney and tools for its management. Introduction to accounting: profit and loss, cash flow, \nbalance sheet, budgets. Sources of finance. Stocks and shares. Options and futures. \nSetting up: legal aspects. Company formation. Brief introduction to business law; duties of \ndirectors. Shares, stock options, profit share schemes and the like. Intellectual Property Rights, \npatents, trademarks and copyright. Company culture and management theory. \nPeople. Motivating factors. Groups and teams. Ego. Hiring and firing: employment law. \nInterviews. Meeting techniques. \nProject planning and management. Role of a manager. PERT and GANTT charts, and critical \npath analysis. Estimation techniques. Monitoring. \nQuality, maintenance and documentation. Development cycle. Productization. Plan for quality. \nPlan for maintenance. Plan for documentation. \nMarketing and selling. Sales and marketing are different. Marketing; channels; marketing \ncommunications. Stages in selling. Control and commissions. \nGrowth and exit routes. New markets: horizontal and vertical expansion. Problems of growth; \nsecond system effects. Management structures. Communication. Exit routes: acquisition, \nfloatation, MBO or liquidation. Futures: some emerging ideas for new computer businesses. \nSummary. Conclusion: now you do it! \nObjectives \nAt the end of the course students should \n \nbe able to write and analyse a business plan; \nknow how to construct PERT and GANTT diagrams and perform critical path analysis; \nappreciate the differences between profitability and cash flow, and have some notion of budget \nestimation; \nhave an outline view of company formation, share structure, capital raising, growth and exit \nroutes; \nhave been introduced to concepts of team formation and management; \nknow about quality documentation and productization processes; \nunderstand the rudiments of marketing and the sales process. \nRecommended reading \n\nLang, J. (2001). The high-tech entrepreneur\u2019s handbook: how to start and run a high-tech \ncompany. FT.COM/Prentice Hall. \n \nStudents will be expected to be able to use Microsoft Excel and Microsoft Project. \n \nFor additional reading on a lecture-by-lecture basis, please see the course website. \n \nStudents are strongly recommended to enter the CU Entrepreneurs Business Ideas Competition \nhttp://www.cue.org.uk/\n \n\nDENOTATIONAL SEMANTICS \n \nAims \nThe aims of this course are to introduce domain theory and denotational semantics, and to \nshow how they provide a mathematical basis for reasoning about the behaviour of programming \nlanguages. \n \nLectures \nIntroduction.The denotational approach to the semantics of programming \nlanguages.Recursively defined objects as limits of successive approximations. \nLeast fixed points.Complete partial orders (cpos) and least elements.Continuous functions and \nleast fixed points. \nConstructions on domains.Flat domains.Product domains. Function domains. \nScott induction.Chain-closed and admissible subsets of cpos and domains.Scott\u2019s fixed-point \ninduction principle. \nPCF.The Scott-Plotkin language PCF.Evaluation. Contextual equivalence. \nDenotational semantics of PCF.Denotation of types and terms. Compositionality. Soundness \nwith respect to evaluation. [2 lectures]. \nRelating denotational and operational semantics.Formal approximation relation and its \nfundamental property.Computational adequacy of the PCF denotational semantics with respect \nto evaluation. Extensionality properties of contextual equivalence. [2 lectures]. \nFull abstraction.Failure of full abstraction for the domain model. PCF with parallel or. \nObjectives \nAt the end of the course students should \n \nbe familiar with basic domain theory: cpos, continuous functions, admissible subsets, least fixed \npoints, basic constructions on domains; \nbe able to give denotational semantics to simple programming languages with simple types; \nbe able to apply denotational semantics; in particular, to understand the use of least fixed points \nto model recursive programs and be able to reason about least fixed points and simple \nrecursive programs using fixed point induction; \nunderstand the issues concerning the relation between denotational and operational semantics, \nadequacy and full abstraction, especially with respect to the language PCF. \nRecommended reading \nWinskel, G. (1993). The formal semantics of programming languages: an introduction. MIT \nPress. \nGunter, C. (1992). Semantics of programming languages: structures and techniques. MIT Press. \nTennent, R. (1991). Semantics of programming languages. Prentice Hall.\n \n\nINFORMATION THEORY \n \nAims \nThis course introduces the principles and applications of information theory: how information is \nmeasured in terms of probability and various entropies, how these are used to calculate the \ncapacity of communication channels, with or without noise, and to measure how much random \nvariables reveal about each other. Coding schemes including error correcting codes are studied \nalong with data compression, spectral analysis, transforms, and wavelet coding. Applications of \ninformation theory are reviewed, from astrophysics to pattern recognition. \n \nLectures \nInformation, probability, uncertainty, and surprise. How concepts of randomness and uncertainty \nare related to information. How the metrics of information are grounded in the rules of \nprobability. Shannon Information. Weighing problems and other examples. \nEntropy of discrete variables. Definition and link to Shannon information. Joint entropy, Mutual \ninformation. Visual depictions of the relationships between entropy types. Why entropy gives \nfundamental measures of information content. \nSource coding theorem and data compression; prefix, variable-, and fixed-length codes. \nInformation rates; Asymptotic equipartition principle; Symbol codes; Huffman codes and the \nprefix property. Binary symmetric channels. Capacity of a noiseless discrete channel. Stream \ncodes. \nNoisy discrete channel coding. Joint distributions; mutual information; Conditional Entropy; \nError-correcting codes; Capacity of a discrete channel. Noisy channel coding theorem. \nEntropy of continuous variables. Differential entropy; Mutual information; Channel Capacity; \nGaussian channels. \nEntropy for comparing probability distributions and for machine learning. Relative entropy/KL \ndivergence; cross-entropy; use as loss function. \nApplications of information theory in other sciences.  \nObjectives \nAt the end of the course students should be able to \n \ncalculate the information content of a random variable from its probability distribution; \nrelate the joint, conditional, and marginal entropies of variables in terms of their coupled \nprobabilities; \ndefine channel capacities and properties using Shannon\u2019s Theorems; \nconstruct efficient codes for data on imperfect communication channels; \ngeneralize the discrete concepts to continuous signals on continuous channels; \nunderstand encoding and communication schemes in terms of the spectral properties of signals \nand channels; \ndescribe compression schemes, and efficient coding using wavelets and other representations \nfor data. \nRecommended reading \n  Mackay's Information Theory, inference and Learning Algorithms \n \n\n Stone's Information Theory: A Tutorial Introduction.\n \n\nLATEX AND JULIA \n \nAims \nIntroduction to two widely-used languages for typesetting dissertations and scientific \npublications, for prototyping numerical algorithms and to visualize results. \n \nLectures \nLATEX. Workflow example, syntax, typesetting conventions, non-ASCII characters, document \nstructure, packages, mathematical typesetting, graphics and figures, cross references, build \ntools. \nJulia. Tools for technical computing and visualization. The Array type and its operators, 2D/3D \nplotting, functions and methods, notebooks, packages, vectorized audio demonstration. \nObjectives \nStudents should be able to avoid the most common LATEX mistakes, to prototype simple image \nand signal-processing algorithms in Julia, and to visualize the results. \n \nRecommended reading \n* Lamport, L. (1994). LATEX \u2013 a documentation preparation system user\u2019s guide and reference \nmanual. Addison-Wesley (2nd ed.). \nMittelbach, F., et al. (2023). The LATEX companion. Addison-Wesley (3rd ed.).\n \n\nPRINCIPLES OF COMMUNICATIONS \n \nAims \nThis course aims to provide a detailed understanding of the underlying principles for how \ncommunications systems operate. Practical examples (from wired and wireless \ncommunications, the Internet, and other communications systems) are used to illustrate the \nprinciples. \n \nLectures \nIntroduction. Course overview. Abstraction, layering. Review of structure of real networks, links, \nend systems and switching systems. [1 lecture] \nRouting. Central versus Distributed Routing Policy Routing. Multicast Routing Circuit Routing [6 \nlectures] \nFlow control and resource optimisation. 1[Control theory] is a branch of engineering familiar to \npeople building dynamic machines. It can be applied to network traffic. Stemming the flood, at \nsource, sink, or in between? Optimisation as a model of networkand user. TCP in the wild. [3 \nlectures] \nPacket Scheduling. Design choices for scheduling and queue management algorithms for \npacket forwarding, and fairness. [2 lectures] \nThe big picture for managing traffic. Economics and policy are relevant to networks in many \nways. Optimisation and game theory are both relevant topics discussed here. [2 lectures] \nSystem Structures and Summary. Abstraction, layering. The structure of real networks, links, \nend systems and switching. [2 lectures] \n1 Control theory was not taught and will not be the subject of any exam question. \n \nObjectives \nAt the end of the course students should be able to explain the underlying design and behaviour \nof protocols and networks, including capacity, topology, control and use. Several specific \nmathematical approaches are covered (control theory, optimisation). \n \nRecommended reading \n* Keshav, S. (2012). Mathematical Foundations of Computer Networking. Addison Wesley. ISBN \n9780321792105 \nBackground reading: \nKeshav, S. (1997). An engineering approach to computer networking. Addison-Wesley (1st ed.). \nISBN 0201634422 \nStevens, W.R. (1994). TCP/IP illustrated, vol. 1: the protocols. Addison-Wesley (1st ed.). ISBN \n0201633469\n \n\nTYPES \n \nAims \nThe aim of this course is to show by example how type systems for programming languages can \nbe defined and their properties developed, using techniques that were introduced in the Part IB \ncourse on Semantics of Programming Languages. The emphasis is on type systems for \nfunctional languages and their connection to constructive logic. \n \nLectures \nIntroduction. The role of type systems in programming languages. Review of rule-based \nformalisation of type systems. [1 lecture] \nPropositions as types. The Curry-Howard correspondence between intuitionistic propositional \ncalculus and simply-typed lambda calculus. Inductive types and iteration. Consistency and \ntermination. [2 lectures] \nPolymorphic lambda calculus (PLC). PLC syntax and reduction semantics. Examples of \ndatatypes definable in the polymorphic lambda calculus. Type inference. [3 lectures] \nMonads and effects. Explicit versus implicit effects. Using monadic types to control effects. \nReferences and polymorphism. Recursion and looping. [2 lectures] \nContinuations and classical logic. First-class continuations and control operators. Continuations \nas Curry-Howard for classical logic. Continuation-passing style. [2 lectures] \nDependent types. Dependent function types. Indexed datatypes. Equality types and combining \nproofs with programming. [2 lectures] \nObjectives \nAt the end of the course students should \n \nbe able to use a rule-based specification of a type system to carry out type checking and type \ninference; \nunderstand by example the Curry-Howard correspondence between type systems and logics; \nunderstand how types can be used to control side-effects in programming; \nappreciate the expressive power of parametric polymorphism and dependent types. \nRecommended reading \n* Pierce, B.C. (2002). Types and programming languages. MIT Press. \nPierce, B. C. (Ed.) (2005). Advanced Topics in Types and Programming Languages. MIT Press. \nGirard, J-Y. (tr. Taylor, P. and Lafont, Y.) (1989). Proofs and types. Cambridge University Press.\n\nADVANCED DATA SCIENCE \n \nPracticals \nThe module will have a practical session for each of the three stages of the pipeline. Each of the \nparts will be tied into an aspect of the final project. \n \nObjectives \nAt the end of the course students will be familiar with the purpose of data science, how it differs \nfrom the closely related fields of machine learning, statistics and artificial intelligence and what a \ntypical data analysis pipeline looks like in practice. As well as emphasising the importance of \nanalysis methods we will introduce a formalism for organising how data science is done in \npractice and what the different aspects the data scientist faces when giving data-driven answers \nto questions of interest. \n \nRecommended reading \nSimon Rogers et al. (2016). A First Course in Machine Learning, Second Edition. [] Chapman \nand Hall/CRC, nil \nShai Shalev-Shwartz et al. (2014). Understanding Machine Learning: From Theory to \nAlgorithms. New York, NY, USA: Cambridge  University Press \nChristopher M. Bishop (2006). Pattern Recognition and Machine Learning (Information Science \nand Statistics). Secaucus, NJ, USA: Springer-Verlag New York, Inc. \nAssessment \nPractical There will be four practicals contributing 20% to the final module mark. Data science is \na topic where there is rarely a single \ncorrect answer therefore the important thing is that an informed attempt has been made to \naddress the questions that can be supported and motivated. \nReport The course will be concluded by an individual report covering the material in the course \nthrough \npractical exercise working with real data. This report will make up 80% of the mark for the \ncourse. \n \n \n\nAFFECTIVE ARTIFICIAL INTELLIGENCE \n \nSynopsis \nAffective Artificial Intelligence (Affective AI) aims to imbue machines with social and emotional \nintelligence (EQ). More specifically, Affective AI aims to create artificially intelligent systems and \nmachines that can recognize, interpret, process, and simulate human social signals and \nbehaviours, expressions, and emotions, to enhance human-AI interaction and communication. \n\\r\\n\\r\\nTo achieve this goal, Affective AI draws upon various scientific disciplines, including \nmachine learning, computer vision, speech / natural language / signal processing, psychology \nand cognitive science, and ethics and social sciences. \n \n  \nBackground & Aims \nAffective Artificial Intelligence (Affective AI) aims to imbue machines with social and emotional \nintelligence (EQ). More specifically, Affective AI aims to create artificially intelligent systems and \nmachines that can recognize, interpret, process, and simulate human social signals and \nbehaviours, expressions, and emotions, to enhance human-AI interaction and communication.  \n \nTo achieve this goal, Affective AI draws upon various scientific disciplines, including machine \nlearning, computer vision, speech / natural language / signal processing, psychology and \ncognitive science, and ethics and social sciences. \n \nAffective AI has direct applications in and implications for the design of innovative interactive \ntechnology (e.g., interaction with chat bots, virtual agents, robots), single and multi-user smart \nenvironments ( e.g., in-car/ virtual / augmented / mixed reality, serious games), public speaking \nand cognitive training, and clinical and biomedical studies (e.g., autism, depression, pain). \n \nThe aim of this module is to impart knowledge and ability needed to make informed choices of \nmodels, data, and machine learning techniques for sensing, recognition, and generation of \naffective and social behaviour (e.g., smile, frown, head nodding/shaking, \nagreement/disagreement) in order to create Affectively intelligent AI systems, with a \nconsideration for various ethical issues (e.g., privacy, bias) arising from the real-world \ndeployment of these systems. \n \n  \n \nSyllabus \nThe following list provides a representative list of topics: \n \nIntroduction, definitions, and overview \nTheories from various disciplines \nSensing from multiple modalities (e.g., vision, audio, bio signals, text) \nData acquisition and annotation \nSignal processing / feature extraction \n\nLearning / prediction / recognition and evaluation \nBehaviour synthesis / generation (e.g., for embodied agents / robots) \nAdvanced topics and ethical considerations (e.g., bias and fairness) \nCross-disciplinary applications (via seminar presentations and discussions) \nGuest lectures (diverse topics \u2013 changes each year) \nHands-on research and programming work (i.e., mini project & report)  \n \n  \n \nObjectives \nOn completion of this module, students will: \n \nunderstand and demonstrate knowledge in key characteristics of affectively intelligent AI, which \ninclude: \nRecognition: How to equip Affective AI systems with capabilities of analysing facial expressions, \nvocal intonations, gestures, and other physiological signals to infer human affective states? \nGeneration: How to enable affectively intelligent AI simulate expressions of emotions in \nmachines, allowing them to respond in a more human-like manner, such as virtual agents and \nhumanoid robots, showing empathy or sympathy? \nAdaptation and Personalization: How to enable affectively intelligent AI adapt system responses \nbased on users' affective states and/or needs, or tailor interactions and experiences to individual \nusers based on their expressivity / emotional profiles, personalities, and past interactions? \nEmpathetic Communication: How to design Affective AI systems to communicate with users in a \nway that demonstrates empathy, understanding, and sensitivity to their affective states and \nneeds? \nEthical and societal considerations: What are the various human differences, ethical guidelines, \nand societal impacts that need to be considered when designing and deploying Affective AI to \nensure that these systems respect users' privacy, autonomy, and well-being? \ncomprehend and apply (appropriate) methods for collection, analysis, representation, and \nevaluation of human affective and communicative behavioural data. \nenhance programming skills for creating and implementing (components of) Affective AI \nsystems. \ndemonstrate critical thinking, analysis and synthesis while deciding on 'when' and 'how' to \nincorporate human affect and social signals in a specific AI system context and gain practical \nexperience in proposing and justifying computational solution(s) of suitable nature and scope. \n  \n \nAssessment - Part II Students \nSeminar presentation: 20% \nParticipating in Q&A and discussions: 10% \nMid-term mini project report and presentation (as a team of 2): 10%  \nFinal mini project report & code (as a team of 2): 60% \n \n\nCATEGORY THEORY \n \nAims \nCategory theory provides a unified treatment of mathematical properties and constructions that \ncan be expressed in terms of 'morphisms' between structures. It gives a precise framework for \ncomparing one branch of mathematics (organized as a category) with another and for the \ntransfer of problems in one area to another. Since its origins in the 1940s motivated by \nconnections between algebra and geometry, category theory has been applied to diverse fields, \nincluding computer science, logic and linguistics. This course introduces the basic notions of \ncategory theory: adjunction, natural transformation, functor and category. We will use category \ntheory to organize and develop the kinds of structure that arise in models and semantics for \nlogics and programming languages. \n \nSyllabus \nIntroduction; some history. Definition of category. The category of sets and functions. \nCommutative diagrams. Examples of categories: preorders and monotone functions; monoids \nand monoid homomorphisms; a preorder as a category; a monoid as a category. Definition of \nisomorphism. Informal notion of a 'category-theoretic' property. \nTerminal objects. The opposite of a category and the duality principle. Initial objects. Free \nmonoids as initial objects. \nBinary products and coproducts. Cartesian categories. \nExponential objects: in the category of sets and in general. Cartesian closed categories: \ndefinition and examples. \nIntuitionistic Propositional Logic (IPL) in Natural Deduction style. Semantics of IPL in a cartesian \nclosed preorder. \nSimply Typed Lambda Calculus (STLC). The typing relation. Semantics of STLC types and \nterms in a cartesian closed category (ccc). The internal language of a ccc. The \nCurry-Howard-Lawvere correspondence. \nFunctors. Contravariance. Identity and composition for functors. \nSize: small categories and locally small categories. The category of small categories. Finite \nproducts of categories. \nNatural transformations. Functor categories. The category of small categories is cartesian \nclosed. \nHom functors. Natural isomorphisms. Adjunctions. Examples of adjoint functors. Theorem \ncharacterising the existence of right (respectively left) adjoints in terms of a universal property. \nDependent types. Dependent product sets and dependent function sets as adjoint functors. \nEquivalence of categories. Example: the category of I-indexed sets and functions is equivalent \nto the slice category Set/I. \nPresheaves. The Yoneda Lemma. Categories of presheaves are cartesian closed. \nMonads. Modelling notions of computation as monads. Moggi's computational lambda calculus. \nObjectives \nOn completion of this module, students should: \n \n\nbe familiar with some of the basic notions of category theory and its connections with logic and \nprogramming language semantics \nAssessment \na graded exercise sheet (25% of the final mark), and \na take-home test (75%) \nRecommended reading \nAwodey, S. (2010). Category theory. Oxford University Press (2nd ed.). \n \nCrole, R. L. (1994). Categories for types. Cambridge University Press. \n \nLambek, J. and Scott, P. J. (1986). Introduction to higher order categorical logic. Cambridge \nUniversity Press. \n \nPitts, A. M. (2000). Categorical Logic. Chapter 2 of S. Abramsky, D. M. Gabbay and T. S. E. \nMaibaum (Eds) Handbook of Logic in Computer Science, Volume 5. Oxford University Press. \n(Draft copy available here.) \n \nClass Size \nThis module can accommodate upto 15 Part II students plus 15 MPhil / Part III students.\n\nDIGITAL SIGNAL PROCESSING \n \nAims \nThis course teaches basic signal-processing principles necessary to understand many modern \nhigh-tech systems, with examples from audio processing, image coding, radio communication, \nradar, and software-defined radio. Students will gain practical experience from numerical \nexperiments in programming assignments (in Julia, MATLAB or NumPy). \n \nLectures \nSignals and systems. Discrete sequences and systems: types and properties. Amplitude, \nphase, frequency, modulation, decibels, root-mean square. Linear time-invariant systems, \nconvolution. Some examples from electronics, optics and acoustics. \nPhasors. Eigen functions of linear time-invariant systems. Review of complex arithmetic. \nPhasors as orthogonal base functions. \nFourier transform. Forms and properties of the Fourier transform. Convolution theorem. Rect \nand sinc. \nDirac\u2019s delta function. Fourier representation of sine waves, impulse combs in the time and \nfrequency domain. Amplitude-modulation in the frequency domain. \nDiscrete sequences and spectra. Sampling of continuous signals, periodic signals, aliasing, \ninterpolation, sampling and reconstruction, sample-rate conversion, oversampling, spectral \ninversion. \nDiscrete Fourier transform. Continuous versus discrete Fourier transform, symmetry, linearity, \nFFT, real-valued FFT, FFT-based convolution, zero padding, FFT-based resampling, \ndeconvolution exercise. \nSpectral estimation. Short-time Fourier transform, leakage and scalloping phenomena, \nwindowing, zero padding. Audio and voice examples. DTFM exercise. \nFinite impulse-response filters. Properties of filters, implementation forms, window-based FIR \ndesign, use of frequency-inversion to obtain high-pass filters, use of modulation to obtain \nband-pass filters. \nInfinite impulse-response filters. Sequences as polynomials, z-transform, zeros and poles, some \nanalog IIR design techniques (Butterworth, Chebyshev I/II, elliptic filters, second-order cascade \nform). \nBand-pass signals. Band-pass sampling and reconstruction, IQ up and down conversion, \nsuperheterodyne receivers, software-defined radio front-ends, IQ representation of AM and FM \nsignals and their demodulation. \nDigital communication. Pulse-amplitude modulation. Matched-filter detector. Pulse shapes, \ninter-symbol interference, equalization. IQ representation of ASK, BSK, PSK, QAM and FSK \nsignals. [2 hours] \nRandom sequences and noise. Random variables, stationary and ergodic processes, \nautocorrelation, cross-correlation, deterministic cross-correlation sequences, filtered random \nsequences, white noise, periodic averaging. \nCorrelation coding. Entropy, delta coding, linear prediction, dependence versus correlation, \nrandom vectors, covariance, decorrelation, matrix diagonalization, eigen decomposition, \n\nKarhunen-Lo\u00e8ve transform, principal component analysis. Relation to orthogonal transform \ncoding using fixed basis vectors, such as DCT. \nLossy versus lossless compression. What information is discarded by human senses and can \nbe eliminated by encoders? Perceptual scales, audio masking, spatial resolution, colour \ncoordinates, some demonstration experiments. \nQuantization, image coding standards. Uniform and logarithmic quantization, A/\u00b5-law coding, \ndithering, JPEG. \nObjectives \napply basic properties of time-invariant linear systems; \nunderstand sampling, aliasing, convolution, filtering, the pitfalls of spectral estimation; \nexplain the above in time and frequency domain representations; \nuse filter-design software; \nvisualize and discuss digital filters in the z-domain; \nuse the FFT for convolution, deconvolution, filtering; \nimplement, apply and evaluate simple DSP applications; \nfamiliarity with a number of signal-processing concepts used in digital communication systems \nRecommended reading \nLyons, R.G. (2010). Understanding digital signal processing. Prentice Hall (3rd ed.). \nOppenheim, A.V. and Schafer, R.W. (2007). Discrete-time digital signal processing. Prentice \nHall (3rd ed.). \nStein, J. (2000). Digital signal processing \u2013 a computer science perspective. Wiley. \n \nClass size \nThis module can accommodate a maximum of 24 students (16 Part II students and 8 MPhil \nstudents) \n \nAssessment - Part II students \nThree homework programming assignments, each comprising 20% of the mark \nWritten test, comprising 40% of the total mark.\n \n\nMACHINE VISUAL PERCEPTION \n \nAims \nThis course aims at introducing the theoretical foundations and practical techniques for machine \nperception, the capability of computers to interpret data resulting from sensor measurements. \nThe course will teach the fundamentals and modern techniques of machine perception, i.e. \nreconstructing the real world starting from sensor measurements with a focus on machine \nperception for visual data. The topics covered will be image/geometry representations for \nmachine perception, semantic segmentation, object detection and recognition, geometry \ncapture, appearance modeling and acquisition, motion detection and estimation, \nhuman-in-the-loop machine perception, select topics in applied machine perception. \n \nMachine perception/computer vision is a rapidly expanding area of research with real-world \napplications. An understanding of machine perception is also important for robotics, interactive \ngraphics (especially AR/VR), applied machine learning, and several other fields and industries. \nThis course will provide a fundamental understanding of and practical experience with the \nrelevant techniques. \n \nLearning outcomes \nStudents will understand the theoretical underpinnings of the modern machine perception \ntechniques for reconstructing models of reality starting from an incomplete and imperfect view of \nreality. \nStudents will be able to apply machine perception theory to solve practical problems, e.g. \nclassification of images, geometry capture. \nStudents will gain an understanding of which machine perception techniques are appropriate for \ndifferent tasks and scenarios. \nStudents will have hands-on experience with some of these techniques via developing a \nfunctional machine perception system in their projects. \nStudents will have practical experience with the current prominent machine perception \nframeworks. \nSyllabus \nThe fundamentals of machine learning for machine perception \nDeep neural networks and frameworks for machine perception \nSemantic segmentation of objects and humans \nObject detection and recognition \nMotion estimation, tracking and recognition \n3D geometry capture \nAppearance modeling and acquisition \nSelect topics in applied machine perception \nAssessment \nA practical exercise, worth 20% of the mark. This will cover the basics and theory of machine \nperception and some of the practical techniques the students will likely use in their projects. This \nis individual work. No GPU hours will be needed for the practical work. \nA machine perception project worth 80% of the marks: \n\nCourse projects will be selected by the students following the possible project themes proposed \nby the lecturer, and will be checked by the lecturer for appropriateness.  \nThe students will form groups of 2-3 to design, implement, report, and present a project to tackle \na given task in machine perception. \nThe final mark will be composed of an implementation/report mark (60%) and a presentation \nmark (20%). Each team member will be evaluated based on her/his contribution. \nEach project will have extensions to be completed only by the ACS students. Each student will \nwrite a different part of the report, whose author will be clearly marked. Each student will further \nsummarise her/his contributions to the project in the same report. \nRecommended Reading List \nComputer Vision: Algorithms and Applications, Richard Szeliski, Springer, 2010. \nDeep Learning, Ian Goodfellow, Yoshua Bengio, and Aaron Courville, MIT Press, 2016. \nMachine Learning and Visual Perception, Baochang Zhang, De Gruyter, 2020.\n \n\nNATURAL LANGUAGE PROCESSING \n \nAims \nThis course introduces the fundamental techniques of natural language processing. It aims to \nexplain the potential and the main limitations of these techniques. Some current research issues \nare introduced and some current and potential applications discussed and evaluated. Students \nwill also be introduced to practical experimentation in natural language processing. \n \nLectures \nOverview. Brief history of NLP research, some current applications, components of NLP \nsystems. \nMorphology and Finite State Techniques. Morphology in different languages, importance of \nmorphological analysis in NLP, finite-state techniques in NLP. \nPart-of-Speech Tagging and Log-Linear Models. Lexical categories, word tagging, corpora and \nannotations, empirical evaluation. \nPhrase Structure and Structure Prediction. Phrase structures, structured prediction, context-free \ngrammars, weights and probabilities. Some limitations of context-free grammars. \nDependency Parsing. Dependency structure, grammar-free parsing, incremental processing.  \nGradient Descent and Neural Nets. Parameter optimisation by gradient descent. Non-linear \nfunctions with neural network layers. Log-linear model as softmax layer. Current findings of \nNeural NLP. \nWord representations. Representing words with vectors, count-based and prediction-based \napproaches, similarity metrics. \nRecurrent Neural Networks. Modelling sequences, parameter sharing in recurrent neural \nnetworks, neural language models, word prediction. \nCompositional Semantics. Logical representations, compositional semantics, lambda calculus, \ninference and robust entailment. \nLexical Semantics. Semantic relations, WordNet, word senses. \nDiscourse. Discourse relations, anaphora resolution, summarization. \nNatural Language Generation. Challenges of natural language generation (NLG), tasks in NLG, \nsurface realisation. \nPractical and assignments. Students will build a natural language processing system which will \nbe trained and evaluated on supplied data. The system will be built from existing components, \nbut students will be expected to compare approaches and some programming will be required \nfor this. Several assignments will be set during the practicals for assessment. \nObjectives \nBy the end of the course students should: \n \nbe able to discuss the current and likely future performance of several NLP applications; \nbe able to describe briefly a fundamental technique for processing language for several \nsubtasks, such as morphological processing, parsing, word sense disambiguation etc.; \nunderstand how these techniques draw on and relate to other areas of computer science. \nRecommended reading \n\nJurafsky, D. & Martin, J. (2023). Speech and language processing. Prentice Hall (3rd ed. draft, \nonline). \n \nAssessment - Part II Students \nAssignment 1 - 10% of marks \nAssignment 2 - 60% of marks \nAssignment 3 - 30% of marks\n \n\nPRACTICAL RESEARCH IN HUMAN-CENTRED AI \n \nAims \nThis is an advanced course in human-computer interaction, with a specialist focus on intelligent \nuser interfaces and interaction with machine-learning and artificial intelligence technologies. The \nformat will be largely Practical, with students carrying out a mini-project involving empirical \nresearch investigation. These studies will investigate human interaction with some kind of \nmodel-based system for planning, decision-making, automation etc. Possible study formats \nmight include: System evaluation, Field observation, Hypothesis testing experiment, Design \nintervention or Corpus analysis, following set examples from recent research publications. \nProject work will be formally evaluated through a report and presentation. \n \nLectures \n(note that Lectures 2-7 also include one hour class discussion of practical work) \n        \u2022 Current research themes in intelligent user interfaces \n        \u2022 Program synthesis \n        \u2022 Mixed initiative interaction \n        \u2022 Interpretability / explainable AI \n        \u2022 Labelling as a fundamental problem \n        \u2022 Machine learning risks and bias \n        \u2022 Visualisation and visual analytics \n        \u2022 Student research presentations \n \nObjectives \nBy the end of the course students should: \n        \u2022 be familiar with current state of the art in intelligent interactive systems \n        \u2022 understand the human factors that are most critical in the design of such systems \n        \u2022 be able to evaluate evidence for and against the utility of novel systems \n        \u2022 have experience of conducting user studies meeting the quality criteria of this field \n        \u2022 be able to write up and present user research in a professional manner \n \nClass Size \nThis module can accommodate upto 20 Part II, Part III and MPhil students. \n \nRecommended reading \nBrad A. Myers and Richard McDaniel (2000). Demonstrational Interfaces: Sometimes You Need \na Little Intelligence, Sometimes You Need a Lot. \n \nAlan Blackwell (2024). Moral Codes: Designing alternatives to AI \n \nAssessment - Part II Students \nThe format will be largely practical, with students carrying out an individual mini-project involving \nempirical research investigation. \n \n\nAssignment 1: six incremental submissions which together contribute 20% to the final module \nmark. \n \nAssignment 2: Final report - 80% of the final module mark \n \n\n",
        "6th Semester \nADVANCED COMPUTER ARCHITECTURE \nAims \nThis course examines the techniques and underlying principles that are used to design \nhigh-performance computers and processors. Particular emphasis is placed on understanding \nthe trade-offs involved when making design decisions at the architectural level. A range of \nprocessor architectures are explored and contrasted. In each case we examine their merits and \nlimitations and how ultimately the ability to scale performance is restricted. \n \nLectures \nIntroduction. The impact of technology scaling and market trends. \nFundamentals of Computer Design. Amdahl\u2019s law, energy/performance trade-offs, ISA design. \nAdvanced pipelining. Pipeline hazards; exceptions; optimal pipeline depth; branch prediction; \nthe branch target buffer [2 lectures] \nSuperscalar techniques. Instruction-Level Parallelism (ILP); superscalar processor architecture \n[2 lectures] \nSoftware approaches to exploiting ILP. VLIW architectures; local and global instruction \nscheduling techniques; predicated instructions and support for speculative compiler \noptimisations. \nMultithreaded processors. Coarse-grained, fine-grained, simultaneous multithreading \nThe memory hierarchy. Caches; programming for caches; prefetching [2 lectures] \nVector processors. Vector machines; short vector/SIMD instruction set extensions; stream \nprocessing \nChip multiprocessors. The communication model; memory consistency models; false sharing; \nmultiprocessor memory hierarchies; cache coherence protocols; synchronization [2 lectures] \nOn-chip interconnection networks. Bus-based interconnects; on-chip packet switched networks \nSpecial-purpose architectures. Converging approaches to computer design \nObjectives \nAt the end of the course students should \n \nunderstand what determines processor design goals; \nappreciate what constrains the design process and how architectural trade-offs are made within \nthese constraints; \nbe able to describe the architecture and operation of pipelined and superscalar processors, \nincluding techniques such as branch prediction, register renaming and out-of-order execution; \nhave an understanding of vector, multithreaded and multi-core processor architectures; \nfor the architectures discussed, understand what ultimately limits their performance and \napplication domain. \nRecommended reading \n* Hennessy, J. and Patterson, D. (2012). Computer architecture: a quantitative approach. \nElsevier (5th ed.) ISBN 9780123838728. (the 3rd and 4th editions are also good)\n \n\nCRYPTOGRAPHY \n \nAims \nThis course provides an overview of basic modern cryptographic techniques and covers \nessential concepts that users of cryptographic standards need to understand to achieve their \nintended security goals. \n \nLectures \nCryptography. Overview, private vs. public-key ciphers, MACs vs. signatures, certificates, \ncapabilities of adversary, Kerckhoffs\u2019 principle. \nClassic ciphers. Attacks on substitution and transposition ciphers, Vigen\u00e9re. Perfect secrecy: \none-time pads. \nPrivate-key encryption. Stream ciphers, pseudo-random generators, attacking \nlinear-congruential RNGs and LFSRs. Semantic security definitions, oracle queries, advantage, \ncomputational security, concrete-security proofs. \nBlock ciphers. Pseudo-random functions and permutations. Birthday problem, random \nmappings. Feistel/Luby-Rackoff structure, DES, TDES, AES. \nChosen-plaintext attack security. Security with multiple encryptions, randomized encryption. \nModes of operation: ECB, CBC, OFB, CNT. \nMessage authenticity. Malleability, MACs, existential unforgeability, CBC-MAC, ECBC-MAC, \nCMAC, birthday attacks, Carter-Wegman one-time MAC. \nAuthenticated encryption. Chosen-ciphertext attack security, ciphertext integrity, \nencrypt-and-authenticate, authenticate-then-encrypt, encrypt-then-authenticate, padding oracle \nexample, GCM. \nSecure hash functions. One-way functions, collision resistance, padding, Merkle-Damg\u00e5rd \nconstruction, sponge function, duplex construct, entropy pool, SHA standards. \nApplications of secure hash functions. HMAC, stream authentication, Merkle tree, commitment \nprotocols, block chains, Bitcoin. \nKey distribution problem. Needham-Schroeder protocol, Kerberos, hardware-security modules, \npublic-key encryption schemes, CPA and CCA security for asymmetric encryption. \nNumber theory, finite groups and fields. Modular arithmetic, Euclid\u2019s algorithm, inversion, \ngroups, rings, fields, GF(2n), subgroup order, cyclic groups, Euler\u2019s theorem, Chinese \nremainder theorem, modular roots, quadratic residues, modular exponentiation, easy and \ndifficult problems. [2 lectures] \nDiscrete logarithm problem. Baby-step-giant-step algorithm, computational and decision \nDiffie-Hellman problem, DH key exchange, ElGamal encryption, hybrid cryptography, Schnorr \ngroups, elliptic-curve systems, key sizes. [2 lectures] \nTrapdoor permutations. Security definition, turning one into a public-key encryption scheme, \nRSA, attacks on \u201ctextbook\u201d RSA, RSA as a trapdoor permutation, optimal asymmetric \nencryption padding, common factor attacks. \nDigital signatures. One-time signatures, RSA signatures, Schnorr identification scheme, \nElGamal signatures, DSA, PS3 hack, certificates, PKI. \nObjectives \nBy the end of the course students should \n\n \nbe familiar with commonly used standardized cryptographic building blocks; \nbe able to match application requirements with concrete security definitions and identify their \nabsence in naive schemes; \nunderstand various adversarial capabilities and basic attack algorithms and how they affect key \nsizes; \nunderstand and compare the finite groups most commonly used with discrete-logarithm \nschemes; \nunderstand the basic number theory underlying the most common public-key schemes, and \nsome efficient implementation techniques. \nRecommended reading \nKatz, J., Lindell, Y. (2015). Introduction to modern cryptography. Chapman and Hall/CRC (2nd \ned.).\n \n\nE-COMMERCE \n \nAims \nThis course aims to give students an outline of the issues involved in setting up an e-commerce \nsite. \n \nLectures \nThe history of electronic commerce. Mail order; EDI; web-based businesses, credit card \nprocessing, PKI, identity and other hot topics. \nNetwork economics. Real and virtual networks, supply-side versus demand-side scale \neconomies, Metcalfe\u2019s law, the dominant firm model, the differentiated pricing model Data \nProtection Act, Distance Selling regulations, business models. \nWeb site design. Stock and price control; domain names, common mistakes, dynamic pages, \ntransition diagrams, content management systems, multiple targets. \nWeb site implementation. Merchant systems, system design and sizing, enterprise integration, \npayment mechanisms, CRM and help desks. Personalisation and internationalisation. \nThe law and electronic commerce. Contract and tort; copyright; binding actions; liabilities and \nremedies. Legislation: RIP; Data Protection; EU Directives on Distance Selling and Electronic \nSignatures. \nPutting it into practice. Search engine interaction, driving and analysing traffic; dynamic pricing \nmodels. Integration with traditional media. Logs and audit, data mining modelling the user. \ncollaborative filtering and affinity marketing brand value, building communities, typical behaviour. \nFinance. How business plans are put together. Funding Internet ventures; the recent hysteria; \nmaximising shareholder value. Future trends. \nUK and International Internet Regulation. Data Protection Act and US Privacy laws; HIPAA, \nSarbanes-Oxley, Security Breach Disclosure, RIP Act 2000, Electronic Communications Act \n2000, Patriot Act, Privacy Directives, data retention; specific issues: deep linking, Inlining, brand \nmisuse, phishing. \nObjectives \nAt the end of the course students should know how to apply their computer science skills to the \nconduct of e-commerce with some understanding of the legal, security, commercial, economic, \nmarketing and infrastructure issues involved. \n \nRecommended reading \nShapiro, C. and Varian, H. (1998). Information rules. Harvard Business School Press. \n \nAdditional reading: \n \nStandage, T. (1999). The Victorian Internet. Phoenix Press. Klemperer, P. (2004). Auctions: \ntheory and practice. Princeton Paperback ISBN 0-691-11925-2.\n \n\nMACHINE LEARNING AND BAYESIAN INFERENCE \nAims \nThe Part 1B course Artificial Intelligence introduced simple neural networks for supervised \nlearning, and logic-based methods for knowledge representation and reasoning. This course \nhas two aims. First, to provide a rigorous introduction to machine learning, moving beyond the \nsupervised case and ultimately presenting state-of-the-art methods. Second, to provide an \nintroduction to the wider area of probabilistic methods for representing and reasoning with \nknowledge. \n \nLectures \nIntroduction to learning and inference. Supervised, unsupervised, semi-supervised and \nreinforcement learning. Bayesian inference in general. What the naive Bayes method actually \ndoes. Review of backpropagation. Other kinds of learning and inference. [1 lecture] \nHow to classify optimally. Treating learning probabilistically. Bayesian decision theory and Bayes \noptimal classification. Likelihood functions and priors. Bayes theorem as applied to supervised \nlearning. The maximum likelihood and maximum a posteriori hypotheses. What does this teach \nus about the backpropagation algorithm? [2 lectures] \nLinear classifiers I. Supervised learning via error minimization. Iterative reweighted least \nsquares. The maximum margin classifier. [2 lectures] \nGaussian processes. Learning and inference for regression using Gaussian process models. [2 \nlectures] \nSupport vector machines (SVMs). The kernel trick. Problem formulation. Constrained \noptimization and the dual problem. SVM algorithm. [2 lectures] \nPractical issues. Hyperparameters. Measuring performance. Cross-validation. Experimental \nmethods. [1 lecture] \nLinear classifiers II. The Bayesian approach to neural networks. [1 lecture] \nUnsupervised learning I. The k-means algorithm. Clustering as a maximum likelihood problem. \n[1 lecture] \nUnsupervised learning II. The EM algorithm and its application to clustering. [1 lecture] \nBayesian networks I. Representing uncertain knowledge using Bayesian networks. Conditional \nindependence. Exact inference in Bayesian networks. [2 lectures] \nBayesian networks II. Markov random fields. Approximate inference. Markov chain Monte Carlo \nmethods. [1 lecture] \nObjectives \nAt the end of this course students should: \n \nUnderstand how learning and inference can be captured within a probabilistic framework, and \nknow how probability theory can be applied in practice as a means of handling uncertainty in AI \nsystems. \nUnderstand several algorithms for machine learning and apply those methods in practice with \nproper regard for good experimental practice. \nRecommended reading \nIf you are going to buy a single book for this course I recommend: \n \n\n* Bishop, C.M. (2006). Pattern recognition and machine learning. Springer. \n \nThe course text for Artificial Intelligence I: \n \nRussell, S. and Norvig, P. (2010). Artificial intelligence: a modern approach. Prentice Hall (3rd \ned.). \n \ncovers some relevant material but often in insufficient detail. Similarly: \n \nMitchell, T.M. (1997). Machine Learning. McGraw-Hill. \n \ngives a gentle introduction to some of the course material, but only an introduction. \n \nRecently a few new books have appeared that cover a lot of relevant ground well. For example: \n \nBarber, D. (2012). Bayesian Reasoning and Machine Learning. Cambridge University Press. \nFlach, P. (2012). Machine Learning: The Art and Science of Algorithms that Make Sense of \nData. Cambridge University Press. \nMurphy, K.P. (2012). Machine Learning: A Probabilistic Perspective. MIT Press.\n \n\nOPTIMISING COMPILERS \n \nAims \nThe aims of this course are to introduce the principles of program optimisation and related \nissues in decompilation. The course will cover optimisations of programs at the abstract syntax, \nflowgraph and target-code level. It will also examine how related techniques can be used in the \nprocess of decompilation. \n \nLectures \nIntroduction and motivation. Outline of an optimising compiler. Optimisation partitioned: analysis \nshows a property holds which enables a transformation. The flow graph; representation of \nprogramming concepts including argument and result passing. The phase-order problem. \nKinds of optimisation. Local optimisation: peephole optimisation, instruction scheduling. Global \noptimisation: common sub-expressions, code motion. Interprocedural optimisation. The call \ngraph. \nClassical dataflow analysis. Graph algorithms, live and avail sets. Register allocation by register \ncolouring. Common sub-expression elimination. Spilling to memory; treatment of \nCSE-introduced temporaries. Data flow anomalies. Static Single Assignment (SSA) form. \nHigher-level optimisations. Abstract interpretation, Strictness analysis. Constraint-based \nanalysis, Control flow analysis for lambda-calculus. Rule-based inference of program properties, \nTypes and effect systems. Points-to and alias analysis. \nTarget-dependent optimisations. Instruction selection. Instruction scheduling and its phase-order \nproblem. \nDecompilation. Legal/ethical issues. Some basic ideas, control flow and type reconstruction. \nObjectives \nAt the end of the course students should \n \nbe able to explain program analyses as dataflow equations on a flowgraph; \nknow various techniques for high-level optimisation of programs at the abstract syntax level; \nunderstand how code may be re-scheduled to improve execution speed; \nknow the basic ideas of decompilation. \nRecommended reading \n* Nielson, F., Nielson, H.R. and Hankin, C.L. (1999). Principles of program analysis. Springer. \nGood on part A and part B. \nAppel, A. (1997). Modern compiler implementation in Java/C/ML (3 editions). \nMuchnick, S. (1997). Advanced compiler design and implementation. Morgan Kaufmann. \nWilhelm, R. (1995). Compiler design. Addison-Wesley. \nAho, A.V., Sethi, R. and Ullman, J.D. (2007). Compilers: principles, techniques and tools. \nAddison-Wesley (2nd ed.).\n \n\nQUANTUM COMPUTING \n \nAims \nThe principal aim of the course is to introduce students to the basics of the quantum model of \ncomputation. The model will be used to study algorithms for searching, factorisation and \nquantum chemistry as well as other important topics in quantum information such as \ncryptography and super-dense coding. Issues in the complexity of computation will also be \nexplored. A second aim of the course is to introduce student to near-term quantum computing. \nTo this end, error-correction and adiabatic quantum computing are studied. \n \nLectures \nBits and qubits. Introduction to quantum states and measurements with motivating examples. \nComparison with discrete classical states. \nLinear algebra. Review of linear algebra: vector spaces, linear operators, Dirac notation, the \ntensor product. \nThe postulates of quantum mechanics. Postulates of quantum mechanics, incl. evolution and \nmeasurement. \nImportant concepts in quantum mechanics. Entanglement, distinguishing orthogonal and \nnon-orthogonal quantum states, no-cloning and no signalling. \nThe quantum circuit model. The circuit model of quantum computation. Quantum gates and \ncircuits. Universality of the quantum circuit model, and efficient simulation of arbitrary two-qubit \ngates with a standard universal set of gates. \nSome applications of quantum information. Applications of quantum information (other than \nquantum computation): quantum key distribution, superdense coding and quantum teleportation. \nDeutsch-Jozsa algorithm. Introducing Deutsch\u2019s problem and Deutsch\u2019s algorithm leading onto \nits generalisation, the Deutsch-Jozsa algorithm. \nQuantum search. Grover\u2019s search algorithm: analysis and lower bounds. \nQuantum Fourier Transform and Quantum Phase Estimation. Definition of the Quantum Fourier \nTransform (QFT), and efficient representation thereof as a quantum circuit. Application of the \nQFT to enable Quantum Phase Estimation (QPE). \nApplication 1 of QFT / QPE: Factoring. Shor\u2019s algorithm: reduction of factoring to period finding \nand then using the QFT for period finding. \nApplication 2 of QFT / QPE: Quantum Chemistry. Efficient simulation of quantum systems, and \napplications to real-world problems in quantum chemistry. \nQuantum complexity. Quantum complexity classes and their relationship to classical complexity. \nComparison with probabilistic computation. \nQuantum error correction. Introducing the concept of quantum error correction required for the \nfollowing lecture on fault-tolerance. \nFault tolerant quantum computing. Elements of fault tolerant computing; the threshold theorem \nfor efficient suppression of errors. \nAdiabatic quantum computing and quantum optimisation. The quantum adiabatic theorem, and \nadiabatic optimisation. Quantum annealing and D-Wave. \nCase studies in near-term quantum computation. Examples of state-of-the-art quantum \nalgorithms and computers, including superconducting and networked quantum computers. \n\nObjectives \nAt the end of the course students should: \n \nunderstand the quantum model of computation and the basic principles of quantum mechanics; \nbe familiar with basic quantum algorithms and their analysis; \nbe familiar with basic quantum protocols such as teleportation and superdense coding; \nsee how the quantum model relates to classical models of deterministic and probabilistic \ncomputation. \nappreciate the importance of efficient error-suppression if quantum computation is to yield an \nadvantage over classical computation. \ngain a general understanding of the important topics in near-term quantum computing, including \nadiabatic quantum computing. \nRecommended reading \nBooks: \nNielsen M.A., Chuang I.L. (2010). Quantum Computation and Quantum Information. Cambridge \nUniversity Press. \nMermin N.D. (2007). Quantum Computer Science: An Introduction. Cambridge University Press. \nMcGeoch, C. (2014). Adiabatic Quantum Computation and Quantum Annealing Theory and \nPractice. Morgan and Claypool. https://ieeexplore.ieee.org/document/7055969 \n \nPapers: \n \nBraunstein S.L. (2003). Quantum computation tutorial. Available at: \nhttps://www-users.cs.york.ac.uk/~schmuel/comp/comp_best.pdf \nAharonov D., Quantum computation [arXiv:quant-ph/9812037] \nAlbash T., Adiabatic Quantum Computing https://arxiv.org/pdf/1611.04471.pdf \nMcCardle S. et al, Quantum computational chemistry https://arxiv.org/abs/1808.10402 \n \nOther lecture notes: \n \nAndrew Childs (University of Maryland): http://cs.umd.edu/~amchilds/qa/ \n \n\nRANDOMISED ALGORITHMS \n \nAims: \nThe aim of this course is to introduce advanced techniques in the design and analysis \nalgorithms, with a strong focus on randomised algorithms. It covers essential tools and ideas \nfrom probability, optimisation and graph theory, and develops this knowledge through a variety \nof examples of algorithms and processes. This course will demonstrate that randomness is not \nonly an important design technique which often leads to simpler and more elegant algorithms, \nbut may also be essential in settings where time and space are restricted.   \n \nLectures: \nIntroduction. Course Overview. Why Randomised Algorithms? A first Randomised Algorithm for \nthe MAX-CUT problem. [1 Lecture]  \n \nConcentration Inequalities. Moment-Generating Functions and Chernoff Bounds. Extension: \nMethod of Bounded Independent Differences. Applications: Balls-into-Bins, Quick-Sort and Load \nBalancing. [approx. 2 Lectures] \n \nMarkov Chains and Mixing Times. Random Walks on Graphs. Application: Randomised \nAlgorithm for the 2-SAT problem. Mixing Times of Markov Chains. Application: Load Balancing \non Networks. [approx. 2 Lectures] \n \nLinear Programming and Applications. Definitions and Applications. Formulating Linear \nPrograms. The Simplex Algorithm. Finding Initial Solutions. How to use Linear Programs and \nBranch & Bound to Solve a Classical TSP instance. [approx. 3 Lectures] \n \nRandomised Approximation Algorithms. Randomised Approximation Schemes. Linearity of \nExpectations, Derandomisation. Deterministic and Randomised Rounding of Linear Programs. \nApplications: MAX3-SAT problem, Weighted Vertex Cover, Weighted Set Cover, MAX-SAT. \n[approx. 2 Lectures] \n \nSpectral Graph Theory and Clustering. Eigenvalues of Graphs and Matrices: Relations between \nEigenvalues and Graph Properties, Spectral Graph Drawing. Spectral Clustering: Conductance, \nCheeger's Inequality. Spectral Partitioning Algorithm [approx. 2 Lectures] \n \nObjectives: \nBy the end of the course students should be able to: \n \nlearn how to use randomness in the design of algorithms, in particular, approximation \nalgorithms; \nlearn the basics of linear programming, integer programming and randomised rounding; \napply randomisation to various problems coming from optimisation, machine learning and data \nscience; \nuse results from probability theory to analyse the performance of randomised algorithms. \n\nRecommended reading \n* Michael Mitzenmacher and Eli Upfal. Probability and Computing: Randomized Algorithms and \nProbabilistic Analysis., Cambridge University Press, 2nd edition. \n \n* David P. Williamson and David B. Shmoys. The Design of Approximation Algorithms, \nCambridge University Press, 2011 \n \n* Cormen, T.H., Leiserson, C.D., Rivest, R.L. and Stein, C. (2009). Introduction to Algorithms. \nMIT Press (3rd ed.). ISBN 978-0-262-53305-8 \n \n \n\nCLOUD COMPUTING \nAims \nThis module aims to teach students the fundamentals of Cloud Computing covering topics such \nas virtualization, data centres, cloud resource management, cloud storage and popular cloud \napplications including batch and data stream processing. Emphasis is given on the different \nbackend technologies to build and run efficient clouds and the way clouds are used by \napplications to realise computing on demand. The course will include practical tutorials on cloud \ninfrastructure technologies. Students will be assessed via a Cloud-based coursework project. \n \nLectures \nIntroduction to Cloud Computing \nData centres \nVirtualization I \nVirtualization II \nMapReduce \nMapReduce advanced \nResource management for virtualized data centres \nCloud storage \nCloud-based data stream processing \nObjectives \nBy the end of the course students should: \n \nunderstand how modern clouds operate and provide computing on demand; \nunderstand about cloud availability, performance, scalability and cost; \nknow about cloud infrastructure technologies including virtualization, data centres, resource \nmanagement and storage; \nknow how popular applications such as batch and data stream processing run efficiently on \nclouds; \nAssessment \nAssignment 1, worth 55% of the final mark. Develop batch processing applications running on a \ncluster assessed through automatic testing, code inspection and a report. \nAssignment 2, worth 30% of the final mark: Design and develop a framework for running the \nbatch processing applications from Assignment 1 on a cluster according to certain resource \nallocation policies. This assigment will be assessed by an expert, testing, code inspection and a \nreport. \nAssignment 3, worth 15% of the final mark: Write two individual project reports describing your \nwork for assignments 1 and 2. Write two sets of scripts to manage and run your code. Reports \nand scripts will be submitted with each assignment. \nRecommended reading \nMarinescu, D.C. Cloud Computing, Theory and Practice. Morgan Kaufmann. \nBarham, P., et. al. (2003). \u201cXen and the Art of Virtualization\u201d. In Proceedings of SOSP 2003. \nCharkasova, L., Gupta, D. and Vahdat, A. (2007). \u201cComparison of the Three CPU Schedulers in \nXen\u201d. In SIGMETRICS 2007. \n\nDean, J. and Ghemawat, S. (2004). \u201cMapReduce: Simplified Data Processing on Large \nClusters\u201d. In Proceedings of OSDI 2004. \nZaharia, M, et al. (2008). \u201cImproving MapReduce Performance in Heterogeneous \nEnvironments\u201d. In Proceedings of OSDI 2008. \nHindman, A., et al. (2011). \u201cMesos: A Platform for Fine-Grained Resource Sharing in Data \nCenter\u201d. In Proceedings of NSDI 2011. \nSchwarzkopf, M., et al. (2013). \u201cOmega: Flexible, Scalable Schedulers for Large Compute \nClusters\u201d. In EuroSys 2013. \nGhemawat, S. (2003). \u201cThe Google File System\u201d. In Proceedings of SOSP 2003. \nChang, F. (2006). \u201cBigtable: A Distributed Storage System for Structured Data\u201d. In Proceedings \nof OSDI 2006. \nFernandez, R.C., et al. (2013). \u201cIntegrating Scale Out and Fault Tolerance in Stream Processing \nusing Operator State Management\u201d. In SIGMOD 2013.\n \n\nCOMPUTER SYSTEMS MODELLING \n \nAims \n \nThe aims of this course are to introduce the concepts and principles of mathematical modelling \n \nand simulation, with particular emphasis on using queuing theory and control theory for \nunderstanding the behaviour of computer and communications systems. \n \nSyllabus \n \n1.     Overview of computer systems modeling using both analytic techniques and simulation. \n \n2.     Introduction to discrete event simulation. \n \na.     Simulation techniques \n \nb.     Random number generation methods \n \nc.     Statistical aspects: confidence intervals, stopping criteria \n \nd.     Variance reduction techniques. \n \n3.     Stochastic processes (builds on starred material in Foundations of Data Science) \n \na.     Discrete and continuous stochastic processes. \n \nb.     Markov processes and Chapman-Kolmogorov equations. \n \nc.     Discrete time Markov chains. \n \nd.     Ergodicity and the stationary distribution. \n \ne.     Continuous time Markov chains. \n \nf.      Birth-death processes, flow balance equations. \n \ng.     The Poisson process. \n \n4.     Queueing theory \n \na.     The M/M/1 queue in detail. \n \n\nb.     The equilibrium distribution with conditions for existence and common performance \nmetrics. \n \nc.     Extensions of the M/M/1 queue: the M/M/k queue, the M/M/infinity queue. \n \nd.     Queueing networks. Jacksonian networks. \n \ne.     The M/G/1 queue. \n \n5.     Signals, systems, and transforms \n \na.     Discrete- and continuous-time convolution. \n \nb.     Signals. The complex exponential signal. \n \nc.     Linear Time-Invariant Systems. Modeling practical systems as an LTI system. \n \nd.     Fourier and Laplace transforms. \n \n6.     Control theory. \n \na.     Controlled systems. Modeling controlled systems. \n \nb.     State variables. The transfer function model. \n \nc.     First-order and second-order systems. \n \nd.     Feedback control. PID control. \n \ne.     Stability. BIBO stability. Lyapunov stability. \n \nf.      Introduction to Model Predictive Control. \n \nObjectives \n \nAt the end of the course students should \n \n\u00b7       Be aware of different approaches to modeling a computer system; their pros and cons \n \n\u00b7       Be aware of the issues in building a simulation of a computer system and analysing the \nresults obtained \n \n\u00b7       Understand the concept of a stochastic process and how they arise in practice \n \n\n\u00b7       Be able to build simple Markov models and understand the critical modelling assumptions \n \n\u00b7       Be able to solve simple birth-death processes \n \n\u00b7       Understand and use M/M/1 queues to model computer systems \n \n\u00b7       Be able to model a computer system as a linear time-invariant system \n \n\u00b7       Understand the dynamics of a second-order controlled system \n \n\u00b7       Design a PID control for an LTI system \n \n\u00b7       Understand what is meant by BIBO and Lyapunov stability \n \nAssessment \n \nThere will be one programming assignment to build a discrete event simulator and using it to \nsimulate an M/M/1 and an M/D/1 queue (worth 15% of the final marks). There will also be two \none-hour in-person assessments for the course on: Stochastic processes and Queueing theory \n(40%) and Signals, Systems, Transforms, and Control Theory (45%). \n \nReference books \n \nKeshav, S. (2012)*. Mathematical Foundations of Computer Networking. Addison-Wesley. A \ncustomized PDF will be made available to class participants. \n \nKleinrock, L. (1975). Queueing systems, vol. 1. Theory. Wiley. \n \nKraniauskas, Peter. Transforms in signals and systems. Addison-Wesley Longman, 1992. \n \nJain, R. (1991). The art of computer systems performance analysis. Wiley. \n \n \n\nCOMPUTING EDUCATION \n \nAims \nThis module considers various aspects of the teaching and learning of computer science in \nschool, including practical experience, and provides an introduction to the field of computing \neducation research. It is offered by the Raspberry Pi Computing Education Research Centre, \npart of the Department of Computer Science and Technology, in conjunction with local schools \nin the Cambridge area. Students will have the opportunity to observe, plan and deliver a lesson, \nand explore the teaching of computing in a secondary education setting. The lesson taught will \nbe on a topic within the computing/computer science curriculum in England but will be \nnegotiated with the teacher mentoring the school placement. In the sessions in the Department, \nstudents will be introduced to elements of pedagogy and assessment alongside current issues \nin computing education. The course can also serve as a useful introduction to research in \ncomputing education for those interested in this area. Assessment will include lesson planning, \npresentations, an in-person viva and a 3000-word report which will include evidence of \nengagement with relevant research underpinning the lessons and activities undertaken. An \ninformation session relating to the module will be available in the preceding Easter Term. The \narrangement of school placements and DBS checks will be carried out in Michaelmas Term, so \nstudents should be prepared to engage in these preparatory activities. \n \nLectures \nThis is not a traditional lecture course and the term will be structured as follows: \n \nWeek 1: Introduction to computing education & visit to school placement \nWeek 2: In school (observation, supporting teacher) \nWeek 3: In school (observation, supporting teacher) \nWeek 4: In school (co-teach / small group teaching) \nWeek 5: Computing pedagogy and assessment (school half-term) \nWeek 6: In school (co-teach / small group teaching) \nWeek 7: In school (teach part/whole lesson) \nWeek 8: Presentations \nObjectives \nBy the end of the course students should: \n \ngain experience of computing education in practice \ndemonstrate an ability to communicate an aspect of computer science clearly and effectively \nbe able to plan and design a teaching activity/lesson with consideration of the audience \nbe able to thoroughly evaluate the design and implementation of a teaching/activity lesson \nbe able to demonstrate an understanding of relevant theories of learning and pedagogy from the \nliterature and how they apply to the learning/teaching of the topic under consideration \ngain an understanding of the breadth and depth of the field of computing education research \nClass Size \nThis module can accommodate up to 10 Part II students. \n \n\nRecommended reading \nBrown, N. C., Sentance, S., Crick, T., & Humphreys, S. (2014). Restart: The resurgence of \ncomputer science in UK schools. ACM Transactions on Computing Education (TOCE), 14(2), \n1-22. https://doi.org/10.1145/2602484 \n \nSentance, S., Barendsen, E., Howard, N. R., & Schulte, C. (Eds.). (2023). Computer science \neducation: Perspectives on teaching and learning in school. Bloomsbury Publishing. \n \nGrover, S. (Ed). 2020. Computer Science in K-12: An A-to-Z Handbook on Teaching \nProgramming. Edfinity. https://www.shuchigrover.com/atozk12cs/ \n \nRaspberry Pi Foundation (2022). Hello World Big Book of Pedagogy. \nhttps://www.raspberrypi.org/hello-world/issues/the-big-book-of-computing-pedagogy \n \nAssessment \nWeek 2: Reflections on an observed lesson (10%, graded out of 20) \nWeek 4: Submission of a lesson plan outline (10% graded out of 20) \nWeek 8: Delivery of an oral presentation relating to the lesson delivery (10% graded out of 20) \nAfter course completion: 3000 word report (60%), graded out of 20 \nViva with assessor (10%) (pass/fail) \nFurther Information \nWe will find placements for students in secondary schools within cycling or bus distance from \nthe Department/centre of Cambridge. There will be a timetabled slot for the course that students \ncan use to go to their school placement, but students may need to be flexible and liaise with the \nschool to find the best time for working with a specific class. \n \n \n\nDEEP NEURAL NETWORKS \n \nObjectives \nYou will gain detailed knowledge of \n \nCurrent understanding of generalization in neural networks vs classical statistical models. \nOptimization procedures for neural network models such as stochastic gradient descent and \nADAM. \nAutomatic differentiation and at least one software framework (PyTorch, TensorFolow) as well as \nan overview of other software approaches. \nArchitectures that are deployed to deal with different data types such as images or sequences \nincluding a. convolutional networks b. recurrent networks and c. transformers. \nIn addition you will gain knowledge of more advanced topics reflecting recent research in \nmachine learning. These change from year to year, examples include: \n \nApproaches to unsupervised learning including autoencoders and self-supervised learning.. \nTechniques for deploying models in low data regimes such as transfer learning and \nmeta-learning. \nTechniques for propagating uncertainty such as Bayesian neural networks. \nReinforcement learning and its applications to robotics or games. \nGraph Neural Networks and Geometric Deep Learning. \nTeaching Style \nThe start of the course will focus on the latest undertanding of current theory of neural networks, \ncontrasting with previous classical understandings of generalization performance. Then we will \nmove to practical examples of network architectures and deployment. We will end with more \nadvanced topics reflecting current research, mostly delivered by guest lecturers from the \nDepartment and beyond. \n \nSchedule \nWeek 1 \nTwo lectures: Generalization and Approximation with Neural Networks \n \nWeek 2 \nTwo lectures: Optimization: Stochastic Gradient Descent and ADAM \n \nWeek 3 \nTwo lectures: Hardware Acceleration and Convolutional Neural Networks \n \nWeek 4 \nTwo lectures: Vanishing Gradients, Recurrent and Residual Networks, Sequence Modeling. \n \nWeek 5 \nTwo lectures: Attention, The Transformer Architecture, Large Language Models \n \n\nWeek 6-8 \nLectures and guest lectures from the special topics below. \n \nSpecial topics \nSelf-Supervised Learning, AutoEncoders, Generative Modeling of Images \nHardware Implementations \nReinforcement learning \nTransfer learning and meta-learning \nUncertainty and Bayesian Neural Networks \nGraph Neural Networks \nAssessment \nAssessment involves solving a number of exercises in a jupyter notebook environment. \nFamiliarity with the python programming language is assumed. The exercises rely on the \npytorch deep learning framework, the syntax of which we don\u2019t cover in detail in the lectures, \nstudents are referred to documentation and tutorials to learn this component of the assessment. \n \nAssignment 1 \n \n30% of the total marks, with guided exercises. \n \nAssignment 2 \n \n70% of the total marks, a combination of guided exercises and a more exploratory mini-project.\n\nEXTENDED REALITY \n \nSyllabus & Schedule \nWeek 1 \nIntroduction \nCourse schedule and concept \nThe history of AR/VR/XR \nApplications of AR/VR/XR \nOverview of the XR pipeline \u2013 3 main components of XR \nDisplay technologies and rendering \nMachine perception and input interfaces \nContent generation \u2013 geometry, appearance, motion \nOverview of the AR/VR/XR frameworks \nPractical 1 \nProject structure \nProject themes \nIntroduction to the XR programming platform \nWeek 2 \n3D scene representations and rendering \nObject representations with polygonal meshes \nEfficient rendering via rasterization \nPractical XR rendering of textured geometries \nTracking and pose estimation for XR \nRigid transforms \nCamera pose estimation \nBody/ hand tracking \nPractical 2 \nProject draft proposal due \nProject feedback \nRendering and displaying a first object \nWeek 3 \n3D scene capture and understanding \nDepth estimation \n3D geometry capture \nAppearance acquisition \nLighting estimation and relighting \nLighting models for 3D scenes \nEstimating lighting in a scene \nRelighting rendered models \nPractical 3 \nAcquiring and utilizing depth from sensors \nRelighting a displayed object \nWeek 4 \nPhysically-based simulation of rigid objects \n\nRigid-body dynamics \nCollision detection \nTime integration \nPhysically-based simulation of non-rigid volumetric phenomena \nParticle systems \nBasic fluid dynamics \nPractical 4 \nPractical due \nProject prototype due \nProject check-in \nSimulating and rendering a simple object \nWeek 5 \nAdvanced topics in XR I: AR/VR displays technologies \nMicro LED-based displays \nWaveguide-based displays \nNext generation displays \nWeek 6 \nAdvanced topics in XR II \u2013 Mobile AR \nComputer vision on limited hardware \nUX/UI challenges \nContent challenges \nProject due \n  \n \nAssessment \nA practical exercise, worth 20% of the mark. This will cover the basics of AR/VR development \nand some of the practical techniques the students will use in their projects. This is individual \nwork. No mobile device or GPU hours are needed. \nAn AR/VR project worth 80% of the marks: \nCourse projects will be designed by the students following the possible project themes proposed \nby the lecturer and will be checked by the lecturer for appropriateness. \nThe students will form groups of 2-3 to design, implement, report, and present the course \nproject. \nThe final mark will be an implementation mark (40%), a report mark (20%), and a \npresentation/demo mark (20%). Each team member will be evaluated based on their \ncontribution. \nEach project will have extensions to be completed only by the ACS students. Each student will \nwrite a different part of the report, whose author will be clearly marked. Each student will further \nsummarise her/his contributions to the project in the same report. \nNote: Submission is by one submission per group but work by each individual must be clearly \nlabelled.\n \n\nFEDERATED LEARNING: THEORY AND PRACTICE \n \nObjectives \nThis course aims to extend the machine learning knowledge available to students in Part I (or \npresent in typical undergraduate degrees in other universities), and allow them to understand \nhow these concepts can manifest in a decentralized setting. The course will consider both \ntheoretical (e.g., decentralized optimization) and practical (e.g., networking efficiency) aspects \nthat combine to define this growing area of machine learning.  \n \nAt the end of the course students should: \n \nUnderstand popular methods used in federated learning \nBe able to construct and scale a simple federated system \nHave gained an appreciation of the core limitations to existing methods, and the approaches \navailable to cope with these issues \nDeveloped an intuition for related technologies like differential privacy and secure aggregation, \nand are able to use them within typical federated settings  \nCan reason about the privacy and security issues with federated systems \nLectures \nCourse Overview. Introduction to Federated Learning. \nDecentralized Optimization. \nStatistical and Systems Heterogeneity. \nVariations of Federated Aggregation. \nSecure Aggregation. \nDifferential Privacy within Federated Systems. \nExtensions to Federated Analytics. \nApplications to Speech, Video, Images and Robotics. \nLab sessions \nFederating a Centralized ML Classifier. \nBehaviour under Heterogeneity. \nScaling a Federated Implementation. \nExploring Privacy with Federated Settings \nRecommended Reading \nReadings will be assigned for each lecture. Readings will be taken from either research papers, \ntutorials, source code or blogs that provide more comprehensive treatment of taught concepts. \n \nAssessment - Part II Students \nFour labs are performed during the course, and students receive 10% of their total grade for \nwork done as part of each lab. (For a total of 40% of the total grade from lab work alone). Labs \nwill primarily provide hands-on teaching opportunities, that are then utilized within the lab \nassignment which is completed outside of the lab contact time. MPhil and Part III students will \nbe given additional questions to answer within their version of the lab assignment which will \ndiffer from the assignment given to Part II CST students. \n \n\nThe remainder of the course grade (60%) will be given based on a hands-on project that applies \nthe concepts taught in lectures and labs. This hands-on project will be assessed based on upon \na combination of source code, related documentation and brief 8-minute pre-recorded talk that \nsummarizes key project elements (any slides used are also submitted as part of the project). \nPlease note, that in the case of Part II CST students, the talk itself is not examinable -- as such \nwill be made optional to those students. \n \nA range of possible practical projects will be described and offered to students to select from, or \nalternatively students may propose their own. MPhil and Part III students will select from a \nproject pool that is separate from those offered to Part II CST students. MPhil and Part III \nprojects will contain a greater emphasis on a research element, and the pre-recorded talks \nprovided by this student group will focus on this research contribution. The project will be \nassessed on the level of student understanding demonstrated, the degree of difficulty, \ncorrectness of implementation -- and for Part III/MPhil students the additional criteria of the \nquality and execution of the research methodology, and depth and quality of results analysis. \n \nThis project can be done individually or in groups -- although individual projects will be strongly \nencouraged. It will be required the project is performed using a code repository that also will \ncontain all documentation -- access to this repository will be shared with course staff (e.g., \nlecturer and TAs). Where needed, marks assigned to students within a group will be \ndifferentiated using this repository as an input. Furthermore if groups are formed, members must \nbe either entirely from Part III/MPhil students or Part II CST, i.e., these two student groups \nshould not mix to form a project group. \n \nProjects will be made available publicly. A maximum word count for written contributions for the \nproject will be enforced.\n \n\nMOBILE HEALTH \n \nAims \nThe course aims to explore how wearable and mobile systems sensors can be used to gather \ndata relevant to understand health, how the data can be analysed with advanced signal \nprocessing and machine learning and the performance of these systems in terms of diagnostics \nand disease progression detection. \n \nSyllabus \nCourse Overview. Introduction to Mobile Health. Evaluation metrics and methodology. Basics of \nSignal Processing. \nInertial Measurement Units, Human Activity Recognition (HAR) and Gait Analysis and Machine \nLearning for IMU data. \nRadios, Bluetooth, GPS and Cellular. Epidemiology and contact tracing, Social interaction \nsensing and applications. Location tracking in health monitoring and public health. \nAudio Signal Processing. Voice and Speech Analysis: concepts and data analysis. Body \nSounds analysis. \nPhotoplethysmogram and Light sensing for health (heart and sleep) \nContactless and wireless behaviour and physiological monitoring \nGenerative Models for Wearable Health Data \nTopical Guest Lectures \nObjectives \nThe course aims to explore how wearable and mobile systems sensors can be used to gather \ndata relevant to understand health, how the data can be analysed with advanced signal \nprocessing and machine learning and the performance of these systems in terms of diagnostics \nand disease progression detection. \n \nRoughly, each lecture contains a theory part about the working of \u201csensor signals\u201d or \u201cdata \nanalysis methods\u201d and an application part which contextualises the concepts. \n \nAt the end of the course students should: Understand how mobile/wearable sensors capture \ndata and their working. Understand different approaches to acquiring and analysing sensor data \nfrom different types of sensors. Understand the concept of signal processing applied to time \nseries data and their practical application in health. Be able to extract sensor data and analyse it \nwith basic signal processing and machine learning techniques. Be aware of the different health \napplications of the various sensor techniques. The course will also touch on privacy and ethics \nimplications of the approaches developed in an orthogonal fashion. \n \nRecommended Reading \nPlease see Course Materials for recommended reading for each session. \n \nAssessment - Part II students \nTwo assignments will be based on two datasets which will be provided to the students: \n \n\nAssignment 1 (shared for Part II and Part III/MPhil): this will be based on a dataset and will be \nworth 40% of the final mark. The task of the assessment will be to perform pre-processing and \nbasic data analysis in a \"colab\" and an answer sheet of no more than 1000 words. \n \nAssignment 2 (Part II): This assignment (worth 60% of the final mark) will be a fuller analysis of \na dataset focusing on machine learning algorithms and metrics. Discussion and interpretation of \nthe findings will be reported in a colab and a report of no more than 1200 words.\n \n\nMULTICORE SEMANTICS AND PROGRAMMING \n \nAims \nIn recent years multiprocessors have become ubiquitous, but building reliable concurrent \nsystems with good performance remains very challenging. The aim of this module is to \nintroduce some of the theory and the practice of concurrent programming, from hardware \nmemory models and the design of high-level programming languages to the correctness and \nperformance properties of concurrent algorithms. \n \nLectures \nPart 1: Introduction and relaxed-memory concurrency [Professor P. Sewell] \n \nIntroduction. Sequential consistency, atomicity, basic concurrent problems. [1 block] \nConcurrency on real multiprocessors: the relaxed memory model(s) for x86, ARM, and IBM \nPower, and theoretical tools for reasoning about x86-TSO programs. [2 blocks] \nHigh-level languages. An introduction to C/C++11 and Java shared-memory concurrency. [1 \nblock] \nPart 2: Concurrent algorithms [Dr T. Harris] \n \nConcurrent programming. Simple algorithms (readers/writers, stacks, queues) and correctness \ncriteria (linearisability and progress properties). Advanced synchronisation patterns (e.g. some \nof the following: optimistic and lazy list algorithms, hash tables, double-checked locking, RCU, \nhazard pointers), with discussion of performance and on the interaction between algorithm \ndesign and the underlying relaxed memory models. [3 blocks] \nResearch topics, likely to include one hour on transactional memory and one guest lecture. [1 \nblock] \nObjectives \nBy the end of the course students should: \n \nhave a good understanding of the semantics of concurrent programs, both at the multprocessor \nlevel and the C/Java programming language level; \nhave a good understanding of some key concurrent algorithms, with practical experience. \nAssessment - Part II Students \nTwo assignments each worth 50% \n \nRecommended reading \nHerlihy, M. and Shavit, N. (2008). The art of multiprocessor programming. Morgan Kaufmann.\n\nBUSINESS STUDIES SEMINARS \nAims \nThis course is a series of seminars by former members and friends of the Laboratory about their \nreal-world experiences of starting and running high technology companies. It is a follow on to \nthe Business Studies course in the Michaelmas Term. It provides practical examples and case \nstudies, and the opportunity to network with and learn from actual entrepreneurs. \n \nLectures \nEight lectures by eight different entrepreneurs. \n \nObjectives \nAt the end of the course students should have a better knowledge of the pleasures and pitfalls \nof starting a high tech company. \n \nRecommended reading \nLang, J. (2001). The high-tech entrepreneur\u2019s handbook: how to start and run a high-tech \ncompany. FT.COM/Prentice Hall. \nMaurya, A. (2012). Running Lean: Iterate from Plan A to a Plan That Works. O\u2019Reilly. \nOsterwalder, A. and Pigneur, Y. (2010). Business Model Generation: A Handbook for \nVisionaires, Game Changers, and Challengers. Wiley. \nKim, W. and Mauborgne, R. (2005). Blue Ocean Strategy. Harvard Business School Press. \n \nSee also the additional reading list on the Business Studies web page.\n \n\nHOARE LOGIC AND MODEL CHECKING \n \nAims \nThe course introduces two verification methods, Hoare Logic and Temporal Logic, and uses \nthem to formally specify and verify imperative programs and systems. \n \nThe first aim is to introduce Hoare logic for a simple imperative language and then to show how \nit can be used to formally specify programs (along with discussion of soundness and \ncompleteness), and also how to use it in a mechanised program verifier. \n \nThe second aim is to introduce model checking: to show how temporal models can be used to \nrepresent systems, how temporal logic can describe the behaviour of systems, and finally to \nintroduce model-checking algorithms to determine whether properties hold, and to find \ncounter-examples. \n \nCurrent research trends also will be outlined. \n \nLectures \nPart 1: Hoare logic. Formal versus informal methods. Specification using preconditions and \npostconditions. \nAxioms and rules of inference. Hoare logic for a simple language with assignments, sequences, \nconditionals and while-loops. Syntax-directedness. \nLoops and invariants. Various examples illustrating loop invariants and how they can be found. \nPartial and total correctness. Hoare logic for proving termination. Variants. \nSemantics and metatheory Mathematical interpretation of Hoare logic. Semantics and \nsoundness of Hoare logic. \nSeparation logic Separation logic as a resource-aware reinterpretation of Hoare logic to deal \nwith aliasing in programs with pointers. \nPart 2: Model checking. Models. Representation of state spaces. Reachable states. \nTemporal logic. Linear and branching time. Temporal operators. Path quantifiers. CTL, LTL, and \nCTL*. \nModel checking. Simple algorithms for verifying that temporal properties hold. \nApplications and more recent developments Simple software and hardware examples. CEGAR \n(counter-example guided abstraction refinement). \nObjectives \nAt the end of the course students should \n \nbe able to prove simple programs correct by hand and implement a simple program verifier; \nbe familiar with the theory and use of separation logic; \nbe able to write simple models and specify them using temporal logic; \nbe familiar with the core ideas of model checking, and be able to implement a simple model \nchecker. \nRecommended reading \n\nHuth, M. and Ryan M. (2004). Logic in Computer Science: Modelling and Reasoning about \nSystems. Cambridge University Press (2nd ed.).\n \n\n",
        "7th Semester \nADVANCED TOPICS IN COMPUTER ARCHITECTURE \nAims \nThis course aims to provide students with an introduction to a range of advanced topics in \ncomputer architecture. It will explore the current and future challenges facing the architects of \nmodern computers. These will also be used to illustrate the many different influences and \ntrade-offs involved in computer architecture. \n \nObjectives \nOn completion of this module students should: \n \nunderstand the challenges of designing and verifying modern microprocessors \nbe familiar with recent research themes and emerging challenges \nappreciate the complex trade-offs at the heart of computer architecture \nSyllabus \nEach seminar will focus on a different topic. The proposed topics are listed below but there may \nbe some minor changes to this: \n \nTrends in computer architecture \nState-of-the-art microprocessor design \nMemory system design \nHardware reliability \nSpecification, verification and test (may be be replaced with a different topic) \nHardware security (2) \nHW accelerators and accelerators for machine learning \nEach two hour seminar will include three student presentations (15mins) questions (5mins) and \na broader discussion of the topics (around 30mins). The last part of the seminar will include a \nshort scene setting lecture (around 20mins) to introduce the following week's topic. \n \nAssessment \nEach week students will compare and contrast two of the main papers and submit a written \nsummary and review in advance of each seminar (except when presenting). \n \nStudents will be expected to give a number of 15 minute presentations. \n \nEssays and presentations will be marked out of 10. After dropping the lowest mark, the \nremaining marks will be scaled to give a final score out of 100. \n \nStudents will give at least one presentation during the course. They will not be required to \nsubmit an essay during the weeks they are presenting. \n \nEach presentation will focus on a single paper from the reading list. Marks will be awarded for \nclarity and the communication of the paper's key ideas, an analysis of the work's strengths and \nweaknesses and the work\u2019s relationship to related work and broader trends and constraints. \n\n \nRecommended prerequisite reading \nPatterson, D. A., Hennessy, J. L. (2017). Computer organization and design: The \nHardware/software interface RISC-V edition Morgan Kaufmann. ISBN 978-0-12-812275-4. \n \nHennessy, J. and Patterson, D. (2012). Computer architecture: a quantitative approach. Elsevier \n(5th ed.) ISBN 9780123838728. (the 3rd and 4th editions are also relevant)\n \n\nADVANCED TOPICS IN PROGRAMMING LANGUAGES \nAims \nThis module explores various topics in programming languages beyond the scope of \nundergraduate courses. It aims to introduce students to ideas, results and techniques found in \nthe literature and prepare them for research in the field. \n \nSyllabus and structure \nThe module consists of eight two-hour seminars, each on a particular topic. Topics will vary from \nyear to year, but may include, for example, \n \nAbstract interpretation \nVerified software \nMetaprogramming \nBehavioural types \nProgram synthesis \nVerified compilation \nPartial evaluation \nGarbage collection \nDependent types \nAutomatic differentiation \nDelimited continuations \nModule systems \nThere will be three papers assigned for each topic, which students are expected to read before \nthe seminar. \nEach seminar will include three 20 minute student presentations (15 minutes + 5 minutes \nquestions), time for general discussion of the topic, and a brief overview lecture for the following \nweek\u2019s topic. \nBefore each seminar, except in weeks in which they give presentations, students will submit a \nshort essay about two of the papers. \n \n \nObjectives \nOn completion of this module, students should \n \nbe able to identify some major themes in programming language research \nbe familiar with some classic papers and recent advances \nhave an understanding of techniques used in the field \n \nAssessment \nAssessment consists of: \n \nPresentation of one of the papers from the reading list (typically once or twice in total for each \nstudent, depending on class numbers) \nOne essay per week (except on the first week and on presentation weeks) \n\nAll essays and presentations carry equal numbers of marks. \n \nEssay marks are awarded for understanding, for insight and analysis, and for writing quality. \nEssays should be around 1500 words (with a lower limit of 1450 and upper limit of 1650). \nPresentation marks are awarded for clarity, for effective communication, and for selection and \norganisation of topics. \n \nThere will be seven submissions (essays or presentations) in total and, as in other courses, the \nlowest mark for each student will be disregarded when computing the final mark. \n \nMarking, deadlines and extensions will be handled in accordance with the MPhil Assessment \nGuidelines. \n \nRecommended reading material and resources \nResearch and survey papers from programming language conferences and journals (e.g. \nPOPL, PLDI, TOPLAS, FTPL) will be assigned each week. General background material may \nbe found in: \n \n\u2022 Types and Programming Languages (Benjamin C. Pierce) \n   The MIT Press \n   ISBN 0-262-16209-1 \n \n\u2022 Advanced Topics in Types and Programming Languages (ed. Benjamin C. Pierce) \n   The MIT Press \n   ISBN 0-262-16228-8 \n \n\u2022 Practical Foundations for Programming Languages (Robert Harper) \n   Cambridge University Press \n   9781107150300\n \n\nAFFECTIVE ARTIFICIAL INTELLIGENCE \n \nSynopsis \nAffective Artificial Intelligence (Affective AI) aims to imbue machines with social and emotional \nintelligence (EQ). More specifically, Affective AI aims to create artificially intelligent systems and \nmachines that can recognize, interpret, process, and simulate human social signals and \nbehaviours, expressions, and emotions, to enhance human-AI interaction and communication. \n\\r\\n\\r\\nTo achieve this goal, Affective AI draws upon various scientific disciplines, including \nmachine learning, computer vision, speech / natural language / signal processing, psychology \nand cognitive science, and ethics and social sciences. \n \n  \nBackground & Aims \nAffective Artificial Intelligence (Affective AI) aims to imbue machines with social and emotional \nintelligence (EQ). More specifically, Affective AI aims to create artificially intelligent systems and \nmachines that can recognize, interpret, process, and simulate human social signals and \nbehaviours, expressions, and emotions, to enhance human-AI interaction and communication.  \n \nTo achieve this goal, Affective AI draws upon various scientific disciplines, including machine \nlearning, computer vision, speech / natural language / signal processing, psychology and \ncognitive science, and ethics and social sciences. \n \nAffective AI has direct applications in and implications for the design of innovative interactive \ntechnology (e.g., interaction with chat bots, virtual agents, robots), single and multi-user smart \nenvironments ( e.g., in-car/ virtual / augmented / mixed reality, serious games), public speaking \nand cognitive training, and clinical and biomedical studies (e.g., autism, depression, pain). \n \nThe aim of this module is to impart knowledge and ability needed to make informed choices of \nmodels, data, and machine learning techniques for sensing, recognition, and generation of \naffective and social behaviour (e.g., smile, frown, head nodding/shaking, \nagreement/disagreement) in order to create Affectively intelligent AI systems, with a \nconsideration for various ethical issues (e.g., privacy, bias) arising from the real-world \ndeployment of these systems. \n \n  \n \nSyllabus \nThe following list provides a representative list of topics: \n \nIntroduction, definitions, and overview \nTheories from various disciplines \nSensing from multiple modalities (e.g., vision, audio, bio signals, text) \nData acquisition and annotation \nSignal processing / feature extraction \n\nLearning / prediction / recognition and evaluation \nBehaviour synthesis / generation (e.g., for embodied agents / robots) \nAdvanced topics and ethical considerations (e.g., bias and fairness) \nCross-disciplinary applications (via seminar presentations and discussions) \nGuest lectures (diverse topics \u2013 changes each year) \nHands-on research and programming work (i.e., mini project & report)  \n \n  \n \nObjectives \nOn completion of this module, students will: \n \nunderstand and demonstrate knowledge in key characteristics of affectively intelligent AI, which \ninclude: \nRecognition: How to equip Affective AI systems with capabilities of analysing facial expressions, \nvocal intonations, gestures, and other physiological signals to infer human affective states? \nGeneration: How to enable affectively intelligent AI simulate expressions of emotions in \nmachines, allowing them to respond in a more human-like manner, such as virtual agents and \nhumanoid robots, showing empathy or sympathy? \nAdaptation and Personalization: How to enable affectively intelligent AI adapt system responses \nbased on users' affective states and/or needs, or tailor interactions and experiences to individual \nusers based on their expressivity / emotional profiles, personalities, and past interactions? \nEmpathetic Communication: How to design Affective AI systems to communicate with users in a \nway that demonstrates empathy, understanding, and sensitivity to their affective states and \nneeds? \nEthical and societal considerations: What are the various human differences, ethical guidelines, \nand societal impacts that need to be considered when designing and deploying Affective AI to \nensure that these systems respect users' privacy, autonomy, and well-being? \ncomprehend and apply (appropriate) methods for collection, analysis, representation, and \nevaluation of human affective and communicative behavioural data. \nenhance programming skills for creating and implementing (components of) Affective AI \nsystems. \ndemonstrate critical thinking, analysis and synthesis while deciding on 'when' and 'how' to \nincorporate human affect and social signals in a specific AI system context and gain practical \nexperience in proposing and justifying computational solution(s) of suitable nature and scope. \n  \n \nAssessment - Part II Students \nSeminar presentation: 20% \nParticipating in Q&A and discussions: 10% \nMid-term mini project report and presentation (as a team of 2): 10%  \nFinal mini project report & code (as a team of 2): 60% \n  \n \n\nAssessment - MPhil / Part III Students \nSeminar presentation: 20% \nParticipating in Q&A and discussions: 10% \nMid-term mini project report and presentation: 10%  \nFinal mini project report & code: 60% \nFurther Information \nCurrent Cambridge undergraduate students who are continuing onto Part III or the MPhil in \nAdvanced Computer Science may only take this module if they did NOT take it as a Unit of \nAssessment in Part II. \n \nThis module is shared with ACS. Assessment will be adjusted for the two groups of students to \nbe at an appropriate level for whichever course the student is enrolled on. More information will \nfollow at the first lecture. \n \nThis module is shared with Part II of the Computer Science Tripos. Assessment will be adjusted \nfor the two groups of students to be at an appropriate level for whichever course the student is \nenrolled on. Further information about assessment and practicals will follow at the first lecture.\n\nCATEGORY THEORY \n \nAims \nCategory theory provides a unified treatment of mathematical properties and constructions that \ncan be expressed in terms of 'morphisms' between structures. It gives a precise framework for \ncomparing one branch of mathematics (organized as a category) with another and for the \ntransfer of problems in one area to another. Since its origins in the 1940s motivated by \nconnections between algebra and geometry, category theory has been applied to diverse fields, \nincluding computer science, logic and linguistics. This course introduces the basic notions of \ncategory theory: adjunction, natural transformation, functor and category. We will use category \ntheory to organize and develop the kinds of structure that arise in models and semantics for \nlogics and programming languages. \n \nSyllabus \nIntroduction; some history. Definition of category. The category of sets and functions. \nCommutative diagrams. Examples of categories: preorders and monotone functions; monoids \nand monoid homomorphisms; a preorder as a category; a monoid as a category. Definition of \nisomorphism. Informal notion of a 'category-theoretic' property. \nTerminal objects. The opposite of a category and the duality principle. Initial objects. Free \nmonoids as initial objects. \nBinary products and coproducts. Cartesian categories. \nExponential objects: in the category of sets and in general. Cartesian closed categories: \ndefinition and examples. \nIntuitionistic Propositional Logic (IPL) in Natural Deduction style. Semantics of IPL in a cartesian \nclosed preorder. \nSimply Typed Lambda Calculus (STLC). The typing relation. Semantics of STLC types and \nterms in a cartesian closed category (ccc). The internal language of a ccc. The \nCurry-Howard-Lawvere correspondence. \nFunctors. Contravariance. Identity and composition for functors. \nSize: small categories and locally small categories. The category of small categories. Finite \nproducts of categories. \nNatural transformations. Functor categories. The category of small categories is cartesian \nclosed. \nHom functors. Natural isomorphisms. Adjunctions. Examples of adjoint functors. Theorem \ncharacterising the existence of right (respectively left) adjoints in terms of a universal property. \nDependent types. Dependent product sets and dependent function sets as adjoint functors. \nEquivalence of categories. Example: the category of I-indexed sets and functions is equivalent \nto the slice category Set/I. \nPresheaves. The Yoneda Lemma. Categories of presheaves are cartesian closed. \nMonads. Modelling notions of computation as monads. Moggi's computational lambda calculus. \nObjectives \nOn completion of this module, students should: \n \n\nbe familiar with some of the basic notions of category theory and its connections with logic and \nprogramming language semantics \nAssessment \na graded exercise sheet (25% of the final mark), and \na take-home test (75%) \nRecommended reading \nAwodey, S. (2010). Category theory. Oxford University Press (2nd ed.). \n \nCrole, R. L. (1994). Categories for types. Cambridge University Press. \n \nLambek, J. and Scott, P. J. (1986). Introduction to higher order categorical logic. Cambridge \nUniversity Press. \n \nPitts, A. M. (2000). Categorical Logic. Chapter 2 of S. Abramsky, D. M. Gabbay and T. S. E. \nMaibaum (Eds) Handbook of Logic in Computer Science, Volume 5. Oxford University Press. \n(Draft copy available here.) \n \nClass Size \nThis module can accommodate upto 15 Part II students plus 15 MPhil / Part III students. \n \nPre-registration evaluation form  \nStudents who are interested in taking this module will be asked to complete a pre-registration \nevaluation form to determine whether they have enough mathematical background to take the \nmodule.  \n \nThis will take place during the week Monday 12 August - Friday 16 August.  \n \nCoursework \nThe coursework in this module will consist of a graded exercise sheet. \n \nFurther Information \nCurrent Cambridge undergraduate students who are continuing onto Part III or the MPhil in \nAdvanced Computer Science may only take this module if they did NOT take 'Catgeory Theory' \nas a Unit of Assessment in Part II. \n \nThis module is shared with Part II of the Computer Science Tripos. Assessment will be adjusted \nfor the two groups of students to be at an appropriate level for whichever course the student is \nenrolled on. Further information about assessment and practicals will follow at the first lecture.\n\nDIGITAL MONEY AND DECENTRALISED FINANCE \n \nAims \nOur society is evolving towards digital payments and the elimination of physical cash. Digital \nalternatives to cash require trade-offs between various properties including unforgeability, \ntraceability, divisibility, transferability without intermediaries, privacy, redeemability, control over \nthe money supply and many more, often in conflict with each other. Since the 1980s, \ncryptographers have proposed a variety of clever technical solutions addressing some of these \nissues but it is only with the appearance of Bitcoin, blockchain and a plethora of copycat \ncryptocurrencies that a new asset class has now emerged, worth in excess of two trillion dollars \n(although plagued by extreme volatility). Meanwhile, most major countries have been planning \nthe introduction of Central Bank Digital Currencies in an attempt to retain control. This \nseminar-style interactive module encourages the students to understand what's happening, \nwhat the underlying problems and opportunities are (technical, social and economic) and where \nwe should go from here. We are at a historical turning point where an enterprising candidate \nmight disrupt the status quo and truly make a difference. \n \nSyllabus \nUnderstanding money: from gold standard to digital cash \nIntro to the Decentralised Finance (DeFi) ecosystem: Bitcoin, blockchain, wallets, smart \ncontracts \nStablecoins and Central Bank Digital Currencies \nDecentralised Autonomous Organisations and Automated Market Makers \nYield Farming, Perpetual Futures and Maximal Extractable Value \nWeb3, Non Fungible Tokens \nCryptocurrency crashes; crimes facilitated by cryptocurrencies \nNew areas of development. Where from here? \nThe module is structured as a reading club with a high degree of interactivity. Aside from the first \nand last session, which are treated differently, the regular two-hour sessions have the following \nstructure (not necessarily in this order). \n \nTwo half-hour debates on each of two papers \nTwo rapid-fire presentations on open questions previously assigned by the lecturers. \nHalf an hour of presentation by the lecturers. \nAbout ten minutes of buffer that can account for switchover time or be absorbed into the \nlecturers' presentation. \nThe first session is scene-setting by the lecturers, with no student presentations. The last \nsession has no paper debates, only one rapid-fire presentation on an open question from each \nof the students. \n \nObjectives \nOn completion of this module, students will gain an in-depth understanding of digital money \nalternatives as well as many modern applications and components of Decentralised Finance. By \ncomparing DeFi with current processes in traditional banking, students will be able to discuss \n\nthe merits and limitations of these new technologies as well as to identify potential new areas of \ndevelopment. \n \nThrough having to write, present and defend very brief extended abstracts, the students will \nimprove their communication skills and in particular the ability to distil and convey the most \nimportant points forcefully and concisely. \n \n \nAssessment \nThe module is an interactive reading club. Each student produces four output triplets throughout \nthe course. Each output triplet consists of three parts: a 1200-word essay, a 200-word extended \nabstract and a 7-minute presentation. \n \nThe essay and abstract must be submitted in advance, using the LaTeX templates provided. \nThe abstract must consist of full sentences, not bullet points, and must fit on a single slide. \nDuring the presentation, the slide with the abstract will be projected on screen as the only visual \naid; the essay will not be accessible to either presenter or audience. Reading out the slide \nverbatim will obviously not count as a great presentation. Presenters must be able to expand \nand defend their ideas live, and answer questions from lecturers and audience. \n \nThe essays, the abstracts and the presentations are graded separately, each out of 10, and \nassigned weights of 60%, 20%, 20%, respectively, within the triplet. \n \nIn a regular 2-hour session (all but the first and last) there will be two paper slots and two \ninvestigation slots, plus a lecturer slot, described next. \n \nEach paper slot (30 min) involves a \u201csupporter\u201d and a \u201cdissenter\u201d for the paper, each \ncontributing one output triplet, and then a panel Q&A where all the other participants quiz the \nsupporter and dissenter. \n \nEach investigation slot (10 min) involves a single student contributing an output triplet on a \ngiven theme. \n \nRoles, papers and themes are assigned by the lecturers the previous week. \n \nIn the first session there will be no student presentations, only lecturer-led exploration. In the \nlast session there will be 12 investigation slots and no paper slots. \n \nOverall, each student will be supporter once, dissenter once and investigator twice, for a total of \n4 output triplets. The output triplets of the last session will be weighed twice as much as the \nothers, i.e. 40% each, while the others 20% each. \n \nAttendance is required at all sessions. Missing a session in which the candidate was due to \npresent will result in the loss of the corresponding presentation marks, while the written work will \n\nstill have to be submitted and will be marked as usual. Missing a session in which the candidate \nwas not due to present will result in the loss of 3%. \n \nRecommended reading \nThere isn't a textbook covering all the material in this course. The following textbook, also \nadopted by R47 Distributed Ledger Technologies (also freely available online), is a thorough \nintroduction to Bitcoin and Blockchain, but we will complement it with research papers and \nindustry white papers for each lecture. \n \nNarayanan, A. , Bonneau, J., Felten, E., Miller, A. and Goldfeder, S. (2016). Bitcoin and \nCryptocurrency Technologies: A Comprehensive Introduction. Princeton University Press. \n(2016 Draft available here: \nhttps://d28rh4a8wq0iu5.cloudfront.net/bitcointech/readings/princeton_bitcoin_book.pdf)\n \n\nDIGITAL SIGNAL PROCESSING \n \nAims \nThis course teaches basic signal-processing principles necessary to understand many modern \nhigh-tech systems, with examples from audio processing, image coding, radio communication, \nradar, and software-defined radio. Students will gain practical experience from numerical \nexperiments in programming assignments (in Julia, MATLAB or NumPy). \n \nLectures \nSignals and systems. Discrete sequences and systems: types and properties. Amplitude, \nphase, frequency, modulation, decibels, root-mean square. Linear time-invariant systems, \nconvolution. Some examples from electronics, optics and acoustics. \nPhasors. Eigen functions of linear time-invariant systems. Review of complex arithmetic. \nPhasors as orthogonal base functions. \nFourier transform. Forms and properties of the Fourier transform. Convolution theorem. Rect \nand sinc. \nDirac\u2019s delta function. Fourier representation of sine waves, impulse combs in the time and \nfrequency domain. Amplitude-modulation in the frequency domain. \nDiscrete sequences and spectra. Sampling of continuous signals, periodic signals, aliasing, \ninterpolation, sampling and reconstruction, sample-rate conversion, oversampling, spectral \ninversion. \nDiscrete Fourier transform. Continuous versus discrete Fourier transform, symmetry, linearity, \nFFT, real-valued FFT, FFT-based convolution, zero padding, FFT-based resampling, \ndeconvolution exercise. \nSpectral estimation. Short-time Fourier transform, leakage and scalloping phenomena, \nwindowing, zero padding. Audio and voice examples. DTFM exercise. \nFinite impulse-response filters. Properties of filters, implementation forms, window-based FIR \ndesign, use of frequency-inversion to obtain high-pass filters, use of modulation to obtain \nband-pass filters. \nInfinite impulse-response filters. Sequences as polynomials, z-transform, zeros and poles, some \nanalog IIR design techniques (Butterworth, Chebyshev I/II, elliptic filters, second-order cascade \nform). \nBand-pass signals. Band-pass sampling and reconstruction, IQ up and down conversion, \nsuperheterodyne receivers, software-defined radio front-ends, IQ representation of AM and FM \nsignals and their demodulation. \nDigital communication. Pulse-amplitude modulation. Matched-filter detector. Pulse shapes, \ninter-symbol interference, equalization. IQ representation of ASK, BSK, PSK, QAM and FSK \nsignals. [2 hours] \nRandom sequences and noise. Random variables, stationary and ergodic processes, \nautocorrelation, cross-correlation, deterministic cross-correlation sequences, filtered random \nsequences, white noise, periodic averaging. \nCorrelation coding. Entropy, delta coding, linear prediction, dependence versus correlation, \nrandom vectors, covariance, decorrelation, matrix diagonalization, eigen decomposition, \n\nKarhunen-Lo\u00e8ve transform, principal component analysis. Relation to orthogonal transform \ncoding using fixed basis vectors, such as DCT. \nLossy versus lossless compression. What information is discarded by human senses and can \nbe eliminated by encoders? Perceptual scales, audio masking, spatial resolution, colour \ncoordinates, some demonstration experiments. \nQuantization, image coding standards. Uniform and logarithmic quantization, A/\u00b5-law coding, \ndithering, JPEG. \nObjectives \napply basic properties of time-invariant linear systems; \nunderstand sampling, aliasing, convolution, filtering, the pitfalls of spectral estimation; \nexplain the above in time and frequency domain representations; \nuse filter-design software; \nvisualize and discuss digital filters in the z-domain; \nuse the FFT for convolution, deconvolution, filtering; \nimplement, apply and evaluate simple DSP applications; \nfamiliarity with a number of signal-processing concepts used in digital communication systems \nRecommended reading \nLyons, R.G. (2010). Understanding digital signal processing. Prentice Hall (3rd ed.). \nOppenheim, A.V. and Schafer, R.W. (2007). Discrete-time digital signal processing. Prentice \nHall (3rd ed.). \nStein, J. (2000). Digital signal processing \u2013 a computer science perspective. Wiley. \n \nClass size \nThis module can accommodate a maximum of 24 students (16 Part II students and 8 MPhil \nstudents) \n \nAssessment - Part II students \nThree homework programming assignments, each comprising 20% of the mark \nWritten test, comprising 40% of the total mark. \nAssessment - Part III and MPhil students \nThree homework programming exercises, each comprising 10% of the mark. \nWritten test, comprising 20% of the total mark. \nIndividual implementation project with 8-page written report, topic agreed with lecturer, \ncomprising 50% of the mark. \nFurther Information \nCurrent Cambridge undergraduate students who are continuing onto Part III or the MPhil in \nAdvanced Computer Science may only take this module if they did NOT take it as a Unit of \nAssessment in Part II. \n \nThis module is shared with Part II of the Computer Science Tripos. Assessment will be adjusted \nfor the two groups of students to be at an appropriate level for whichever course the student is \nenrolled on. Further information about assessment and practicals will follow at the first lecture.\n\nINTRODUCTION TO COMPUTATIONAL SEMANTICS \n \nAims \nThis is a lecture-style course that introduces students to various aspects of the semantics of \nNatural Languages (mainly English): \n \nLexical Semantics, with an emphasis on theory and phenomenology  \nCompositional Semantics \nDiscourse and pragmatics-related aspects of semantics \nObjectives \nGive an operational definition of what is meant by \u201cmeaning\u201d (for instance, above and beyond \nsyntax); \nName the types of phenomena in language that require semantic consideration, in terms of \nlexical, compositional and discourse/pragmatic aspects, in other words, argue why semantics is \nimportant; \nDemonstrate an understanding of the basics of various semantic representations, including \nlogic-based and graph-based semantic representations, their properties, how they are used and \nwhy they are important, and how they are different from syntactic representations; \nKnow how such semantic representations are derived during or after parsing, and how they can \nbe analysed and mapped to surface strings; \nUnderstand applications of semantic representations e.g. reasoning, validation, and methods \nhow these are approached. \nWhen designing NL tasks that clearly require semantic processing (e.g. knowledge-based QA), \nto be aware of and reuse semantic representations and algorithms when designing the task, \nrather than reinventing the wheel. \nPractical advantages of this course for NLP students \nKnowledge of underlying semantic effects helps improve NLP evaluation, for instance by \nproviding more meaningful error analysis. You will be able to link particular errors to design \ndecisions inside your system. \nYou will learn methods for better benchmarking of your system, whatever the task may be. \nSupervised ML systems (in particular black-box systems such as Deep Learning) are only as \nclever as the datasets they are based on. In this course, you will learn to design datasets so that \nthey are harder to trick without real understanding, or critique existing datasets. \nYou will be able to design tests for ML systems that better pinpoint which aspects of language \nan end-to-end system has \u201cunderstood\u201d. \nYou will learn to detect ambiguity and ill-formed semantics in human-human communication. \nThis can serve to write more clearly and logically. \nYou will learn about decomposing complex semantics-reliant tasks sensibly so that you can \nreuse the techniques underlying semantic analyzers in a modular way. In this way, rather than \nbeing forced to treat complex tasks in an end-to-end manner, you will be able to profit from \npartial explanations and a better error analysis already built into the system. \nSyllabus \nOverview of the course \nEvents and semantic role labelling \n\nReferentiality and coreference resolution \nTruth-conditional semantics \nGraph-based meaning representation \nCompositional semantics \nContext-free Graph Rewriting \nSurface realisation \nNegation and psychological approach to semantics \nDynamic semantics \nGricean pragmatics \nVector space models \nCross-modality \nAcquisition of semantics \nDiachronic change of semantics \nSummary of the course \n  \n \nAssessment \n5 take-home exercises worth 20% each: \n \nTake-home assignment 1 is given in Lecture 4 and due is Lecture 6 \n \nStudents are given 10 English sentences and asked to provide their semantic analysis \naccording to truth-conditions. Students will be expected to return 10 logical expressions as their \nanswers. No word limit. Assessment criteria: correctness of the 10 logical expressions; 2 points \nfor each logical expression. \n \nTake-home assignment 2 is given in Lecture 7 and due is Lecture 9  \n \nStudents are given 10 English sentences and asked to provide their syntactico-semantic \nderivations according to the compositionality principle. Students will be expected to return \nderivation graphs as their answers. No word limit. Assessment criteria: correctness of the 10 \nderivation graphs; 2 points for each derivation. \n \nTake-home assignment 3 is given in Lecture 11 and due is Lecture 13  \n \nAll students are assigned with a paper on modelling common ground in dialogue system. \nStudents will receive related but different papers. Each student will write a review of their \nassigned paper, including a comprehensive summary and their own thoughts. Word limit: 1000 \nwords. Assessment criteria: 15 points on whether a student understands the paper correctly; 5 \npoints on whether a student is able to think critically. \n \nTake-home assignment 4 is given in Lecture 13 and due is Lecture 15 \n \n\nAll students are assigned with a paper on language-vision interaction. Students will receive \nrelated but different papers. Each student will write a review of their assigned paper, including a \ncomprehensive summary and their own thoughts. Word limit: 1000 words. Assessment criteria: \n15 points on whether a student understands the paper correctly; 5 points on whether a student \nis able to think critically. \n \nTake-home assignment 5 is given in Lecture 14 and due is two weeks later. \n \nContent: All students are assigned with a paper on bootstrapping language acquisition. All \nstudents will receive the same paper. Each student will write a review of the paper, including a \ncomprehensive summary and their own thoughts. Word limit: 1000 words. Assessment criteria: \n15 points on whether a student understands the paper correctly; 5 points on whether a student \nis able to think critically.\n \n\nINTRODUCTION TO NATURAL LANGUAGE SYNTAX AND PARSING \n \nAims \nThis module aims to provide a brief introduction to linguistics for computer scientists and then \ngoes on to cover some of the core tasks in natural language processing (NLP), focussing on \nstatistical parsing of sentences to yield syntactic representations. We will look at how to \nevaluate parsers and see how well state-of-the-art tools perform given current techniques. \n \nSyllabus \nLinguistics for NLP focusing on morphology and syntax \nGrammars and representations \nStatistical and neural parsing \nTreebanks and evaluation \nObjectives \nOn completion of this module, students should: \n \nunderstand the basic properties of human languages and be familiar with descriptive and \ntheoretical frameworks for handling these properties; \nunderstand the design of tools for NLP tasks such as parsing and be able to apply them to text \nand evaluate their performance; \nPractical work \nWeek 1-7: There will be non-assessed practical exercises between sessions and during \nsessions. \nWeek 8: Download and apply two parsers to a designated text. Evaluate the performance of the \ntools quantitatively and qualitatively. \nAssessment \nThere will be one presentation worth 10% of the final mark; presentation topics will be allocated \nat the start of term. \nAn assessed practical report of not more 8 pages in ACL conference format. It will contribute \n90% of the final mark. \nRecommended reading \nJurafsky, D. and Martin, J. (2008). Speech and Language Processing. Prentice-Hall (2nd ed.). \n(See also 3rd ed. available online.)\n \n\nINTRODUCTION TO NETWORKING AND SYSTEMS MEASUREMENTS \n \nAims \nSystems research refers to the study of a broad range of behaviours arising from complex \nsystem design, including: resource sharing and scheduling; interactions between hardware and \nsoftware; network topology, protocol and device design and implementation; low-level operating \nsystems; Interconnect, storage and more. This module will: \n \nTeach performance characteristics and performance measurement methodology and practice \nthrough profiling experiments; \nExpose students to real-world systems artefacts evident through different measurement tools; \nDevelop scientific writing skills through a series of laboratory reports; \nProvide research skills for characterization and modelling of systems and networks using \nmeasurements. \nPrerequisites \nIt is strongly recommended that students have previously (and successfully) completed an \nundergraduate networking course -- or have equivalent experience through project or \nopen-source work. \n \nSyllabus \nIntroduction to performance measurements, performance characteristics [1 lecture] \nPerformance measurements tools and techniques [2 lectures + 2 lab sessions] \nReproducible experiments [1 lecture + 1 lab session] \nCommon pitfalls in measurements [1 lecture] \nDevice and system characterisation [1 lecture + 2 lab sessions] \nObjectives \nOn completion of this module, students should: \n \nDescribe the objectives of measurements, and what they can achieve; \nCharacterise and model a system using measurements; \nPerform reproducible measurements experiments; \nEvaluate the performance of a system using measurements; \nOperate measurements tools and be aware of their limitations; \nDetect anomalies in the network and avoid common measurements pitfalls; \nWrite system-style performance evaluations. \nPractical work \nFive 2-hour in-classroom labs will ask students to develop and use skills in performance \nmeasurements as applied to real-world systems and networking artefacts. Results from these \nlabs (and follow-up work by students outside of the classroom) will by the primary input to lab \nreports. \n \nThe first three labs will provide an introduction and hands on experience with measurement \ntools and measurements methodologies, while the last two labs will focus on practical \nmeasurements and evaluation of specific platforms. Students may find it useful to work in pairs \n\nwithin the lab, but must prepare lab reports independently. The module lecturer will give a short \nintroductory at the start of each lab, and instructors will be on-hand throughout labs to provide \nassistance. \n \nLab participation is not directly included in the final mark, but lab work is a key input to lab \nreports that are assessed. Guided lab experiments resulting in practical write ups. \n \nAssessment \nEach student will write two lab reports. The first lab report will summarise the experiments done \nin the first three labs (20%). The second will be a lab report (5000 words) summarising the \nevaluation of a device or a system (80%). \n \nRecommended reading \nThe following list provides some background to the course materials, but is not mandatory. A \nreading list, including research papers, will be provided in the course materials. \n \nGeorge Varghese. Network algorithmics. Chapman and Hall/CRC, 2010. \nMark Crovella and Balachander Krishnamurthy. Internet measurement: infrastructure, traffic and \napplications. John Wiley and Sons, Inc., 2006. \nBrendan Gregg. Systems Performance: Enterprise and the Cloud, Prentice Hall Press, Upper \nSaddle River, NJ, USA, October 2013. \nRaj Jain, The Art of Computer Systems Performance Analysis: Techniques for Experimental \nDesign, Measurement, Simulation, and Modeling, Wiley - Interscience, New York, NY, USA, \nApril 1991.\n \n\nLARGE-SCALE DATA PROCESSING AND OPTIMISATION \n \nAims \nThis module provides an introduction to large-scale data processing, optimisation, and the \nimpact on computer system's architecture. Large-scale distributed applications with high volume \ndata processing such as training of machine learning will grow ever more in importance. \nSupporting the design and implementation of robust, secure, and heterogeneous large-scale \ndistributed systems is essential. To deal with distributed systems with a large and complex \nparameter space, tuning and optimising computer systems is becoming an important and \ncomplex task, which also deals with the characteristics of input data and algorithms used in the \napplications. Algorithm designers are often unaware of the constraints imposed by systems and \nthe best way to consider these when designing algorithms with massive volume of data. On the \nother hand, computer systems often miss advances in algorithm design that can be used to cut \ndown processing time and scale up systems in terms of the size of the problem they can \naddress. Integrating machine learning approaches (e.g. Bayesian Optimisation, Reinforcement \nLearning) for system optimisation will be explored in this course. \n \nSyllabus \nThis course provides perspectives on large-scale data processing, including data-flow \nprogramming, graph data processing, probabilistic programming and computer system \noptimisation, especially using machine learning approaches, thus providing a solid basis to work \non the next generation of distributed systems. \n \nThe module consists of 8 sessions, with 5 sessions on specific aspects of large-scale data \nprocessing research. Each session discusses 3-4 papers, led by the assigned students. One \nsession is a hands-on tutorial on on dataflow programming fundamentals. The first session \nadvises on how to read/review a paper together with a brief introduction on different \nperspectives in large-scale data \nprocessing and optimisation. The last session is dedicated to the student presentation of \nopensource project studies. \n \nIntroduction to large-scale data processing and optimisation \nData flow programming: Map/Reduce to TensorFlow \nLarge-scale graph data processing: storage, processing model and parallel processing \nDataflow programming hands-on tutorial \nProbabilistic Programming \nOptimisations in ML Compiler \nOptimisations of Computer systems using ML \nPresentation of Open Source Project Study \nObjectives \nOn completion of this module, students should: \n \nUnderstand key concepts of scalable data processing approaches in future computer systems. \n\nObtain a clear understanding of building distributed systems using data centric programming \nand large-scale data processing. \nUnderstand a large and complex parameter space in computer system's optimisation and \napplicability of Machine Learning approach. \nCoursework \nReading Club: \nThe preparation for the reading club will involve 1-3 papers every week. At each session, \naround 3-4 papers are selected under the given topic, and the students present their review \nwork. \nHands-on tutorial session of data flow programming including writing an application of \nprocessing streaming in Twitter data and/or Deep Neural Networks using Google TensorFlow \nusing cluster computing. \nReports \nThe following three reports are required, which could be extended from the assignment of the \nreading club, within the scope of data centric systems. \n \nReview report on a full length paper (max 1800 words) \nDescribe the contribution of the paper in depth with criticisms \nCrystallise the significant novelty in contrast to other related work \nSuggestions for future work \nSurvey report on sub-topic in large-scale data processing and optimisation (max 2000 words) \nPick up to 5 papers as core papers in the survey scope \nRead the above and expand reading through related work \nComprehend the view and finish an own survey paper \nProject study and exploration of a prototype (max 2500 words) \nWhat is the significance of the project in the research domain? \nCompare with similar and succeeding projects \nDemonstrate the project by exploring its prototype \nReports 1 should be handed in by the end of 5th week; Report 2 in the 8th week and Report 3 \non the first day of Lent Term. \n \nAssessment \nThe final grade for the course will be provided as a percentage, and the assessment will consist \nof two parts: \n \n25%: for reading club (participation, presentation) \n75%: for the three reports: \n15%: Intensive review report \n25%: Survey report \n35%: Project study \nRecommended reading \nM. Abadi et al. TensorFlow: A System for Large-Scale Machine Learning, OSDI, 2016. \nD. Aken et al.: Automatic Database Management System Tuning Through Large-scale Machine \nLearning, SIGMOD, 2017. \n\nJ. Ansel et al. Opentuner: an extensible framework for program autotuning. PACT, 2014. \nV. Dalibard, M. Schaarschmidt, E. Yoneki. BOAT: Building Auto-Tuners with Structured Bayesian \nOptimization, WWW, 2017. \nJ. Dean et al. Large scale distributed deep networks. NIPS, 2012. \nG. Malewicz, M. Austern, A. Bik, J. Dehnert, I. Horn, N. Leiser, G. Czajkowski. Pregel: A System \nfor Large-Scale Graph Processing, SIGMOD, 2010. \nA. Mirhoseini et al. Device Placement Optimization with Reinforcement Learning, ICML, 2017. \nD. Murray, F. McSherry, R. Isaacs, M. Isard, P. Barham, M. Abadi. Naiad: A Timely Dataflow \nSystem, SOSP, 2013. \nM. Schaarschmidt, S. Mika, K. Fricke and E. Yoneki: RLgraph: Modular Computation Graphs for \nDeep Reinforcement Learning, SysML, 2019. \nZ. Jia, O. Padon, J. Thomas, T. Warszawski, M. Zaharia,  A. Aiken: TASO: Optimizing Deep \nLearning Computation with Automated Generation of Graph Substitutions: SOSP, 2019. \nH. Mao et al.: Park: An Open Platform for Learning-Augmented Computer Systems, \nOpenReview, 2019. \nJ. Shao, et al.: Tensor Program Optimization with Probabilistic Programs, NeurIPS, 2022.  \nA complete list can be found on the course material web page. See also 2023-2024 course \nmaterial on the previous course Large-Scale Data Processing and Optimisation.\n \n\nMACHINE LEARNING AND THE PHYSICAL WORLD \nAims \nThe module \u201cMachine Learning and the Physical World\u201d is focused on machine learning \nsystems that interact directly with the real world. Building artificial systems that interact with the \nphysical world have significantly different challenges compared to the purely digital domain. In \nthe real world data is scares, often uncertain and decisions can have costly and irreversible \nconsequences. However, we also have the benefit of centuries of scientific knowledge that we \ncan draw from. This module will provide the methodological background to machine learning \napplied in this scenario. We will study how we can build models with a principled treatment of \nuncertainty, allowing us to leverage prior knowledge and provide decisions that can be \ninterrogated. \n \nThere are three principle points about machine learning in the real world that will concern us. \n \nWe often have a mechanistic understanding of the real world which we should be able to \nbootstrap to make decisions. For example, equations from physics or an understanding of \neconomics. \nReal world decisions have consequences which may have costs, and often these cost functions \nneed to be assimilated into our machine learning system. \nThe real world is surprising, it does things that you do not expect and accounting for these \nchallenges requires us to build more robust and or interpretable systems. \nDecision making in the real world hasn\u2019t begun only with the advent of machine learning \ntechnologies. There are other domains which take these areas seriously, physics, environmental \nscientists, econometricians, statisticians, operational researchers. This course identifies how \nmachine learning can contribute and become a tool within these fields. It will equip you with an \nunderstanding of methodologies based on uncertainty and decision making functions for \ndelivering on these challenges. \n \nObjectives \nYou will gain detailed knowledge of \n \nsurrogate models and uncertainty \nsurrogate-based optimization \nsensitivity analysis \nexperimental design \nYou will gain knowledge of \n \ncounterfactual analysis \nsurrogate-based quadrature \nSchedule \nWeek 1: Introduction to the unit and foundation of probabilistic modelling \n \nWeek 2: Gaussian processes and probablistic inference \n \n\nWeek 3: Simulation and Sequential decision making under uncertainty \n \nWeek 4: Emulation and Experimental Design \n \nWeek 5: Sensitivity Analysis and Multifidelity Modelling \n \nWeek 6-8: Case studies of applications and Projects \n \nPractical work \nDuring the first five weeks of the unit we will provide a weekly worksheet that will focus on \nimplementation and practical exploration of the material covered in the lectures. The worksheets \nwill allow you to build up a these methods without relying on extensive external libraries. You are \nfree to use any programming language of choice however we highly recommended the use of \n=Python=. \n \nAssessment \nTwo individual tasks (15% each) \nGroup Project (70%). Each group will work on an application of uncertainty that covers the \nmaterial of the first 5 weeks of lectures in the unit. Each group will submit a report which will \nform the basis of the assessment. In addition to the report each group will also attend a short \noral examination based on the material covered both in the report and the taught material. \nRecommended Reading \nRasmussen, C. E. and Williams, C. K. I. (2006). Gaussian Processes for Machine Learning. MIT \nPress \n \nBishop, C. (2006). Pattern recognition and machine learning. Springer. \nhttps://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-an\nd-Machine-Learning-2006.pdf \n \nLaplace, P. S. (1902). A Philosophical Essay on Probabilities. John Wiley & Sons. \nhttps://archive.org/details/philosophicaless00lapliala\n \n\nMACHINE VISUAL PERCEPTION \nAims \nThis course aims at introducing the theoretical foundations and practical techniques for machine \nperception, the capability of computers to interpret data resulting from sensor measurements. \nThe course will teach the fundamentals and modern techniques of machine perception, i.e. \nreconstructing the real world starting from sensor measurements with a focus on machine \nperception for visual data. The topics covered will be image/geometry representations for \nmachine perception, semantic segmentation, object detection and recognition, geometry \ncapture, appearance modeling and acquisition, motion detection and estimation, \nhuman-in-the-loop machine perception, select topics in applied machine perception. \n \nMachine perception/computer vision is a rapidly expanding area of research with real-world \napplications. An understanding of machine perception is also important for robotics, interactive \ngraphics (especially AR/VR), applied machine learning, and several other fields and industries. \nThis course will provide a fundamental understanding of and practical experience with the \nrelevant techniques. \n \nLearning outcomes \nStudents will understand the theoretical underpinnings of the modern machine perception \ntechniques for reconstructing models of reality starting from an incomplete and imperfect view of \nreality. \nStudents will be able to apply machine perception theory to solve practical problems, e.g. \nclassification of images, geometry capture. \nStudents will gain an understanding of which machine perception techniques are appropriate for \ndifferent tasks and scenarios. \nStudents will have hands-on experience with some of these techniques via developing a \nfunctional machine perception system in their projects. \nStudents will have practical experience with the current prominent machine perception \nframeworks. \nSyllabus \nThe fundamentals of machine learning for machine perception \nDeep neural networks and frameworks for machine perception \nSemantic segmentation of objects and humans \nObject detection and recognition \nMotion estimation, tracking and recognition \n3D geometry capture \nAppearance modeling and acquisition \nSelect topics in applied machine perception \nAssessment \nA practical exercise, worth 20% of the mark. This will cover the basics and theory of machine \nperception and some of the practical techniques the students will likely use in their projects. This \nis individual work. No GPU hours will be needed for the practical work. \nA machine perception project worth 80% of the marks: \n\nCourse projects will be selected by the students following the possible project themes proposed \nby the lecturer, and will be checked by the lecturer for appropriateness.  \nThe students will form groups of 2-3 to design, implement, report, and present a project to tackle \na given task in machine perception. \nThe final mark will be composed of an implementation/report mark (60%) and a presentation \nmark (20%). Each team member will be evaluated based on her/his contribution. \nEach project will have extensions to be completed only by the ACS students. Each student will \nwrite a different part of the report, whose author will be clearly marked. Each student will further \nsummarise her/his contributions to the project in the same report. \nRecommended Reading List \nComputer Vision: Algorithms and Applications, Richard Szeliski, Springer, 2010. \nDeep Learning, Ian Goodfellow, Yoshua Bengio, and Aaron Courville, MIT Press, 2016. \nMachine Learning and Visual Perception, Baochang Zhang, De Gruyter, 2020. \nFurther Information \nCurrent Cambridge undergraduate students who are continuing onto Part III or the MPhil in \nAdvanced Computer Science may only take this module if they did NOT take it as a Unit of \nAssessment in Part II. \n \nThis module is shared with Part II of the Computer Science Tripos. Assessment will be adjusted \nfor the two groups of students to be at an appropriate level for whichever course the student is \nenrolled on. Further information about assessment and practicals will follow at the first lecture.\n\nMOBILE, WEARABLE SYSTEMS AND MACHINE LEARNING \n \nAims \nThis module aims to introduce the latest research advancements in mobile systems and mobile \ndata machine learning, spanning a range of domains including systems, data gathering, \nanalytics and machine learning, on device machine learning and applications such as health, \ntransportation, behavior monitoring, cyber-physical systems, autonomous vehicles, drones. The \ncourse will cover current and seminal research papers in the area of research. \n \nSyllabus \nThe course will consist of one introductory lecture, seven two-hour, and one three-hour, \nsessions covering a variety of topics roughly including the following material (some variation in \nthe topics might happen from year to year): \n \nSystem, Energy and Security \nBackscatter Communication, Battery Free and Energy Harvesting Devices \nNew Sensing Modalities \nMachine Learning on Wearable Data \nOn Device Machine Learning \nMobile and Wearable Health \nMobile and Wearable Systems of Sustainability \nEach week, three class participants will be assigned to introduce assigned three papers via \n20-minute presentations, conference-style and highlighting critically its features. Each \npresentation will be followed by 10 minutes of questions. This will be followed by 10 minutes of \ngeneral discussion. Slides will be used for presentation. \n \nStudents will give one or more presentations each term. Each student will submit a paper review \neach week for one of the three papers presented except for the week they will be presenting \nslides. Each review will follow a template and be up to 1,000 words. Each review will receive a \nmaximum of 10 points. As a result, each student will produce 6-7 reviews and at least one \npresentation, probably two. All participants are expected to attend and participate in every class; \nthe instructor must be notified of any absences in advance. \n \nObjectives \nOn completion of this module students should have an understanding of the recent key research \nin mobile and sensor systems and mobile analytics as well as an improved critical thinking over \nresearch papers. \n \nAssessment \nAggregate mark for 7 assignments from 6-7 reports and 1-2 presentations. Each report or \npresentation will contribute one seventh of the course mark. \nA tick for presence and participation to each class will also be awarded. \nRecommended reading \n\nReadings from the most recent conferences such as AAAI, ACM KDD, ACM MobiCom, ACM \nMobiSys, ACM SenSys, ACM UbiComp, ICLR, ICML Neurips, and WWW pertinent to mobile \nsystems and data.\n \n\nNETWORK ARCHITECTURES \nAims \nThis module aims to provide the world with more network architects. The 2011-2012 version \nwas oriented around the evolution of IP to support new services like multicast, mobility, \nmultihoming, pub/sub and, in general, data oriented networking. The course is a paper reading \nwhich puts the onus on the student to do the work. \n \nSyllabus \nIPng [2 lectures, Jon Crowcroft] \nNew Architectures [2 lectures, Jon Crowcroft] \nMulticast [2 lectures, Jon Crowcroft] \nContent Distribution and Content Centric Networks [2 lectures, Jon Crowcroft] \nResource Pooling [2 lectures, Jon Crowcroft] \nGreen Networking [2 lectures, Jon Crowcroft] \nAlternative Router Implementions [2 lectures, Jon Crowcroft] \nData Center Networks [2 Lectures, Jon Crowcroft] \nObjectives \nOn completion of this module, students should be able to: \n \ncontribute to new network system designs; \nengineer evolutionary changes in network systems; \nidentify and repair architectural design flaws in networked systems; \nsee that there are no perfect solutions (aside from academic ones) for routing, addressing, \nnaming; \nunderstand tradeoffs in modularisation and other pressures on clean software systems \nimplementation, and see how the world is changing the proper choices in protocol layering, or \nnon layered or cross-layered. \nCoursework \nAssessment is through three graded essays (each chosen individually from a number of \nsuggested or student-chosen topics), as follows: \n \nAnalysis of two different architectures for a particular scenario in terms of cost/performance \ntradeoffs for some functionality and design dimension, for example: \nATM \u2013 e.g. for hardware versus software tradeoff \nIP \u2013 e.g. for mobility, multi-homing, multicast, multipath \n3GPP \u2013 e.g. for plain complexity versus complicatedness \nA discursive essay on a specific communications systems component, in a particular context, \nsuch as ad hoc routing, or wireless sensor networks. \nA bespoke network design for a narrow, well specified specialised target scenario, for example: \nA customer baggage tracking network for an airport. \nin-flight entertainment system. \nin-car network for monitoring and control. \ninter-car sensor/control network for automatic highways. \nPractical work \n\nThis course does not feature any implementation work due to time constraints. \n \nAssessment \nThree 1,200-word essays (worth 25% each), and \nan annotated bibliography (25%). \nRecommended reading \nPre-course reading: \n \nKeshav, S. (1997). An engineering approach to computer networking. Addison-Wesley (1st ed.). \nISBN 0201634422 \nPeterson, L.L. and Davie, B.S. (2007). Computer networks: a systems approach. Morgan \nKaufmann (4th ed.). \n \nDesign patterns: \n \nDay, John (2007). Patterns in network architecture: a return to fundamentals. Prentice Hall. \n \nExample systems: \n \nKrishnamurthy, B. and Rexford, J. (2001). Web protocols and practice: HTTP/1.1, Networking \nprotocols, caching, and traffic measurement. Addison-Wesley. \n \nEconomics and networks: \n \nFrank, Robert H. (2008). The economic naturalist: why economics explains almost everything. \n \nPapers: \n \nCertainly, a collection of papers (see ACM CCR which publishes notable network researchers' \nfavourite ten papers every 6 months or so).\n \n\nOVERVIEW OF NATURAL LANGUAGE PROCESSING \n \nAims \nThis course introduces the fundamental techniques of natural language processing. It aims to \nexplain the potential and the main limitations of these techniques. Some current research issues \nare introduced and some current and potential applications discussed and evaluated. Students \nwill also be introduced to practical experimentation in natural language processing. \n \nLectures \nOverview. Brief history of NLP research, some current applications, components of NLP \nsystems. \nMorphology and Finite State Techniques. Morphology in different languages, importance of \nmorphological analysis in NLP, finite-state techniques in NLP. \nPart-of-Speech Tagging and Log-Linear Models. Lexical categories, word tagging, corpora and \nannotations, empirical evaluation. \nPhrase Structure and Structure Prediction. Phrase structures, structured prediction, context-free \ngrammars, weights and probabilities. Some limitations of context-free grammars. \nDependency Parsing. Dependency structure, grammar-free parsing, incremental processing.  \nGradient Descent and Neural Nets. Parameter optimisation by gradient descent. Non-linear \nfunctions with neural network layers. Log-linear model as softmax layer. Current findings of \nNeural NLP. \nWord representations. Representing words with vectors, count-based and prediction-based \napproaches, similarity metrics. \nRecurrent Neural Networks. Modelling sequences, parameter sharing in recurrent neural \nnetworks, neural language models, word prediction. \nCompositional Semantics. Logical representations, compositional semantics, lambda calculus, \ninference and robust entailment. \nLexical Semantics. Semantic relations, WordNet, word senses. \nDiscourse. Discourse relations, anaphora resolution, summarization. \nNatural Language Generation. Challenges of natural language generation (NLG), tasks in NLG, \nsurface realisation. \nPractical and assignments. Students will build a natural language processing system which will \nbe trained and evaluated on supplied data. The system will be built from existing components, \nbut students will be expected to compare approaches and some programming will be required \nfor this. Several assignments will be set during the practicals for assessment. \nObjectives \nBy the end of the course students should: \n \nbe able to discuss the current and likely future performance of several NLP applications; \nbe able to describe briefly a fundamental technique for processing language for several \nsubtasks, such as morphological processing, parsing, word sense disambiguation etc.; \nunderstand how these techniques draw on and relate to other areas of computer science. \nRecommended reading \n\nJurafsky, D. & Martin, J. (2023). Speech and language processing. Prentice Hall (3rd ed. draft, \nonline). \n \nAssessment - Part II Students \nAssignment 1 - 10% of marks \nAssignment 2 - 60% of marks \nAssignment 3 - 30% of marks \nCoursework \nSubmit work for 3 assignments as part of practical sessions on information extraction. \n \nThese include an annotation exercise, a feature-based classifier along with code repository and \ndocumentation, and a 4,000-word report on results and analysis from an extended information \nextraction experiment. \n \nPractical work \nStudents will build a natural language processing system which will be trained and evaluated on \nsupplied data. The system will be built from existing components, but students will be expected \nto compare approaches and some programming will be required for this. \n \nAssessment - Part III and MPhil Students \nAssessment will be based on the practicals: \n \nFirst practical exercise (10%, ticked) \nSecond practical exercise (25%, code repository or notebook with documentation) \nFinal report (65%, 4,000 words, excluding references) \nFurther Information \nAlthough the lectures don't assume any exposure to linguistics, the course will be easier to \nfollow if students have some understanding of basic linguistic concepts. The following may be \nuseful for this: The Internet Grammar of English \n \nCurrent Cambridge undergraduate students who are continuing onto Part III or the MPhil in \nAdvanced Computer Science may only take this module if they did NOT take it as a Unit of \nAssessment in Part II. \n \nStudents from other departments may attend the lectures for this module if space allows. \nHowever students wanting to take it for credit will need to make arrangements for assessment \nwithin their own department. \n \nThis module is shared with Part II of the Computer Science Tripos. Assessment will be adjusted \nfor the two groups of students to be at an appropriate level for whichever course the student is \nenrolled on. Further information about assessment and practicals will follow at the first lecture.\n\nPRACTICAL RESEARCH IN HUMAN-CENTRED AI \n \nAims \nThis is an advanced course in human-computer interaction, with a specialist focus on intelligent \nuser interfaces and interaction with machine-learning and artificial intelligence technologies. The \nformat will be largely Practical, with students carrying out a mini-project involving empirical \nresearch investigation. These studies will investigate human interaction with some kind of \nmodel-based system for planning, decision-making, automation etc. Possible study formats \nmight include: System evaluation, Field observation, Hypothesis testing experiment, Design \nintervention or Corpus analysis, following set examples from recent research publications. \nProject work will be formally evaluated through a report and presentation. \n \nLectures \n(note that Lectures 2-7 also include one hour class discussion of practical work) \n        \u2022 Current research themes in intelligent user interfaces \n        \u2022 Program synthesis \n        \u2022 Mixed initiative interaction \n        \u2022 Interpretability / explainable AI \n        \u2022 Labelling as a fundamental problem \n        \u2022 Machine learning risks and bias \n        \u2022 Visualisation and visual analytics \n        \u2022 Student research presentations \n \nObjectives \nBy the end of the course students should: \n        \u2022 be familiar with current state of the art in intelligent interactive systems \n        \u2022 understand the human factors that are most critical in the design of such systems \n        \u2022 be able to evaluate evidence for and against the utility of novel systems \n        \u2022 have experience of conducting user studies meeting the quality criteria of this field \n        \u2022 be able to write up and present user research in a professional manner \n \nClass Size \nThis module can accommodate upto 20 Part II, Part III and MPhil students. \n \nRecommended reading \nBrad A. Myers and Richard McDaniel (2000). Demonstrational Interfaces: Sometimes You Need \na Little Intelligence, Sometimes You Need a Lot. \n \nAlan Blackwell (2024). Moral Codes: Designing alternatives to AI \n \nAssessment - Part II Students \nThe format will be largely practical, with students carrying out an individual mini-project involving \nempirical research investigation. \n \n\nAssignment 1: six incremental submissions which together contribute 20% to the final module \nmark. \n \nAssignment 2: Final report - 80% of the final module mark \n  \n \nAssessment - MPhil / Part III Students \nThere will be a minor assessment component (20%) in which students compile a reflective diary \nthroughout the term, reporting on the weekly sessions. Diary entries should include citations to \nany key references, notes of possible further reading, summary of key points, questions relevant \nto the personal project, and points of interest noted in relation to the work of other students. \n \nThe major assessment component (80%) will involve a report on research findings, in the style \nof a submission to the ACM CHI or IUI conferences. This work will be submitted incrementally \nthrough the term, in order that feedback can be provided before final assessment of the full \nreport. Phased submissions will cover the following aspects of the empirical study: \n \nResearch question \nMethod \nLiterature Review \nIntroduction \nResults \nDiscussion / Conclusion \nFeedback on each phased submission will include an indicative mark for guidance, but with the \nunderstanding that the final grade will be based on the final delivered report, and that this may \ngo up (or possibly down), depending on how well the student responds to earlier feedback. \n \nReflective diary entries will also be assessed, but graded at a relatively coarse granularity \ncorresponding to the ACS grading bands, and with minimal written feedback. Informal and \ngeneric feedback will be offered verbally in class, and also potentially supplemented with peer \nassessment. \n \nFurther Information \nCurrent Cambridge undergraduate students who are continuing onto Part III or the MPhil in \nAdvanced Computer Science may only take this module if they did NOT take it as a Unit of \nAssessment in Part II. \n \nThis module is shared with Part II of the Computer Science Tripos. Assessment will be adjusted \nfor the two groups of students to be at an appropriate level for whichever course the student is \nenrolled on. Further information about assessment and practicals will follow at the first lecture.\n\nPRINCIPLES OF MACHINE LEARNING SYSTEMS \n \nPrerequisites: It is recommended that students have successfully completed introductory \ncourses, at an undergraduate level, in: 1) operating systems, 2) computer architecture, and 3) \nmachine learning. In addition, this course will heavily focus on deep neural network \nmethodologies and assume familiarity with common neural architectures and related algorithms. \nIf these topics were not covered within the machine learning course taken, students should \nsupplement by reviewing material in a book like: \"Dive into Deep Learning\". Finally, students are \nassumed to be comfortable with programming in Python. \nMoodle, timetable \n \nAims \nThis course will examine the emerging principles and methodologies that underpin scalable and \nefficient machine learning systems. Primarily, the course will focus on an exciting cross-section \nof algorithms and system techniques that are used to support the training and inference of \nmachine learning models under a spectrum of computing systems that range from constrained \nembedded systems up to large-scale distributed systems. It will also touch upon the new \nengineering practices that are developing in support of such systems at scale. When needed to \nappreciate issues of scalability and efficiency, the course will drill down to certain aspects of \ncomputer architecture, systems software and distributed systems and explore how these \ninteract with the usage and deployment of state-of-the-art machine learning. \n \nSyllabus \nTopics covered may include the following, with confirmation a month before the course begins: \n \nSystem Performance Trade-offs \nDistributed Learning Algorithms  \nModel Compression  \nDeep Learning Compilers  \nFrameworks and Run-times  \nScalable Inference Serving  \nDevelopment Practices  \nAutomated Machine Learning  \nFederated Learning  \nPrimarily, topics are covered with conventional lectures. However, where appropriate, material \nwill be delivered through hands-on lab tutorials. Lab tutorials will make use of hardware \nincluding ARM microcontrollers and multi-GPU machines to explore forms of efficient machine \nlearning (any necessary equipment will be provided to students) \n \nAssessment \nEach student will be assessed on 3 labs which will be worth 30% of their grade. They will also \nundertake a written project report which will be worth 70% of the grade. This report will detail an \ninvestigation into a particular aspect of machine learning systems, this report will be made \navailable publicly.\n \n\nPROOF ASSISTANTS \nAims \nThis module introduces students to interactive theorem proving using Isabelle and Coq. It \nincludes techniques for specifying formal models of software and hardware systems and for \nderiving properties of these models. \n \n  \n \nSyllabus \nIntroduction to proof assistants and logic. \nReasoning in predicate logic and typed set theory. \nReasoning in dependent type theory. \nInductive definitions and recursive functions: modelling them in logic, reasoning about them. \nModelling operational semantics definitions and proving properties. \n  \n \nObjectives \nOn completion of this module, students should: \n \npossess good skills in the use of Isabelle and Coq; \nbe able to specify inductive definitions and perform proofs by induction; \nbe able to formalise and reason about a variety of specifications in a proof assistant. \nCoursework \nPractical sessions will allow students to develop skills. Some of the exercises will serve as the \nbasis for assessment. \n \nAssessment \nAssessment will be based on two marked assignments (due in weeks 5 and 8) with students \nperforming small formalisation and verification projects in a proof assistant together with a \nwrite-up explaining their work (word limit 2,500 per project for the write-up). Each of the \nassignments will be worth 100 marks, distributed as follows: \n \n50 marks for completing basic formalisation and verification tasks assessing grasp of the \nmaterial taught in the lecture. Students will submit their work as theory files for either the \nIsabelle or Coq proof assistant, and they will be assessed for correctness and completeness of \nthe specifications and proofs. \n20 marks for completing designated more challenging tasks, requiring the use of advanced \ntechniques or creative proof strategies. \n30 marks for a clear write-up explaining the design decisions made during the formalisation and \nthe strategy used for the proofs, where 10 of these marks will be reserved for write-ups of \nexceptional quality, e.g. demonstrating particular insight. \nThe main tasks in the assignments will be designed to assess the student's proficiency with the \nbasic material and techniques taught in the lectures and the practical sessions, while the more \nchallenging tasks will give exceptional students the opportunity to earn distinction marks \n\n \nRecommended reading \nNipkow, T., Klein, G. (2014). Concrete Semantics with Isabelle/HOL. (The first part of this book, \n\u201cProgramming and Proving in Isabelle/HOL\u201d, comes with the Isabelle distribution.)  \n \nNipkow, T., Paulson, L.C. and Wenzel, M. (2002). A proof assistant for higher-order logic. \nSpringer LNCS 2283.  \n \nThe Software Foundations series of books, in particular the first two: \n \nPierce, B., Azevedo de Amorim, A., Casinghino, C., Gaboardi, M., Greenberg, M., Hri\u0163cu, C., \nSj\u00f6berg, V., Yorgey, B. (2023). Logical Foundations  \nPierce, B., Azevedo de Amorim, A., Casinghino, C., Gaboardi, M., Greenberg, M., Hri\u0163cu, C., \nSj\u00f6berg, V., Tolmach, A., Yorgey, B. (2023). Programming Language Foundations  \nChlipala, A. (2022). Certified programming with dependent types: a pragmatic introduction to the \nCoq proof assistant. MIT Press.  \n \nSergey, I. (2014). Programs and Proofs: Mechanizing Mathematics with Dependent Types.  \n \nAll of these are freely available online.\n \n\nQUANTUM ALGORITHMS AND COMPLEXITY \n \nAims \nThis module is a research-focused introduction to the theory of quantum computing. The aim is \nto prepare the students to conduct research in quantum algorithms and quantum complexity \ntheory. \n \nSyllabus \nTopics will vary from year to year, based on developments in cutting edge research. \nRepresentative topics include: \n \nQuantum algorithms: \n \nQuantum learning theory \nQuantum property testing \nShadow tomography \nQuantum walks \nQuantum state/unitary synthesis. \nStructure vs randomness in quantum algorithms \nQuantum complexity theory: \n \nThe quantum PCP conjecture \nEntanglement and MIP \nState complexity, AdS/CFT, and quantum gravity \nQuantum locally testable codes \nPseudorandom states and unitaries \nQuantum zero-knowledge proofs \nObjectives \nOn completion of this module, students should: \n \nbe familiar with contemporary quantum algorithms \ndevelop an understanding of the power and limitations of quantum computation \nconduct research in quantum algorithms and complexity theory \nAssessment \nMini research project (70%) \nPresentation (20%) \nAttendance and participation (10%) \nTimelines for the assignment submissions:  \n \nsubmit questions/observations on a weekly basis (i.e., Weeks 2-8) \ndeliver talks in Weeks 5-8 \nsubmit their mini-project at the end of term \nRecommended reading material and resources \n\u201cQuantum Computing Since Democritus\u201d by Scott Aaronson \n\n \n\u201cIntroduction to Quantum Information Science\u201d by Scott Aaronson \n \n\u201cQuantum Computing: Lecture Notes\u201d by Ronald de Wolf \n \n\u201cQuantum Computation and Quantum Information\u201d by Nielsen and Chuang\n \n\nUNDERSTANDING QUANTUM ARCHITECTURE \nPrerequisites: Requires introductory linear algebra - concepts such as eigenvalues, Hermitian \nmatrices, unitary matrices. Taking the Part II Quantum Computing course (or similar) is helpful \nbut not required. Familiarity with computer architecture and compilers is helpful. \nMoodle, timetable \n \nAims \nThis course covers the architecture of a practical-scale quantum computer. We will examine the \nresource requirements of practical quantum applications, understand the different layers of the \nquantum stack, the techniques used in these layers and examine how these layers come \ntogether to enable practical quantum advantage over classical computing. \n \nSyllabus \nThe course has two parts: a series of lectures to cover important aspects of the quantum stack \nand a set of student presentations. The following are a list of representative topics: \n \n- Basics of quantum computing \n- The fault-tolerant quantum stack \n- Compilation \n- Instruction sets \n- Implementations of quantum error correction \n- Implementation of magic state distillation \n- Resource estimation \n \nStudent presentations will be based on a reading list of important papers in quantum \narchitecture.  \n  \n \nObjectives \nAt the end of the course, students will have a broad understanding of the quantum computing \nstack. They will understand how major qubit technologies work, design challenges in real \nquantum hardware, how quantum applications are mapped to a system and the importance of \nquantum error correction for scalability.  \n \nAssessment \nSeminar presentation: 20% \nRead one paper from a provided reading list \nPrepare a 20 minute presentation on it + 10 minutes of answering questions. \nThe student presentation should be similar to a conference presentation - convey the problem, \nwhat are prior solutions, what did the paper do, what are the results, what are future directions \nFor the 20% of the score, the score will be split as 15% and 5%. 15% based on how well the \nstudent understands and explains the paper to the rest of the audience. 5% based on the Q&A \nsession. \n \n\n  \n \nCourse project: 80% (split across a proposal, mid-term report, final report) \nA. Research proposal - at least 500 words (10% of total) \nB. Mid-term report - at least 1000 words (including aspects like the research problem, literature \nreview, what questions will be evaluated, any early ideas or methods (20% of total)  \nC. Final report - up to 4 pages double column in a conference paper style with 3000-4000 \nwords. In addition to the mid term report, should include aspects like results and future \ndirections. (50% of the total) \nD. Report on contributions (in case of group work by 2 students) - 1 paragraph on individual \ncontribution of the student. 1 paragraph on teammate's contribution. \nStudents may work alone in the project, in which case they need only components A-C. \nStudents may work in pairs of two, but they should in addition individually submit D. The \ninstructor may conduct a short viva in case contributions from both team members are not clear \nor imbalanced. \nRecommended reading \nNielsen M.A., Chuang I.L. (2010). Quantum Computation and Quantum Information. Cambridge \nUniversity Press. \n \nMermin N.D. (2007). Quantum Computer Science: An Introduction. Cambridge University Press. \n \nAssessing requirements to scale to practical quantum advantage (2022) Beverland et al.\n\n",
        "8th Semester \nADVANCED TOPICS IN CATEGORY THEORY \n \n \nTeaching \nThe teaching style will be lecture-based, but supported by a practical component where \nstudents will learn to use a proof assistant for higher category theory, and build a small portfolio \nof proofs. Towards the end of the course we will explore some of the exciting computer science \nresearch literature on monoidal and higher categories, and students will choose a paper and \npresent it to the class. \n \nAims \nThe module will introduce advanced topics in category theory. The aim is to train students to \nengage and start modern research on the mathematical foundations of higher categories, the \ngraphical calculus, monoids and representations, type theories, and their applications in \ntheoretical computer science, both classical and quantum. \n \nObjectives \nOn completion of this module, students should: \n \nBe familiar with the techniques of compositional category theory. \nHave a strong understanding of basic categorical semantic models. \nBegun exploring current research in monoidal categories and higher structures. \nSyllabus \nPart 1, lecture course: \nThe first part of the course introduces concepts from monoidal categories and higher categories, \nand explores their application in computer science. \n\u2010 Monoidal categories and the graphical calculus \n\u2010 The proof assistant homotopy.io \n\u2010 Coherence theorems and higher category theory \n\u2010 Linearity, superposition, duality, quantum entanglement \n\u2010 Monoids, Frobenius algebras and bialgebras \n\u2010 Type theory for higher category theory \n \nPart 2, exploring the research frontier: \nIn the second part of the course, students choose a research paper to study, and give a \npresentation to the class. \nThere is a nice varied literature related to the topics of the course, and the lecturer will supply a \nlist of suggested papers.  \n \nClasses \nThere will be three exercise sheets for homework, with accompanying classes by a teaching \nassistant to go over them. \n \n\nAssessment \nProblem sheets (50%) \nClass presentation (20%) \nPractical portfolio (30%) \nReading List \nChris Heunen and Jamie Vicary, \u201cCategory for Quantum Theory: An Introduction\u201d, Oxford \nUniversity Press\n \n\nADVANCED TOPICS IN COMPUTER SYSTEMS \n \nAims \nThis module will attempt to provide an overview of \u201csystems\u201d research. This is a very broad field \nwhich has existed for over 50 years and which has historically included areas such as operating \nsystems, database systems, file systems, distributed systems and networking, to name but a \nfew. The course will thus necessarily cover only a tiny subset of the field. \n \nMany good ideas in systems research are the result of discussing and debating previous work. \nA primary aim of this course therefore will be to educate students in the art of critical thinking: \nthe ability to argue for and/or against a particular approach or idea. This will be done by having \nstudents read and critique a set of papers each week. In addition, each week will include \npresentations from a number of participants which aim to advocate or criticise each piece of \nwork. \n \nSyllabus \nThe syllabus for this course will vary from year to year so as to cover a mixture of older and \nmore contemporary systems papers. Contemporary papers will be generally selected from the \npast 5 years, primarily drawn from high quality conferences such as SOSP, OSDI, ASPLOS, \nFAST, NSDI and EuroSys. Example topics might include: \n \nSystems Research and System Design \nOS Structure and Virtual Memory \nVirtualisation \nConsensus \nScheduling \nPrivacy \nData Intensive Computing \nBugs \nThe reading each week will involve a load equivalent to 3 full length papers. Students will be \nexpected to read these in detail and prepare a written summary and review. In addition, each \nweek will contain one or more short presentations by students for each paper. The types of \npresentation will include: \n \nOverview: a balanced presentation of the paper, covering both positive and negative aspects. \nAdvocacy: a positive spin on the paper, aiming to convince others of its value. \nCriticism: a negative take on the paper, focusing on its weak spots and omissions. \nThese presentation roles will be assigned in advance, regardless of the soi disant absolute merit \nof the paper or the preference of the student. Furthermore, all students \u2013 regardless of any \nassigned presentation role in a given week \u2013 will be expected to participate in the class by \nasking questions and generally entering into the debate. \n \nObjectives \n\nOn completion of this module students should have a broad understanding of some key papers \nand concepts in computer systems research, as well as an appreciation of how to argue for or \nagainst any particular idea. \n \nCoursework and practical work \nCoursework will be the production of the weekly paper reviews. Practical work will be presenting \npapers as appropriate, as well as ongoing participation in the class. \n \nAssessment \nAssessment consists of: \n \nOne essay per week for 7 weeks (10% each) \nPresentation (20%) \nParticipation in class over the term (10%) \nRecommended reading \nMost of the reading for this course will be in the form of the selected papers each week. \nHowever, the following may be useful background reading to refresh your knowledge from \nundergraduate courses: \n \nSilberschatz, A., Peterson, J.L. and Galvin, P.C. (2005). Operating systems concepts. \nAddison-Wesley (7th ed.). \n \nTanenbaum, A.S. (2008). Modern Operating Systems. Prentice-Hall (3rd ed.). \n \nBacon, J. and Harris, T. (2003). Operating systems. Addison-Wesley (3rd ed.). \n \nAnderson, T. and Dahlin, M. (2014). Operating Systems: Principles and Practice. Recursive \nBooks (2nd ed.). \n \nHennessy, J. and Patterson, D. (2006). Computer architecture: a quantitative approach. Elsevier \n(4th ed.). ISBN 978-0-12-370490-0. \n \nKleppmann M (2016) Designing Data-Intensive Applications, O'Reilly (1st ed.)\n \n\nADVANCED TOPICS IN MACHINE LEARNING \nAims \nThis course explores current research topics in machine learning in sufficient depth that, at the \nend of the course, participants will be in a position to contribute to research on their chosen \ntopics. Each topic will be introduced with a lecture which, building on the material covered in the \nprerequisite courses, will make the current research literature accessible. Each lecture will be \nfollowed by up to three sessions which will typically be run as a reading group with student \npresentations on recent papers from the literature followed by a discussion, or a practical, or \nsimilar. \n \nStructure \nEach student will attend 3 topics and each topic's sessions will be spread over 5 contact hours. \nStudents will be expected to undertake readings for their selected topics. There will be some \ngroup work. \n \nThere will be a briefing session in Michaelmas term. \n \nSyllabus \nStudents choose five topics in preferential order from a list to be published in Michaelmas term. \nThey will be assigned to three topics out of their list. Students are assessed on one of these \ntopics which may not necessarily be their first choice topic. \n \nThe topics to be offered in 2024-25 are yet to be decided but to give an indicative idea of the \ntypes of topics, the ones offered in 2023-24 were: \n \nImitation learning  Prof A. Vlachos \nThe Future of Large Language Models: data architectures ethics Dr M. Tomalin \nPhysics and Geometry in Machine Learning Dr C. Mishra \nDiffusion Models and SDEs Francisco Vargas, Dr C. H. Ek \nExplainable Artificial Intelligence M. Espinosa, Z. Shams, Prof M. Jamnik \nUnconventional approaches to AI Dr S. Banerjee \nNarratives in Artificial Intelligence and Machine Learning Prof N. Lawrence \nMultimodal Machine Learning K. Hemker, N. Simidjievski, Prof M. Jamnik \nDeep Reinforcement Learning S. Morad, Dr C. H. Ek \nAutomation in Proof Assistants A. Jiang, W. Li, Prof M. Jamnik \nOn completion of this module, students should: \n \nbe in a strong position to contribute to the research topics covered; \nunderstand the fundamental methods (algorithms, data analysis, specific tasks) underlying each \ntopic; \nand be familiar with recent research papers and advances in the field. \nCoursework \nStudents will typically work in groups to give a presentation on assigned papers. Alternatively, a \ntopic may include practical sessions. Each topic will typically consist of one preliminary lecture \n\nfollowed by 3 reading and discussion sessions, or several lectures followed by a practical \nsession. A typical topic can accommodate up to 9 students presenting papers. There will be at \nleast 10 minutes general discussion per session. \n \nFull coursework details will be published by October. \n \nAssessment \nCoursework will be marked by the topic leaders and second marked by the module conveners. \n \nParticipation in all assigned topics, 10% \nPresentation or practical work or similar (for one of the chosen topics), 20% \nTopic coursework (for one of the chosen topics), 70% \nIndividual topic coursework will be published late Michaelmas term. \n \nAssessment criteria for topic coursework will follow project assessment criteria here: \nhttps://www.cl.cam.ac.uk/teaching/exams/acs_project_marking.pdf \n \nPlease note that students will be assessed on one of their three chosen topics but this may not \nbe their first choice. \n \nRecommended reading \nTo be confirmed by each topic convenor.\n \n\nCOMPUTING FOR COLLECTIVE INTELLIGENCE \nPrerequisites: Familiarity with core machine learning paradigms - Basic calculus (analytical \nskills) - Basic discretre optimization (combinatorics) \nMoodle, timetable \n \nObjectives \nThere is a substantial body of academic work demonstrating that complex life is built by \ncooperation across scales: collections of genes cooperate to produce organisms, cells \ncooperate to produce multi-cellular organisms and multicellular animals cooperate to form \ncomplex social groups. Arguably, all intelligence is collective intelligence. Yet, to-date, many \nartificially intelligent agents (both embodied and virtual), are generally not conceived from the \nground up to interact with other intelligent agents (be it machines or humans). The canonical AI \nproblem is that of a monolithic and solitary machine confronting a non-social environment. This \ncourse aims to balance this trend by (i) equipping students with conceptual and practical \nknowledge on collective intelligence from a computational standpoint, and (ii) by conveying \nvarious computational paradigms by which collective intelligence can be modelled as well as \nsynthesized. \n \nAssessment \nThe assessment will be based on a reading-group paper presentation (individual or in pairs), \naccompanied by a 2-page summary, and a technical position paper (individual work) handed in \nafter the course. The position paper will be assessed based on its technical correctness, \nstrength or arguments, \nand clarity of research vision, and will be accompanied by a brief 5-minute pre-recorded talk that \nsummarizes key arguments (any slides used are also submitted as part of the project). \n \nPaper presentation and 1-page summary: 25% \nTechnical position paper: 75% (4000 word limit) \nRecommended reading \nSwarm Intelligence : From Natural to Artificial Systems, Bonabeau et al (1999) \nJoined-Up Thinking, Hannah Critchlow, (2024) \nSupercooperators, Martin Nowak (2011) \nA Course on Cooperative Game Theory, Chakravarty et al., (2015) \nGoverning the Commons: The Evolution of Institutions for Collective Action. Ostrom (1990).  \n \n\nCRYPTOGRAPHY AND PROTOCOL ENGINEERING \nAims \nWe all use cryptographic protocols every day: whenever we access an https:// website or send a \nmessage via WhatsApp or Signal, for example. In this module, students will get hands-on \nexperience of how those protocols are implemented. Students start by writing their own secure \nmessaging protocol from scratch, and then move on to more advanced examples of \ncryptographic protocols for security and privacy, such as private information retrieval (searching \nfor data without revealing what you\u2019re searching for). \n \nSyllabus \nThis is a practical course in which the first half of each of the 2-hour weekly sessions is a lecture \nthat introduces a topic, while the second half is lab time during which the students can work on \ntheir implementations, discuss the topics in small groups, and get help from the lecturers. The \nmodule is structured into three blocks as follows:  \n \nCryptographic primitives (2 weeks). Using common building blocks such as symmetric ciphers \nand hash functions. Implementing selected aspects of elliptic curve Diffie-Hellman and digital \nsignatures. Protocol security, Dolev-Yao model, side-channel attacks.  \nSecure communication (3 weeks). Authenticated key exchange (e.g. SIGMA, TLS), \npassword-authenticated key exchange (e.g. SPAKE2), forward secrecy, post-compromise \nsecurity, double ratchet, the Signal protocol.  \nPrivate database lookups (3 weeks). Lattice-based post-quantum cryptography, learning with \nerrors, homomorphic encryption, private information retrieval.  \nIn each block, students will be given a description of the algorithms and protocols covered (e.g. \na research paper or an RFC standards document), and a code template that the students \nshould use for their implementation. The implementation language will probably be Python (to \nbe confirmed after the practical materials have been developed ). Students will also be given a \nset of test cases to check their code, and will have the opportunity to test their implementation \ncommunicating with other students\u2019 implementations. Finally, for each block, students will write \nand submit a lab report explaining their approach and the findings from their implementation. \n \nObjectives \nOn completion of this module, students should: \n \nUnderstand that cryptography is not magic! The mathematics can look intimidating, but this \nmodule will show students that they needn\u2019t be afraid of cryptography. \nGain appreciation for the complexity and careful implementation of real-world cryptography \nlibraries ,and understand why one should prefer vetted implementations. \nBe familiar with the mathematical notation and concepts commonly used in descriptions of \ncryptographic protocols, and be able to understand and implement research papers and RFCs \nin this field. \nBe comfortable with cryptographic primitives commonly used in protocols, and able to correctly \nuse software libraries that implement them. \n\nHave gained hands-on experience of attacks on cryptographic protocols, and an appreciation \nfor the difficulty of making these protocols correct. \nHave been exposed to ideas and techniques from recent research, which may be useful for their \nown future research. \nHave practised technical writing through lab reports. \nAssessment \nStudents will be provided with code skeletons in one or two different programming languages \n(probably Python, maybe Rust) to avoid them struggling with setup issues. For each \nassignment, students are required to produce a working implementation of the protocols \ndiscussed in the course, using the programming language and skeleton provided. Here, \n\u201cworking\u201d means that the algorithms are functionally correct, but they are not production-quality \n(in particular, no side-channel countermeasures such as constant-time algorithms are required). \nFor some primitives (e.g. symmetric ciphers and hash functions) existing libraries should be \nused, whereas other cryptographic algorithms will be implemented from scratch. Students \nshould submit their code as a Docker container that can be run against specified test cases.  \n \nAfter each of the three blocks, students will be asked to submit a lab report of approximately \n2,000 words along with their code for that block. In the lab report, the students should explain \nhow their implementation works and why it is correct, as well as any findings from the work (e.g. \nany limitations or trade-offs they found with the protocols). The report structure and marking \nscheme will be specified in advance. The lab reports provide an opportunity for students to \nprovide critical insights and creativity. For instance, they might compare their implementation \nwith existing real-world implementations which provide additional features and guarantees.  \n \nEach of the three lab reports (along with the associated code) will be marked, forming the basis \nof assessment, and feedback on the reports will be given to the students before the next \nsubmission is due, so that students can take the feedback on board for their next submission. \nOf the three reports, the worst mark will be discarded, and the other two reports each contribute \n50% of the final mark. As such, students might decide to submit only two reports.  \n \nStudents may discuss their work with others, but the implementations and lab reports must be \nindividual work. \n \nRecommended reading \nTextbook: Jonathan Katz and Yehuda Lindell. Introduction to Modern Cryptography (3rd edition). \nCRC Press, 2020. \n \nThe textbook covers prerequisite knowledge on cryptographic primitives, such as ciphers, \nMACs, hash functions, and signatures. This book is also used in the Part II Cryptography \ncourse. For the protocols we cover in this course, we are not aware of a suitable textbook; \ninstead we will use research papers, lecture slides, and RFCs as resources, which will be \nprovided at the start of the module.\n \n\nDISTRIBUTED LEDGER TECHNOLOGIES: FOUNDATIONS AND APPLICATIONS \n \nAims \nThis reading group course examines foundations and current research into distributed ledger \n(blockchain) technologies and their applications. Students will read, review, and present seminal \nresearch papers in this area. Once completed, students should be able to integrate blockchain \ntechnologies into their own research and gain familiarity with a range of research skills. \n \nLectures \nIntroduction \nConsensus protocols \nBitcoin and its variants \nEthereum, smart contracts, and other permissionless DLTs \nHybrid and permissioned DLTs \nApplications \nLearning objectives \nThere are two broad objectives: to acquire familiarity with a body of work in the area of \ndistributed ledgers and to learn some specific research skills: \n \nHow to read a paper \nHow to review a paper \nHow to analyze a paper\u2019s strengths and weaknesses \nWritten and oral presentation skills \nAssessment \nYou are expected to read all assigned papers and submit paper reviews each week. Each \nreview must either follow the provided review form [PDF] [Latex source]. Each \u201creview\u201d is worth \n5% of your total mark, and is marked out of 100 with 60 a passing grade. Marks will be awarded \nand penalties for late submission applied according to ACS Assessment Guidelines.  \n \nOne paper review for the first week, then two paper reviews each week for 6 weeks (13 reviews, \n5% each) 65% (approx 600 words per review) \nSummative essay 25% (max 3000 words) \nPresentation 5% \n100% attendance in class 5% (2 marks deducted per missed class) \nRecommended Reading \nNarayanan, A. , Bonneau, J., Felten, E., Miller, A. and Goldfeder, S. (2016). Bitcoin and \nCryptocurrency Technologies: A Comprehensive Introduction. Princeton University Press. \n(2016 Draft available here: \nhttps://d28rh4a8wq0iu5.cloudfront.net/bitcointech/readings/princeton_bitcoin_book.pdf)\n \n\nEXPLAINABLE ARTIFICIAL INTELLIGENCE \nPrerequisites: A solid background in statistics, calculus and linear algebra. We strongly \nrecommend some experience with machine learning and deep neural networks (to the level of \nthe first chapters of Goodfellow et al.\u2019s \u201cDeep Learning\u201d). Students are expected to be \ncomfortable reading and writing Python code for the module\u2019s practical sessions. \nMoodle, timetable \n \nAims \nThe recent palpable introduction of Artificial Intelligence (AI) models to everyday \nconsumer-facing products, services, and tools brings forth several new technical challenges and \nethical considerations. Amongst these is the fact that most of these models are driven by Deep \nNeural Networks (DNNs), models that, although extremely expressive and useful, are \nnotoriously complex and opaque. This \u201cblack-box\u201d nature of DNNs limits their ability to be \nsuccessfully deployed in critical scenarios such as those in healthcare and law. Explainable \nArtificial Intelligence (XAI) is a fast-moving subfield of AI that aims to circumvent this crucial \nlimitation of DNNs by either  \n \n(i) constructing human-understandable explanations for their predictions,  \n \nor (ii) designing novel neural architectures that are interpretable by construction.  \n \nIn this module, we will introduce the key ideas behind XAI methods and discuss some of the \nimportant application areas of these methods (e.g., healthcare, scientific discovery, debugging, \nmodel auditing, etc.). We will approach this by focusing on the nature of what constitutes an \nexplanation, and discussing different ways in which explanations can be constructed or learnt to \nbe generated as a by-product of a model. The main aim of this module is to introduce students \nto several commonly used approaches in XAI, both theoretically in lectures and through \nhands-on exercises in practicals, while also bringing recent promising directions within this field \nto their attention. We hope that, by the end of this module, students will be able to directly \ncontribute to XAI research and will understand how methods discussed in this module may be \npowerful tools for their own work and research. \n \nSyllabus \nOverview and taxonomy of XAI (why is explainability needed, definition of terms, taxonomy of \nthe XAI space, etc.)  \nPerturbation-based feature attribution (e.g., LIME, Anchors, SHAP, etc.) \nPropagation-based feature attribution methods (e.g., Relevance Propagation, Saliency \nmethods, etc.) \nConcept-based explainability (Net2Vec, T-CAV, ACE, etc.) \nInterpretable architectures (CBMs and variants, Concept Whitening, SENNs, etc.) \nNeurosymbolic methods (DeepProbLog, Neural Reasoners, etc.) \nSample-based Explanations (Influence functions, ProtoPNets, etc.) \nCounterfactual explanations \nProposed Schedule \n\nThe 16 hours of lectures across 8 weeks will be divided as follows:  \nWeek 1: 1h lecture + 1h lecture  \nWeek 2: 1h lecture + 1h reading group & presentations  \nWeek 3: 1h lecture + 1h reading group & presentations  \nWeek 4: 2h practical  \nWeek 5: 1h lecture + 1h reading group & presentations  \nWeek 6: 2h practical  \nWeek 7: 1h lecture + 1h reading group & presentations  \nWeek 8: 1h reading group & presentation + 1h reading group & presentations \n \nIn weeks where lectures are planned, one-hour lecture slots will intercalate with one hour of \nstudent paper presentations. During each paper presentation session, three students will \npresent a paper related to the topic covered in the earlier lecture that week for about 10 minutes \neach + 5 minutes of questions. At the end of all paper presentations, there will be a discussion \non all the papers. The order of student presentations will be allocated randomly during the first \nweek so that students know in advance when they are expected to present. For the sake of \nfairness, we will release the paper to be presented by each student a week before their \npresentation slot. \n \nObjectives \nBy the end of this module, students should be able to: \n \nRecognise and identify key concepts in XAI together with their connection to related subfields in \nAI such as fairness, accountability, and trustworthy AI. \nUnderstand how to use, design, and deploy model-agnostic perturbation methods such as \nLIME, Anchors, and RISE. In particular, students should understand the connection between \nfeature importance and cooperative game theory, and its uses in methods such as SHAP. \nIdentify the uses and limitations of propagation-based feature importance methods such as \nSaliency, SmoothGrad, GradCAM, and Integrated Gradients. Students should be able to \nimplement each of these methods on their own and connect the theoretical ideas behind them \nto practical code, exploiting modern frameworks\u2019 auto-differentiation. \nUnderstand what concept learning is and what limitations it overcomes compared to traditional \nfeature-based methods. Specifically, students should understand how probing a DNN\u2019s latent \nspace may be exploited for learning useful concepts for explainability. \nReason about the key components of inherently interpretable architectures and neuro-symbolic \nmethods and understand how interpretable neural networks can be designed from first \nprinciples. \nElaborate on what sample-based explanations are and how influence functions and prototypical \narchitectures such as ProtoPNet can be used to construct such explanations. \nExplain what counterfactual explanations are, how they are related to causality, and under which \nconditions they may be useful. \nUpon completion of this module, students will have the technical background and tools to use \nXAI as part of their own research or partake in XAI research itself. Moreover, we hope that by \ndetailing a clear timeline of how this relatively young subfield has developed, students may be \n\nable to better understand what are some fundamental open questions in this area and what are \nsome promising directions that are currently actively being explored. \n  \n \nAssessment \n(10%) student presentation: each student will be randomly assigned a presentation slot at the \nbeginning of the course. We will then distribute a paper for them to present in their slot a week \nbefore the presentation\u2019s scheduled time. Students are expected to prepare a 10-minute \npresentation of their assigned paper where they will present the motivation of their assigned \nwork and discuss the main methodology and findings reported in that paper. We will encourage \nstudents to focus on fully communicating the intuition of the work in their assigned papers and \ntry and connect it with ideas that we have previously discussed in previous lectures. All students \nnot presenting each week will be asked to submit one question pertaining to each paper \npresented that week before the paper presentations. \n \n(20%) practical exercises: We will run two practical sessions where students will be asked to \nperform a series of exercises that require them to use concepts we have introduced in lecture \nup to that point. For each practical session, we will prepare a colab notebook to guide the \nstudent through exercises and we will ask the students to submit their solutions through this \ncolab notebook. We expect students to complete about \u2154 of the exercises in the practical class \nand complete the rest at home as homework. Each practical will be worth 10%. \n \n(70%) mini-project: At the end of week 3, we will hand out a list of papers for students to select \ntheir mini-projects from. Each mini-project will consist of a student selecting a paper from our list \nand reimplementing and expanding the key idea in the paper. We encourage students to be as \ncreative as they want with how they drive their mini-project once the paper has been selected. \nFor example, they can reimplement the technique in the paper and combine it with \nmethodologies from other works we discussed in lecture, or they can apply their paper\u2019s \nmethodology to a new domain, datasets, or setup, where the technique may offer interesting \nand potentially novel insights. We will ask all students to submit a report in a workshop format of \nup to 4,000 words. This report, due roughly a week after Lent term ends, should describe their \nmethodology, experiments, and results. A crucial aspect of this report involves explaining the \nrationale behind different choices in methodology and experiments, as well as elaborating on \nthe choices made and hypotheses tested throughout their mini-project (potentially showing a \ndeep understanding of the work they are basing their mini-project on). To aid students with \nselecting their projects and making progress on them, we will hold regular office hours when \nstudents can come to discuss their progress and questions with us. \n \nRecommended reading \nTextbooks \n \n* Christoph Molnar, \u201cInterpretable Machine Learning\u201d. (2022): \nhttps://christophm.github.io/interpretable-ml-book/ \n \n\nOnline Courses and Tutorial \n \n* Su-in Lee and Ian Cover, \u201cCSEP 590B Explainable AI\u201d University of Washington* Explaining \nMachine Learning Predictions: State-of-the-art, Challenges, and Opportunities \n \n* On Explainable AI: From Theory to Motivation, Industrial Applications, XAI Coding & \nEngineering Practices - AAAI 2022 Turorial \n \nSurvey papers \n \n* Arrieta, Alejandro Barredo, et al. \"Explainable Artificial Intelligence (XAI): Concepts, \ntaxonomies, opportunities and challenges toward responsible AI.\" Information fusion 58 (2020): \n82-115. \n \n* Rudin, Cynthia, et al. \"Interpretable machine learning: Fundamental principles and 10 grand \nchallenges.\" Statistics Surveys 16 (2022): 1-85.\n \n\nFEDERATED LEARNING: THEORY AND PRACTICE \nPrerequisites: It is strongly recommended that students have previously (and successfully) \ncompleted an undergraduate machine learning course - or have equivalent experience through \nopen-source material (e.g., lectures or similar). An example course would be: Part1B \"Artificial \nIntelligence\". Students should feel comfortable with SGD and optimization methods used to train \ncurrent popular neural networks and simple forms of neural network architectures. \nMoodle, timetable \n \nObjectives \nThis course aims to extend the machine learning knowledge available to students in Part I (or \npresent in typical undergraduate degrees in other universities), and allow them to understand \nhow these concepts can manifest in a decentralized setting. The course will consider both \ntheoretical (e.g., decentralized optimization) and practical (e.g., networking efficiency) aspects \nthat combine to define this growing area of machine learning.  \n \nAt the end of the course students should: \n \nUnderstand popular methods used in federated learning \nBe able to construct and scale a simple federated system \nHave gained an appreciation of the core limitations to existing methods, and the approaches \navailable to cope with these issues \nDeveloped an intuition for related technologies like differential privacy and secure aggregation, \nand are able to use them within typical federated settings  \nCan reason about the privacy and security issues with federated systems \nLectures \nCourse Overview. Introduction to Federated Learning. \nDecentralized Optimization. \nStatistical and Systems Heterogeneity. \nVariations of Federated Aggregation. \nSecure Aggregation. \nDifferential Privacy within Federated Systems. \nExtensions to Federated Analytics. \nApplications to Speech, Video, Images and Robotics. \nLab sessions \nFederating a Centralized ML Classifier. \nBehaviour under Heterogeneity. \nScaling a Federated Implementation. \nExploring Privacy with Federated Settings \nRecommended Reading \nReadings will be assigned for each lecture. Readings will be taken from either research papers, \ntutorials, source code or blogs that provide more comprehensive treatment of taught concepts. \n \nAssessment - Part II Students \n\nFour labs are performed during the course, and students receive 10% of their total grade for \nwork done as part of each lab. (For a total of 40% of the total grade from lab work alone). Labs \nwill primarily provide hands-on teaching opportunities, that are then utilized within the lab \nassignment which is completed outside of the lab contact time. MPhil and Part III students will \nbe given additional questions to answer within their version of the lab assignment which will \ndiffer from the assignment given to Part II CST students. \n \nThe remainder of the course grade (60%) will be given based on a hands-on project that applies \nthe concepts taught in lectures and labs. This hands-on project will be assessed based on upon \na combination of source code, related documentation and brief 8-minute pre-recorded talk that \nsummarizes key project elements (any slides used are also submitted as part of the project). \nPlease note, that in the case of Part II CST students, the talk itself is not examinable -- as such \nwill be made optional to those students. \n \nA range of possible practical projects will be described and offered to students to select from, or \nalternatively students may propose their own. MPhil and Part III students will select from a \nproject pool that is separate from those offered to Part II CST students. MPhil and Part III \nprojects will contain a greater emphasis on a research element, and the pre-recorded talks \nprovided by this student group will focus on this research contribution. The project will be \nassessed on the level of student understanding demonstrated, the degree of difficulty, \ncorrectness of implementation -- and for Part III/MPhil students the additional criteria of the \nquality and execution of the research methodology, and depth and quality of results analysis. \n \nThis project can be done individually or in groups -- although individual projects will be strongly \nencouraged. It will be required the project is performed using a code repository that also will \ncontain all documentation -- access to this repository will be shared with course staff (e.g., \nlecturer and TAs). Where needed, marks assigned to students within a group will be \ndifferentiated using this repository as an input. Furthermore if groups are formed, members must \nbe either entirely from Part III/MPhil students or Part II CST, i.e., these two student groups \nshould not mix to form a project group. \n \nProjects will be made available publicly. A maximum word count for written contributions for the \nproject will be enforced.  \n \n  \nAssessment - MPhil / Part III Students \n50% final mini-project. This hands-on project will be assessed based on upon a combination of \nsource code, related documentation and brief 8-minute pre-recorded talk that summarizes key \nproject elements (any slides used are also submitted as part of the project). \n \n30% midterm lab report (a written report of a bit more depth of thought than required in a lab, \nthe report provides the results of experiments -- with the skills to perform these experiments \ncoming from lab sessions),  \n \n\n20% for two individual hands-on labs. Labs will primarily provide hands-on teaching \nopportunities, that are then utilized within the lab assignment which is completed outside of the \nlab contact time. There will also be additional questions to answer within their version of the lab \nassignment. \n \nFor the mini-project, a range of possible practical projects will be described and offered to \nstudents to select from, or alternatively students may propose their own. The project will be \nassessed on the level of student understanding demonstrated, the degree of difficulty, \ncorrectness of implementation, the quality and execution of the research methodology, and \ndepth and quality of results analysis. \n \nThe project can be done individually or in groups -- although individual projects will be strongly \nencouraged. It will be required the project is performed using a code repository that also will \ncontain all documentation -- access to this repository will be shared with course staff (e.g., \nlecturer and TAs). Where needed, marks assigned to students within a group will be \ndifferentiated using this repository as an input. Furthermore if groups are formed, members must \nbe either entirely from Part III or entirely from MPhil students, i.e., these two student groups \nshould not mix to form a project group. \n \nProjects will be made available publicly. A maximum word count for written contributions for the \nproject will be enforced.   \n \nAdvanced Material sessions - MPhil / Part III Students \nThese sessions are mandatory for the MPhil and Part III students. Part II CST students may \nattend if they wish \n \nTopics and announced the first week of class. Selected to extend beyond topics covered in \nlectures and labs. Content is a mixture of sessions run in the lecture room the are either (1) \npresentations by guest lectures by an invited domain expert or (2) class-wide discussions \nregarding one or more related academic papers. Paper discussions will require students to read \nthe paper ahead of the lecture, and a brief discussion primer presentation will be given students \nbefore discussions begin.  \n \nThis module is shared with Part II of the Computer Science Tripos. Assessment will be adjusted \nfor the two groups of students to be at an appropriate level for whichever course the student is \nenrolled on. Further information about assessment and practicals will follow at the first lecture.\n\nGEOMETRIC DEEP LEARNING \n \nPrerequisites: Experience with machine learning and deep neural networks is recommended. \nKnowledge of concepts from graph theory and group theory is useful, although the relevant \nparts will be explicitly retaught. \nMoodle, timetable \n \nAims and Objectives \nMost of the patterns we see in nature can be elegantly reasoned about using spatial \nsymmetries\u2014transformations that leave underlying objects unchanged. This observation has \nhad direct implications for the development of modern deep learning architectures that are \nseemingly able to escape the \u201ccurse of dimensionality\u201d and fit complex high-dimensional tasks \nfrom noisy real-world data. Beyond this, one of the most generic symmetries\u2014the \npermutation\u2014will prove remarkably powerful in building models that reason over graph \nstructured-data, which is an excellent abstraction to reason about naturally-occurring, \nirregularly-structured data. Prominent examples include molecules (represented as graphs of \natoms and bonds, with three-dimensional coordinates provided), social networks and \ntransportation networks. Several already-impacted application areas include traffic forecasting, \ndrug discovery, social network analysis and recommender systems. The module will provide the \nstudents the capability to analyse irregularly- and nontrivially-structured data in an effective way, \nand position geometric deep learning in a proper context with related fields. The main aim of the \ncourse is to enable students to make direct contributions to the field, thoroughly assimilate the \nkey concepts in the area, and draw relevant connections to various other fields (such as NLP, \nFourier Analysis and Probabilistic Graphical Models). We assume only a basic background in \nmachine learning with deep neural networks. \n \nLearning outcomes \nThe framework of geometric deep learning, and its key building blocks: symmetries, \nrepresentations, invariance and equivariance \nFundamentals of processing data on graphs, as well as impactful application areas for graph \nrepresentation learning \nTheoretical principles of graph machine learning: permutation invariance and equivariance \nThe three \"flavours\" of spatial graph neural networks (GNNs) (convolutional, attentional, \nmessage passing) and their relative merits. The Transformer architecture as a special case. \nAttaching symmetries to graphs: CNNs on images, spheres and manifolds, Geometric Graphs \nand E(n)-equivariant GNNs \nRelevant connections of geometric deep learning to various other fields (such as NLP, Fourier \nAnalysis and Probabilistic Graphical Models) \nLectures \nThe lectures will cover the following topics: \n \nLearning with invariances and symmetries: geometric deep learning. Foundations of group \ntheory and representation theory. \n\nWhy study data on graphs? Success stories: drug screening, travel time estimation, \nrecommender systems. Fundamentals of graph data processing: network science, spectral \nclustering, node embeddings. \nPermutation invariance and equivariance on sets and graphs. The principal tasks of node, edge \nand graph classification. Neural networks for point clouds: Deep Sets, PointNet; universal \napproximation properties. \nThe three flavours of spatial GNNs: convolutional, attentional, message passing. Prominent \nexamples: GCN, SGC, ChebyNets, MoNet, GAT, GATv2, IN, MPNN, GraphNets. Tradeoffs of \nusing different GNN variants. \nGraph Rewiring: how to apply GNNs when there is no graph? Links to natural language \nprocessing---Transformers as a special case of attentional GNNs. Representative \nmethodologies for graph rewiring: GDC, SDRF, EGP, DGCNN. \nExpressive power of graph neural networks: the Weisfeiler-Lehman hierarchy. GINs as a \nmaximally expressive GNN. Links between GNNs and graph algorithms: neural algorithmic \nreasoning. \nCombining spatial symmetries with GNNs: E(n)-equivariant GNNs, TFNs, SE(3)-Transformer. A \ndeep dive into AlphaFold 2. \nWorked examples: Circulant matrices on grids, the discrete Fourier transform, and convolutional \nnetworks on spheres. Graph Fourier transform and the Laplacian eigenbasis. \nPracticals \nThe practical is designed to complement the knowledge learnt in lectures and teach students to \nderive additional important results and architectures not directly shown in lectures. The practical \nwill be given as a series of individual exercises (each either code implementation or \nproof/derivation). Each of these exercises can be individually assessed based on a specified \nmark budget. \n \nPossible practical topics include the study of higher-order GNNs and equivariant message \npassing. \n \nAssessment \n(60%) Group Mini-project (writeup) at the end of the course. The mini projects can either be \nself-proposed, or the students can express their preference for one of the provided topics, the \nlist of which will be announced at the start of term. The projects will consist of implementing \nand/or extending graph representation learning models in the literature, applying them to \npublicly available datasets. Students will undertake the project in pairs and submit a joint \nwriteup limited to 4,000 words (in line with other modules); appendix of work logs to be included \nbut ungraded; \n \n(10%) Short presentation and viva: students will give a short presentation to explain their \nindividual contribution to the mini-project and there will be a short viva following. \n \n(30%) Practical work completion. Completing the exercises specified in the practical to a \nsatisfactory standard. The practical assessor should be satisfied that the student derived their \nanswers using insight gained from the course; coupled with original thought, not by simple \n\ncopy-pasting of relevant related work. The students would submit code and a short report, which \nwould then be marked in line with the predetermined mark budget for each practical item. \nThe students will learn how to run advanced architectures on GPU but no specific need for \ndedicated GPU resources. Practicals will be made possible to do on CPU; if required, students \ncan use GPUs on publicly available free services (such as Colab) for their mini-project work. \n \nReferences \nThe course will be based on the following literature: \n \n\"Geometric Deep Learning: Grids, Graphs, Groups, Geodesics, and Gauges\", by Michael \nBronstein, Joan Bruna, Taco Cohen and Petar Veli\u010dkovi\u0107 \n\"Graph Representation Learning\", by Will Hamilton \n\"Deep Learning\", by Ian Goodfellow, Yoshua Bengio and Aaron Courville.\n \n\nMOBILE HEALTH \nAims \nThe course aims to explore how wearable and mobile systems sensors can be used to gather \ndata relevant to understand health, how the data can be analysed with advanced signal \nprocessing and machine learning and the performance of these systems in terms of diagnostics \nand disease progression detection. \n \nSyllabus \nCourse Overview. Introduction to Mobile Health. Evaluation metrics and methodology. Basics of \nSignal Processing. \nInertial Measurement Units, Human Activity Recognition (HAR) and Gait Analysis and Machine \nLearning for IMU data. \nRadios, Bluetooth, GPS and Cellular. Epidemiology and contact tracing, Social interaction \nsensing and applications. Location tracking in health monitoring and public health. \nAudio Signal Processing. Voice and Speech Analysis: concepts and data analysis. Body \nSounds analysis. \nPhotoplethysmogram and Light sensing for health (heart and sleep) \nContactless and wireless behaviour and physiological monitoring \nGenerative Models for Wearable Health Data \nTopical Guest Lectures \nObjectives \nThe course aims to explore how wearable and mobile systems sensors can be used to gather \ndata relevant to understand health, how the data can be analysed with advanced signal \nprocessing and machine learning and the performance of these systems in terms of diagnostics \nand disease progression detection. \n \nRoughly, each lecture contains a theory part about the working of \u201csensor signals\u201d or \u201cdata \nanalysis methods\u201d and an application part which contextualises the concepts. \n \nAt the end of the course students should: Understand how mobile/wearable sensors capture \ndata and their working. Understand different approaches to acquiring and analysing sensor data \nfrom different types of sensors. Understand the concept of signal processing applied to time \nseries data and their practical application in health. Be able to extract sensor data and analyse it \nwith basic signal processing and machine learning techniques. Be aware of the different health \napplications of the various sensor techniques. The course will also touch on privacy and ethics \nimplications of the approaches developed in an orthogonal fashion. \n \nRecommended Reading \nPlease see Course Materials for recommended reading for each session. \n \nAssessment - Part II students \nTwo assignments will be based on two datasets which will be provided to the students: \n \n\nAssignment 1 (shared for Part II and Part III/MPhil): this will be based on a dataset and will be \nworth 40% of the final mark. The task of the assessment will be to perform pre-processing and \nbasic data analysis in a \"colab\" and an answer sheet of no more than 1000 words. \n \nAssignment 2 (Part II): This assignment (worth 60% of the final mark) will be a fuller analysis of \na dataset focusing on machine learning algorithms and metrics. Discussion and interpretation of \nthe findings will be reported in a colab and a report of no more than 1200 words. \n \nAssessment  - Part III and MPhil students \nTwo assignments will be based on datasets which will be provided to the students: \n \nAssignment 1: this will be based on a dataset and will be worth 40% of the final mark. The task \nof the assessment will be to perform pre-processing and basic data analysis in a \"colab\" and an \nanswer sheet of no more than 1000 words. \n \nAssignment 2: The second assignment (worth 60% of the final mark) will be based on multiple \ndatasets. The students will be asked to compare and contract ML algorithms/solutions (trained \nand tested on the different data) and discuss the findings and interpretation in terms of health \ncontext. This will be in the form of a colab and a report of 1800 words. \n \nFurther Information \nCurrent Cambridge undergraduate students who are continuing onto Part III or the MPhil in \nAdvanced Computer Science may only take this module if they did NOT take it as a Unit of \nAssessment in Part II. \n \nThis module is shared with Part II of the Computer Science Tripos. Assessment will be adjusted \nfor the two groups of students to be at an appropriate level for whichever course the student is \nenrolled on. Further information about assessment and practicals will follow at the first lecture.\n\nMULTICORE SEMANTICS AND PROGRAMMING \nPrerequisites: Some familiarity with discrete mathematics (sets, partial orders, etc.) and with \nsequential Java programming will be assumed. Experience with operational semantics and with \nsome concurrent programming would be helpful. \nMoodle, timetable \n \nAims \nIn recent years multiprocessors have become ubiquitous, but building reliable concurrent \nsystems with good performance remains very challenging. The aim of this module is to \nintroduce some of the theory and the practice of concurrent programming, from hardware \nmemory models and the design of high-level programming languages to the correctness and \nperformance properties of concurrent algorithms. \n \nLectures \nPart 1: Introduction and relaxed-memory concurrency [Professor P. Sewell] \n \nIntroduction. Sequential consistency, atomicity, basic concurrent problems. [1 block] \nConcurrency on real multiprocessors: the relaxed memory model(s) for x86, ARM, and IBM \nPower, and theoretical tools for reasoning about x86-TSO programs. [2 blocks] \nHigh-level languages. An introduction to C/C++11 and Java shared-memory concurrency. [1 \nblock] \nPart 2: Concurrent algorithms [Dr T. Harris] \n \nConcurrent programming. Simple algorithms (readers/writers, stacks, queues) and correctness \ncriteria (linearisability and progress properties). Advanced synchronisation patterns (e.g. some \nof the following: optimistic and lazy list algorithms, hash tables, double-checked locking, RCU, \nhazard pointers), with discussion of performance and on the interaction between algorithm \ndesign and the underlying relaxed memory models. [3 blocks] \nResearch topics, likely to include one hour on transactional memory and one guest lecture. [1 \nblock] \nObjectives \nBy the end of the course students should: \n \nhave a good understanding of the semantics of concurrent programs, both at the multprocessor \nlevel and the C/Java programming language level; \nhave a good understanding of some key concurrent algorithms, with practical experience. \nAssessment - Part II Students \nTwo assignments each worth 50% \n \nRecommended reading \nHerlihy, M. and Shavit, N. (2008). The art of multiprocessor programming. Morgan Kaufmann. \n \nCoursework \nCoursework will consist of assessed exercises. \n\n \nPractical work \nPart 2 of the course will include a practical exercise sheet.  The practical exercises involve \nbuilding concurrent data structures and measuring their performance.  The work can be \ncompleted in C, Java, or similar languages. \n \nAssessment - Part III and MPhil Students \nAssignment 1, for 50% of the overall grade. Written coursework comprising a series of questions \non hardware and software relaxed-memory concurrency semantics. \nAssignment 2, for 50% of the overall grade. Written coursework comprising a series of questions \non the design of mutual exclusion locks and shared-memory data structures. \nFurther Information \nCurrent Cambridge undergraduate students who are continuing onto Part III or the MPhil in \nAdvanced Computer Science may only take this module if they did NOT take it as a Unit of \nAssessment in Part II. \n \nThis module is shared with Part II of the Computer Science Tripos. Assessment will be adjusted \nfor the two groups of students to be at an appropriate level for whichever course the student is \nenrolled on. Further information about assessment and practicals will follow at the first lecture.\n\nREINFORCEMENT LEARNING \n \nAims \nThe aim of this module is to introduce students to the foundations of the field of reinforcement \nlearning (RL), discuss state-of-the-art RL methods, incentivise students to produce critical \nanalysis of recent methods, and prompt students to propose novel solutions that address \nshortcomings of existing methods. If judged by the headlines, RL has seen an unprecedented \nsuccess in recent years. However, the vast majority of RL methods still have shortcomings that \nmight not be apparent at first glance. The aim of the course is to inspire students by \ncommunicating the promising aspects of RL, but ensure that students develop the ability to \nproduce a critical analysis of current RL limitations. \n \nObjectives \nStudents will learn the following technical concepts: \n \nfundamental RL terminology and mathematical formalism; a brief history of RL and its \nconnection to neuroscience and biological systems \nRL methods for discrete action spaces, e.g. deep Q-learning and large-scale Monte Carlo Tree \nSearch \nmethods for exploration, modelling uncertainty, and partial observability for RL \nmodern policy gradient and actor-critic methods \nconcepts needed to construct model-based RL and Model Predictive Control methods \napproaches to make RL data-efficient and ways to enable simulation-to-reality transfer \nexamples of fine-tuning foundation models and large language models (LLMs) with human \nfeedback; safe RL concepts; examples of using RL for safety validation \nexamples of using RL for scientific discovery \nStudents will also gain experience with analysing RL methods to uncover their strengths and \nshortcomings, as well as proposing extensions to improve performance. \n \nFinally, students will gain skills needed to create and deliver a successful presentation in a \nformat similar to that of conference presentations. \n \nWith all of the above, students who take part in this module will be well-prepared to start \nconducting research in the field of reinforcement learning. \n  \n \nSyllabus \nTopic 1: Introduction and Fundamentals \n \nOverview of RL: foundational ideas, history, and books; connection to neuroscience and \nbiological systems, recent industrial applications and research demonstrations \nMathematical fundamentals: Markov decision processes, Bellman equations, policy and value \niteration, temporal difference learning \nTopic 2: RL in Discrete Action Spaces \n\n \nQ-learning, function approximation and deep Q-learning; nonstationarity in RL and its \nimplications for deep learning; example applications (video games; initial example: Atari) \nMonte Carlo Tree Search; example applications (AlphaGo) \nTopic 3: Exploration, Uncertainty, Partial Observability \n \nMulti-armed bandits, Bayesian optimisation, regret analysis \nPartially observable Markov decision process; belief, memory, and sequence modelling \n(probabilistic methods, recurrent networks, transformers) \nTopic 4: Policy Gradient and Actor-critic Methods for Continuous Action Spaces \n \nImportance sampling, policy gradient theorem, actor-critic methods (SPG, DDPG) \nProximal policy optimisation; example applications \nTopic 5: Model-based RL and Model Predictive Control \n \nLearning dynamics models (graph networks, stochastic processes, diffusion models, \nphysics-based models, ensembles); planning with learned models \nModel predictive control; example applications (real-time control) \nTopic 6: Data-efficient RL and Simulation-to-reality Transfer \n \nData-efficient learning with probabilistic methods from real data (e.g. policy search in robotics), \nreal-to-sim inference and differentiable simulation, data-efficient simulation-to-reality transfer \nRL for physical systems (successful examples in locomotion, open problems in contact-rich \nmanipulation, applications to logistics, energy, and transport systems); examples of RL for \nhealthcare. \nTopic 7: RL with Human Feedback ; Safe RL and RL for Validation \n \nFine-tuning large language models (LLMs) and other foundation models with human feedback \n(TRLX,RL4LMs, a light-weight overview of RLHF) \nA review of SafeRL, example: optimising commercial HVAC systems using policy improvement \nwith constraints; improving safety using RL for validation: examples in autonomous driving and \nautonomous flying and aircraft collision avoidance \nTopic 8: RL for Scientific Discovery; Student Presentations \n \nExamples of RL for molecular design and drug discovery, active learning for synthesising new \nmaterials, RL for nuclear fusion experiments \nStudent presentations (based on essays and mini-projects) for other topics in RL, e.g. \nmulti-agent RL, hierarchical RL, RL for hyperparameter optimisation and NN architecture \nsearch, RL for multi-task transfer, lifelong RL, RL in biological systems, etc. \nAssessment \nThe assessment for this module consists of: \n \nEssay and mini-project (60%)  \n\nStudents will choose a concrete RL method from the literature, then complete a 2-part essay \ndescribed below: \nEssay Part 1 (1500 words, 20%): Introduce a formal description of the method and explain how \nthe method extended the state-of-the-art at the time of its publication \nEssay Part 2 or a mini-project (2500 words, 40%): Provide a critical analysis of the chosen RL \nmethod. \nPresentation of the essay and mini-project results (15%)  \nstudents will be expected to create and record a 15-minute video presentation of the analysis \ndescribed in their essay and mini-project. \nShort test of RL theory fundamentals (15%)  \nto test the understanding of RL fundamentals (~15 minutes, in-class, closed book) \nParticipation in seminar discussions (10%)  \ntake an active part in the seminars by asking clarifying questions and mentioning related works. \nRecommended Reading \nBooks \n \n[S&B] Reinforcement Learning: An Introduction (second print edition). Richard S. Sutton, \nAndrew G. Barto. [Available from the book\u2019s website as a free PDF updated in 2022] \n \n[CZ] Algorithms for Reinforcement Learning. Csaba Szepesvari. [Available from the book\u2019s \nwebsite as a free PDF updated in 2019] \n \n[MK] Algorithms for Decision Making. Mykel J. Kochenderfer Tim A. WheelerKyle H. Wray. \n[Available from the book\u2019s website as a free PDF updated in 2023] \n \n[DB] Reinforcement Learning and Optimal Control. Dimitri Bertsekas. [Available from the book\u2019s \nwebsite as a free PDF updated in 2023] \n \nPresentation guidelines \n \n[KN] Ten simple rules for effective presentation slides. Kristen M.Naegle. PLoS computational \nbiology 17, no. 12 (2021).\n \n\nTHEORIES OF SOCIO-DIGITAL DESIGN FOR HUMAN-CENTRED AI \n \nPrerequisites: This course is appropriate to any ACS student, and will assist in the development \nof critical thinking, argument and long-form writing skills. Prior experience of essay-based \nhumanities subjects at university or secondary school will be beneficial, but not required. \nMoodle, timetable \n \nAims \nThis module is a theoretically-oriented advanced introduction to the broad field of \nhuman-computer interaction, extending to consider topics such as intelligent user interfaces, \ncritical and speculative design, participatory design, cognitive models of users, human-centred, \nas well as more-than-human-centred design, and others. The course will not address purely \nengineering approaches to the development of user interfaces (unless there is a clear \ntheoretical question being addressed), but allow students to effectively link the political, social, \nand ethical considerations in HCI to specific technical and conceptual design challenges. The \nmodule builds on the Practical Research in Human-Centred AI course offered in Michaelmas \n(developed with researchers at Microsoft Research Cambridge) and a collaboration with the \nLeverhulme Centre for the Future of Intelligence at Cambridge. Participants may include visitors \nfrom these groups and/or interdisciplinary research students and academic guests from other \nUniversity departments, including Cambridge Digital Humanities. \n \nSyllabus \nThe syllabus will remain broadly within the area of human-computer interaction, including \ntheories of design practice and the social contexts of technology use. Individual seminar topics \nwill be selected in response to contemporary and recent research developments, in consultation \nwith members of the class and visiting contributors. \n \nRepresentative topics in the next year are likely to include: \n \nResponsible AI and HCI \nIntersectional design for social justice \nParticipatory design and co-design \nSustainable AI and more-than-human-centred design \nUsefulness and usability of ethical design toolkits \nThe EU AI Act in design practice \nAI and interface design for transparency \nDesigning otherwise: on anarchist HCI \n  \nObjectives \nOn completion of this module, students should have developed facility in discussing and \ncritiquing the aims of their research, especially for an audience drawn from other academic \ndisciplines, including the following skills: \n \nTheoretical motivation and defence of a research question. \n\nConsideration of a research proposal from one or more alternative theoretical perspectives. \nPotential critique of the theoretical basis for a programme of research. \nCoursework \nIn advance of each seminar, all participants must read the essential reading - generally one \nlong, plus one or two short papers. Further reading is suggested for some seminars. Some \nweeks, instead of shorter papers, students will be asked to explore specific design tools or \nresources. \n \nBefore seminars 2-8, each student will submit a written commentary on the paper, either \ngenerated using a large language model (LLM), or written in the style of an LLM, supplemented \nby a short original observation. \n \nEach member of the class must select one of the 8 seminar themes as the subject for a more \nextended critical review essay. The critical review should be written as an academic research \nessay, not in LLM style. \n \nEach seminar will include an informal design exercise to be carried out in small groups. The \npurpose of these exercises is to reflect on a variety of practical design resources, by applying \nthose resources to concrete case studies. \n \nThroughout the course, students will be expected to keep a \"reflective diary\", briefly reporting \nthemes that arise in discussion, reflections on the design exercises, and ways in which the \nreadings relate to their own research interests. \n  \n \nPractical work \nThere is no practical work element in this course. \n \nAssessment \nWritten critical review of a single discussion text (20%) \nReflective diary submitted at the end of the module (60%) \nIn advance of the session, each student will submit a written commentary on the paper, either \ngenerated using a large language model (LLM), or written in the style of an LLM. A short original \nobservation should be added (20%) \nRecommended reading \nTo be assigned during the module, as discussed above. Most set readings will be available \neither from the ACM Digital Library, or from institutional repositories of the authors. \n \nFurther Information \nStudents from the MPhil in the Ethics of AI, Data and Algorithms, MSt in AI, and MPhil in Digital \nHumanities courses also attend the lectures for this module.\n \n\nTHEORY OF DEEP LEARNING \nPrerequisites: A strong background in calculus, probability theory and linear algebra, familiarity \nwith differential equations, optimization and information theory is required. Students need to \nhave studied Deep Neural Networks, familiarity with deep learning terminology (e.g. \narchitectures, benchmark problems) as well as deep learning frameworks (pytorch, jax or \nsimilar) is assumed \nMoodle, timetable \n \nAims \nThe objectives of this course is to expose you to one of the most active contemporary research \ndirections within machine learning: the theory of deep learning (DL). While the first wave of \nmodern DL has focussed on empirical breakthroughs and ever more complex techniques, the \nattention is now shifting to building a solid mathematical understanding of why these techniques \nwork so well in the first place. The purpose of this course is to review this recent progress \nthrough a mixture of reading group sessions and invited talks by leading researchers in the \ntopic, and prepare you to embark on a PhD in modern deep learning research. Compared to \ntypical, non-mathematical courses on deep learning, this advanced module will appeal to those \nwho have strong foundations in mathematics and theoretical computer science. In a way, this \ncourse is our answer to the question \u201cWhat should the world\u2019s best computer science students \nknow about deep learning in 2023?\u201d \n \nLearning Outcomes \nThis course should prepare the best students to start a PhD in the theory and mathematics of \ndeep learning, and to start formulating their own hypotheses in this space. You will be \nintroduced to a range of empirical and mathematical tools developed in recent years for the \nstudy of deep learning behaviour, and you will build an awareness of the main open questions \nand current lines of attack. At the end of the course you will: \n \nbe able to explain why classical learning theory is insufficient to describe the phenomenon of \ngeneralization in DL \nbe able to design and interpret empirical studies aimed at understanding generalization \nbe able to explain the role of overparameterization: be able to use deep linear models as a \nmodel to study implicit regularisation of gradient-based learning \nbe able to state PAC-Bayes and Information-theoretic bounds, and apply them to DL \nbe able to explain the connection between Gaussian processes and neural networks, and will \nbe able to study learning dynamics in the neural tangent kernel (NTK) regime. \nbe able to formulate your own hypotheses about DL and choose tools to prove/test them \nleverage your deeper theoretical understanding to produce more robust, rigorous and \nreproducible solutions to practical machine learning problems. \nSyllabus \nEach week we'll have two to four student-lead presentations about a research paper chosen \nfrom a reading list. The reading list loosely follows the weekly breakdown below (but we adapt it \neach year based as this is an active research area): \n \n\nWeek 1: Introduction to the topic \nWeek 2: Empirical Studies of Deep Learning Phenomena \nWeek 3: Interpolation Regime and \u201cDouble Descent\u201d Phenomena \nWeek 4: Implicit Regularization in Deep Linear Models \nWeek 5: Approximation Theory \nWeek 6: Networks in the Infinite Width Limit \nWeek 7: PAC-Bayes and Information Theoretic Bounds for SGD \nWeek 8: Discussion and Coursework Spotlight Session \n \nAssessment \nStudents will be assessed on the following basis: \n \n20% for presentation/content contributed to the module: Each student will have an opportunity \nto present one of the recommended papers during Weeks 1-7 (30 minute slot including Q&A). \nFor the presentation, students should aim to communicate the core ideas behind the paper, and \nclearly present the results, conclusions, and future directions. Where possible, students are \nencouraged to comment on how the work itself fits into broader research goals. \n10% for active participation (regular attendance and contribution to discussions during the Q&A \nsessions). \n70% for a group project report, with a word limit of 4000. Either (1) an original research \nproposal/report with a hypothesis, review of related literature, and ideally preliminary findings, \nor, (2) reproduction and ideally extension of an existing relevant paper. \nCoursework reports are marked in line with general ACS guidelines, reports receiving top marks \nwill have have demonstrable research value (contain an original research idea, extension of \nexisting work, or a thorough reproduction effort which is valuable to the research community). \nAdditionally, some projects will be suggested during the first weeks of the course, although \nstudents are encouraged to come up with their own ideas. Students may be required to \nparticipate in group projects, with groups of size 2-3 (the class groups will be separated). For \nany given project, individual contributions would be noted for assessment though a viva \ncomponent. \nRelationship with related modules \nThis course can be considered as an advanced follow-up to the Part IIB course on Deep Neural \nNetworks. That course introduces some high level concepts that this course significantly \nexpands on. \n \nThis module complements L48: Machine Learning in the Physical World and L46: Principles of \nMachine Learning Systems, which focus on applications and hardware/systems aspects of ML \nrespectively. \n \nRecommended reading \nPNAS Colloquium on the Science of Deep Learning \nMathematics of Machine Learning book by Marc Deisenroth, Aldo Faisal and Cheng Soon Ong. \nProbabilistic Machine Learning: An Introduction book by Kevin Murphy \nMatus Telgarsky's lecture notes on deep learning theory  \n\nThese are in addition to the papers which will be discussed in the lectures.\n \n\nUNDERSTANDING NETWORKED-SYSTEMS PERFORMANCE \nPrerequisites: A student must possess knowledge comparable to the undergraduate subjects on \nUnix Tools, C/C++ programming and Computer Networks. Optionally; a student will be \nadvantaged by doing an introduction to Computer Systems Modelling; such as the Part 2 \nComputer Systems Modelling subject as well as having a background in measurement \nmethodologies such as that of L50: Introduction to networking and systems measurements \nprovided in the ACS/Part III curriculum. \nMoodle, timetable \n \nAims \nThis is a practical course, actively building and extending software tools to observe the detailed \nbehaviour of transaction-oriented datacenter-like software. Students will observe and \nunderstand sources of user-facing tail latency, including that stemming \nfrom resource contention, cross-program interference, bad software locking, and simple design \nerrors. \nA 2-hour weekly hybrid, lecture/lab format permits students continuous monitored progress with \ncomplexity of tasks building naturally upon the previous weeks learning. \n \nObjectives \nUpon successful completion of the course, students will be able to: \n \nMake order-of-magnitude estimates of software, hardware, and I/O speeds \nMake valid measurements of actual software, hardware, and I/O speeds \nCreate observation facilities, including logs and dashboards, as part of a software system \ndesign \nCreate tracing facilities to fully observe the execution of complex software \nTime-align traces across multiple computers \nDisplay dense tracing information in meaningful ways \nReason about the sources of real-time and transaction delays, including cross-program \nresource interference, remote procedure call delays, and software locking surprises \nFix programs based on the above reasoning, making their response times faster and more \nrobust \n \nSyllabus \nMeasurement \n   Week 1 Intro, Measuring CPU time, rdtsc, Measuring memory access times, Measuring disks, \n   Week 2 gettimeofday, logs Measuring networks, Remote Procedure Calls, Multi-threads, locks \nObservation \n   Week 3 RPC, logs, displaying traces, interference \n   Week 4 Antagonist programs, Logging, dashboards, profiling. \nKUtrace \n   Week 5 Kernel patches, hello world, post-processing \n   Week 6 KUtrace multi-CPU time display \n   Week 7 Client-server KUtrace, with antagonists and interference \n\n   Week 8 Other trace mysteries \n \n \nAssessment \nThis is a Lab-based module; assessment is based upon reports covering guided laboratory work \nperformed each week. Assessment will be via two submissions: \n \n20% assignment deadline week 4 based upon Lab work from Weeks 1-4; word target 1000, limit \n2000 \n80% assignment deadline week 8 based upon lab work from weeks 5-8; word target 2000, limit \n4000 \nThe intent of the first assessment point is to provide rich feedback to students based upon a \n20% assignment permitting focussed and improved work to be executedfor the final assignment. \n \nAll work in this module is expected to be the effort of the student. Enough equipment is \nprovisioned to allow each student an independent set of apparatus. While classmembers may \nfind sharing operational experience valuable all assessment is based upon a students sole \nsubmission based upon. their own experiments and findings. \n \nReading Material \nCore text \n \nRichard L. Sites, Understanding Software Dynamics, Addison-Wesley Professional Computing \nSeries, 2022 \nFurther reading: papers and presentations that you might find interesting. None are required \nreading \u2013 they are well-written and/or informative. \n \nJohn K. Ousterhout et al., A trace-driven analysis of the UNIX 4.2 BSD file system ACM \nSIGOPS Operating Systems Review, Vol. 19, No. 5, Dec, 1985 \nhttps://dl.acm.org/doi/pdf/10.1145/323627.323631 \nLuiz Andr\u00b4e Barroso, Jimmy Clidaras, Urs H\u00a8olzle, The Datacenter as a Computer: An \nIntroduction to the Design of Warehouse-Scale Machines, Second Edition \nJohn L. Hennessy, David A. Patterson, Computer Architecture, A Quantitative Approach 5th \nEdition \nGeorge Varghese, Network Algorithmics. Morgan Kaufmann \nActually keeping datacenter software up and running \u2013 how Google runs production systems \nSite Reliability Engineering Edited by Betsy Beyer, Chris Jones, Jennifer Petoff and Niall \nRichard Murphy free PDF: https://landing.google.com/sre/book.html \nHow NOT to run datacenters (27 minute video, funny/sad) dotScale 2014 - Robert Kennedy - \nLife in the Trenches of healthcare.gov https://www.youtube.com/watch?v=GLQyj-kBRdo \nAn extended (optional) reading list will also be provided. \n \n"
    ]
}